[
		{"url": "https://pingcap.com/blog/FOSDEM-2018-Rust-Devroom-reflection/",
		"title": "Bringing TiKV to Rust Devroom at FOSDEM 2018", 
		"content": "At the crack of dawn on February 1, I landed in Brussels, Belgium, for the first time in my life. The goal of my trip wasn’t to taste the local cuisine, tour world-famous museums, or grab a pint of the local brew (though I ended up doing all those things anyway). It was to deliver a talk three days later at &amp;ldquo;FOSDEM 2018 Rust Devroom&amp;rdquo; about our experience at PingCAP using Rust to build TiKV, a distributed transactional Key-Value storage engine.FOSDEM 2018 Rust Devroom is the 1st edition of a dedicated Rust venue co-located with FOSDEM 2018. It was, by all accounts, very popular; at least a few hundred people visited the Rust Devroom, where my talk was supposed to be held. Because the room could only hold about 120 people, people had to line up to get into talks. When I showed up for my event, there was already a long line snaking through the hallway. I lined up with everyone else and almost didn’t get into my own talk!People lining up to listen to their favorite Rust talk. Photo credit: Andrew HobdenYou can find the slides of my presentation here and the full video here. But we are all busy people, so just in case you don’t have time to go through it all, here’s a summary of the highlights and where you can go learn more!Building TiKV, a distributed key-value store with RustBuilding a distributed key-value store with RustPresenting in front of a group of Rustaceans at FOSDEM 2018. Photo credit: Andrew HobdenIn my talk, I shared our experiences on the following topics from our process of building TiKV: Why another database and the challenges of building a modern distributed transactional Key-Value store: consistency, horizontal scalability, scalability, stability, performance, ACID compliance, High Availability, etc.   How we build the TiKV core system using Rust, including the backend store, the gRPC framework, and the consensus replication mechanism. Backend store: TiKV adopts RocksDB as the backend storage engine for its high-performance and fast storage, and usesRaft and Multi-raft to ensure data safety and horizontal scalability. We implemented the Raft Consensus algorithm in Rust. Transaction: As a distributed Key-Value database, TiKV also supports transaction and is ACID compliant. To maintain consistency among the Multi-raft groups, TiKV adopts an optimized two-phase commit (2PC) protocol and supports Multiversion Concurrency Control (MVCC), Snapshot Isolation, and Optimistic Transaction. For different machines to communicate to each other, TiKV uses gRPC, a high-performance universal RPC framework. We develop agRPC library for Rust built on C Core library and futures.  How we monitor the cluster and diagnose problems: to monitor the metrics of the cluster and gain insights, TiKV adopts Prometheus as the monitoring solution. We build a Prometheus instrumentation library for Rust applications, which is listed as a third-party client on the Official Prometheus Client Libraries. How we use the failure injection test to guarantee data safety. Inspired by FreeBSD&amp;rsquo;s failpoints, TiKV implemented a fail point to inject errors by users at runtime in Rust.  The following diagram shows the TiKV architecture:TiKV ArchitectureOur team has big plans beyond just building a full-featured distributed transactional Key-Value database like TiKV. We have already built a stateless SQL layer, TiDB, mainly for Online transaction processing (OLTP) that works directly with TiKV, and TiSpark, an Apache Spark driver which sits on top of TiKV for handling heavy Online analytical processing (OLAP) workloads. Our ultimate vision is to build a Hybrid Transactional/Analytical Processing database that empowers businesses to meet both workloads with a single database and enables real-time business analysis based on live transactional data.Architecture of TiDB, a Hybrid Transactional/Analytical Processing (HTAP) databaseTiKV, TiDB, and TiSpark have been widely adopted in production environments by companies, ranging from e-commerce and gaming to financial services and bike-sharing. We are working to publish specific use case stories from our customers soon, and the best way to receive them is to subscribe to our blog. Stay tuned for more!Last but not least, we welcome everyone to fork, star, use, and contribute to the following projects that we’ve written in Rust for TiKV: Rustwrapper for RocksDB Raft Consensus algorithm implemented in Rust gRPC library for Rust built on C Core library and futures Prometheus instrumentation library for Rust applications Fail points for Rust  We look forward to building a strong and vibrant Rust community together!"},
		{"url": "https://pingcap.com/weekly/2018-02-12-tidb-weekly/",
		"title": "Weekly update (February 05 ~ February 11, 2018)", 
		"content": " Weekly update in TiDB Last week, we landed 67 PRs in the TiDB repositories.Added  CreateIndex supports the LOCK option. Add GoVersion info for tidb_version Add a session variable to show the configuration Support show stats_healthy to check if a table needs to be analyzed  Removed  Remove the useless field in jsonColumn Clean up the abandoned storage engine  Fixed  Check the CreateTable statement charset option Fix the bug of show index printing non-public index when add index operation is not finished Treat a decimal truncate as a warning in update  Improved  Run GC workers parallelly Pass the operator label from Plan to Executor Prepare the candidate index to improve performance Use pseudo estimation when the stats of some table is outdated Ignore the error and keep GC always working Set a min count for AutoAnalyze to avoid the auto analysis of small tables Improve importer tools:  Support randDate by statistics Support generating other types of columns randomly by statistics Generate a string by statistics Support VARCHAR in set Generate the integer data by Histogram  Refine metrics in TiDB:  Rename _count to _num Unify metrics naming Add a metric for pseudo estimation Make metrics content clearer and compacter Move domain metrics and add the privilege load counter Add metrics for DDL and the server Add metrics for the DDL worker Add metrics for expensive executors and statement nodes Add metrics for stats Fix inconsistent labels for the panic counter Move DDL metrics from the ddl package to the metrics package Refine TiKV client metrics Add metrics for the DDL owner Add metrics and logs for ticlient   Weekly update in TiSpark Last week, we landed 6 PRs in the TiSpark repositories.Improved  Change the remote repository for the test data Add JDBC write guide usage  Fixed  Fix key range corner cases Fix the bug of group by without aggregates Fix the resetFilters method Fix the character decoding charset  Weekly update in TiKV and PD Last week, we landed 19 PRs in the TiKV and PD repositories.Added  Add the read pool Introduce the label property and the label scheduler Support configuring the label scheduler and the label property  Fixed  Fix the bug caused by limit=0 or by no limit  Improved  Add the file name and the line number for the box error Record the leader missing information Filter Store when counting the average leader and score Add stale_command in ASYNC_REQUESTS_COUNTER_VEC Check whether range.start &amp;lt; range.end Change the unit of heartbeat from ts to ms Limit the maximum total size of snapshot Collect gRPC logs Save the critical metadata in metrics Use local metrics Adjust the status of the store  New contributor (Thanks!)  TiDB: caojiafeng  "},
		{"url": "https://pingcap.com/blog/2018-02-11-108/",
		"title": "TiDB 1.0.8 Release Notes", 
		"content": " On Feburary 11, 2018, TiDB 1.0.8 is released with the following updates:TiDB:  Fix issues in the Outer Join result in some scenarios Optimize the performance of the InsertIntoIgnore statement Fix the issue in the ShardRowID option Add limitation (Configurable, the default value is 5000) to the DML statements number within a transaction Fix an issue in the Table/Column aliases returned by the Prepare statement Fix an issue in updating statistics delta Fix a panic error in the Drop Column statement Fix an DML issue when running the Add Column After statement Improve the stability of the GC process by ignoring the regions with GC errors Run GC concurrently to accelerate the GC process Provide syntax support for the CREATE INDEX statement  PD:  Reduce the lock overheat of the region heartbeats Fix the issue that a hot region scheduler selects the wrong Leader  TiKV:  Use DeleteFilesInRanges to clear stale data and improve the TiKV starting speed Using Decimal in Coprocessor sum Sync the metadata of the received Snapshot compulsorily to ensure its safety  To upgrade from 1.0.7 to 1.0.8, follow the rolling upgrade order of PD -&amp;gt; TiKV -&amp;gt; TiDB."},
		{"url": "https://pingcap.com/weekly/2018-02-05-tidb-weekly/",
		"title": "Weekly update (January 29 ~ February 04, 2018)", 
		"content": " Weekly update in TiDB Last week, we landed 35 PRs in the TiDB repositories.Added  Support the load stats command  Removed  Remove iota in DDL package to make the constant clearer  Fixed  limit and offset can be parameter markers in the prepared statement IndexOption can be a list in creating a table Fix the bug of some field length missing in creating a table Fix the bug of parsing Datetime overflow Trim leading zeros before parsing integer literal Fix the float truncate bug  Improved  Importer tools support loadStats by path Support mock table info for importer tools Let DO statement be a read only statement Improve an error handling in ddl Reduce memory allocation in buildDataSource Make Explain clearer Add the metrics package and recover Panic of Worker Refine the joinResult generator to return maxChunkSize chunk Limit lock count for ScanLock request Enhance the IndexRange calculation Refine metrics in TiDB:  Refine DistSQL metrics Add metrics for the meta package Move and refine the server metrics Add metrics for DDL syncer Update metrics for session   Weekly update in TiSpark Last week, we landed 6 PRs in the TiSpark repositories.Improved  Switch integration test to scalatest framework  Fixed  Fix Average on BIGINT overflow Fix scala version for some libraries  Weekly update in TiKV and PD Last week, we landed 22 PRs in the TiKV and PD repositories.Added  PD: implement GetAllStores API TiKV: implement GetAllStores API Add PD health API Support setting leader priority to etcd members Sync the PD leader with the etcd leader Add the future pool  Fixed  Fix hot Region DistinctScoreFilter Fix and update Region size Fix cleanup for raw KV  Improved  Check Range for DAG and analysis Sync the snapshots Remove the sync snapshot file Add operator duration metrics Support simple command line args Add Region statistics Clear stale data using DeleteFilesInRanges  New contributor (Thanks!)  TiDB: Boik  "},
		{"url": "https://pingcap.com/weekly/2018-01-29-tidb-weekly/",
		"title": "Weekly update (January 22 ~ January 28, 2018)", 
		"content": " Weekly update in TiDB Last week, we landed 28 PRs in the TiDB repositories.Added  Add the importer tool Add metrics for GC failure count Add metrics for TiDB-server panic Add metrics for async secondary lock cleanup Support create time in information_schema  Removed  Remove GetSessionVars() in expression evaluation Remove varsutil package, and make Systems a private member of SessionVars  Fixed  Avoid the generation of mysql.TypeNewDate Push down binaryliterals as varstring  Improved  Keep slow query log entry in order Make list.MemoryUsage() more efficient Limit statement count in a transaction Remove WithCancel in copIterator Uniform the way of iterating rows within a Chunk  Weekly update in TiSpark Last week, we landed 4 PRs in the TiSpark repositories.Improved  Improve index double read performance by around 30% Verify and add document for Hive integration  Fixed  Fix test for between expression Add placeholder for unsupported type preventing schema reading crash  Weekly update in TiKV and PD Last week, we landed 17 PRs in the TiKV and PD repositories.Added  Add ImportSST/UploadSST API Add drop region admin command  Fixed  Convert int to decimal in sum Fix join failure when log dir is the same as data dir Disable the portable on MacOS  Improved  Limit data size for scan lock Add metrics for checkers Remove schedule empty apply task Ignore Prometheus encoding error Debug trait for snapshot Reduce the lock overhead Update the region size  New contributor (Thanks!)  TiDB: yuananf  "},
		{"url": "https://pingcap.com/blog/tidb-devcon-2018-recap/",
		"title": "TiDB DevCon 2018 Recap - News, Latest Development, and Roadmap", 
		"content": " On January 20th, 2018, a chilly Saturday in the middle of the winter, more than 200 coders, hackers, and techies streamed into Garage Café, a chic coffee shop in the heart of Beijing’s techhub, Zhongguancun. They weren’t there to get coffee. They weren’t there to stay warm. They were there to be part of TiDB DevCon 2018, a technology party for the developers, by the developers.TiDB DevCon 2018 attendees signing-in on the event bannerA packed audience at TiDB DevCon 2018At this party, the team behind TiDB announced exciting news, shared the project’s latest development, and unveiled the future plans of the TiDB project. Here are some important news and highlights:PingCAP co-founder and CEO, Max Liu, giving the opening address at TiDB DevCon 2018 Team Community Partners TiDB 1.1 Alpha Release  Performance &amp;amp; latency TiDB Lightning  Tools and Ecosystem  TiDB deployment Import &amp;amp; Export Tools  TiDB Syncer TiDB Binlog TiDB Lightning  Wormhole&amp;ndash;Enterprise synchronization tool TiDB Insight Monitoring &amp;amp; Alerting  2018 Roadmap  Team The team behind TiDB, PingCAP, grew from 35 members in 2016 to almost 90 members in 2017.Community  Github stars on the TiDB project (TiDB and TiKV combined) reached 14000+ in January 2018, almost doubled its number in January 2017.  Number of contributors reached 213, an increase of 126%. More than 200 users are running TiDB in production environments, 50 times more than 2016. Three distinguished TiDB contributors were recognized as TiDB committers. For more information about how to join the TiDB community, see TiDB Community repo and How to become a TiDB committer. PingCAP co-founder &amp;amp; CTO, Ed Huang (left), recognizing three distinguished TiDB contributors as TiDB committer  Partners Designed to be a cloud-native HTAP database, TiDB has been fully integrated and available on UCloud and Tencent Cloud. Close partnerships with other public cloud vendors will be announced soon. Stay tuned!Some of TiDB’s largest customers, Mobike, Toutiao, SpeedyCloud, Qunar.com, Ele.me, also shared their use cases and best practices:A Senior Manager of the databse team at Mobike, sharing their best practices of TiDBTech Lead of the Database Middleware and Distributed Database Team at Toutiao, sharing their best practices of TiDBTech lead of the architecture team in the Technical Innovation department at Ele.me, sharing their use case with TiDBSenior Database Administrator at Qunar.com, sharing their best pratices of TiDBSenior Database Architect at SpeedyCloud, sharing their query cache in TiDBTiDB 1.1 Alpha Release On January 19th, one day before DevCon, TiDB 1.1 Alpha was released with the following improvements:Performance &amp;amp; latency  Average QPS of TiDB increased by 50%. To support requirement from Toutiao, TiDB 1.1 Alpha is able to scale auto-increment ID and achieve 16x speed improvement for queries like alter table t shard_row_id_bits = 4 and is also able to handle millions of TPS. Read QPS speed of TiKV improved by 2x. Counting table is now 70% faster than the previous version. Hash-join is 3 times faster than the previous version. Some complex queries using index are now more than 10x faster than the previous version. Cost based optimizer (CBO) is smarter. Loading data using the following statement is now 15x faster: load data local infile &#39;output.csv&#39; into table xxx;  TiDB Lightning With TiDB Lightning, it takes less than 5 hours to import 1 TB of raw SQL file from MySQL to TiDB&amp;ndash;10x faster than before.Tools and Ecosystem TiDB deployment Deploying a TiDB cluster is much easier than ever before. Developers can use Docker Compose to launch a local TiDB cluster with one command on their laptops and play around with it. Moreover, the container-based components of TiDB managed by a special software named tidb-operator can be simply deployed and maintained on a Kubernetes environment in production.Import &amp;amp; Export Tools TiDB Syncer TiDB Syncer is a smart tool to migrate data from MySQL to TiDB in real-time. It poses as a MySQL slave and listens to the changes of master by parsing the MySQL Binary Logs, and replaying the changes sequentially on the TiDB instance.TiDB SyncerTiDB Binlog TiDB Binlog is a tool designed to help users to synchronize data to downstream database in real-time. For example, users can use TiDB binlog to synchronize data from one TiDB instance to another as a Hot-backup cluster. In addition, TiDB Binlog can also be used for other purposes, such as incremental backup, message triggering and subscription or driving stream computing and analytics.BinlogTiDB Lightning Lightning is a high-speed data importing tool, customized for TiDB:TiDB LightningWormhole&amp;ndash;Enterprise synchronization tool Wormhole is an enterprise edition tool for data synchronization. It contains the following features: Web UI dashboard Multi-source and destination configuration Distributed scheduling Sharding source supported Pre-checking Lightweight ETL in real-time Cloud DB integration with:  AliCloud RDS TencentCloud CDB   Wormhole&amp;ndash;Enterprise synchronization toolTiDB Insight TiDB Insight provides users with a real-time visualization and overview into the region distribution, leader regions status, hotspots, region &amp;amp; leader transfer, traffic flow, and the health of the entire cluster.TiDB InsightMonitoring &amp;amp; Alerting The TiDB monitoring system collects metrics from TiDB cluster and is integrated with Grafana to visualize data. Alerts are defined based on Prometheus&amp;rsquo;s flexible query language. And the alerting system can keep users informed of any abnormal status in their TiDB cluster via email, SMS, Slack messages, or other customized ways.TiDB Monitoring &amp;amp; Alerting2018 Roadmap After recapping all the news and developments from an exciting and productive 2017, PingCAP co-founder and CEO, Max Liu, unveiled TiDB’s 2018 roadmap.PingCAP co-founder and CEO, Max Liu, sharing TiDB’s 2018 roadmap Region level SQL query cache (2018 Q1) View (2018 Q1) Region merging (2018 Q1) Partition table (2018 Q2-Q3) Customized Raft storage engine (2018 Q3) Blob storage to reduce compaction (2018 Q3 working with RocksDB team) Full-text search (2018 Q4) Geo-Index (2018 Q4-2019 Q1) Window function (2018 Q4)  Last but not least, we want to thank all the attendees for braving the cold to participate in the first (but definitely not the last) TiDB DevCon, all the speakers from our customer companies to share their use cases and best practices, and all the amazing contributors to the vibrant and growing TiDB community. We know you have big hopes for 2018; we won’t let you down.DevCon 2018 Group Photo"},
		{"url": "https://pingcap.com/blog/2018-01-22-107/",
		"title": "TiDB 1.0.7 Release Notes", 
		"content": " On January 22, 2018, TiDB 1.0.7 is released with the following updates:TiDB:  Optimize the FIELD_LIST command Fix data race of the information schema Avoid adding read-only statements to history Add the session variable to control the log query Fix the resource leak issue in statistics Fix the goroutine leak issue Add schema info API for the http status server Fix an issue about IndexJoin Update the behavior when RunWorker is false in DDL Improve the stability of test results in statistics Support PACK_KEYS syntax for the CREATE TABLE statement Add row_id column for the null pushdown schema to optimize performance  PD:  Fix possible scheduling loss issue in abnormal conditions Fix the compatibility issue with proto3 Add the log  TiKV:  Support Table Scan Support the remote mode in tikv-ctl Fix the format compatibility issue of tikv-ctl proto Fix the loss of scheduling command from PD Add timeout in Push metric  To upgrade from 1.0.6 to 1.0.7, follow the rolling upgrade order of PD -&amp;gt; TiKV -&amp;gt; TiDB."},
		{"url": "https://pingcap.com/weekly/2018-01-22-tidb-weekly/",
		"title": "Weekly update (January 15 ~ January 21, 2018)", 
		"content": " Weekly update in TiDB Last week, we landed 43 PRs in the TiDB repositories.Added  Add an interface for Chunk to count the memory usage Add a session variable to log the query string  Removed  Remove the old, never used IndexLookUpJoin  Fixed  group_concat should not modify the argument during execution Correct the unsigned pk&amp;rsquo;s behavior  Improved  Optimize the com_field_list command and make Use Database faster Improve the sort efficiency on lookupTableTask.rows Enlarge the default distsql_scan_concurrency Log a warning when the memory usage of HashJoinExec exceeds threshhold Refine the behavior when updating StatsDelta Split the presentation and evaluation layers of aggregation functions Use BatchGet to speed up LOAD DATA  Weekly update in TiSpark Last week, we landed 13 PRs in the TiSpark repositories.Improved  Refactor the type and expression systems totally  Reorganize the type and expressions Use the visitor instead of raw expressions when processing traversal Fixed type infer logics Refactor predicates processing logic according to the visitor pattern Fixed index encoding  Change CodecDataInput to use non-synchronized ByteArrayInputStream Add TiKV, PD, and TiDB&amp;rsquo;s config files for Docker test environment  Weekly update in TiKV and PD Last week, we landed 18 PRs in the TiKV and PD repositories.Added  simulator: add hot region cases  Fixed  proto: compatible with proto3 heartbeat: rebind the stream server: delete overlap from etcd  Improved  Refactor the raftstore Callback API Remove the unused rocksdb.backup_dir Adjust Scheduler metrics impl Runnable for Scheduler  "},
		{"url": "https://pingcap.com/blog-cn/tidb-1.1-alpha-release/",
		"title": "TiDB 1.1 Alpha Release", 
		"content": " 2018 年 1 月 19 日，TiDB 发布 1.1 Alpha 版。该版本对 MySQL 兼容性、SQL 优化器、系统稳定性、性能做了大量的工作。TiDB  SQL parser  兼容更多语法  SQL 查询优化器  统计信息减小内存占用 优化统计信息启动时载入的时间 更精确的代价估算 使用 Count-Min Sketch 更精确的估算点查的代价 支持更复杂的条件，更充分使用索引  SQL 执行器  使用 Chunk 结构重构所有执行器算子，提升分析型语句执行性能，减少内存占用 优化 INSERT INGORE 语句性能 下推更多的类型和函数 支持更多的 SQL_MODE 优化 Load Data 性能，速度提升 10 倍 优化 Use Database 性能 支持对物理算子内存使用进行统计  Server  支持 PROXY protocol   PD  增加更多的 API 支持 TLS 给 Simulator 增加更多的 case 调度适应不同的 region size Fix 了一些调度的 bug  TiKV  支持 Raft learner 优化 Raft Snapshot，减少 IO 开销 支持 TLS 优化 RocksDB 配置，提升性能 Coprocessor 支持更多下推操作 增加更多的 Failpoint 以及稳定性测试 case 解决 PD 和 TiKV 之间重连的问题 增强数据恢复工具 TiKV-CTL 的功能 region 支持按 table 进行分裂 支持 delete range 功能 支持设置 snapshot 导致的 IO 上限 完善流控机制  源码地址：https://github.com/pingcap/tidb如今，在社区和 PingCAP 技术团队的共同努力下，TiDB 1.1 Alpha 版已发布，在此感谢社区的小伙伴们长久以来的参与和贡献。 作为世界级开源的分布式关系型数据库，TiDB 灵感来自于 Google Spanner/F1，具备『分布式强一致性事务、在线弹性水平扩展、故障自恢复的高可用、跨数据中心多活』等核心特性。TiDB 于 2015 年 5 月在 GitHub 创建，同年 12 月发布 Alpha 版本，而后于 2016 年 6 月发布 Beta 版，12 月发布 RC1 版， 2017 年 3 月发布 RC2 版，6月份发布 RC3 版，8月份发布 RC4 版，并在 10 月发版 TiDB 1.0。 "},
		{"url": "https://pingcap.com/blog/2018-01-19-11-alpha/",
		"title": "TiDB 1.1 Alpha Release Notes", 
		"content": " On January 19, 2018, TiDB 1.1 Alpha is released. This release has great improvement in MySQL compatibility, SQL optimization, stability, and performance.TiDB:  SQL parser  Support more syntax  SQL query optimizer  Use more compact structure to reduce statistics info memory usage Speed up loading statistics info when starting tidb-server Provide more accurate query cost evaluation Use Count-Min Sketch to evaluate the cost of queries using unique index more accurately Support more complex conditions to make full use of index  SQL executor  Refactor all executor operators using Chunk architecture, improve the execution performance of analytical statements and reduce memory usage Optimize performance of the INSERT INGORE statement Push down more types and functions to TiKV Support more SQL_MODE Optimize the Load Data performance to increase the speed by 10 times Optimize the Use Database performance Support statistics on the memory usage of physical operators  Server  Support the PROXY protocol   PD:  Add more APIs Support TLS Add more cases for scheduling Simulator Schedule to adapt to different Region sizes Fix some bugs about scheduling  TiKV:  Support Raft learner Optimize Raft Snapshot and reduce the IO overhead Support TLS Optimize the RocksDB configuration to improve performance Optimize count (*) and query performance of unique index in Coprocessor Add more failpoints and stability test cases Solve the reconnection issue between PD and TiKV Enhance the features of the data recovery tool TiKV-CTL Support splitting according to table in Region Support the Delete Range feature Support setting the IO limit caused by snapshot Improve the flow control mechanism  "},
		{"url": "https://pingcap.com/weekly/2018-01-15-tidb-weekly/",
		"title": "Weekly update (January 08 ~ January 14, 2018)", 
		"content": " Weekly update in TiDB Last week, we landed 43 PRs in the TiDB repositories.Added  Support the ODBC syntax of time/date/timestamp literal Add MaxProcs and make the runtime.GOMAXPROCS parameter configurable  Fixed  Fix a bug about index join Close HashJoin goroutines as soon as possible to avoid unexpected errors fetchShowTableStatus should append an integer to the third column instead of a string Correct the behavior when RunWorker is false Refine the typeInfer of group_concat  Improved  Avoid the Children type assertion Merge IntColumnRange with NewRange Upgrade the username length limit to 32 to be compatible with MySQL 5.7 Garbage collects useless stats info InsertExec eliminates unnecessary CastValue operations Speed up row count estimation Support encoding a Chunk row Use dep instead of glide Optimize the insert ignore statement Support Chunk in:  HashJoinExec ExecuteExec   Weekly update in TiKV and PD Last week, we landed 25 PRs in the TiKV and PD repositories.Added  Add the node case Support Region split Support query regions by read/write flow  Fixed  Change the version Fix a RocksDB task name error Cancel call when refreshing client  Improved  Use dep instead of glide Optimize write type parsing for MVCC properties collector Update README and upgrade Dockerfile golang:1.9.2 Specify gRPC event engine Add the debug log Fix some ineffassign to improve the GoReport Result Rename AggregationExecutor to HashAggExecutor Improve the GoReport Result Dump the ConfChange type Raft log entry Make meta key return array  New contributor (Thanks!)  PD: maiyang  "},
		{"url": "https://pingcap.com/blog/2018-01-08-106/",
		"title": "TiDB 1.0.6 Release Notes", 
		"content": " On January 08, 2018, TiDB 1.0.6 is released with the following updates:TiDB:  Support the Alter Table Auto_Increment syntax Fix the bug in Cost Based computation and the Null Json issue in statistics Support the extension syntax to shard the implicit row ID to avoid write hot spot for a single table Fix a potential DDL issue Consider the timezone setting in the curtime, sysdate and curdate functions Support the SEPARATOR syntax in the GROUP_CONCAT function Fix the wrong return type issue of the GROUP_CONCAT function.  PD:  Fix store selection problem of hot-region scheduler  TiKV: None.To upgrade from 1.0.5 to 1.0.6, follow the rolling upgrade order of PD -&amp;gt; TiKV -&amp;gt; TiDB."},
		{"url": "https://pingcap.com/weekly/2018-01-08-tidb-weekly/",
		"title": "Weekly update (January 01 ~ January 07, 2018)", 
		"content": " Weekly update in TiDB Last week, we landed 37 PRs in the TiDB repositories.Added  Support the PACK_KEYS option in the CreateTable statement. Show job&amp;rsquo;s start time in the result of admin show ddl ....  Fixed  Fix a bug when initializing HTTP stats handler. Fix a bug when estimating row count for outdated histograms. Consider time zone for builtin functions curtime/sysdate/curdate.  Improved  Refactor Chunk.AppendRow to handle virtual. Refactor hybrid type expressions. Only do a shallow copy when evaluating a &amp;ldquo;Column&amp;rdquo; expression. Refine the design of schema. Use more compact structure for histograms. Convert the aggregation operator max/min to topN. Shard the implicit row ID to avoid hot spot. Support Chunk for StreamAggExec. Support Chunk for HashAggExec.  Weekly update in TiSpark Last week, we landed 4 PRs in the TiSpark repositories.Improved  Add potential missing test cases. Simplify Aggregation pushdown.  Fixed  Fix the dependency for jackson and joda-time. Fix the error handling logic.  Weekly update in TiKV and PD Last week, we landed 10 PRs in the TiKV and PD repositories.Added  Add timer to support worker timeout later. tikv-ctl: add bad-regions subcommand. coprocessor/endpoint: initialize runtime in response.  Fixed  tests: fix some failpoints name. scheduler: fix hot region scheduler select store problem. Fix the score when region size is zero.  Improved  raftstore: add Lease. util: remove BatchRunnable and use Runnable instead. tests: remove redundant storage tests. Change github.com/Sirupsen/logrus to github.com/sirupsen/logrus.  New contributor (Thanks!)  TiDB: Cruth kvinc  "},
		{"url": "https://pingcap.com/weekly/2018-01-02-tidb-weekly/",
		"title": "Weekly update (December 25 ~ December 31, 2017)", 
		"content": " Weekly update in TiDB Last week, we landed 36 PRs in the TiDB repositories.Added  Add the Iterator interface in Chunk.  Removed  Remove the useless aggregation function during buildQuantifierPlan.  Fixed  Flen and Decimal of TypeNewDecimal should not be -1. To pass sysbench Prepare tests, set Fields for SelectStmt in PrepareExec. Fix the case that fails to split a table. Correct the type inference of the sum and avg functions. Make set transaction read only work, compatible with JDBC connector.  Improved  Change BuildRange to build column/index/table range to improve performance. Make KvEncoder support encoding Prepare SQL. Refine codes about maxOneRow. Use chunk.Iterator for joinGenerator. Merge ranger.IndexRange and ranger.ColumnRange to ranger.NewRange. Reduce allocation for DetachCondsForSelectivity, and add a Filter function. Support alter table auto_increment. Support pushing down stream aggregation on mocktikv. Support Chunk in executors:  NestedLoopApply UnionScanExec   Weekly update in TiSpark Last week, we landed 12 PRs in the TiSpark repositories.Improved  Dynamically downgrade the index scan plan to table scan plan. Optimize the column logic of Aggregation pushing down. Add the date type pushing down logic. Reduce dependencies. Add tests for issues. Add switching to ignore the unsupported type. Use the parent child POM mode instead of submodule.  Fixed  Fix the NPE issue of Aggregation pushing down. Remove time test cases. Modify integration scripts and .gitignore files. Remove the Scala lang provided in the POM file. Delete the unnecessary variable.  Weekly update in TiKV and PD Last week, we landed 18 PRs in the TiKV and PD repositories.Added  Support builtin aggregation functions bit_and, &#39;bit_or and bit_xor in Coprocessor. Upgrade the version of gRPC-rs to 0.2. Make delete_range configurable. Enable table split by default. Add trend API in PD. Increase the priority of the raftstore thread. Add ScatterRegion API in PD. Move SST instead of copying in injestion. Update the version of fail-rs to 0.2.  Fixed  Fix the return type of avg and sum in Coprocessor. Catch stale command in the scheduler. Fix build error on macOS.  Improved  Clean up the select interface in Coprocessor. Collect metrics more efficiently in Coprocessor. Make fail point tests more stable.  New contributor (Thanks!)  TiKV: Rain Li  "},
		{"url": "https://pingcap.com/blog/pingcap-reflection-and-gratitude/",
		"title": "2017 Reflection and Gratitude", 
		"content": " In open source, we trust!2017 has witnessed the growth of PingCAP, from Beijing to Silicon Valley, and the evolution of TiDB, from RC1 to the 1.0 release, and then to the 1.0.5 release. As our CEO Max said in the TiDB 1.0 announcement, &amp;ldquo;because of the hard work and dedication of not just every member of our team, but also every contributor, user, and partner in our open source community.&amp;ldquo;As 2017 draws to a close, let’s take a look back upon the highlights that shape PingCAP and TiDB:Community  1818 forks 13000+ stars 1106 watches 210+ contributors 41 weekly updates 26 PingCAP Infra Meetups 1 of the gRPC popular projects Listed on the CNCF Cloud Native Landscape Merged 14 PRs to RocksDB and 5 PRs to etcd 40 changed files in 1 PR from Samsung Electronics  Cloud  A cloud-native HTAP Database on UCloud and Tencent Cloud  Product  30+ customers in APAC region 1.0 Release, marking the production readiness of TiDB 1st version of TiSpark: sitting SparkSQL on top of TiKV. Distributed HTAP made possible  Meetup  1st Rust China Meetup Siddon&amp;rsquo;s talk atRust meetup in Bay area Siddon&amp;rsquo;s talk at RocksDB meetup in Bay area 2nd invitation to Percona Live for Speech (Edward)  Company  $15M in Series B funding led by China Growth Capital 1st of many steps towards a global impact by opening the Silicon Valley office  We connect, contribute, collaborate, and we will never stop.Thank you all, our beloved contributors, customers, and partners, for an amazing 2017!Hello, 2018!See the following infographic for a recap of PingCAP in 2017:"},
		{"url": "https://pingcap.com/blog/2017-12-26-105/",
		"title": "TiDB 1.0.5 Release Notes", 
		"content": " On December 26, 2017, TiDB 1.0.5 is released with the following updates:TiDB  Add the max value for the current Auto_Increment ID in the Show Create Table statement. Fix a potential goroutine leak. Support outputting slow queries into a separate file. Load the TimeZone variable from TiKV when creating a new session. Support the schema state check so that the Show Create Tableand Analyze statements process the public table/index only. The set transaction read only should affect the tx_read_only variable. Clean up incremental statistic data when rolling back. Fix the issue of missing index length in the Show Create Table statement.  PD  Fix the issue that the leaders stop balancing under some circumstances.  869 874  Fix potential panic during bootstrapping.  TiKV  Fix the issue that it is slow to get the CPU ID using the get_cpuid function. Support the dynamic-level-bytes parameter to improve the space collection situation.  To upgrade from 1.0.4 to 1.0.5, follow the rolling upgrade order of PD -&amp;gt; TiKV -&amp;gt; TiDB."},
		{"url": "https://pingcap.com/weekly/2017-12-25-tidb-weekly/",
		"title": "Weekly update (December 18 ~ December 24, 2017)", 
		"content": " Weekly update in TiDB Last week, we landed 48 PRs in the TiDB repositories.Added  Support the builtin aggregation function bit_or.  Removed  Remove the old JSON type. Remove the HashSemiJoin plan and executor.  Fixed  Support showing the current auto_increment id in the result of show create table. Fix a bug of NewIndexLookUpJoin&#39;s Next(). Fix the trigger condition for AutoAnalyze. Only rebuild the range when using prepared cache.  Improved  Merge ApplyExec and NestedLoopJoin into NestedLoopApply. Refine the UnionAll plan building. Metrics: record details of the RPC type and store id. Replace JSON with BinaryJSON. Collect and store the query feedback. Make the function ExtractColumns more efficient. Add BinaryJSON functions. Enhance the index join, making it can be used for more scenarios. Graceful shutdown will wait clients to close. Support Chunk in executors:  LoadData GrantExec RevokeExec DeallocateExec SimpleExec AnalyzeExec TableScanExec ShowDDLjobsExec TableDualExec ReplaceExec DeleteExec MergeJoinExec   Weekly update in TiSpark Last week, we landed 10 PRs in the TiSpark repositories.Added  Add counting single column tests. Add request.timezone.offset config. Add the test framework for each issue. Add index related integration tests. Add debug utils. Add null data for DAG test cases.  Removed  Remove timezone in the TisparkTest.sql file.  Fixed  Fix the Load tests problem. Fix the test framework concerning null tests.  Weekly update in TiKV and PD Last week, we landed 13 PRs in the TiKV and PD repositories.Added  Support Raft Learner. Add transaction tests with fail-rs. Add more configurations for the RocksDB column family. Add more complex configurations for the PD simulator. Coprocessor: collect output counts for each executor.  Fixed  PD: support join without scheme. Return the stale epoch error for retry. Add the require option to the addr command flag. Update rustc_serialize to pass newer Rust version compilation.  Improved  Coprocessor: speed up the CM sketch test. Add key_only to avoid fetching the data value. Avoid calling Raft status to reduce allocation.  New contributors (Thanks!) TiKV: Jiahao Huang Andrew Hobden  "},
		{"url": "https://pingcap.com/blog/Time-in-Distributed-Systems/",
		"title": "Tick or Tock? Keeping Time and Order in Distributed Databases", 
		"content": " Preface At re:Invent 2017, Amazon Web Services (AWS) announced Amazon Time Sync Service, a highly accurate and reliable time reference that is natively accessible from Amazon EC2 instances. It is much like the Google TrueTime published in 2012. Why do Google and AWS both want to make efforts to provide global time service? Is there any inspiration for building distributed database? This topic is important to think about.Time synchronization remains a hard nut to crack in distributed systems, especially for distributed databases such as TiDB where time is used to confirm the order of the transaction to guarantee the ACID compliance.In this post, I will introduce the existing solutions to tackle the time synchronization issue in distributed systems, as well as their pros and cons. I will also share why we chose to use the timestamp oracle (TSO) from Google Percolator in TiDB.Order of the events Linearizability is important for distributed systems, especially for distributed databases. We can’t allow reading stale value after the update. For example, if account A has $100, and transfers $10 to B. After the transaction finishes, we can’t read $100 again from account A.Another simple explanation is: if we write a data at time T1 like set a = 10, after T1, we must always read 10 as the value of a, not 11 or any other values.But how can we ensure we can read the newest data? How can we know the order of two events?We usually use &amp;ldquo;happened before&amp;rdquo;(hb or -&amp;gt;) to describe the relationship of two causal events. For two events e1 and e2, e1 -&amp;gt; e2, we can say that e1 happened before e2, or e1 causes e2.If we have only one process, determining the order is easy, because all the events can only happen in the process in sequence. However, in a distributed system, things become more complex. The events may happen in different places, and it becomes hard to determine the order of all the events.At first, we may consider using wall time, but the time is not the same in all the processes &amp;ndash; one process may run faster, and the other process may walk slower. So we can’t use the time directly to check the order of the events. Luckily, we have other ways to do it.Logical clock A simple way is to use logical clock which was proposed by Lamport in 1978 for timestamping and ordering events in a distributed system. The algorithm is: Every process starts with an initialized clock counter. A process increments the value of its counter before each event in that process. When a process sends a message, it includes the counter value with the message. When a process receives a message, it increases its clock counter value to be bigger than both its current clock counter value and the value in the received message. If two events have the same clock value, we may think they happen simultaneously so we should use process ID or any other unique ID to differentiate them.  Using this can easily determine the order of events. For example, assuming we have processes P1, P2, both with an initialized counter value 0. P1 increases the counter value to 1 and executes event A. P2 wants to do something but needs to know the result of the event A, so it increases its counter to 1 and sends a message with counter 1 to P1. P1 receives the message and increases its counter to 2, then increases the counter to 3 and replies to P2. P2 receives the message and increases the counter to 4 then executes event B with counter 5.  We use C(A) and C(B) as the counter value of the events, if events A and B happen in one process, and A happens before B, we can know that C(A) &amp;lt; C(B). If A and B happen in different processes, we can also know C(A) &amp;lt; C(B) based on the message, so if A happens before B, we can infer C(A) &amp;lt; C(B). But if C(A) &amp;lt; C(B), it doesn’t necessarily mean that A happens before B.If the two events are not causally related (no communication between the processes), we can’t determine the order of the events. We can use vector clock to fix this. But whether it’s logical clock or vector clock, they both have a disadvantage: we can’t know what time the event happens because both of the two clocks merely record the order of the events instead of the time.To get the chronological order of the events, we have to go back to square one and use real time. But we can’t depend on Network Time Protocol (NTP) directly because it has some errors and the time is not accurate, so what should we do?TrueTime In Google Spanner, it uses TrueTime API to fix the problem of time. Spanner uses GPS and Atomic Clock to correct the time and can guarantee clock uncertainty bound (ε) is very small. The ε value is less than 7ms in 2012; it may be less now.The TrueTime API is very simple:  Method Return   TT.now() TTinterval: [earliest, latest]   TT.after(t) true if t has definitely passed   TT.before(t) true if t has definitely not arrived   Spanner can’t get an accurate time point but only an interval. We can use function now to get an interval. Assuming event A happens at the time point tt.a, and tt.b is for event B. We can know that tt.a.earliest &amp;lt;= tt.a &amp;lt;= tt.a.latest. For event A and B, if A happens before B, we can infer tt.a.latest &amp;lt; tt.b.earliest.Because TrueTime has the clock uncertainty bound ε, so for every transaction commit, it must wait 2ε time to guarantee linearizability, but ε is so small that the performance is still high.The biggest hurdle to adopting TrueTime is that it depends on special hardware, such as GPS clocks and atomic clocks, which many companies do not have.Hybrid Logical Clock Hybrid logical clock (HLC) is another way for timekeeping and timestamping in distributed systems.Based on NTP, HLC can only read time from NTP, but it won’t change it. HLC contains two parts: physical clock + logical clock. For example, assuming: pt: the physical time l: the maximum of pt information learned so far c: the logical clock  To compare the order of two events, we can first check their l time, if equal, we can check c time, for any two events e and f, if e happened before f, we can know (l.e, c.e) &amp;lt; (l.f, c.f).The HLC algorithm for node j: Initialize l.j = 0 and c.j = 0 when node j starts up. Send a message to another node, or a local event happens:l’.j = l.j l.j = max(l’.j, pt.j) if (l.j = l’.j) then c.j = c.j + 1 else c.j = 0 Timestamp with l.j, c.j Receive a message from node m.l’.j = l.j l.j = max(l’.j, l.m, pt.j) if (l.j = l’.j = l.m) then c.j = max(c.j, c.m) + 1 else if (l.j = l’.j) then c.j = c.j + 1 else if (l.j = l.m) then c.j = c.m + 1 else c.j = 0 Timestamp with l.j, c.j  As we can see, HLC is very easy to implement and doesn’t depend on hardware. But HLC is not the silver bullet to solve the time synchronization problem of distributed systems. HLC still needs to guarantee |l.e - pt.e| &amp;lt;= ε to make HLC bounded, because sometimes the user wants to use the physical timestamp to query the events directly, and if the HLC is not unbounded, we can’t know whether the event happens or not at this time.HLC still has a bound value ε, so for the transaction, we still need to handle the wait time problem which exists in Spanner with TrueTime.To tolerate the NTP synchronization error to the greatest extent, we may use a big value for ε. Some system uses 250 ms or 500 ms by default. Obviously, these default values are big and can cause a high latency for the transaction. The large ε value has little impact when supporting multiple data centers because the network latency is high as well and might be even higher than the value of the clock offset. When we send a message to the remote node, we don’t need to wait for too much time after we subtract the network transfer time.But what can we do if the NTP is not working as expected? Start panicking? Or just ignore this error?Why we choose TSO? As a distributed relational database, TiDB supports cross-instance transactions by using an optimized two-phase commit protocol (2PC) from Google Percolator. In the practical …"},
		{"url": "https://pingcap.com/weekly/2017-12-18-tidb-weekly/",
		"title": "Weekly update (December 11 ~ December 17, 2017)", 
		"content": " Weekly update in TiDB Last week, we landed 46 PRs in the TiDB repositories.Added  Support SEPARATOR in the group_concat aggregate function. Support the BinaryJSON type. Add a config for the SQL parser to enable parsing syntax for window function. Support the http index MVCC interface.  Fixed  Show the index column length if necessary in show create table statement. Clear the delta info when rolling back a transaction. Only set defaultValues in aggregation push down. Fix a bug when applying meets index join.  Improved  Support analyzing all indices statements. Load timezone from TiKV when starting a new session. Support date_format push down. NewIndexLookUpJoin executor for Chunk. Attach the requiredProp info to physical plans. Support tls connection to pd and tikv. Support Chunk in executors:  DDLExec CancelDDLJobsExec ShowDDLExec PrepareExec CheckTableExec SelectLockExec ExplainExec SetExecutor ExistsExec UpdateExec JoinResultGenerator   Weekly update in TiSpark Last week, we landed 10 PRs in the TiSpark repositories.Fixed  Fix the PD cache invalidation not synced for the TiSpark side. Fix the Region Request failure not synced with the TiSpark driver. Fix the inconsistent datatype mapping for Datetime type. Fix inconsistent datatype mapping for DOUBLE type. Fix not printing index name in plan. Fix the schema change that causes a SQL failure. Fix inconsistency of the timestamp unit.  Weekly update in TiKV and PD Last week, we landed 14 PRs in the TiKV and PD repositories.Added  Coprocessor: support date_format. Pd/Client: support get_region_info. Worker: add pending capacity. Storage: add more fail points.  Fixed  Tests: use the same RocksDB configuration as production. PD: fix the stale region info when overlapping. PD: fix tls for join. PD: fix the panic that the cluster status is nil.  Improved  Update Prometheus. Use the lower case of logrus. Correct a typo.  "},
		{"url": "https://pingcap.com/blog/2017-12-11-104/",
		"title": "TiDB 1.0.4 Release Notes", 
		"content": " TiDB 1.0.4 Release Notes On December 11, 2017, TiDB 1.0.4 is released with the following updates:TiDB  Speed up the loading of the statistics when starting the tidb-server Improve the performance of the show variables statement Fix a potential issue when using the Add Index statement to handle the combined indexes Fix a potential issue when using the Rename Table statement to move a table to another database Accelerate the effectiveness for the Alter/Drop User statement  TiKV  Fix a possible performance issue when a snapshot is applied  Fix the performance issue for reverse scan after removing a lot of data Fix the wrong encoded result for the Decimal type under special circumstances  To upgrade from 1.0.3 to 1.0.4, follow the rolling upgrade order of PD -&amp;gt; TiKV -&amp;gt; TiDB."},
		{"url": "https://pingcap.com/weekly/2017-12-11-tidb-weekly/",
		"title": "Weekly update (December 04 ~ December 10, 2017)", 
		"content": " Weekly update in TiDB Last week, we landed 45 PRs in the TiDB repositories.Added  Add hints to force to choose HashJoin. Provide the HTTP API of table disk usage for tidb-ctl.  Fixed  Fix a bug when updating the JSON field. Delete the auto ID key when renaming the table. Fix a bug in JoinResultGenerator. Fix a bug when backfilling the index with nil. The value of a session variable should not be modified when getting a global variable.  Improved  Move the Cancel function from session to clientConn. Reduce a comparison in the union scan. Move the binary-tree library from petar/GoLLRB to google/btree. Add the Format interface on ExprNode to convert AST back to string. Improve the performance of ShowVariables. Add Chunk/List to hold a slice of Chunk. Add the explain info for TopN. Support coprocessor streaming API for the TiKV client. Support sql_mode IGNORE SPACE. Support the builtin aggregation function bit_xor. Refactor SQL query optimizer:  Replace Sort with LogicalSort and PhysicalSort. Add Physical operator for Lock, Limit and UnionAll. Make Show no longer a logical/physical plan. Add Physical plan for MaxOneRow, Dual and Exists. Add Physical plan for Projection and TopN. Extract getChildrenPossibleProp to a file. Add PhysicalStreamAgg and remove AggType.  Support Chunk in executors:  InsertExec MaxOneRowExec TopN UnionExec   Weekly update in TiSpark Last week, we landed 12 PRs in the TiSpark repositories.Added  Implement the Coprocessor DAG mode reading. Implement TopN pushdown. Add the switch for disabling a specific expression pushdown for old versions of TiKV. Add the Cache invalidation notification for the client.  Fixed  Fix the reading error when all values in group aggregate to null. Fix a potential error escape from Handler. Fix inconsistent PD cache eviction and add capture for leaked exception. Fix the pushdown type problem of timestamp.  Weekly update in TiKV and PD Last week, we landed 21 PRs in the TiKV and PD repositories.Added  Add TiltCase and Main. Support lower_bound for iterator. Add on_tick for BatchRunnable. Support getting the unique index. Add role_observer in the coprocessor. Record the wait time of APPLY_TASK. Support disabling the block cache for test. Support setting bytes_per_sync and wal_bytes_per_sync.  Fixed  Use the same configuration for tikv-ctl and tikv-server. Fix zero truncation in decimal decoding. Call the post_apply hook. Support the TLS connection in pd-recover.  Improved  Improve scheduler metrics. Avoid cloning prs in Raft. Update Limiter using all pending operators. Use a bit field to present the operator kind. Update RocksDB with optimized ingesting SST. Reduce the log in the MVCC layer. Refactor tests for the RPC client. Optimize the dev build and fix typo.  New contributors (Thanks!)  TiDB:  Evgeniy Kulikov denofiend  docs-cn:  Guangkuo Bian   "},
		{"url": "https://pingcap.com/blog/Silicon-Valley-Office-Announcement/",
		"title": "PingCAP Plants its Seed in Silicon Valley", 
		"content": " PingCAP Plants its Seed in Silicon Valley PingCAP, a cutting-edge distributed Hybrid Transactional/Analytical Processing (HTAP) database company, is excited to announce the opening of its Silicon Valley office, located at the GSV Labs in Redwood City, California. GSV (Global Silicon Valley) Labs is a global innovation platform that houses more than 170 startups, investors, and partners in its 60,000 square foot space in the heart of Silicon Valley. Its member startups work in a wide range of technologies and industries, from Big Data and healthcare, to VR and education.Image sourceFrom day one, PingCAP was conceived and built to be a global technology company with global impact. With the GA 1.0 release of its flagship product TiDB in October 2017, PingCAP has gained significant adoption in the APAC region from leading companies like Mobike and Gaea. Opening a Silicon Valley office is a natural progression of its growth trajectory.&amp;ldquo;Even before we wrote the first line of code, we wanted the impact of our vision to be global. That’s why we adopted the open source way from the beginning,&amp;rdquo; said Max Liu, co-founder and CEO. “I’m thrilled to be taking our first of many steps in pursuit of that mission.”With a physical presence in Silicon Valley, PingCAP plans to aggressively drive product adoption in the U.S., build a strong local community of developers and contributors, and recruit a team of top-notch engineers and evangelists to further develop and raise awareness of its products. To see current U.S.-based job openings, please visit: Jobs at PingCAP."},
		{"url": "https://pingcap.com/weekly/2017-12-04-tidb-weekly/",
		"title": "Weekly update (November 27 ~ December 03, 2017)", 
		"content": " Weekly update in TiDB Last week, we landed 43 PRs in the TiDB repositories.Added  Add an option to disable Chunk. Add the schema info API of the http status server.  Removed  Remove the Align method of IndexRange.  Fixed  Set the priority for IndexLookupExecutor when reading the table. Fix the length metadata of the decimal column returned to the client. Fix the bug about auto-increment key after renaming a table from the old DB to another DB. Fix large float64 to bigint. Notify TiDB of updating privileges after Alter User and Drop user.  Improved  Refine codes and make backoffer the is-a go context. Refactor NewDomain. Speed up loading full stats info. Speed up loading stats info when the server is started. Remvove the hasGby field. Limit the Chunk size to MaxChunkSize. Remove the useless object clone. Split Selection to the logical plan and physical plan. Move Range to the package ranger. No longer treat DML as a logical/physical plan. Support Chunk in:  LimitExec Sort SelectionExec   Weekly update in TiSpark Last week, we landed 6 PRs in the TiSpark repositories.Added  Support the version print for both the client and TiSpark.  Fixed  Fix the inconsistent timezone behavior of Datetime. Fix wrongly folded common aggregation expressions. Fix the inconsistent PD cache eviction and add capture for leaked exception.  Weekly update in TiKV Last week, we landed 17 PRs in the TiKV and PD repositories.Added  Implement the count-min sketch in TiKV Coprocessor. Add the fake TiKV client. Add TLS support in TiKV and PD.  Fixed  Fix weird leader distribution after adding a new node and restarting. Fix wrong key range generation in tests. Fix wrong default features.  Improved  Limit the generation IO of region snapshot. Update to protobuf 3. Reduce the slow log in endpoint and scheduler. Use readyNotify of etcd. Always link to RocksDB statically. Use the latest rust-prometheus. Update RocksDB to 5.8.  New contributors (Thanks!)  TiDB: Johnny Bergström Docs: Du Chuan  "},
		{"url": "https://pingcap.com/blog/optimizing-raft-in-tikv/",
		"title": "A TiKV Source Code Walkthrough – Raft Optimization", 
		"content": " Paxos or Raft is frequently used to ensure data consistency in the distributed databases. But Paxos is known for its complexity and is rather difficult to understand while Raft is very simple. Therefore, a lot of emerging databases tend to use Raft as the consensus algorithm at its bottom layer. TiKV is no exception.Simple as Raft is, its performance is not ideal if we follow exactly the way introduced in the Paper. Therefore, optimizations are essential. This blog introduces how we optimize Raft to ensure its performance in TiKV. It presumes that the audience is very familiar with Raft algorithm and don’t need much explanation. (If not, please see Raft in TiKV).A simple Raft process Below is the simple Raft process: The leader receives the request sent by the client. The Leader appends the request to its log. The Leader sends the corresponding log entry to other followers. The Leader waits for the result of followers. If the majority of nodes have committed this log, then Leader applies. The Leader returns the result to the client. The Leader continues to handle the next request.  You can see, the above flow is a typical sequential operation and if we exactly follow this workflow, the performance would be far from ideal.Batch and Pipeline The first approach that comes to our minds is to use batch to solve the performance problem. As is known to all, using batch could remarkably improve the performance in most cases. For example, as for the writes to RocksDB, we usually don&amp;rsquo;t write one value each time; instead, we use WriteBatch to cache a batch of updates and write them all. For Raft, the Leader can gather multiple requests at a time and send this batch to its Follower. Of course, we also need a maximum size to limit the amount of data sent each time.If we merely use batch, the Leader couldn&amp;rsquo;t proceed to the subsequent flow until its Follower returns the result. Thus, we use Pipeline to speed up the process. The Leader maintains a NextIndex variable to represent the next log position that will be sent to the Follower. Usually, once the Leader establishes a connection with the Follower, we will consider that the network is stable and connected. Therefore, when the Leader sends a batch of logs to the Follower, it can directly update NextIndex and immediately sends the subsequent log without waiting for the return of the Follower. If the network goes wrong or the Follower returns a few errors, the Leader needs to readjust NextIndex and resends log.Append Logs Parallelly We can execute the 2nd and 3rd steps of the above simple Raft process in parallel. In other words, the Leader can send logs to the Followers in parallel before appending logs. The reason is that in Raft if a log is appended by the majority of nodes, we consider the log committed. Thus, even if the Leader cannot append the log and goes panic after it sends a log to its Follower, the log can still be considered committed as long as N/2 + 1 followers have received and appended the log. The log will then be applied successfully.Since appending log involves disk writing and overhead, we’d better make the Follower receive log and append as quickly as possible when the Leader is writing to the disk.Note that though the Leader can send the log to the Follower before appending log, the Follower cannot tell the Leader that it has successfully appended this log in advance. If the Follower does so but fails, the Leader will still think that the log has been committed. In this case, the system might be at the risk of data loss.Asynchronous Apply As I mentioned previously, when a log is appended by the majority of the nodes, we consider it committed. When the committed log is applied has no impact on data consistency. So when a log is committed, we can use another thread to apply this log asynchronously.So the entire Raft process becomes as follows: The leader receives a request sent by a client. The Leader sends the corresponding log to other followers and appends locally. The leader continues to receive requests from other clients and executes step 2. The Leader finds that the log has been committed, and apply the log in another thread. Leader returns the result to the corresponding client after asynchronously applying the log.  The benefit of using asynchronous apply is that we are now able to append and apply log in parallel. Although to a client, its single request still needs to go through the whole Raft process; to multiple clients, the overall concurrency and throughput have improved.SST Snapshot In Raft, if a Follower lags far behind the Leader, the Leader will probably send a snapshot to the Follower directly. In TiKV, Placement Driver sometimes schedules a few replicas inside a Raft Group onto other machines. All of this involves Snapshot.Below is a Snapshot process in the current implementation: The Leader scans all data of a region and creates a snapshot file. The Leader sends the snapshot file to Follower. The Follower receives the snapshot file, reads it and writes to RocksDB in batches.  If there are multiple Followers of the Raft Group processing the snapshot file within one node, RocksDB’s write load will be huge, which easily leads to the condition that the whole write process slows down or stalls when RocksDB struggles to cope with so many compactions.Fortunately, RocksDB offers the SST mechanism, with which we can directly create an SST snapshot file. Then the Follower loads the SST file to RocksDB by calling DB::IngestExternalFile() and passes the file paths as a vector of std::string. For more information, see Ingesting SST files.Asynchronous Lease Read or Append Log TiKV uses ReadIndex and Lease Read to optimize the Raft Read operation, but these two operations are performed within the Raft thread, which is the same as the appended log process of Raft. However fast the appended log is written to RocksDB, this process still delays Lease Read.Thus, currently, we are trying to asynchronously implement Lease Read in another thread. We’ll move the Leader Lease judgment to another thread, and the thread in Raft will update Lease regularly through messages. In this way, we can guarantee that the write process of Raft will not influence that of read.We are also trying to append the Raft log in another thread at the same time. We will compare the performance of the two approaches and choose the better one later.Summary We will continuously optimize the Raft process in the future. And up to now, our hard work pays off as we have significant improvements in performance. But we know that there are more difficulties and challenges to resolve. We are looking forward to experienced experts who are good at performance optimization. If you have interest in our project and want to improve Raft, do not hesitate to contact us: info@pingcap.com.Fore more information  Raft in TiKV The Design and Implementation of Multi-raft  "},
		{"url": "https://pingcap.com/blog/2017-11-28-103/",
		"title": "TiDB 1.0.3 Release Notes", 
		"content": " TiDB  Optimize the performance in transaction conflicts scenario Add the TokenLimit option in the config file Output the default database in slow query logs Remove the DDL statement from query duration metrics Optimize the query cost estimation Fix the index prefix issue when creating tables Support pushing down the expressions for the Float type to TiKV Fix the issue that it is slow to add index for tables with discrete integer primary index Reduce the unnecessary statistics updates Fix a potential issue during the transaction retry  PD  Support adding more types of schedulers using API  TiKV  Fix the deadlock issue with the PD client Fix the issue that the wrong leader value is prompted for NotLeader Fix the issue that the chunk size is too large in the coprocessor  To upgrade from 1.0.2 to 1.0.3, follow the rolling upgrade order of PD-&amp;gt;TiKV-&amp;gt;TiDB."},
		{"url": "https://pingcap.com/weekly/2017-11-27-tidb-weekly/",
		"title": "Weekly update (November 20 ~ November 26, 2017)", 
		"content": " Weekly update in TiDB Last week, we landed 60 PRs in the TiDB repositories.Added  Support the Create View syntax. Support the PAD_CHAR_TO_FULL_LENGTH sql_mode. Support the PROXY protocol.  Fixed  Fix a bug in retry(). Fix a bug about parse duration when the fsp round overflows 60 seconds. Fix the missing index update about automatic updating for TIMESTAMP. Fix a bug when val &amp;gt; MaxInt32 in the from_unixtime argument. Clean up the goroutine after closing a domain. Deep clone TopN when pushing it down through Join. Add overflow Truncate when converting Str to Float. The password used to grant the privilege should be a hash string. Throw out error if the index hint not exist. Fix an invalid time cast bug.  Improved  Use Chunk in TableReader. Support Chunk in IndexLookupReader. Support Chunk in ProjectionExec. Make chunk.Row iterable. Load the stats table asynchronously. Support vectorized execution of expressions. Avoid updating delta when the modified count is 0. Limit the length of the index name. Support the builtin aggregation function bit_and.  Weekly update in TiSpark Last week, we landed 5 PRs in the TiSpark repositories.Added  Add the code format for scala.  Fixed  Fix a column binding error which prevents the pushdown. Fix the integration test loading. Fix the Stale Region error.  Improved  Upgrade to GRPC 1.7.  Weekly update in TiKV Last week, we landed 10 PRs in the TiKV repositories.Added  Add the fake TiKV. Support the namespace configuration.  Fixed  Fix potential disk full due to too many stale snapshots. Fix Chunk count detection. Fix a deadlock when trying to reconnect to PD.  Improved  Update to protobuf 3. Adjust the gRPC metrics. Clean up codes.  New contributors (Thanks!) TiDB: ZhengQian Zheng Dayu  "},
		{"url": "https://pingcap.com/weekly/2017-11-20-tidb-weekly/",
		"title": "Weekly update (November 13 ~ November 19, 2017)", 
		"content": " Weekly update in TiDB Last week, we landed 48 PRs in the TiDB repositories.Removed  Remove redundant ResolveIndices. Remove useless error return.  Fixed  Fix the index recognized as prefix index when the column length is enlarged. Check the MaxInt64 and MinInt64 to avoid range error. Fix the estimation in betweenRowCount. Refine sql_mode no_backslash_escapes. Add deep copies for the update operation. Refine projection elimination when projection is the inner child of an outer join. Fix a data race in dataReaderBuilder. Support not in and correct the behavior.  Improved  Remove returned value isNull in Row methods. Replace *ast.Row with types.Row and prepare to use Chunk. Increase the batch size slowly for double read, which benefits small queries. Remove unionscan schema and add LogicalUnionScan. Build the logical plan to check the column name validation when doPrepare. Covert max/min to Limit + Sort operators. Support adding columns with parentheses. Prealloced space. Ignore the DDL statement for query duration metrics. Refine the use of idAllocator. Use baseExecutor for all Executors. Support decoding data to Chunk. Make tokenLimit configurable.  New contributors (Thanks!)  liubo ZhengQian Zhengwanbo  Weekly update in TiSpark Last week, we landed 3 PRs in the TiSpark repositories.Fixed  Fix the unsupported expression error of not being pushed back. Fix the thread pool error of not closing properly. Fix the Bit type being pushed down.  Weekly update in TiKV Last week, we landed 23 PRs in the TiKV repositories.Added  Add more observers. Support configuring namespaces.  Fixed  Fix a range check in debugging API. Fix CI execution. Make the progresses stored in Raft leader never false positive. Fix the Chunk size of coprocessor response. Fix a potential dead lock when resolving address. Return the correct leader when the NotLeader error occurs. Fix wrong log info.  Improved  As a part of introducing streaming, remove lifetime in the transaction layer. Code cleanup.  Move mod debug from raftstore into server Fix typos Move opt into clusterInfo Clean up scheduelr Move server methods to server.go  Use git describe to assign PDReleaseVersion. Prealloc pre-makes space. Use CGO_ENABLED instead of ENABLE_CGO. Add the log signal before exit. Limit the duration metric buckets of gRPC messages. Shorten write guard lifetime when sending the snapshot.  New contributors (Thanks!)  Drogon  "},
		{"url": "https://pingcap.com/blog-cn/rust-key-value-store/",
		"title": "使用 Rust 构建分布式 Key-Value Store", 
		"content": " 引子 构建一个分布式 Key-Value Store 并不是一件容易的事情，我们需要考虑很多的问题，首先就是我们的系统到底需要提供什么样的功能，譬如： 一致性：我们是否需要保证整个系统的线性一致性，还是能容忍短时间的数据不一致，只支持最终一致性。 稳定性：我们能否保证系统 7 x 24 小时稳定运行。系统的可用性是 4 个 9，还有 5 个 9？如果出现了机器损坏等灾难情况，系统能否做的自动恢复。 扩展性：当数据持续增多，能否通过添加机器就自动做到数据再次平衡，并且不影响外部服务。 分布式事务：是否需要提供分布式事务支持，事务隔离等级需要支持到什么程度。  上面的问题在系统设计之初，就需要考虑好，作为整个系统的设计目标。为了实现这些特性，我们就需要考虑到底采用哪一种实现方案，取舍各个方面的利弊等。后面，我将以我们开发的分布式 Key-Value TiKV 作为实际例子，来说明下我们是如何取舍并实现的。TiKV TiKV 是一个分布式 Key-Value store，它使用 Rust 开发，采用 Raft 一致性协议保证数据的强一致性，以及稳定性，同时通过 Raft 的 Configuration Change 机制实现了系统的可扩展性。TiKV 提供了基本的 KV API 支持，也就是通常的 Get，Set，Delete，Scan 这样的 API。TiKV 也提供了支持 ACID 事务的 Transaction API，我们可以使用 Begin 开启一个事务，在事务里面对 Key 进行操作，最后再用 Commit 提交一个事务，TiKV 支持 SI 以及 SSI 事务隔离级别，用来满足用户的不同业务场景。Rust 在规划好 TiKV 的特性之后，我们就要开始进行 TiKV 的开发。这时候，我们面临的第一个问题就是采用什么样的语言进行开发。当时，摆在我们眼前的有几个选择： Go，Go 是我们团队最擅长的一门语言，而且 Go 提供的 goroutine，channel 这些机制，天生的适合大规模分布式系统的开发，但灵活方便的同时也有一些甜蜜的负担，首先就是 GC，虽然现在 Go 的 GC 越来越完善，但总归会有短暂的卡顿，另外 goroutine 的调度也会有切换开销，这些都可能会造成请求的延迟增高。 Java，现在世面上面有太多基于 Java 做的分布式系统了，但 Java 一样有 GC 等开销问题，同时我们团队在 Java 上面没有任何开发经验，所以没有采用。 C++，C++ 可以认为是开发高性能系统的代名词，但我们团队没有特别多的同学能熟练掌握 C++，所以开发大型 C++ 项目并不是一件非常容易的事情。虽然使用现代 C++ 的编程方式能大量减少 data race，dangling pointer 等风险，我们仍然可能犯错。  当我们排除了上面几种主流语言之后，我们发现，为了开发 TiKV，我们需要这门语言具有如下特性： 静态语言，这样才能最大限度的保证运行性能。 无 GC，完全手动控制内存。 Memory safe，尽量避免 dangling pointer，memory leak 等问题。 Thread safe，不会遇到 data race 等问题。 包管理，我们可以非常方便的使用第三方库。 高效的 C 绑定，因为我们还可能使用一些 C library，所以跟 C 交互不能有开销。  综上，我们决定使用 Rust，Rust 是一门系统编程语言，它提供了我们上面想要的语言特性，但选择 Rust 对我们来说也是很有风险的，主要有两点： 我们团队没有任何 Rust 开发经验，全部都需要花时间学习 Rust，而偏偏 Rust 有一个非常陡峭的学习曲线。 基础网络库的缺失，虽然那个时候 Rust 已经出了 1.0，但我们发现很多基础库都没有，譬如在网络库上面只有 mio，没有好用的 RPC 框架，HTTP 也不成熟。  但我们还是决定使用 Rust，对于第一点，我们团队花了将近一个月的时间来学习 Rust，跟 Rust 编译器作斗争，而对于第二点，我们就完全开始自己写。幸运的，当我们越过 Rust 那段阵痛期之后，发现用 Rust 开发 TiKV 异常的高效，这也就是为啥我们能在短时间开发出 TiKV 并在生产环境中上线的原因。一致性协议 对于分布式系统来说，CAP 是一个不得不考虑的问题，因为 P 也就是 Partition Tolerance 是一定存在的，所以我们就要考虑到底是选择 C - Consistency 还是 A - Availability。我们在设计 TiKV 的时候就决定 - 完全保证数据安全性，所以自然就会选择 C，但其实我们并没有完全放弃 A，因为多数时候，毕竟断网，机器停电不会特别频繁，我们只需要保证 HA - High Availability，也就是 4 个 9 或者 5 个 9 的可用性就可以了。既然选择了 C，我们下一个就考虑的是选用哪一种分布式一致性算法，现在流行的无非就是 Paxos 或者 Raft，而 Raft 因为简单，容易理解，以及有很多现成的开源库可以参考，自然就成了我们的首要选择。在 Raft 的实现上，我们直接参考的 etcd 的 Raft。etcd 已经被大量的公司在生产环境中使用，所以它的 Raft 库质量是很有保障的。虽然 etcd 是用 Go 实现的，但它的 Raft library 是类似 C 的实现，所以非常便于我们用 Rust 直接翻译。在翻译的过程中，我们也给 etcd 的 Raft fix 了一些 bug，添加了一些功能，让其变得更加健壮和易用。现在 Raft 的代码仍然在 TiKV 工程里面，但我们很快会将独立出去，变成独立的 library，这样大家就能在自己的 Rust 项目中使用 Raft 了。使用 Raft 不光能保证数据的一致性，也可以借助 Raft 的 Configuration Change 机制实现系统的水平扩展，这个我们会在后面的文章中详细的说明。存储引擎 选择了分布式一致性协议，下一个就要考虑数据存储的问题了。在 TiKV 里面，我们会存储 Raft log，然后也会将 Raft log 里面实际的客户请求应用到状态机里面。首先来看状态机，因为它会存放用户的实际数据，而这些数据完全可能是随机的 key - value，为了高效的处理随机的数据插入，自然我们就考虑使用现在通用的 LSM Tree 模型。而在这种模型下，RocksDB 可以认为是现阶段最优的一个选择。RocksDB 是 Facebook 团队在 LevelDB 的基础上面做的高性能 Key-Value Storage，它提供了很多配置选项，能让大家根据不同的硬件环境去调优。这里有一个梗，说的是因为 RocksDB 配置太多，以至于连 RocksDB team 的同学都不清楚所有配置的意义。关于我们在 TiKV 中如何使用，优化 RocksDB，以及给 RocksDB 添加功能，fix bug 这些，我们会在后面文章中详细说明。而对于 Raft Log，因为任意 Log 的 index 是完全单调递增的，譬如 Log 1，那么下一个 Log 一定是 Log 2，所以 Log 的插入可以认为是顺序插入。这种的，最通常的做法就是自己写一个 Segment File，但现在我们仍然使用的是 RocksDB，因为 RocksDB 对于顺序写入也有非常高的性能，也能满足我们的需求。但我们不排除后面使用自己的引擎。因为 RocksDB 提供了 C API，所以可以直接在 Rust 里面使用，大家也可以在自己的 Rust 项目里面通过 rust-rocksdb 这个库来使用 RocksDB。分布式事务 要支持分布式事务，首先要解决的就是分布式系统时间的问题，也就是我们用什么来标识不同事务的顺序。通常有几种做法： TrueTime，TrueTime 是 Google Spanner 使用的方式，不过它需要硬件 GPS + 原子钟支持，而且 Spanner 并没有在论文里面详细说明硬件环境是如何搭建的，外面要自己实现难度比较大。 HLC，HLC 是一种混合逻辑时钟，它使用 Physical Time 和 Logical Clock 来确定事件的先后顺序，HLC 已经在一些应用中使用，但 HLC 依赖 NTP，如果 NTP 精度误差比较大，很可能会影响 commit wait time。 TSO，TSO 是一个全局授时器，它直接使用一个单点服务来分配时间。TSO 的方式很简单，但会有单点故障问题，单点也可能会有性能问题。  TiKV 采用了 TSO 的方式进行全局授时，主要是为了简单。至于单点故障问题，我们通过 Raft 做到了自动 fallover 处理。而对于单点性能问题，TiKV 主要针对的是 PB 以及 PB 以下级别的中小规模集群，所以在性能上面只要能保证每秒百万级别的时间分配就可以了，而网络延迟上面，TiKV 并没有全球跨 IDC 的需求，在单 IDC 或者同城 IDC 情况下，网络速度都很快，即使是异地 IDC，也因为有专线不会有太大的延迟。解决了时间问题，下一个问题就是我们采用何种的分布式事务算法，最通常的就是使用 2 PC，但通常的 2 PC 算法在一些极端情况下面会有问题，所以业界要不通过 Paxos，要不就是使用 3 PC 等算法。在这里，TiKV 参考 Percolator，使用了另一种增强版的 2 PC 算法。这里先简单介绍下 Percolator 的分布式事务算法，Percolator 使用了乐观锁，也就是会先缓存事务要修改的数据，然后在 Commit 提交的时候，对要更改的数据进行加锁处理，然后再更新。采用乐观锁的好处在于对于很多场景能提高整个系统的并发处理能力，但在冲突严重的情况下反而没有悲观锁高效。对于要修改的一行数据，Percolator 会有三个字段与之对应，Lock，Write 和 Data： Lock，就是要修改数据的实际 lock，在一个 Percolator 事务里面，有一个 primary key，还有其它 secondary keys， 只有 primary key 先加锁成功，我们才会再去尝试加锁后续的 secondary keys。 Write，保存的是数据实际提交写入的 commit timestamp，当一个事务提交成功之后，我们就会将对应的修改行的 commit timestamp 写入到 Write 上面。 Data，保存实际行的数据。  当事务开始的时候，我们会首先得到一个 start timestamp，然后再去获取要修改行的数据，在 Get 的时候，如果这行数据上面已经有 Lock 了，那么就可能终止当前事务，或者尝试清理 Lock。当我们要提交事务的时候，先得到 commit timestamp，会有两个阶段： Prewrite：先尝试给 primary key 加锁，然后尝试给 second keys 加锁。如果对应 key 上面已经有 Lock，或者在 start timestamp 之后，Write 上面已经有新的写入，Prewrite 就会失败，我们就会终止这次事务。在加锁的时候，我们也会顺带将数据写入到 Data 上面。 Commit：当所有涉及的数据都加锁成功之后，我们就可以提交 primay key，这时候会先判断之前加的 Lock 是否还在，如果还在，则删掉 Lock，将 commit timestamp 写入到 Write。当 primary key 提交成功之后，我们就可以异步提交 second keys，我们不用在乎 primary keys 是否能提交成功，即使失败了，也有机制能保证数据被正常提交。  在 TiKV 里面，事务的实现主要包括两块，一个是集成在 TiDB 中的 tikv client，而另一个则是在 TiKV 中的 storage mod 里面，后面我们会详细的介绍。RPC 框架 RPC 应该是分布式系统里面常用的一种网络交互方式，但实现一个简单易用并且高效的 RPC 框架并不是一件容易的事情，幸运的是，现在有很多可以供我们进行选择。TiKV 从最开始设计的时候，就希望使用 gRPC，但 Rust 当时并没有能在生产环境中可用的 gRPC 实现，我们只能先基于 mio 自己做了一个 RPC 框架，但随着业务的复杂，这套 RPC 框架开始不能满足需求，于是我们决定，直接使用 Rust 封装 Google 官方的 C gRPC，这样就有了 grpc-rs。这里先说一下为什么我们决定使用 gRPC，主要有如下原因： gRPC 应用广泛，很多知名的开源项目都使用了，譬如 Kubernetes，etcd 等。 gRPC 有多种语言支持，我们只要定义好协议，其他语言都能直接对接。 gRPC 有丰富的接口，譬如支持 unary，client streaming，server streaming 以及 duplex streaming。 gRPC 使用 protocol buffer，能高效的处理消息的编解码操作。 gRPC 基于 HTTP/2，一些 HTTP/2 的特性，譬如 duplexing，flow control 等。  最开始开发 rust gRPC 的时候，我们先准备尝试基于一个 rust 的版本来开发，但无奈遇到了太多的 panic，果断放弃，于是就将目光放到了 Google gRPC 官方的库上面。Google gRPC 库提供了多种语言支持，譬如 C++，C#，Python，这些语言都是基于一个核心的 C gRPC 来做的，所以我们自然选择在 Rust 里面直接使用 C gRPC。因为 Google 的 C gRPC 是一个异步模型，为了简化在 rust 里面异步代码编写的难度，我们使用 rust Future 库将其重新包装，提供了 Future API，这样就能按照 Future 的方式简单使用了。关于 gRPC 的详细介绍以及 rust gRPC 的设计还有使用，我们会在后面的文章中详细介绍。监控 很难想象一个没有监控的分布式系统是如何能稳定运行的。如果我们只有一台机器，可能时不时看下这台机器上面的服务还在不在，CPU 有没有问题这些可能就够了，但如果我们有成百上千台机器，那么势必要依赖监控了。TiKV 使用的是 Prometheus，一个非常强大的监控系统。Prometheus 主要有如下特性： 基于时序的多维数据模型，对于一个 metric，我们可以用多种 tag 进行多维区分。 自定义的报警机制。 丰富的数据类型，提供了 Counter，Guage，Histogram 还有 Summary 支持。 强大的查询语言支持。 提供 pull 和 push 两种模式支持。 支持服务的动态发现和静态配置。 能跟 Grafana 深度整合。  因为 Prometheus 并没有 Rust 的客户端，于是我们开发了 rust-prometheus。Rust Prometheus 在设计上面参考了 Go Prometehus 的 API，但我们只支持了 最常用的 Counter，Guage 和 Histogram，并没有实现 Summary。后面，我们会详细介绍 Prometheus 的使用，以及不同的数据类型的使用场景等。测试 要做好一个分布式的 Key-Value Store，测试是非常重要的一环。 只有经过了最严格的测试，我们才能有信心去保证整个系统是可以稳定运行的。从最开始开发 TiKV 的时候，我们就将测试摆在了最重要的位置，除了常规的 unit test，我们还做了更多，譬如： Stability test，我们专门写了一个 stability test，随机的干扰整个系统，同时运行我们的测试程序，看结果的正确性。 Jepsen，我们使用 Jepsen 来验证 TiKV 的线性一致性。 Namazu，我们使用 Namazu 来干扰文件系统以及 TiKV 线程调度。 Failpoint，我们在 TiKV 很多关键逻辑上面注入了 fail point，然后在外面去触发这些 fail，在验证即使出现了这些异常情况，数据仍然是正确的。  上面仅仅是我们的一些测试案例，当代码 merge 到 master 之后，我们的 CI 系统在构建好版本之后，就会触发所有的 test 执行，只有当所有的 test 都完全跑过，我们才会放出最新的版本。在 Rust 这边，我们根据 FreeBSD 的 Failpoint 开发了 fail-rs，并已经在 TiKV 的 Raft 中注入了很多 fail，后面还会在更多地方注入。我们也会基于 Rust 开发更多的 test 工具，用来测试整个系统。小结 上面仅仅列出了我们用 Rust 开发 TiKV 的过程中，一些核心模块的设计思路。这篇文章只是一个简单的介绍，后面我们会针对每一个模块详细的进行说明。还有一些功能我们现在是没有做的，譬如 open tracing，这些后面都会慢慢开始完善。我们的目标是通过 TiKV，在分布式系统领域，提供一套 Rust 解决方案，形成一个 Rust ecosystem。这个目标很远大，欢迎任何感兴趣的同学加入。"},
		{"url": "https://pingcap.com/blog/2017-11-13-102/",
		"title": "TiDB 1.0.2 Release Notes", 
		"content": " TiDB: Optimize the cost estimation of index point query Support the Alter Table Add Column (ColumnDef ColumnPosition) syntax Optimize the queries whose where conditions are contradictory Optimize the Add Index operation to rectify the progress and reduce repetitive operations Optimize the Index Look Join operator to accelerate the query speed for small data size Fix the issue with prefix index judgment  Placement Driver (PD): Improve the stability of scheduling under exceptional situations  TiKV: Support splitting table to ensure one region does not contain data from multiple tables Limit the length of a key to be no more than 4 KB More accurate read traffic statistics Implement deep protection on the coprocessor stack Fix the LIKE behavior and the do_div_mod bug   "},
		{"url": "https://pingcap.com/weekly/2017-11-13-tidb-weekly/",
		"title": "Weekly update (November 06 ~ November 12, 2017)", 
		"content": " Weekly update in TiDB 2017-11-13Last week, we landed 45 PRs in the TiDB repositories.Added  Support more SQL modes in TiDB:  the NO_UNSIGNED_SUB sql_mode the REAL_AS_FLOAT sql_mode the PIPES_AS_CONCAT sql_mode the high_not_precedence sql_mode the ONLY_FULL_GROUP_BY sql_mode  Parse more privilege types like RELOAD, EVENT and so on. Support part of window function AST.  Removed  Remove joinBuilder. Remove resolver.go.  Fixed  Return error instead of panic if a subquery in JOIN ON condition. Set the error to be undetermined error if not recovered from an RPC error. Fix the issue that DDL is always running and saves the reorg doneHandle regularly. Let the in clause handle more cases.  Improved  Use types.Row to write the result set and prepare to use Chunk. Support the alter table add column(col_name column_definition) syntax. Build and use the Count-Min sketch. Stop reusing the Executor in IndexLookUpJoin and remove doRequestForDatums(). Begin OpenTracing from dispatch() and change the interface to Execute(ctx, sql) to avoid too many noises. Remove the read-only statement from transaction auto retry. Make the batch size of index join increase slowly. Let select push across projects to support generated column index. Support insert into from selectStmt that has brackets. Remove the type assertion on types.DatumRow. Improve hash join to support all the join types. Implement the Count-Min Sketch. Enable OpenTracing for the TableReader, IndexReader, IndexLookup executor.  Weekly update in TiSpark Added  Add R language support. Add Python language support. Add Index Read Support and related task concurrent read.  Improved  Improve integration tests and frameworks.  Fixed  Fix racing condition in thread pool allocation.  Weekly update in TiKV Last week, We landed 18 PRs in the TiKV repositories.Added  Support splitting table Support resolving multiple locks in a batch. Use size as a factor of score when scheduling. Support OpenTracing for pd-client. Support set namespace for meta. Support checking the region stats using API.  Fixed  Verify before destroying a peer. Fix the LIKE behavior for coprocessor. Fix the do_div_mod bug. Fix a nil pointer dereference bug in pd-client. Clear hot region related gauges if a region is not hot any more.  Improved  Add meta decoding for tables. Better region key output API. Unify ApplyContext and ExecContext. Use recover_safe! to hide the panic stack trace. Remove the pending peers before adding a new peer.  New contributers (Thanks!)  wudi xiaojian cai  "},
		{"url": "https://pingcap.com/weekly/2017-11-06-tidb-weekly/",
		"title": "Weekly update (October 30 ~ November 05, 2017)", 
		"content": " Weekly update in TiDB Last week, we landed 51 PRs in the TiDB repositories.Added  Provide the command option and log the success/fail information for slow-query.  Removed  Remove the old planner. Remove xeval. Remove the localstore storage engine.  Fixed  Change the selection plan to dual plan directly if the condition is always false. Insert column char(4) with latin1 charset by incorrect padding. Remove the check of initialized auto ID. Support alter table add column(...). Fix a bug in JSON decoding. Use utf8_bin for unsupported column collation. Fix the bug of alter table table_options, other_alter_specification.  Improved  Move the safepoint checker to tikvStore. Enable pushing float down to TiKV. Fix estimation in index point query. No longer add enforcer. Improve index join to support all join types. Extract the process of getting full CMP type to method. Opentracing for Execute, ParseSQL, Compile, and runStmt. Reduce growslice in dumpRowValuesBinary.  New Contributor (Thanks!)  wudi  Weekly update in TiKV Last week, We landed 16 PRs in the TiKV repositories.Added  Add recursion limit check for coprocessor. Reset the RocksDB statistics periodically.  Fixed  Save the region meta information to cache even if it is failed to be written to the disk. Fix the read statistics. Clear the hot store related gauges when store is not hot any more. Fix the LIKE behavior.  Improved  Move operator influence into ShouldBalance. Reduce allocation. Remove the pending peer before adding a new peer. Refactor the split check. Add limitation for key size.  New Contributor (Thanks!)  wudi  "},
		{"url": "https://pingcap.com/blog/2017-11-01-101/",
		"title": "TiDB 1.0.1 Release Notes", 
		"content": " TiDB: Support canceling DDL Job. Optimize the IN expression. Correct the result type of the Show statement. Support log slow query into a separate log file. Fix bugs.  TiKV: Support flow control with write bytes. Reduce Raft allocation. Increase coprocessor stack size to 10MB. Remove the useless log from the coprocessor.   "},
		{"url": "https://pingcap.com/weekly/2017-10-30-tidb-weekly/",
		"title": "Weekly update (October 23 ~ October 29, 2017)", 
		"content": " Weekly update in TiDB Last week, we landed 52 PRs in the TiDB repositories.Added  Support the window function syntax.  Fixed  Fix an issue when the values builtin function meets the null value. Support the signed field option for the numeric type. Fix an issue of index reader. Fix an issue that the binlog client is not initialized correctly. Correct the schema type of ShowStmt. The default value length for Join should be changed for column pruning. Add retry for dialPumpClient. Parse but ignore the REPLICATION CLIENT/SLAVE, USAGE privileges in the GRANT statement. Fix an issue with visibility check. Fix an issue with renaming tables. Parse PARTITION BY RANGE COLUMNS. Support the straight_join syntax. Support the row_count built-in function. Load the column histograms as needed. Start safePointChecker before running a worker. Truncate the information field for show processlist and support show full processlist.  Improved  Build the prepared statement plan in the Optimization phase. Support the ALTER TABLE t ENGINE = &#39;string&#39; syntax. Remove reorgTableDeleteLimit. Fold the true item in the CNF predicator. Use max uint64 as the start timestamp for the ANALYZE statement . Improve merge join to support all join types. Add highlight to log. Add the Row interface. Simple improvement for range calculation for Primary Key. Change the behavior of the the in function. Support a plan cache for prepared statements. Introduce Chunk to replace the current datum slice Row. Introducing OpenTracing into TiDB:  For two phase commit. Tracing config and gRPC interceptor middleware.   Weekly update in TiSpark Added  Add redundant aggregation elimination.  Fixed  Fix meta deserialize overflow. Fix broken integration test. Fix potential time type NPE. Fix and upgrade Bazel for CI. Fix test dependency in CI.  New Contributor  Ry  Weekly update in TiKV Last week, We landed 21 PRs in the TiKV repositories.Added  Add adjacent region balancing scheduler. Add WAL bytes per the sync configuration. Flow control based on total writing bytes. Implement the in function for coprocessor. Filter the stores with lots of pending peers when balancing. Add region and hot region scheduling API. Record the approximate size of a store in PD.  Fixed  raftstore: fix an issue with stale peer check. Record snapshot metrics in SCHED_STAGE_COUNTER_VEC. Fix the illegal JSON format in the hotspot/stores API.  Improved  Inline get_limit_at_size. Group the metrics by namespaces. Document defer macro LIFO behavior. Remove redundant verbose KV for coordinator. Adjust table ID calculation. Set stack size to 10 MB for the coprocessor threads.  New contributor  odeits  "},
		{"url": "https://pingcap.com/weekly/2017-10-23-tidb-weekly/",
		"title": "Weekly update (October 9 ~ October 22, 2017)", 
		"content": " Weekly update in TiDB Last two weeks, we landed 83 PRs in the TiDB repositories.Added  Support writing slow query log into separate files. Dummy implementation for the SHOW PROFILES statement. Add metrics for automatic analyzing. Support the operation of cancel DDL jobs. Add a new http status API to get meta regions.  Removed  Remove the self field in baseBuiltinFunc completely. Remove foldable from baseBuiltinFunc.  Fixed  Fix a bug occurred in select sum(float col)*0.1 Cast types for the expression of assignments of updateList. Fix a PhysicalReader range bug when data is MaxInt64. Add unsigned and zerofill flags to field type year. Fix a select distinct bug. Fix a bug when auto_increment meets unsigned. Change the DNV of default null column to 0. Correct the signature building of Values. Add schema state check when executing the show create table or analyze statistic statement. Set missing result field Org_table and Database to Navicat for MySQL compatibility. Fix a bug in the copy function. Fix a bug when converting time to scalar. Fix a bug when building histograms for columns. Fix a bug when merging sample collectors. Fix ineffectual assignments. Fix the issue that the show grants statement displays empty entries. Set proper parent for newly projection-eliminated child. Put RPC handler in response instead of returning it. Quit the builtin function SLEEP when it is killed. Fix the issue that the unsigned integer column length is not consistent with MySQL. Add the ParseErrorWith function to make the parse error compatible with MySQL.  Improved  Rename the DDL job state variable. Rename the package inspectkv to admin. Change reorg wait time from 1ms to 50ms. Support int1, int2, int3, int4, int8 type syntax. Estimate NDV more precisely. Return MySQL error code for Unsafe SafePoint. Estimate NDV as pseudo when its value is zero. Make some builtin functions foldable. Return NULL when error is not nil. Improvement for multi-delete. Split the detach process from BuildRange. Use single method to set parent and children. Use pattern match to check dbRecord in mysql.db. Check sc.IgnoreZeroInDate when parsing the string or number type to date/datetime/timestamp. Enforce errcheck in Makefile. Open auto analyze by default. Improve the fold constant. Avoid type assertion for ast.ExprNode. Avoid come assertion for StmtNode. Use ParseTimeFormNum instead of ParseTime. Support plan cache for the SELECT statement.  Weekly update in TiSpark Added  Add support for explain  Fixed  Fix tispark-sql table alias problem  Weekly update in TiKV In the last two weeks, We landed 46 PRs in the TiKV repositories.Added  Register classifiers by name when using namespace. Add the operator priority mechanism. Support setting a region to tombstone status. Add MVCC scan support to debug API. Support fail point. Support removing table_id/store_id from namespace. Persist scheduler list to etcd. Support namespace(experimental). Support health check in pd-ctl. Store case-insensitive labels. Support adding/removing region peer in pd-ctl.  Fixed  Fixes typos in raft. Fix table namespace classifier. Fix split check tests.  Improved  Refactor clusterInfo. Refactor the debug API for tikv-ctl. Adjust scheduler interface. Reduce allocation in Raft. Report read statistics to pd-worker directly. Move out the PD work from raftstore. Use try shorthand. Move significant send to Raft router. Adjust label checks. Optimize approximate region size for region heartbeat. Flow control based on current writing KV count.  "},
		{"url": "https://pingcap.com/blog/2017-10-17-announcement/",
		"title": "PingCAP Launches TiDB 1.0", 
		"content": " PingCAP Launches TiDB 1.0, A Scalable Hybrid Database Solution October 16, 2017 - PingCAP Inc., a cutting-edge distributed database technology company, officially announces the release of TiDB 1.0. TiDB is an open source distributed Hybrid Transactional/Analytical Processing (HTAP) database that empowers businesses to meet both workloads with a single database.In the current database landscape, infrastructure engineers often have to use one database for online transactional processing (OLTP) and another for online analytical processing (OLAP). TiDB aims to break down this separation by building a HTAP database that enables real-time business analysis based on live transactional data. With TiDB, engineers can now spend less time managing multiple database solutions, and more time delivering business value for their companies. One of TiDB’s many users, a financial securities firm, is leveraging this technology to power its application for wealth management and user personas. With TiDB, this firm can easily process web-scale volumes of billing records and conduct mission-critical time sensitive data analysis like never before.&amp;ldquo;Two and a half years ago, Edward, Dylan and I started this journey to build a new database for an old problem that has long plagued the infrastructure software industry. Today, we are proud to announce that this database, TiDB, is production ready,&amp;rdquo; said Max Liu, co-founder and CEO of PingCAP. “Abraham Lincoln once said, ‘the best way to predict the future is to create it.’ The future we predicted 771 days ago we now have created, because of the hard work and dedication of not just every member of our team, but also every contributor, user, and partner in our open source community. Today, we celebrate and pay gratitude to the power of the open source spirit. Tomorrow, we will continue to create the future we believe in.”TiDB has already been deployed in production in more than 30 companies in the APAC region, including fast-growing Internet companies like Mobike, Gaea, and YOUZU. The use cases span multiple industries from online marketplace and gaming, to fintech, media, and travel.TiDB features Horizontal ScalabilityTiDB grows as your business grows. You can increase the capacity for storage and computation simply by adding more machines.Compatible with MySQL ProtocolUse TiDB as MySQL. You can replace MySQL with TiDB to power your application without changing a single line of code in most cases and with nearly no migration cost.Automatic Failover and High AvailabilityYour data and applications are always-on. TiDB automatically handles malfunctions and protects your applications from machine failures or even downtime of an entire data-center.Consistent Distributed TransactionsTiDB is analogous to a single-machine RDBMS. You can start a transaction that crosses multiple machines without worrying about consistency. TiDB makes your application code simple and robust.Online DDLEvolve TiDB schemas as your requirement changes. You can add new columns and indexes without stopping or affecting your ongoing operations.Try TiDB Now!Use cases How TiDB tackles fast data growth and complex queries for yuanfudao.comMigration from MySQL to TiDB to handle tens of millions of rows of data per dayFor more information: TiDB internal: Data Storage Computing Scheduling  Release NotesBest PracticeDocumentsWeekly Update"},
		{"url": "https://pingcap.com/blog-cn/ga-liuqi/",
		"title": "写在 TiDB 1.0 发布之际 | 预测未来最好的方式就是创造未来", 
		"content": "如果只能用一个词来描述此刻的心情，我想说恍如隔世，这样说多少显得有几分矫情，或许内心还是想在能矫情的时候再矫情一次，毕竟当初做这一切的起因是为了梦想。还记得有人说预测未来最好的方式就是创造未来，以前看到这句话总觉得是废话，如今看到这一切在自己身上变成现实的一刻，感受是如此的真切，敲击键盘的手居然有点颤抖，是的，预测未来最好的方式就是创造未来。还记得刚开始做的时候，只有很少的几个人相信这个事情可以做，毕竟难度比较高，就像有些户外旅行，只有方向，没有路。从零开始到发布 1.0 版本，历时 2 年 6 个月，终于还是做出来了。这是开源精神的胜利，是真正属于工程师们的荣耀。这个过程我们一直和用户保持沟通和密切协作，从最早纯粹的为 OLTP 场景的设计，到后来迭代为 HTAP 的设计，一共经历了 7 次重构，许多看得见的汗水，看不见的心跳，也许这就是相信相信的力量，总有那么一群人顶着世俗的压力，用自己的信念和力量在改变世界。在这个过程中，质疑的声音变少了，越来越多的人从观望，到为我们鼓舞助威，帮助我们快速成长。特别感谢那些从 beta 版本开始一路相随的用户，没有你们的信任，耐心和参与，就没有今天的 PingCAP。开心的时刻总是特别想对很多帮助和支持我们的童鞋们说声谢谢，没有你们就没有 PingCAP，特别感谢每一位项目的贡献者。也许你已经知道了，我们专门为你们定制了一面荣誉墙，那里的色彩记录了你们的每一次贡献，如果你仍在埋头工作，来不及知道，我想请你过去逛逛，不负好时光。这个世界还是有人相信未来是可以被创造的。感谢开源精神，让我们这样一个信仰创造未来的团队，可以站在未来的入口，因为相信和努力，获得源源不绝的正向的力量。面对未来，让我们可以摒弃对未知的恐惧和对不完美的妥协。也感谢那些曾经的诋毁和吐槽，让我们不敢懈怠，砥砺前行。然而 1.0 版本只是个开始，是新的起点，愿我们一路相扶，不负远途。"},
		{"url": "https://pingcap.com/blog/2017-10-16-ga/",
		"title": "TiDB 1.0 release notes", 
		"content": " On October 16, 2017, TiDB 1.0 is now released! This release is focused on MySQL compatibility, SQL optimization, stability, and performance.TiDB:  The SQL query optimizer:  Adjust the cost model Analyze pushdown Function signature pushdown  Optimize the internal data format to reduce the interim data size Enhance the MySQL compatibility Support the NO_SQL_CACHE syntax and limit the cache usage in the storage engine Refactor the Hash Aggregator operator to reduce the memory usage Support the Stream Aggregator operator  PD:  Support read flow based balancing Support setting the Store weight and weight based balancing  TiKV:  Coprocessor now supports more pushdown functions Support pushing down the sampling operation Support manually triggering data compact to collect space quickly Improve the performance and stability Add a Debug API for debugging TiSpark Beta Release: Support configuration framework Support ThriftSever/JDBC and Spark SQL  Acknowledgement Special thanks to the following enterprises and teams!  Archon Mobike Samsung Electronics SpeedyCloud Tencent Cloud UCloud  Thanks to the open source software and services from the following organizations and individuals:  Asta Xie CNCF CoreOS Databricks Docker Github Grafana gRPC Jepsen Kubernetes Namazu Prometheus RedHat RocksDB Team Rust Team  Thanks to the individual contributors:  8cbx Akihiro Suda aliyx alston111111 andelf Andy Librian Arthur Yang astaxie Bai, Yang bailaohe Bin Liu Blame cosmos Breezewish Carlos Ferreira Ce Gao Changjian Zhang Cheng Lian Cholerae Hu Chu Chao coldwater Cole R Lawrence cuiqiu cuiyuan Cwen Dagang David Chen David Ding dawxy dcadevil Deshi Xiao Di Tang disksing dongxu dreamquster Drogon Du Chuan Dylan Wen eBoyy Eric Romano Ewan Chou Fiisio follitude Fred Wang fud fudali gaoyangxiaozhu Gogs goroutine Gregory Ian Guanqun Lu Guilherme Hübner Franco Haibin Xie Han Fei hawkingrei Hiroaki Nakamura hiwjd Hongyuan Wang Hu Ming Hu Ziming Huachao Huang HuaiyuXu Huxley Hu iamxy Ian insion iroi44 Ivan.Yang Jack Yu jacky liu Jan Mercl Jason W Jay Jay Lee Jianfei Wang Jiaxing Liang Jie Zhou jinhelin Jonathan Boulle Karl Ostendorf knarfeh Kuiba leixuechun li Li Shihai Liao Qiang Light lijian Lilian Lee Liqueur Librazy Liu Cong Liu Shaohui liubo0127 liyanan lkk2003rty Louis louishust luckcolors Lynn Mae Huang maiyang maxwell mengshangqi Michael Belenchenko mo2zie morefreeze MQ mxlxm Neil Shen netroby ngaut Nicole Nie nolouch onlymellb overvenus PaladinTyrion paulg Priya Seth qgxiaozhan qhsong Qiannan qiukeren qiuyesuifeng queenypingcap qupeng Rain Li ranxiaolong Ray Rick Yu shady ShawnLi Shen Li Sheng Tang Shirly Shuai Li ShuNing ShuYu Wang siddontang silenceper Simon J Mudd Simon Xia skimmilk6877 sllt soup Sphinx Steffen sumBug sunhao2017 Tao Meng Tao Zhou tennix tiancaiamao TianGuangyu Tristan Su ueizhou UncP Unknwon v01dstar Van WangXiangUSTC wangyanjun wangyisong1996 weekface wegel Wei Fu Wenbin Xiao Wenting Li Wenxuan Shi winkyao woodpenker wuxuelian Xiang Li xiaojian cai Xuanjia Yang Xuanwo XuHuaiyu Yang Zhexuan Yann Autissier Yanzhe Chen Yiding Cui Yim youyouhu Yu Jun Yuwen Shen Zejun Li Zhang Yuning zhangjinpeng1987 ZHAO Yijun Zhe-xuan Yang ZhengQian ZhengQianFang zhengwanbo ZhiFeng Hu Zhiyuan Zheng Zhou Tao Zhoubirdblue zhouningnan Ziyi Yan zs634134578 zxylvlp zyguan zz-jason  "},
		{"url": "https://pingcap.com/blog/2017-10-10-nextcon/",
		"title": "Scale the Relational Database with NewSQL", 
		"content": " This is the speech Li SHEN gave at the 3rd NEXTCON: Cloud+Data NEXT Conference Seattle on September 16th, 2017. Speaker introduction Why we build a new relational database TiDB Project - Goal Architecture The core components of TiDB  The Storage stack Dynamic Multi-Raft Safe Split ACID Transaction Something we haven&amp;rsquo;t mentioned Placement Driver The SQL Layer What Happens behind a query SQL Layer Overview Cost-Based Optimizer  Tools matter Spark on TiKV Future plans  Speaker introduction Hello everyone, I am glad to be here in this beautiful city and share this talk with you. The talk‘s name is “Scale the Relational Database with NewSQL”. It is about a new distributed relational database named TiDB. In this talk I’ll draw a detailed picture of TiDB to help you understand why and how we build it.First of all, I’d like to introduce myself and our company PingCAP. My name is Shen Li, an infrastructure software engineer and the VP of Engineering at PingCAP. Our company is a startup based in Beijing, China. We focus on building database solutions. We are building a distributed transactional database TiDB and have more than thirty adoptions for production use in China.So today we will cover the following topics: Motivations The goals of TiDB The core components of TiDB The tools around TiDB Spark on TiKV Future plans  Back to the topWhy we build a new relational database Ok. Let&amp;rsquo;s begin. I want to ask a question: what would you do when your RDBMS is becoming the bottleneck of your application? Maybe most of you guys have experienced the following situations: RDBMS is becoming the performance bottleneck of your backend service The amount of data stored in the database is overwhelming You want to do some complex queries on a sharding cluster  e.g., simple JOIN or GROUP BY  Your application needs ACID transaction on a sharding cluster  In the old days, all you can do is to either refactor your application or use database middleware, something like MySQL proxy. But once you decide to use the sharding solution, it’s one-way path, and you will never get rid of the sharding keys and have to say goodbye to complex queries&amp;hellip; So how to scale your relational database is a pain point for the entire industry.Back to the topTiDB Project - Goal And there comes TiDB, when we were designing TiDB, we want to achieve the following goals: - Make sharding and data movement transparent to users so that the developers can be freed to focus on application development. - 100% OLTP and 80% OLAP support. TiDB aims to be a Hybrid database that supports both OLTP and OLAP workload. This is feasible because TiDB supports transactions and has our own full-featured distributed SQL engine (including the parser , optimizer and query executor). We build it from the ground up. - Twenty-four/Seven availability, even in case of datacenter outages. Thanks to the Raft consensus algorithm, TiDB can ensure the data strong consistency and availability all the time. - TiDB has to be compatible with the MySQL protocol, by implementing the MySQL syntax and protocol. In this way, our users can easily migrate their existing application to TiDB with nearly zero costs. And also use their familiar MySQL tools to manage the database. - Open source, of course. You can find all our projects on GitHub.Back to the topArchitecture Let&amp;rsquo;s see the TiDB architecture. In this diagram, there are three components: - The SQL layer, which is TiDB servers. TiDB servers are stateless. They don&amp;rsquo;t store data, just do the computing. - The distributed storage layer, which is TiKV. TiKV is a distributed key-value database, acting as the underlying storage layer of TiDB and it’s the place where data is actually stored. This layer uses Raft consensus algorithm to replicate data and guarantee data safety. - And Placement Driver, aka PD. The brain of the entire cluster and provides a God&amp;rsquo;s view.These three components communicate with each other through gRPC.Back to the topThe core components of TiDB The Storage stack One thing I need to point out here is that TiDB doesn&amp;rsquo;t depend on any distributed file system.TiKV is the underlying storage layer where data is actually stored. More specifically, data is stored in RocksDB locally which is the bottom layer of the TiKV architecture as you can see from this diagram. On top of RocksDB, we build a Raft layer. So what is Raft? Raft is a consensus algorithm that equals to Paxos in fault-tolerance and performance. It has several key features such as leader election, auto failover, and membership changes. And Raft ensures that data is safely replicated with strong consistency.We have exposed the Raw Key-Value API at this layer, if you want a scalable, high-performance, highly available Key-Value database, and don’t care about a cross-row ACID transaction; you can use raw KV API for higher performance.The middle layer is MVCC, Multi-version concurrency control. The top two layers are transaction and gRPC API. The API here is the transactional KV API.TiKV is written in Rust and the reason is that the storage layer is performance critical and stability is the first-class citizen of course. We only got c/c++ in the past, and now we have Rust is great for infrastructure system software like database, operation system… No extra cost for GC and runtime, high performance, and another great thing is that Rust does a lot of innovation works in preventing memory leaks and data race, that matters a lot for us.Back to the topTiKV is a key-value storage engine. Keys and values are both byte arrays. Logically, we could regard TiKV as a giant sorted key-value map. In order to achieve data distribution, we split the key space into multiple ranges. Each range has a metadata which contains start_key and end_key.Back to the topNow we know that the actual data is stored in RocksDB. And we also know that the key space is split into ranges. Region is a set of continuous key-value pairs in byte-order. Let’s take a look at the diagram here: The data is split into a set of continuous key-value pairs which we name them from a to z. Region 1 stores “a” to “e”, Region 2 “f” to “j”, Region 3 “k” to “o”, etc. notice that region is a logical concept, all the regions in a physical node share the same RocksDB instance.In each RocksDB instance, there are several regions and each region is replicated to other instances by Raft. The replicas of the same Region, Region 4 for example, make a Raft group.The metadata of the Raft groups is stored in PD. And of course, PD is a cluster, replicating the metadata by Raft, too. I will introduce it later.Back to the topDynamic Multi-Raft In TiKV, we adopt a multi-raft model. What’s multi-raft? It’s a way to split and merge regions dynamically, and of course, safely. We name this approach “safe split/merge”.For example, Region 1 from “a” to “e” is safely split into Region 1.1 “a” to “c” and Region 1.2 “d” to “e”, we need to guarantee no data is lost during the split.This explains how one Region is split, but how about its replicas on other nodes? Let’s go to next few slides.Back to the topSafe Split This is the initial state for Region 1. You can see there is a Raft group with three TiKV nodes.Region 1 on TiKV1 is the Leader and the other two replicas are the followers. However, there comes a situation that there are too much data in Region 1 and it needs to be split.It’s easy if there is only one Region 1. But in this case, we have three replicas. How can all the replicas be split safely? The answer is also Raft. Let’s see how it works.Back to the topThe split is initiated by the Leader, which means Region 1 is split to Region 1.1 and Region 1.2 firstly in the Leader as you can see from the diagram.Back to the topWhen the split-log is written to WAL in the Leader, the split log is replicated by Raft and sent to the followers. Then, the followers apply the split log, just like any other normal Raft log.Back to the topAnd finally, once the …"},
		{"url": "https://pingcap.com/weekly/2017-10-09-tidb-weekly/",
		"title": "Weekly update (September 25 ~ October 08, 2017)", 
		"content": " Weekly update in TiDB Last two weeks, we landed 62 PRs in the TiDB repositories.Added  Support the SyncLog Key-Value request option. Support the NotFillCache Key-Value request option. Support the combination SQL modes.  Removed  Close the aggregation pushdown by default and remove the CBO switch. Remove some useless code. Remove the usage of TypeClass completely.  Fixed  Change the like function to be case sensitive. Prepare to enforce errcheck, step 1. Make the prepare statement retry when the schema is out-dated. Revoke etcd on ctx.Done to prevent the situation that no owner entry left. Pre-calculate the lower and upper scalar. Add length limitation for index comment. Make insert with calculated value behave the same as MySQL. Drop invalid cached region.  Improved  Turn on the analyze statement pushdown switch. Upgrade pd-client to fix a bug. Rewrite the unit test for Minus and Plus functions. The analyze statement uses NewSelectResult. Remove the usage of evalExprToXXX functions to improve performance. Change structured slow log to friendly JSON output. Refactor parser step 2: to be compatible with MySQL syntax. Refactor the SelectDAG function parameter. Speed up the add index operation. Optimize the SortExec executor. Split separated region for newly created table. Change the schema validator data structure to queue. Change the way of transferring the handle. Support more signatures pushdown.  Weekly update in TiSpark Added  Add the Configuration Framework  Fixed  Fix invalid leader store ID cases Remove unused parameter for TiSpark context during the startup  Weekly update in TiKV Last two weeks, We landed 39 PRs in the TiKV repositories.Added  Use expression in DAG. Namespace isolation: http interface and classifier. Report read statistics to PD. Pushdown sampling: implement histogram. Support ppc64le. Pushdown the analyze request. Support the must_sync flag for raft ready. Introduce Disconnected and LowSpace states for store. Add an API to compact a range for the specified column family.  Fixed  Stop the thread pool on shutdown. Fix block send problem in grpc. Fix potential unreachable drop. Fix tablecodec.  Improved  Select response of DAG don’t need row meta. Add drop message metrics. Debug: implement size. Debug: implement region information. Add slow log for applying. Update gRPC to 1.6.1. Check whether peers need to split after initialization. Refactor Key-Value for PD. Config: change all the names to kebab-case](https://github.com/pingcap/pd/pull/772). Flush the leader message as soon as possible. Return error instead of panic if an expression is illegal. Update etcd to v3.2.6 for PD. Synchronize data before applying snapshots. Update the client URLs continuously. Adjust the region heartbeat metrics.  New contributors  Priya Seth  "},
		{"url": "https://pingcap.com/blog/2017-09-26-whyrust/",
		"title": "Why did we choose Rust over Golang or C/C&#43;&#43; to develop TiKV?", 
		"content": " What is Rust Rust is a systems programming language sponsored by Mozilla Research. It moves fast and steady with a 6-week release cycle ever since its 1.0 version in May 2015.See the following list for some of the features that most attract us: The design principles of Rust resemble with C++ in Abstraction without overhead and RAII (Resource acquisition is initialization). The minimum runtime and efficient C bindings empower Rust to be as efficient as C and C++, thus making it very suitable for the systems programming field where high performance matters the most. The powerful type system and unique life cycle management facilitate the memory management during the compiling, which ensures the memory and thread safety and makes the program run very fast after the compiling. Rust provides pattern matching and type inference like a functional programming language and makes the code simple and elegant. The macros and traits allow Rust to be highly abstract and save quite a few boilerplates during the engineering especially when it comes to the libraries.  The Rust Ecosystem Because of the excellent package management tool, Cargo, Rust has many types of libraries, such as Hyper for HTTP, Tokio and mio for asynchronous I/O, basically all the libraries that are required to construct a backend application.Generally speaking, Rust is mainly used to develop server-side applications with high performance at this stage. In addition, its innovation in the type system and syntax gives it a unique edge in developing Domain-Specific Libraries (DSL).The Rust adoption As a new programming language, Rust is unique. To name just a few projects that are using Rust, The backend distributed storage system of Dropbox Servo, the new kernel of Firefox Redox, the new operating system TiKV, the storage layer of TiDB, a distributed database developed by PingCAP.  As one of the listed Friends of Rust, TiKV has been one of the top projects in Rust according to the Github trending.TiKV is a distributed key-value database. It is the core component of the TiDB project and is the open source implementation of Google Spanner. We chose Rust to build such a large distributed storage project from scratch. In this blog, I will uncover the rationale.In the past long period of time, C or C++ has dominated the development of infrastructure software such as databases. Java or Golang has problems such as GC jitter especially in case of high read/write pressure. On the one hand, Goroutine, the light-weight thread and the fascinating feature of Golang, has significantly reduced the complexity of developing concurrent applications at the cost of the extra overhead in context switching in the Goroutine runtime. For an infrastructure software like a database, the importance of performance goes without saying. On the other hand, the system needs to remain its &amp;ldquo;Certainty&amp;rdquo; which makes it convenient for performance tuning. But introducing GC and another runtime contributes to the opposite. So for quite a long time, C/C++ seems to be the only choice.TiKV originates from the end of 2015. Our team was struggling among different language choices such as Pure Go, Go + Cgo, C++11, or Rust. Pure Go: Our core team has rich experience in Go. The SQL layer of TiDB is developed in Go and we have benefited quite a lot from the high efficiency brought by Go. However, when it comes to the development of the storage layer, Pure Go is the first option to rule out for one simple reason: we have decided to use RocksDB as the bottom layer which is written in C++. The existing LSM-Tree implementations (like goleveldb) in Go were hardly as mature as RocksDB. Cgo: If we had to use Go, we had to use Cgo to bridge but Cgo had its own problems. At the end of 2015, the performance might be greatly impacted if calling Cgo in Go code rather than calling Cgo in the same thread with Goroutine. Besides, databases require frequent calls to the underneath storage libraries, aka RocksDB. It was highly inefficient if the extra overhead was needed every time the RocksDB functions were called. Of course, some workarounds could be introduced to enlarge the throughput of calling Cgo, such as packaging the calls within a certain period to be a Cgo batch call that will increase the latency of a single request and erase the Cgo overhead. But, the implementation might be very complex while the GC problem was not entirely solved. At the storage layer, we want to use the memory as efficiently as possible. Hacky workarounds such as extensive use of syscall.Mmap or object reuse might damage the readability of the code. C++11: There ought to be absolutely no issue with C++11. RocksDB is developed using C++11. But given the team background and what we want to do, we didn’t choose C++11. The reasons are as follows: The core team members are experienced C++ developer with rich experience in large C++ projects. But the seemingly inevitable problems in large projects like Dangling pointers, memory leak, or data race make them shudder at the thought. Of course, the probability of these problems could be lowered if well guided, or having a stringent code review and coding rules in place. But if a problem occurred, it might be costly and burdened to debug. Not to mention that we have no controls if the third-party libraries could not meet our coding rules. There are too many and too different programming paradigms in C++ as well as too many tricks. It demands extra costs to unify the coding style especially when there are more and more new members who might not be familiar with C++. After years of using languages with GC, it is very hard to go back time for manually managing the memory. The lack of package management and CI tools. It appears not to be trivial, but the automated tools are very important for a large project because it is directly related to the development efficiency and the speed of iterating. What’s more, the C++ libraries are far from enough and some of them need to be created by ourselves.  Rust: The 1.0 version of Rust is released in May 2015 with some charming features: Memory safety High performance which is empowered by LLVM. The runtime is practically no different from C++. It also has affinities to the C/C++ packages. Cargo, the powerful package management tools Modern syntax Almost consistent troubleshooting and performance tuning experience. We can directly reuse some of the tools like perf which we are already very familiar with. FFI (Foreign Function Interface), call directly into the C APIs in RocksDB free of losses.  The first and foremost reason is memory safety. As mentioned earlier, the issues in the memory management and data race might seem to be easy for C++ veterans. But I believe the utmost solution, which is what Rust is doing, is to put constraints in the compiler and solve it from the very beginning. For large projects, never ever bet the quality solely on human beings. To err is human. Though Rust is hard to begin with, I think it’s totally worth the while. Besides, Rust is a very modern programming language with its extraordinary type system, pattern modeling, powerful macros, traits, etc. Once you are familiar with it, it can greatly improve the efficiency which might be the same as if we chose C++ counting the time to debug. According to our experience, it takes about 1 month for a software engineer to code in Rust from zero experience. The efficiency is almost the same between an experienced Rust engineer and a Golang engineer.  To sum up, Rust, as an emerging programming language, seems to be new to most of the developers in China, but it has become the most promising challenger to C/C++. Rust was also crowned the &amp;ldquo;most loved&amp;rdquo; technology in StackOverflow&amp;rsquo;s 2016 developer survey. So from a long term, Rust will shine in scenarios where memory safety and performance matter the most."},
		{"url": "https://pingcap.com/weekly/2017-09-25-tidb-weekly/",
		"title": "Weekly update (September 18 ~ September 24, 2017)", 
		"content": " Weekly update in TiDB 2017-09-25Last week, we landed 63 PRs in the TiDB repositories.Added  Use new expression framework by default. Support the DOT explain format. Support the syntax for EXPLAIN FORMAT = stringlit Support the TIME/TIMESTAMP literal  Removed  Remove expression/typeinfer.go entirely. Abandon the selection controller.  Fixed  Roll back the ID allocator when a transaction fails to commit. Fix the returned column length of all the SHOW statements. Fix the Navicat for MySQL compatibility issue of the SHOW CREATE TABLE statement. Fix a bug in GC worker. Fix a bug of merge JOIN/stream aggregation and ORDER BY. Support the aggregation function that contains an aggregation function. Abort an unsafe transaction subject to GC command Fix a bug in parseDatetime.  Improved  Refactor the aggregation to reduce memory usage. Use continuous-value assumption to estimate cost. Speed up the add index operation. Don&amp;rsquo;t return the killed session when SHOW PROCESSLIST. Refine the return type, charset and collation for the get variable expression. First step to make parser to be totally compatible with MySQL. Add real tables for global/session status in the performance schema. Implement the ANALYZE columns pushing down. Rewrite the builtin function MOD Use the Goroutine pool to avoid runtime.morestack. Remove the usage of &amp;ldquo;TypeClass&amp;rdquo; in:  builtin_arithmetic.go builtin_string.go expression.go builtin_cast.go builtin_math.go builtin_op.go   Weekly update in TiSpark Fixed  Fix the Region client error in handling for store no match and leader switch Fix an issue in PD Client leader switch.  Weekly update in TiKV Last week, We landed 30 PRs in the TiKV repositories.Added  Add benchmarks for JSON binary codec. Implement fmsketch for coprocessor. Add the SplitRegion API. Synchronize the Raft log when necessary. Add the debug framework. Add the namespace checker for PD. Support setting the store state for PD. Support initial schedulers’ name from the config file. Add the table namespace classifier for PD. Support scheduling based on regions’ read flow for PD. Support getting region’s namespace in PD.  Fixed  Fix setting a nonempty store to the tombstone state.  Improved  Replace opp_neg with overflowing_neg. Enhance synchronizing the WAL log. Add metrics for transactions. Use 96MB as the default size for regions. Use counters instead of histogram when statistics scan details. Fix resource downloading and extracting when bench JSON. Add more statistics for RocksDB. Update the futures. Refactor the cluster information for PD.  New contributors (Thanks!)  David Ding Hu Ming OuYang Jin Liang SHANG  "},
		{"url": "https://pingcap.com/blog-cn/talk-about-opensource/",
		"title": "谈谈开源(一)", 
		"content": "  源码面前，了无秘密 &amp;mdash;- 侯捷 前言 很多人的『开源』是一个比较时髦且有情怀的词汇，不少公司也把开源当做 KPI 或者是技术宣传的手段。但是在我们看来，大多数人开源做的并不好，大多数开源项目也没有被很好的维护。比如前一段时间微博上流传关于 Tengine 的讨论，一个优秀的开源项目不止是公布源代码就 OK 了，还需要后续大量的精力去维护，包括制定 RoadMap、开发新功能、和社区交流、推动项目在社区中的使用、对使用者提供一定程度的支持，等等。目前我们在国内没看到什么特别好的文章讲如何运营一个开源项目，或者是如何做一个顶级的开源项目。TiDB 这个项目从创建到现在已经有两年多，从开发之初我们就坚定地走开源路线，陆续开源了 TiDB、TiKV、PD 这三个核心组件，获得了广泛的关注，项目在 GitHub 的 Trending 上面也多次登上首页。在这两年中，我们在这方面积累了一些经验和教训，这里和大家交流一下我们做开源过程中的一些感受，以及参与开源项目（至少是指 TiDB 相关项目）的正确姿势。什么是开源  Open-source software (OSS) is computer software with its source code made available with a license in which the copyright holder provides the rights to study, change, and distribute the software to anyone and for any purpose.&amp;mdash;- From Wikipedia 本文讨论的开源是指开源软件，简而言之，开源就是拥有源代码版权的人，允许其他人在一定许可证所述范围内，访问源代码，并用于一些自己的目的。 最基本的要求就是其他人可以访问源代码，另外获取代码后能做什么，就需要一个专门的许可证来规范（可以是自己写的，也可以用一个别人写好的）。里面一般会规定诸如对修改代码、新增代码、后续工作是否需要开源以及专利相关的事项。 OK，我们写一个 main.py 里面有一行 print &amp;quot;Hello World!&amp;quot;，再和某个许可证文件一起扔到 GitHub 上，我们就有一个满足最低要求的开源项目了。为什么要开源 很多人觉得代码是一个软件公司最宝贵的资产，把这些最宝贵的资产让别人免费获取，对你们有什么好处？如果对手拿走了你们的代码，另起炉灶和你们竞争怎么办？或者是用户直接获取源代码，用于自己的环境中，那你们如何收钱呢？ 对一个技术型公司来说，最宝贵的资产其实是人，对一个开源项目来说，最核心的资产是一个活跃的开源社区以及他人对这个项目的认可。 我们从这两方面来看一下开源在这两方面的影响。 Branding 很明显，开源是一种非常好的 PR、Branding 的手段，大多数大公司做开源也是这个目的，可以以一种成本几乎为零的方式宣传企业名，树立技术型企业形象。一个知名且良好的企业形象，对于各个方面都很有好处。比如国外有一个知名的技术媒体叫 HackNews，我司的产品曾经多次登上其首页，获得了大量的关注。其实那几次都不是我们自己发的帖子，而是其他人关注到我们的产品，自行做的传播。 人才获取 人才招聘最大的难处就是如何鉴别这个人的能力，他是否能干活、是否是靠刷题通过了面试。如何能和这个人工作一段时间，看到他是如何完成日常工作，那么对于这个人的能力了解会更进一步。为了实现这个目的，传统的手段是 Some How 找到和这个人共事过的人，听取他的意见。这样做首先要看运气，有的时候要转几层关系才能找到这样的人，并且不一定得到的是正确、真实的答案。 但是如果这个人已经给你的项目贡献了一些代码，并且代码质量比较高、贡献过程中和你的沟通很顺畅，那么一方面说明这个人软硬实例都不错，另一方面说明这个人对你做的事情很有兴趣。TiDB 有大量的正式、实习员工都是从 Contributor 中转化来的，以至于我们担心别把所有的人都招进来，社区没了 :) 。 社区贡献 可以这么说，如果没有开源社区，整个互联网都不会是现在这样。想象一下如果没有 Linux、MySQL、GCC、Hadoop、Lucence 这些东西，那么整个互联网的基础技术栈将不复存在（当然，肯定会出现另外一套东西，但是可能不会像开源的这套这么完善）。无数的开源社区贡献者贡献自己的力量，共同维持这样一个互助互利的社区，支撑社会技术进步。 我们也从开源社区中获得了很多支持，包括大家报的问题、提的建议以及来自全球一百四十多名贡献者提交的代码。随着项目的发展，我相信社区贡献代码的比例会持续提升。 提升项目质量 当一个项目以开源方式运营时，代码质量是项目的脸面，大家无论是在提交代码的时候，还是在 Comment 别人的 PR 的时候，都会非常谨慎，因为你的一举一动全世界都能看到，毕竟谁也不想人前露怯是吧。 对基础软件的意义 对于一个数据库这样的基础软件，最重要的就是正确性、稳定性和性能。前两点尤其重要，要保证这两点，一方面需要在开发和测试过程中尽可能提高质量，另一方面广泛的使用也非常重要。只有当你的产品有足够多的人试用，甚至用于生产环境，才可能有足够多的问题反馈以及产品建议。开发人员能做的测试毕竟是有限的，很多场景、环境或者是业务负载是我们想象不到的。来自实际用户的问题反馈有助于我们提升产品质量，来自用户的建议有利于我们提升产品易用性。只有长期在生产环境中运行过的基础软件的，才算是合格的基础软件的。  所以我们认为开源是基础软件的大趋势，无论是 Hadoop、MySQL、Spark 这样的知名产品，或者是 Linux 基金会、Apache 基金会、CNCF 基金会这样的巨头，都证明了这个观点。国内目前大公司比较热门的开源项目，也都集中在基础软件领域，比如百度的 Brpc、Palo、Tera，以及腾讯的 PaxosStore。PingCAP 开源了哪些项目 这里简单讲一下我们开源的几个 Repo 都是做什么： TiDB：数据库的 SQL 层 TiKV：数据库的分布式存储引擎 PD：集群的管理节点 Docs：项目的英文文档 Docs-cn：项目的中文文档  大家可以在 GitHub 上浏览我们的代码，看到我们完整的开发过程。开源模式下的开发流程 PingCAP 攻城狮小申典型的一天：8:00 起床，先登录 Slack 看一下昨晚定时跑的测试任务是否结果正常，然后关注一下 Slack 上各种 Channel 以及微信群、邮箱是否有什么重要的消息9:00 洗漱完+吃完早饭，逗一会可爱的女儿（也可能是被女儿逗），然后去上班9:30 到达公司，开始干活。* 打开电脑看看 GitHub 上面有什么新的 Issue * 看看自己的 PR 有没有被别人 Comment，如果有 Comment 的话，尽快解决；如果还没人看的话，at 一下相关的同学，求 Review * 看看有没有别人的 PR 需要自己 Review，特别是 at 自己的那些 PR * 带上耳机开始写点代码 * Slack 有人 at 我，赶紧回复一下 * Slack 上我关注的 Channel 中有人在讨论问题，我很感兴趣，加入进去讨论一会 * 同事要做一个新的 Feature，写了设计文档，我点进去看了一遍提了几个 Comment 12:00 肚子可耻的饿了，呼朋唤友去吃饭，路上顺便讨论讨论技术以及八卦13:00 吃饭归来，看看邮件、Slack、微信留言，处理一下紧急的事情13:30 小睡一会14:00 小睡结束，接一杯咖啡，开始下午的工作，键盘敲起来。。。。。15:30 参与同事的设计评审会议，通过视频会议系统和远程的同事一起讨论设计方案，拍板后开干16:30 休息一下，然后继续敲代码、Review PR18:00 大部分同事已经去吃饭了，我准备开车回家吃饭去20:30 吃完饭，收拾完，没什么事情，打开电脑看一会邮件、Issue、PR22:30 休息一会，准备洗澡睡觉如何做一个开源项目 首先你需要根据自己的诉求、商业模式等选择一个开源协议，常见的有 GPL 、BSD、Apache 和 Mit ，这些开源协议的区别在阮一峰老师的这篇博客中解释的很清楚了，推荐大家阅读。协议选定之后，再选择一个代码托管平台，目前的标准选择是 GitHub，注册一个 GitHub 账号，申请一个 Orgnization 之后，就可以开始用了，如果不需要私有 Repo 的话，那么不需要交任何费用。开始代码开发，提交第一次 Commit，完成 Readme 的撰写（一个好的 Readme 真的很重要）。后续的开发都需要通过 Pull Request 进行，最好不要直接 Push Master。一个严肃的项目需要把 Master 加入 Protected Branch，禁止直接 Push。为了保证后续的代码提交都是 Work 的，最好在 GitHub 中集成至少一个 CI 服务，常用的有 TravisCI、CircleCI （最近一段时间 CircelCI 似乎总是出问题）。然后在 PR 的设置页面上要求 PR 通过了 CI 才能合并。如果有人试用项目时发现一些问题，会通过 Issue 反馈，所以需要关注 Issue ，尽快给予回复。另外将 Issue 通过 Label 分门别类是一个好的实践，便于大家快速搜索、分类 Issue。比如我们会将一部分简单些的 Issue 标记为 Help Wanted，如果有新加入社区的同学想要开始贡献代码，那么这些 Issue 就是不错的起点。当参与的人越来越多，那么会有一部分人开始贡献代码，Maintainer 需要 Review 其他人的 PR，保证能项目自身的代码质量要求、编码风格一致。最后一点，一个好的项目需要配备完善的文档，帮助大家使用项目。包括架构、简要介绍、详细介绍、FAQ、使用范例、接口文档、安装部署以及最佳实践等等。这点也是大多数项目所忽略的。如何参与开源项目 试用 最简单的参与方式是试用开源项目，这也是开源最大的一个好处，所有人都可以随时试用，相当于有很多人帮助项目作者做测试。毕竟如果只有作者自己做测试，遇到的环境、场景、应用方式会比较单一，总有一些你想像不到的地方会出问题。所以每一个测试出来的问题都很宝贵，我们都会尽可能快的评估和回复。报 Issue 试用过程中大家可能会遇到各种问题，特别是文档中没有提及的问题，反馈问题的最佳方式是在 GitHub 上新建 Issue，这样所有的人都可以看到，而且通过 Issue 来反馈我们也会更重视一些，有人会定期扫一遍未处理的 Issue。当然，建立 Issue 之前先搜索是否和已有的 Issue 重复是个好习惯。在 Issue 中尽可能详细的描述清楚遇到的问题，以及一个可操作的复现步骤，包括所用 Binary 的版本、部署方式、客户端以及服务的日志、操作系统的日志（如 dmesg 的输出）。如果不能复现，也尽可能详细地提供 Log。这些对开发人员追踪 Bug 会非常有用。提出建议 如果对项目有什么建议，也可以通过新建 Issue 来反馈， 我们一般会给出是否会支持，如果要支持的话，大概会在什么时候支持。提 PR 当你使用 TiDB 遇到问题或者需要新的 Feature，而觉得自己有能力 Fix 或者是当前官方还没有精力 Fix 时，可以尝试自己修改代码，解决问题。目前 TiDB 项目的 Contributor 有 140 多个，分散在全球十几个国家。其中不乏深度参与的用户。如果是小的功能或者是简单的 Bug Fix，可以在相关的 Issue 下面吼一声，让大家知道你在做这个事情即可，这样不会有人做重复的工作。如果做的过程中遇到了什么问题，也可以在相关的 Issue 中和 Maintainer 讨论。如果要做的是比较大的功能，那么最好先和官方做一轮讨论，然后写一个尽可能详细的 Design，讨论 OK 后，开始开发。讲一点好玩的事情 在开源项目中总能或多或少的发现奇葩的 Issue，比如这个看到这个 Issue 真的是震惊了。"},
		{"url": "https://pingcap.com/meetup/meetup-2017-09-20/",
		"title": "【Infra Meetup No.56】MonetDB/X100 Paper 解读（内附视频）", 
		"content": "  上周六，PingCAP Infra Meetup 第 56 期特设论文专场，我司核心工程师张建与大家一起分享并解读了“MonetDB/X100: Hyper-Pipelining Query Execution” 论文。此篇论文作为分析型数据库领域内引用次数最多的论文之一，它为何如此火爆？在今天的文章里你应该可以找到答案。 精彩视频 精彩现场 在 PingCAP Infra Meetup 第 56 期论文专场，来了很多对 MonetDB/X100 论文感兴趣的小伙伴们。分享一开始，我司联合创始人兼 CEO 刘奇就为何选择 MonetDB/X100 这篇论文分享了自己看法。刘奇提到:&amp;ldquo;如果大家有阅读近两年新出的一些 Paper，会发现里面引用率最高的一篇文章就是 MonetDB/X100。MonetDB/X100 发表于 2005 年，其实不算新。但读过该论文的人会发现目前主流的 OLAP 系统相关的技术，基本上都能在这篇论文中找到影子，如文中提到了列存、Pipeline，甚至是 JIT。他做 JIT 的思路不一样，都是比较早就有的，所以这是一篇很不错的论文。现在也可以看到很多性能比较的时候，大家新做了一个系统，说我的性能非常好，会拿出来 benchmark 说你看我打败了 MonetDB。另外还有一些比较创新的项目，多是基于 MonetDB 改造的。一个就是英特尔最近出的一篇论文，他把 MonetDB 改造一下，把正则表达式的搜索，放到 FPGA 里面去。英特尔最近出了一款服务器，这个服务器的 CPU 和 FPGA 是放在一起的，他们得到 Performance 最小提倡是 2.3 倍以上，大概意思上就是说，MonetDB 在这上面做一个简单的改造，就可以适应到更新的硬件。在 2012年的时候，第一个提供论文、代码的基于 MonetDB 的 GPU 的 Database 也出来了。当时是在 TPCH 的 query 里面，有一些复杂的 query，提升是非常的明显。所以大家可以看到，基于 MonetDB 改造的，在 FPGA 或者 GPU上运行的系统都有，实际上这是一个非常优秀的学术的原形，今年得了十年最佳论文奖。&amp;rdquo;接下来，我司核心工程师张建就这篇最佳论文做了拆分讲解，为了让大家更好的理解今天分享的主题，张建首先简单介绍了一下 SQL 的执行流程、Volcano 执行框架以及CPU 的硬件特性，进一步展示了当时一些数据库的 performance 情况，并分析了为什么其执行效率低。就大家关心的 MonetDB/X100 的设计思路以及总体架构，分析了一些 MonetDB/X100 的执行算子，通过具体的列举简单介绍了 MonetDB/X100 的执行过程以及动态生成的 Vectorized Primitive 的特点。技术干货节选 以上就是张建带给大家的部分论文精彩内容，小伙伴们可以观看现场演讲视频，慢慢 Enjoy~ 也敬请期待我们下一期的内容 :)点击下载 PPT"},
		{"url": "https://pingcap.com/weekly/2017-09-18-tidb-weekly/",
		"title": "Weekly update (September 11 ~ September 17, 2017)", 
		"content": " Weekly update in TiDB Last week, we landed 46 PRs in the TiDB repositories.Added  Add required tables of MySQLX . Add TOML configuration file support.  Removed  Remove the performance schema instrumentation.  Fixed  Fix show create table with foreign key. Cast values only for modified columns in the update statement to avoid unnecessary check. Fix an OOM issue when analyzing table. Fix a cast (date as datetime) error. Fix a bug about the DATE literal.  Improved  Change the FMSketch hash function from &amp;ldquo;fnv&amp;rdquo; to &amp;ldquo;murmur&amp;rdquo;. Add the PARTITION BY KEY grammar support. Refine setting/getting the SQL_MODE variable. Support the NVARCHAR syntax. Refine the parser of localtime and localtimestamp. Push down the ANALYZE INDEX statement. Support stream aggregation in the new optimization plan. Hide the security information in SHOW PROCESSLIST. Add a goroutine pool package utility. Refactor the following expression/builtin-function evaluation framework:  ADDDATE, SUBDATE TIMEDIFF VALUES ROW SETVAR, GETVAR GREATEST, LEAST CONVERT_TZ MAKETIME ADDTIME, SUBTIME   Weekly update in TiSpark Added  Add the JDBC Thrift Server / spark-sql support. Add the back-off retry policy. Add the Meta table prefix support.  Fixed  Fix the Null decoding error. Fixed some Styling issues.  Improved  Added the Integration test framework.  New contributors (Thanks!)  Cheng Lian  Weekly update in TiKV Last week, We landed 34 PRs in the TiKV repositories.Added  Balance regions based on the read flow. Make the Raft tick configurable for PD. Add a classifier interface for namespace. Balance by the namespace.  Fixed  Fix a race condition for threadpool. Fix the active region statistics. Fix cleaning up data between regions. Stop the threadpool before quitting the scheduler. Fix the marshal operator. Fix the operator timeout issue. Fix a bug in cast_str_as_int. Wrap InitLogger in sync.Once for thread safety. Fix a compiling error.  Improved  Add tests for the gRPC services. Add two queues cache for PD. Cache elf sections for backtrace. Optimize JSON. Skip checking region 0. Adjust metrics for the coprocessor. Remove the stale peers as soon as possible. Enable pin_l0_filter_and_index by default. Enhance metrics for the scheduler. Optimize the LIKE expression. Check the leader request when becoming a follower after receiving message with higher term. Use util::time::instant instead of std::time::instant. Add metrics for compression ratio at different levels. Remove the statistics that may overflow.  New contributors (Thanks!)  Rain Li OuYang Jin  "},
		{"url": "https://pingcap.com/blog/2017-09-15-rocksdbintikv/",
		"title": "RocksDB in TiKV", 
		"content": " This is the speech Siddon Tang gave at the RocksDB meetup on August 28, 2017. Speaker Introduction Agenda Why did we choose RocksDB? How are we using RocksDB?  TiKV Architecture Region Raft InsertWithHint Prefix Iterator Table Property for Region Split Check Table Property for GC Check Ingest the SST File Others  How are we contributing? Future Plans  Speaker Introduction Hi every one, thanks for having me here, the RocksDB team.Today, I will talk about how we use RocksDB in TiKV. Before we start, I will introduce myself briefly. My name is Siddon Tang, chief engineer of PingCAP. Now I am working on TiDB, the next generation SQL database; and TiKV, a distributed transactional key-value store. I am an open source lover and I have developed some open source projects like LedisDB (BTW, the backend engine is also RocksDB), go-mysql, go-mysql-elasticsearch, etc…Agenda What I will talk about today is as follows: Why did we choose RocksDB in TiKV? How are we using RocksDB in TiKV? How are we contributing to RocksDB? Our future plan  Why did we choose RocksDB? OK, let’s begin. Why did we decided to use RocksDB instead of LevelDB, WiredTiger, or any other engines. Why? I have a long list of reasons: First of all, RocksDB is fast. We can keep high write/read speed even there’s a lot of data in a single instance. And of course, RocksDB is stable. I know that RocksDB team does lots of stress tests to guarantee the stability； And it’s easy to be embedded. We can call RocksDB’s C API in Rust directly through FFI, because TiKV is written in Rust. Not to mention that it has many useful features. We can use them directly in production to improve the performance. In addition, RocksDB is still in fast development. Many cool features are added, and the performance is being improved continuously. What’s more, RocksDB has an very active community. If we have questions, we can easily ask for help. Many RocksDB team members and us are even WeChat (a very popular IM tool in China) friends, we can talk to each other directly.  Back to the topHow are we using RocksDB? TiKV Architecture After we decided to use RocksDB, the next question is how to use it in TiKV. Let me start with the TiKV architecture briefly.First of all, all data in a TiKV node shares two RocksDB instances. One is for data, and the other is for Raft log.Region Region is a logical concept: it covers a range of data. Each region has several replicas, residing on multiple machines. All these replicas form a Raft group.Raft TiKV uses the Raft consensus algorithm to replicate data, so for every write request, we will first write the request to the Raft log, after the log is committed, we will apply the Raft log and write the data.The key format for our Raft log saved in RocksDB is: region ID plus log ID, the log ID is monotonically increased.InsertWithHint We will append every new Raft log to the region. For example, we first append log 1 for region 1, then we might append log 2 for the same region later. So we use memtable insert with the hint feature, and this feature improves the insert performance by fifteen percent at least.The version is embedded in the key as a suffix, and used for ACID transaction support. But transaction management is not our topic today, so I just skip it.Back to the topPrefix Iterator As you can see, we save the key with a timestamp suffix, but can only seek the key without the timestamp, so we set a prefix extractor and enable the memtable bloom filter, which helps us improve the read performance by ten percent at least.Table Property for Region Split Check If we insert a lot of data into a region, the data size will soon exceed the threshold which we predifine and need to be split.In our previous implementation, we must first scan the data in the range of the region, then calculate the total size of the data, if the total size is larger than the threshold, we split the region.Scanning a region has a high I/O cost, so now, we use table properties instead. We record the total data size in the SST table property when doing compaction, get all table properties in the range, then add up the total size.Although the final calculated total size is approximate, it is more effective, we can avoid the useless scan to reduce the I/O cost.Table Property for GC Check We use multiple versions for a key, and will remove the old keys periodically. But we don’t know whether we need to do GC in a range or not, in the past, we simply scanned all the data.However, since we only need to do GC before a specified safe point, and most keys have only one version, scanning these keys every time is wasteful.So we create an MVCC property collector to collect the version information, including the maximum and minimum timestamp, the row number and version number. Then every time before scanning a range, we can check these properties to see if we can skip the GC procedure or not.For example, if we find the minimal timestamp in the table property is bigger than the safe point, we can immediately skip scanning the range.Back to the topIngest the SST File And in our previous implementation, if we wanted to do bulk load, we must scan all the key-values in the range and save them into a file. Then in another RocksDB, read all the key-values from this file and inserted them in batches.As you can see, this flow is very slow and can cause high pressure in RocksDB. So now, we use the IngestFile feature instead. At first, we scan the key-values and save them to an SST file, then we ingest the SST file directly.Others More than that, we enable sub compaction, pipelined write, and use direct I/O for compaction and flush. These cool features also help to improve the performance.How are we contributing? We are not only using RocksDB, we also do our best to contribute to the community. We have done many stress tests and have found some serious data corruption bugs. Like these issues. #1339: sync write + WAL may still lose newest data #2722: some deleted keys might appear after compaction #2743: delete range and memtable prefix bloom filter bug  Thank goodness, we haven’t found any of our users meet these problems in production.We also added features and fixed some bugs, like these. Because TiKV can only call the RocksDB C API, we also add many missing C APIs for RocksDB. #2170: support PopSavePoint for WriteBatch #2463: fix coredump when release nullptr #2552: cmake, support more compression type many C APIs  Future Plans In the future, we are planning DeleteRange API, which is a very useful for us. But now we found some bugs 2752 and 2833, we are trying our best to fix them, of course, together with the RocksDB team.And we will try to use BLOB DB when it’s stable. On the other hand, we will also try different memtable types to speed up the insert performance, and use partitioned indexes and filters for SATA disks.Back to the top"},
		{"url": "https://pingcap.com/blog/2017-09-12-futuresandgrpc/",
		"title": "Futures and gRPC in Rust", 
		"content": " This is the speech Tang Liu (tl@pingcap.com) gave at the Bay Area Rust Meetup August 2017. See the video. Speaker Introduction Async Programming  Why not Sync? Why Async?  Callback Hell Coroutine Makes it Easy Future, Another Way   Futures in Rust  Futures Combinator Synchronization Stream Sink Task  gRPC  Why gRPC? HTTP/2 gRPC based on HTTP/2  Combine Futures and gRPC  C gRPC Keywords Pseudo Flow Unary Client Streaming Server Streaming Duplex Streaming  Unary Future Implementation  Client Unary Unary Future Resolve Future  Benchmark Misc  Speaker Introduction Hi everyone! I am very glad to join the meetup here. Thanks, the Rust team.Today I will talk about the Futures and gRPC in Rust. Before we start, let me introduce myself briefly. My name is Siddon Tang, and siddontang on Github, chief engineer at PingCAP. I have been working on the next generation SQL database, TiDB, and a distributed key-value store, TiKV. By the way, TiKV is also written in Rust. I’m also an open source lover, and have some open source projects, such as LedisDB, go-mysql, go-mysql-elasticsearch, rust-prometheus, etc.Today, I will first discuss Async briefly, then I will introduce Futures in Rust. Of course, you guys here are very familiar with them, so I will just go through it quickly. Then I will talk about gRPC, and in the end, I will show you how we use futures to wrap the gRPC in Rust.Async Programming Let’s begin. About Async.Why not Sync? The first thing is why not Sync. As we all know, the Sync programming is easier. If the load of your service is low, using Sync may be better. You just need to start some threads to handle the concurrence.But if we want to support a high performance service, such as a database, Sync is not enough. Sync I/O can block the execution, which reduces the performance. Although we can use threads, but thread is heavy and wastes system resources. What’s more, frequent thread switching is inefficient and causes the performance to reduce seriously.Why Async? So we chose Async.There is no blocking in Async programming, so we don’t have to wait the slow I/O and can do other things. When the I/O is ready, the system can notify us and we can handle it again. This is very efficient and therefore, the performance is high. But as you can see, the Async way is much more complex and it is hard to write the code correctly. The code logic is split into pieces when the I/O is not ready and we have to switch to do other things.Callback Hell auto r = make_shared&amp;lt;int&amp;gt;(); do_async_foo() { do_foo_a(|| { do_finish(|| { *r = 10; }) }) }) Sometimes, we have to use the callback mechanism to handle the I/O or other asynchronous notifications, and may sink into the callback hell, like this. Oh, so many callbacks.Coroutine Makes it Easy Of course, if we have to write code like this, it might drive us crazy. Luckily, we have at least two ways to bypass it. First, it is the coroutine.Coroutine is a lightweight thread which is supported in many languages. Like the boost coroutine library, WebChat libco library in C plus plus, yield and greenlet in Python, and of course, goroutine in Go.Here is a simple example to use goroutine and channel in Go. The two internal cool features allow us to write high performance concurrent code easily. I personally believe that it is the main reason that Go becomes more and more popular nowadays.Future, Another Way let future = do_async( future() ) .then( future_a() ) .then( future_b() ) .then( future_c() ); future.wait(); Some languages don’t provide coroutine, but we can have another workaround, which is future. Future is a kind-of promise. When we begin to resolve a future, the result of the future may not be ready now and cannot be retrieved now, but after the future is performed later, we can get the result again.You can wait the future to be finished, and multiple futures can be combined into a future chain.Futures in Rust So what about futures in Rust?In Rust, future has already been supported by Alex. Thanks, Alex!Based on the Rust trait, the future is zero cost, which means that you don’t need to do extra heap allocation or dynamic dispatch. Future is easy to use, you can combine many futures into a chain, and use the combinator like an Iterator API.The future is demand-driven, not callback, you should poll the future explicitly to check whether the future is ready or not. No callback can also avoid the heap allocation, and we can cancel the future easily too.Futures Future is a trait and the main function is poll.pub trait Future { type Item; type Error; // check the future is resolved, return Ready or NotReady 	fn poll(&amp;amp;mut self) -&amp;gt; Poll&amp;lt;Self::Item, Self::Error&amp;gt;; // wait until the future is resolved 	fn wait() -&amp;gt; result::Result&amp;lt;Self::Item, Self::Error&amp;gt;; } pub type Poll&amp;lt;T, E&amp;gt; = Result&amp;lt;Async&amp;lt;T&amp;gt;, E&amp;gt;; pub enum Async&amp;lt;T&amp;gt; { Ready(T), NotReady, } We must implement the poll for our customized future. The poll can return NotReady, which means the future is not ready and we should poll later. If Ready is returned, the future is finished and we can get the result. We can also wait the future to finish explicitly. If we call wait, the execution will be blocked until the future is performed.Future ExampleHere are some very simple future examples. For the ok future, wait will return Ready, the result is 1; for the empty future, poll will return NotReady.let f = ok::&amp;lt;u32, u32&amp;gt;(1); assert_eq!(f.wait().unwrap(), 1); let mut f = empty::&amp;lt;u32, u32&amp;gt;(); assert_eq!(f.poll(), Ok(Async::NotReady)); Combinator We can use combinator to chain the futures.let f = ok::&amp;lt;u32, u32&amp;gt;(1).map(|x| x + 1); assert_eq!(f.wait().unwrap(), 2); let f1 = ok::&amp;lt;u32, u32&amp;gt;(1); let f2 = ok::&amp;lt;u32, u32&amp;gt;(2); let (_, next) = f1.select(f2).wait().ok().unwrap(); assert_eq!(next.wait().unwrap(), 2); For example, we can use the ok future plus map combinator, and the end result is 2. We can use select to wait for two futures. If either future is ready, the select future is finished and returns the result plus a next future to be resolved later.Synchronization The future library provides two synchronization channels. One-shot is for SPSC and channel is for MPSC. Both can be used for communication cross threads.Stream Stream is like Future, and you can resolve the value one by one until the stream is finished.pub trait Stream { type Item; type Error; // check the future is resolved, return Ready or NotReady 	// Ready(Some) means next value is on the stream 	// Ready(None) means the stream is finished 	fn poll(&amp;amp;mut self) -&amp;gt; Poll&amp;lt;Option&amp;lt;Self::Item&amp;gt;, Self::Error&amp;gt;; } If the poll of the stream returns Ready Some, it means you can still get the next value of the stream. If Ready None is returned, it means the stream is finished and you can never poll the stream again.Sink Sink is a way to send value asynchronously.pub trait Sink { type SinkItem; type SinkError; fn start_send(self, item: Self::SinkItem) -&amp;gt; StartSend&amp;lt;Self::SinkItem, Self::SinkError&amp;gt;; fn poll_complete(&amp;amp;mut self) -&amp;gt; Poll&amp;lt;(), Self::SinkError&amp;gt;; fn close(&amp;amp;mut self) -&amp;gt; Poll&amp;lt;(), Self::SinkError&amp;gt;; } We can use start_send to send one value, and use poll_complete to flush the sink and check whether all values are sent or not, or you can use close to close the sink.Task The task is used to drive the future computation.# If the future is not ready? let handle = task::current(); # If the event of interest occurs? handle.notify(); # What to do after notify? executor.poll(f); If the future is not ready, we can use task current to get a task handle. We can use task notify to wake up the task when it is ready, and the executor should poll the future again.That’s all about the Rust futures for today.gRPC Now let’s talk about gRPC.If you want to develop a service, the first thing you need to decide is how the client communicates with your service. You may implement your own protocol and RPC. Although it is …"},
		{"url": "https://pingcap.com/weekly/2017-09-11-tidb-weekly/",
		"title": "Weekly update (September 04 ~ September 10, 2017)", 
		"content": " Weekly update in TiDB Last week, we landed 49 PRs in the TiDB repositories.Added  Add the column size limit when creating table. Add the syntax for admin show ddl jobs. SSL/TLS support.  Fixed  Fix an ORDER BY bug. Add entry limit for transactions when doing DDL job. Fix a bug during the limit operator pushdown. Check the default value of the column option in the CREATE TABLE statement. Fix the DEFAULT output in SHOW CREATE TABLE. Fix a bug during the TopN operator pushdown. Fix an OOM issue when analyzing tables in some cases.  Improved  Do some prework before pushing down the analyze table statement. Rewrite fieldTp2EvalTp() to use mysql.TypeXXX instead of TypeClass. Add timezone for CastAsTime. Rewrite hashCode() for requireProp. Use temporary session in gc_worker instead of global singleton. Change the log package to logrus. Let allocID() return int to be more efficient. Rewrite hex and bit literals implementation. Support client specified collation. Refactor IndexLookUpExecutor. Update the time Add() implementation. Refactor the following expression/builtin-function evaluation framework:  FROM_UNIXTIME EXTRACT EXPORT_SET GET_FORMAT INTERVAL FIELD CONVERT FORMAT INSERT Builtin abot JSON CURRENT_TIME TIME_TO_SEC, SEC_TO_TIME INTDIV   New Contributor (Thanks!)  Wang Guoliang  Weekly update in TiKV Last week, We landed 32 PRs in the TiKV repositories.Added  Implement div_real and div_decimal for expression. Add JSON functions for DAG. Add the LIKE signature for DAG. Add the short name for raftdb in tikv-ctl. Implement the bit operation for expression. Implement the int_as_true and decimal/real_as_false for expression. Add the IfJson, IfNullJson signature and some flags for DAG.  Fixed  Fixed capacity parsing. Fixed an unwrapping panic in scheduler. Drop the delete-range feature temporarily.  Improved  Add more tests for config structs. Refactor 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13 on PD. Cleanup the threadpool. Add the panic detail information. Optimize merging array elements. Use time.Since instead of time.Now().Sub. Check if the log is initialized when starting up.  New Contributor (Thanks!)  Wang Guoliang  "},
		{"url": "https://pingcap.com/blog/2017-09-08-rocksdbbug/",
		"title": "How we Hunted a Data Corruption bug in RocksDB", 
		"content": " Data was corrupted. A cluster panicked. The crime scene was compromised. What happened? Detective Huang (huachao@pingcap.com) went all lengths to locate the criminal and solved it once and for all.Background As a distributed open source HTAP database, TiDB uses TiKV as its storage engine. Inside TiKV, we use RocksDB as the local storage. RocksDB is a great project. It&amp;rsquo;s mature, fast, tunable, and widely used in very large scale production environments. We have been working very closely with the RocksDB team. Recently, we found a bug in the DeleteRange feature in RocksDB.Before we begin, let&amp;rsquo;s introduce some basic knowledge about RocksDB first.RocksDB is a log structured storage engine. All writes are append-only, and every write is assigned a globally increasing sequence number to distinguish it.Let&amp;rsquo;s use (key,sequence,entry_type) to denote an entry in RocksDB. If we put &amp;ldquo;a&amp;rdquo;, &amp;ldquo;b&amp;rdquo;, and &amp;ldquo;c&amp;rdquo;, RocksDB will append three PUT entries to the disk: (a,1,PUT),(b,2,PUT),(c,3,PUT). If we put it in a table, it goes like this:  key sequence entry_type   c 3 PUT   b 2 PUT   a 1 PUT   If we delete &amp;ldquo;c&amp;rdquo;, RocksDB will not erase the (c,3,PUT) entry in place, but append a DELETE entry: (c,4,DELETE) instead, so the table becomes:  key sequence entry_type   c 4 DELETE   c 3 PUT   b 2 PUT   a 1 PUT   Then if we try to get &amp;ldquo;c&amp;rdquo;, RocksDB will encounter entry (c,4,DELETE) first and return that &amp;ldquo;c&amp;rdquo; is not found.Of course, RocksDB cannot keep all the entries forever. It needs to compact and drop old entries when necessary. RocksDB organizes entries into SST (Sorted String Table) files, we can think of an SST as a big sorted array of key-value pairs in the disk, and SST files will be compacted into different levels later.All the entries are appended to a WAL (Write Ahead Log) and a Memtable first. When the Memtable is large enough, it will be flushed to a new SST in Level-0. SST files in Level-0 can be overlapped with each other because they haven&amp;rsquo;t been compacted yet. When the files in Level-0 are large enough, RocksDB will compact the SST files in Level-0 with overlapped SST files in Level-1, and then output new SST files to Level-1 without overlap. Then files in Level-1 will be compacted to the next level and so on. In short, only SST files in Level-0 can be overlapped, SST files under Level-0 must not overlap with each other in the same level.Additionally, RocksDB logs some compaction information in its LOG file, including all the input and output file numbers of each compaction, and other useful statistics. RocksDB also records all deleted and added files of each compaction in the MANIFEST file, plus the information of all the files of each level. The LOG file and the MANIFEST file are two important sources to track RocksDB&amp;rsquo;s behaviors, we will use them later.Well, that&amp;rsquo;s an overly simplified introduction to RocksDB, but enough to go on now.DeleteRange Now we know how to delete a key in RocksDB, but what if we want to delete a range of keys? For example, if we want to delete keys in a range [a,c), we can first scan keys in [a,c), which are &amp;ldquo;a&amp;rdquo; and &amp;ldquo;b&amp;rdquo; in the above example, and delete them one by one. The result of the above example will be:(a,1,PUT),(b,2,PUT),(c,3,PUT),(c,4,DELETE),(a,5,DELETE),(b,6,DELETE) In the table, it looks like:   key sequence entry_type   a 6 DELETE   b 5 DELETE   c 4 DELETE   c 3 PUT   a 2 PUT   b 1 PUT  That&amp;rsquo;s how we delete a range in TiKV now, for example, to drop a table or destroy a replica. It&amp;rsquo;s easy, but has two drawbacks: A replica can contain a lot of keys, it&amp;rsquo;s costly to scan them all Deleting all keys appends a lot of delete entries. So as you can see, deleting data can result in more data temporarily (before compaction), and more IO amplification.  Let&amp;rsquo;s see how the DeleteRange feature from RocksDB comes to rescue. In the above example, instead of scanning range [a,c) and delete &amp;ldquo;a&amp;rdquo; and &amp;ldquo;b&amp;rdquo; separately, we just append a DeleteRange entry ([a,c),5,DELETE_RANGE), which results in:(a,1,PUT),(b,2,PUT),(c,3,PUT),(c,4,DELETE),([a,c),5,DELETE_RANGE) In the table, it looks like:   key sequence entry_type   [a,c) 5 DELETE_RANGE   c 4 DELETE   c 3 PUT   a 2 PUT   b 1 PUT  Now, if we try to get &amp;ldquo;a&amp;rdquo;, we will encounter the DELETE_RANGE first, and return that &amp;ldquo;a&amp;rdquo; is not found. That&amp;rsquo;s good, we don&amp;rsquo;t need to scan all data anymore, and the size of a DeleteRange entry can be ignored in face of a large range. So we planned to use the DeleteRange feature in TiKV and started to test it.The BUG Before telling the story, let&amp;rsquo;s see how we check data in tests first. TiKV has a useful consistency check mechanism, which has helped to expose some serious bugs. When it is enabled, TiKV runs the consistency check periodically. Consistency check will calculate a hash of each replica and compare the hashes of all replicas to check whether they are consistent. If the hashes don&amp;rsquo;t match, something must be terribly wrong, so it will panic.Everything worked great until one of our test clusters panicked?The panicked cluster, which we named it Cluster A, was running a branch with the DeleteRange feature, so that&amp;rsquo;s why we started hunting the DeleteRange bug.Back to the topRound 1 There are 4 TiKV instances KV1, KV2, KV3, and KV4 in Cluster A, and the consistency check showed that we had an abnormal replica R2 in KV2, and two normal replicas R1 and R3 in KV1 and KV3. We used tikv-ctl to print out the diff of R2 and R1 to see what&amp;rsquo;s wrong. The diff showed that R2 had lost some ranges of data, and some deleted data reappeared too. Since we were testing the DeleteRange branch in this cluster, we guessed that this could happen if a DeleteRange entry dropped some entries in a wrong way during the compaction.We tried to collect more evidence to locate the problem, but unfortunately, the crime scene had been destroyed after we used tikv-ctl to open the underlying data directory, because tikv-ctl started RocksDB, which reorganized the data in some background compaction, and we hadn&amp;rsquo;t made any backup.However, the problem was still there, we did with what we got. We analyzed the log of the abnormal KV2 and found out that R2 sent a snapshot to R1 in 2017-08-15 11:46:17, and panicked in 2017-08-15 12:11:04. Since R1 was restored from R2&amp;rsquo;s snapshot in 11:46:17 and R1 was OK in 12:11:04, it meant that R2 should be OK in 11:46:17, so what happened between 11:46:17~12:11:04? We assumed that one of the compaction in RocksDB between that time was wrong and started investigating RocksDB&amp;rsquo;s LOG and MANIFEST files. The LOG file showed that some compactions were done in that time. We could do a cross-reference with the MANIFEST file to get more details. However, remembered that the crime scene had been destroyed? It meant that the MANIFEST file was truncated and the SST files generated in that time were deleted and we were stalled.So we decided to deploy another test cluster, which we called it Cluster B, and hopefully, it would panic again soon so that we could have another fresh crime scene. Meanwhile, we kept investigating here and there, but still without any clue. We had no choice but tried to locate the bug in the source code of RocksDB directly. We believed that the bug was related to either the DeleteRange feature or the compaction, so we dived into there. After a few days, we got some suspicious places but still nothing solid, except to realize that the DeleteRange implementation was more complicated than we expected.Round 2 A few days later, Cluster B panicked again, so we were given another chance to hunt, let&amp;rsquo;s go. This time, we protected the crime scene very well and made a backup before doing anything.The consistency check showed that we had an abnormal replica R3 in KV3, and two normal replicas R1 and R2 in KV1 and KV2. And …"},
		{"url": "https://pingcap.com/meetup/meetup-2017-09-06/",
		"title": "【Infra Meetup NO.55】TiDB Pre-GA 版本新特性介绍以及后续功能展望（内附视频）", 
		"content": "  上周六，PingCAP Infra Meetup 第 55 期，由我司 Engineering VP 申砾为大家分享《 TiDB Pre-GA 版本新特性介绍以及后续功能展望》。在活动现场，小伙伴们就 TiDB 新特性提出了很多问题，申砾在现场与大家有一番深度的交流与讨论。精彩现场小编立马为你呈现。 精彩视频 精彩现场 上周，TiDB 正式发布了 Pre-GA 版本。针对 Pre-GA 版本的新特性，PingCAP Infra Meetup 第 55 期特设定 Pre-GA 详解专场。活动当天，现场来了很多关注 TiDB 的粉丝们。简单开场后，我司 Engineering VP 申砾同学介绍到本期内容主要围绕新版本带来的变化和内部实现细节，以及这种新型的 HTAP 数据库解决的实际问题和典型应用场景等做深度解析。技术干货节选 TiDB Pre-GA 版本对 MySQL 兼容性、SQL 优化器、系统稳定性、性能方面做了大量优化工作。本次分享中，申砾就各个组件的优化做了详解：TiDB ：在优化器方面 RC4 已经从一个假的基于代价产品模型，切换成一个真的基于代价产品模型，也真的是用统计信息去算。在 RC3 版本中，一些代价实际上是有规则算法的，比如说，A 等于 10 设置一个过滤比例，A 大于 10 又算另外一个过滤比例，这都是一些规则，RC4 是基于代价的一个传统模型。 Pre-GA 新特性也主要对代价模型做了一些调整。 其次，在索引选择上做了优化，可以支持不同类型字段比较的索引选择，这一优化用户反馈查询速度明显变快。 再者，支持 Join Reorder，对于 OLTP 层面来说，Join Reorder 不太会用到，但对于一些比较复杂的场景，比如说有的用户使用参报表。这个时候有可能会出现 Join 报表，特别是在 TCH 里面，多表 Join 比较常见。下一步计划也将统计信息导入 TiSpark 里，指导 TiSpark 做 Join Reorder 。  在执行器方面 首先在 MySQL 兼容性做了大量的工作，特别是像时间类型这种比较复杂的类型，字面值的表达方式多种多样，还要考虑各种不合法的字面值，不合法的字面值需要处理成零，还是 Null，其实挺多细节在里面。 支持 Natural Join，我们以前支持 Using 来制订，现在我们支持 Natural Join 这个语法。这个是由我们的 Contributor 一个大二的学生贡献出来的 PR，在此特别感谢。 支持 JSON，然后支持 JSON 字段上各种操作。同时支持去修改使用某一个字段，并支持对 JSON 字段中某一属性去建索引，这样才可以更快速的用其中某一个字段做过滤条件。 减小执行器内存占用，大部分内存消耗在读出来的数据上面，我们把读出来的数据表示结构做了精简，在数据量比较大的时候，大概能比之前的版本少 20% 的内存。 支持在 SQL 语句中设置优先级，并根据查询类型自动设置部分语句的优先级。 完成表达式框架重构，执行速度提升 30% 左右。  在介绍完 TiDB 后，申砾接下来介绍了下 PD 方面的优化。PD：主要工作还是支持手动切换 PD 集群 Leader，可以手动的把 PD Leader 做一个迁移。TiKV ： Raft Log 使用独立的 RocksDB 实例。以前是一个 TIKV ,一个 RocksDB，数据和 Raft Log 都写在里面，现在是把数据和 Raft Log 分成两个 RocksDB 实例。这样对未来底层的优化有了更多的可能性。例如 Raft Log 可以不用写入在 RocksDB 中。 使用 DeleteRange 加速数据迁移，比如把一个副本从 PGA 迁到 PGB ,然后 PGA 这个副本的数据没有用了，以前的做法是一点点删除，现在可以用 DeleteRange 直接删除。 在 TiKV 上支持更多运算符下推，让 where 条件包含这些运算符的聚合操作可以下推到 TiKV 上进行计算。 至少有 10% 的性能提升，稳定性提升。  最后，申砾就大家关注的 TiDB GA 版本发布及后续功能优化做了分享。TiDB GA 将在 9 月底左右发布，核心主要围绕稳定性、性能、正确性、兼容性方面做大量工作。申砾就后续功能做了详解，主要是以下几点： 一个更好用的调度器，这个调度器指的是 PD 去调度 TiKV 的 Region 的位置。通过给不同的 TIKV 节点设置不同的权重， PD 会往这些 TIKV 节点 根据这个节点的权重调度不同数量的 Region。 增加 SQL Plan Cache ，通过增加 SQL 语法解析 Cache 可以让相同逻辑的 SQL 在命中语法解析 Cache 时速度变得更快。 增加更多的并行算子，最大化利用多核 CPU 的能力。 增加物理备份，直接使用 RocksDB 的 SST 文件进行备份，这样可以有比较快的备份恢复速度。  以上就是申砾给大家带来的部分精彩内容，干货满满，小伙伴们还可以观看完整版演讲视频，慢慢 Enjoy~ 也敬请期待我们下一期的内容 :)点击下载 PPT"},
		{"url": "https://pingcap.com/weekly/2017-09-04-tidb-weekly/",
		"title": "Weekly update (August 28 ~ September 03, 2017)", 
		"content": " Weekly update in TiDB Last week, we landed 42 PRs in the TiDB repositories.Added  Add JSON into builtin if function.  Fixed  Fix bugs when doing natural JOIN or JOIN with using clause. Check whether date is zero and returns error when casting int as the time type. Support date time format when parse duration. Fix the issue that SHOW CREATE TABLE COMMENT is not escaped and the issue that FieldType.CompactStr() is escaped. Support empty bit-value literal syntax b&#39;&#39;. Fix the OCT() bug when it meets a binary value. Fix float and binary literal bugs. Fix index join key incompatible encoded problem. EvalDuration supports the time format in int. Fix the batch insert bug on duplicate keys. Fix the bug for parsing the UTC_TIME/UTC_TIMESTAMP/CUR_TIME/CURRENT_TIME/CURRENT_TIMESTAMP builtin  Improved  Make NOW() folded in constant folding stage. Refine the time stamp index selection. Disable auto analyze. Support sig pushdown in mocktikv. Support more types when getting default FLEN and DECIMAL. Refactor the following expressions/builtin-function evaluation framework:  GET_LOCK, RELEASE_LOCK FOUND_ROWS, DATABASE, CURRENT_USER, USER, CONNECTION_ID, VERSION STR_TO_DATE BIT_COUNT RLIKE MAKE_SET, QUOTE DATE_FORMAT PERIOD_ADD, PERIOD_DIFF UNIX_TIMESTAMP   Weekly update in TiKV Last week, We landed 32 PRs in the TiKV repositories.Added  Use delete range to destroy a replica. Add builtin_cast-III for DAG/expression. Support decoding Time from tipb.Expr. Add statistics for coprocessor. Support builtin UnaryMinus* for DAG. Add more math functions for DAG. Implement eval for expression. Support storing weight when balancing data. Adjust the decimal parsing interface for coprocessor Implement coalesce for corprocessor. Implement case_when for DAG.  Improved  Refactor storeInfo. Speed up reconnecting to PD leader. Use FnvHashMap. Cleanup code for coprocessor. Add more tests for exprssion/builtin_cast. Use RocksDB 5.7.3. Add more write statistics. Make error message more friendly. Refactor replica selector for PD.  "},
		{"url": "https://pingcap.com/blog/2017-09-01-tidbmeetsjepsen/",
		"title": "When TiDB Meets Jepsen", 
		"content": "  What is Jepsen? How does Jepsen work?  DB Client Checker Nemesis Generator  Jepsen tests that TiDB goes through  The Bank Test The Set Test The Register Test  Miscellaneous  What is Jepsen? Written by Kyle Kingsbury in Clojure. Jepsen is a test framework for distributed systems verification. Kingsbury has used it to verify the consistency of many famous distributed systems (etcd, ZooKeeper, CockroachDB, etc.) and found multiple bugs in some of these systems. The verification process and reflections on the consistency verification are presented in the author’s blog.How does Jepsen work? Jepsen consists of 6 nodes, one control node and five controlled nodes (by default they&amp;rsquo;re named n1, n2, n3, n4, and n5). The control node sends all instructions from shell script to SQL statements, to some or all of the controlled nodes. Jepsen provides a few key APIs for distributed systems verification:DB DB encapsulates the download, deployment, startup and shutdown commands of the target system. The core function consists of setup and teardown. When Jepsen is testing TiDB, setup is responsible for downloading binaries and starting PD (Placement Driver), TiKV and TiDB in turn while teardown shutdowns the entire TiDB cluster and deletes logs.Client Client encapsulates the client that each test needs to provide and each client provides two interfaces: setup and invoke. Setup is responsible for connecting with TiDB and invoke includes SQL statements that the client calls to TiDB during the test and the statements are subject to test cases.Checker Checker is used for verifying the history produced by the test, determining whether the test result is as expected. Below shows the format of the history:Nemesis Nemesis introduces failures across the cluster, such as the common network partition, network latency, and node downtime. In TiDB testing, there are the following types of Nemesis: parts: network partition majority-ring: each node sees different majority of other nodes start-stop: send SIGSTOP to some nodes start-kill: send SIGKILL to some nodes  After introducing the parts type into the tests, a time-out error occurs when executing some statements as shown below:Generator Generator, the event generator of Jepsen, interweaves the operations of Client with that of Nemesis and generates specific execute statements for the entire test process.Jepsen tests that TiDB goes through TiDB has gone through 3 Jepsen tests: bank, set and register.The Bank Test The Bank Test is used for verifying snapshot isolation. This test simulates various transfers in a bank system and each initial bank system is as follows:1 to 5 represents the account name respectively and 10 means the account balance. The test will randomly generate transfer information:The above diagram means that the amount of 5 is transferred from Account 1 to Account 2. Meanwhile, the test randomly reads the deposit information of all accounts. For example, at some point, the deposit information of an account can be as below:Below is a screenshot in a test:In snapshot isolation, all transfers should ensure that the total amount of all accounts in each moment should be the same. TiDB passed the test even if all kinds of nemesis have been introduced.The Set Test This test concurrently inserts many rows into a table from different nodes and performs a final read of all rows to verify their presence. At the same time, due to the introduction of Nemesis, it is normal that values that return time-out will or will not appear in this table.Below is a screenshot in a test:Once again, TiDB passed the test.The Register Test It is easy to understand this test: create a table and insert a value. We will then regard this value as a register and concurrently execute the read, write and cas (compare and swap) operations to it from different nodes in the test.Later, we use a series of action history produced by Jepsen (as shown above) for Linearizability verification. This algorithm is the core of Jepsen and the reason why Jepsen is well-known in the industry.Miscellaneous Each time when there is an update in the TiDB code, we will internally trigger continuous integration (CI) to execute Jepsen and use Jepsen to guarantee the data consistency of TiDB. If you are interested in distributed test and consistency verification, please join us.TiDB Jepsen: https://github.com/pingcap/jepsen/tree/master/tidb"},
		{"url": "https://pingcap.com/blog-cn/tidb-meets-spark/",
		"title": "When TiDB Meets Spark", 
		"content": " 本文整理自 TiSpark 项目发起人马晓宇在 Strata Data Conference 上分享的《When TiDB Meets Spark》演讲实录。 先介绍我自己，我是 PingCAP 的马晓宇，是 TiDB OLAP 方向的负责人，也是 TiSpark 项目的发起人，主要是做 OLAP 方面的 Feature 和 Product 相关的工作，之前是网易的 Big Data Infra Team Leader，先前的经验差不多都是在 SQL、Hadoop 和所谓大数据相关的一些东西。今天主要会讲的议程大概这么几项。首先稍微介绍一下 TiDB 和 TiKV，因为 TiSpark 这个项目是基于它们的，所以你需要知道一下 TiDB 和 TiKV 分别是什么，才能比较好理解我们做的是什么事情。另外正题是 TiSpark 是什么，然后 TiSpark 的架构，除了 Raw Spark 之外，我们提供了一些什么样的不一样的东西，再然后是 Use Case，最后是项目现在的状态。首先说什么是 TiDB。你可以认为 TiDB 是现在比较火的 Spanner 的一个开源实现。它具备在线水平扩展、分布式 ACID Transaction、HA、Auto failover 等特性，是一个 NewSQL 数据库。然后什么是 TiKV，可能我们今天要说很多次了。TiKV 其实是 TiDB 这个产品底下的数据库存储引擎，更形象，更具体一点，这是一个架构图。大家可以看到，TiDB 做为一个完整的数据库来说，它是这样的一个架构，上层是 DB 层，DB 层是负责做 DB 相关的东西，比如说一部分的 Transaction，SQL 的解析，然后执行 Query Processing 相关的一些东西。底下是 KV 层，存储层。存储层就是存储数据，通过 Raft 协议来做 Replica 的，旁边还有 Placement Driver(简称 PD)，如果对 Hadoop比较了解，你可以认为它有点像 NameNode，它会存储每一个 Region 分别存了哪些 Key，然后 Key Range 是什么。当然它在需要的时候也会做一些数据搬迁的调度，以及 Leader 的自动负载均衡等。最后 PD 提供了统一中央授时功能。所有这些组件，都是通过 gRPC 来进行通讯的。我们回到正题来说，什么叫 TiSpark。TiSpark 就是 Spark SQL on TiKV。为什么说是 on TiKV，而不是 on TiDB，因为我们让 Spark SQL 直接跑在分布式存储层上而绕过了 TiDB。这三个组件，TiDB ／ TiKV ／ TiSpark 一起，作为一个完整的平台，提供了 HTAP（Hybrid Transactional/Analytical Processing）的功能。再具体一点说 TiSpark 实现了什么：首先是比较复杂的计算下推，然后 Key Range Pruning，支持索引(因为它底下是一个真正的分布式数据库引擎，所以它可以支持索引)，然后一部分的 Cost Based Optimization 基于代价的优化。CBO 这里有两部分，一部分是说，因为我们有索引，所以在这种情况下，大家知道会面临一个问题，比如说我有十个不同索引，我现在要选择哪一个索引对应我现在这个查询的谓词条件更有利。选择好的索引，会执行比较快，反之会慢。 另外一个是，刚才大家可能有听华为的 Hu Rong 老师介绍，他们在 Spark 上面做 Join Reorder，对于我们来说，也有类似的东西，需要做 Join Reorder 。这里底下有两个是 Planned 但还没有做。一个是回写，就是说现在 TiSpark 是一个只读的系统。另外我们考虑把常用的一些传统数据库的优化手段，也搬到我们这边来。现在开始说一下整个架构是什么样的。后面会有一个具体的解说，先看一下架构图。在 Spark Driver 上，需要接入 TiSpark 的接口，现在 TiSpark 也支持 JDBC。Worker / Executor 那边也需要一个这样的架构。 整个部署，采用 Spark 外接 JAR 的方式，并没有说需要到我整个把 Spark 部署全都换掉属于我们的版本，只需要提交一个 JAR 包就可以。每个 TiSpark 组件会与 TiKV 进行通讯，Driver 这边会和 Placement Driver 进行通讯，然后这边具体干了什么，后面会解释。在 Spark Driver 这边，因为这个架构上没有 TiDB 什么事，所以说 DB 本身干的事情，我们需要再干一遍，比如说 Schema 存在 TiKV 存储引擎里面，然后里面包括 Tables 的元信息，也就是告诉你数据库里面，分别有什么表，每个表里面分别有什么列，这些东西都属于 Schema 信息。因为我们没有直接连接 TiDB，所以说 Schema 信息需要我们自己去解析。比较重要的功能通过将 Spark SQL 产生的 LogicalPlan，Hook LogicalPlan，然后去做过滤，主要是: 哪一些谓词可以转化成索引相关的访问； 哪一些可以转化成 Key Range 相关的，还有哪一些其它计算可以下推，这些 Plan 节点我们会先过滤处理一遍。然后把 TiKV 可以算的部分推下去，TiKV 算不了的反推回 Spark； 在基于代价的优化部分 Join Reorder 只是在 Plan 状态； Data Location 是通过 Placement Driver 的交互得到的。Java 这边，会跟 Placement Driver 进行交互，说我要知道的是每个（Task）分别要发哪一台机器，然后分别要知晓哪一块的数据。  之后切分 Partition 的过程就稍微简单一点，按照机器分割区间。之后需要做 Encoding / Decoding：因为还是一样的，抛弃了数据库之后，所有的数据从二进制需要还原成有 Schema 的数据。一个大数据块读上来，我怎么切分 Row，每个 Row 怎么样还原成它对应的数据类型，这个就需要自己来做。计算下推，我需要把它下推的 Plan 转化成 Coprocessor 可以理解的信息。然后当作 Coprocessor 的一个请求，发送到 Coprocessor，这也是 TiKV-Client 这边做的两个东西。这些是怎么做的？因为 Spark 提供的两个所谓 Experimental 接口。这两个分别对应的是 Spark Strategy 和 Spark Optimizer，如果做过相关的工作你们可能会知道，你 Hook 了 SQL 引擎的优化器和物理计划生成部分。那两个东西一旦可以改写的话，其实你可以更改数据库的很多很多行为。当然这是有代价的。什么代价？这两个看名字，Experimental Methods，名字提示了什么，也就是在版本和版本之间，比如说 1.6 升到 2.1 不保证里面所有暴露出来的东西都还能工作。可以看到，一个依赖的函数或者类，如果变一些实现，比如说 LogicalPlan 这个类原来是三个参数，现在变成四个参数，那可能就崩了，会有这样的风险。我们是怎么样做规避的呢？这个项目其实是切成两半的，一半是 TiSpark，另一半是重很多的 TiKV-Client 。TiKV Java Client是负责解析对 TiKV 进行读取和数据解析，谓词处理等等，是一个完整的 TiKV 的独立的 Java 实现的接口。也就是说你有 Java 的系统，你需要操作 TiKV 你可以拿 TiKV Client 去做。底下项目就非常薄，你可以说是主体，就是真的叫 TiSpark 的这个项目，其实也就千多行代码。做的事情就是利用刚才说的两个 Hook 点把 Spark 的 LogicalPlan 导出来，我们自己做一次再变换之后，把剩下的东西交还给 Spark 来做的。这一层非常薄，所以我们不会太担心每个大版本升级的时候，我们需要做很多很多事情，去维护兼容性。刚才说的有几种可能比较抽象，现在来一个具体的例子，具体看这个东西怎么 Work，可以看一个具体的例子。这是一个查询，根据所给的学号和学院等条件计算学生平均值。这张表上，有两个索引，一个索引是主键索引，另外一个索引是在 Secondary Index ，建立在 School 上。lottery 是一个用户自定义函数，并不在 TiDB 和 TiKV 的支持范围之内。首先是说谓词怎么样被处理，这里有几种不同的谓词，比如关于学生 ID 的：大于等于 8000，小于 10100，以及有两个单独学号；然后是一个 school = ‘engineer’，还有一个 UDF 叫 lottery，单独挑选一些运气不好的学生。第一步，整个处理，假设说我们索引选中的是在 studentID 上的聚簇索引。studentID 相关的谓词可以转化为区间 [8000, 10100), 10323, 10327。然后是 school=‘engineer’，因为它没有被任何索引选择，所以是一个独立的条件。这两种不同的条件，一个是跟聚簇索引相关的，可以转化成 Key Range，另外一个是跟索引没有关系的独立的谓词。两者会经过不同的处理，聚簇索引相关的谓词转化成 Key Range，独立的谓词 school=‘engineer’ 会变成 Coprocessor 的 Reqeust，然后进行 gRPC 的编码，最后把请求发过去。聚簇索引相关谓词转化的 Key Range 会通过查询 Placement Driver 取得 Region 的分布信息，进行相应的区间切割。假设说有三个 Region。Region 1 是 [0, 5000)，是一个闭开区间，然后 Region 2 是 [5000, 10000)。接着 Region 3 是 [10000, 15000)。对应我们上面的 Request 下推的区间信息你可以看到，谓词区间对应到两个 Region：Region 2 和 Region 3，Region1 的数据根本不用碰，Region 2 的数据会被切成 [8000, 10000)，因为对应的数据区间只有 [8000, 10000)。然后剩下的 [10000, 10100) 会单独放到 Region 3 上面，剩下的就是编码 school=‘engineering’ 对应的 Coprocessor Request。最后将编码完成的请求发送到对应的 Region。 上面就是一个谓词处理的逻辑。多个索引是怎么选择的呢？是通过统计信息。TiDB 本身是有收集统计信息的， TiSpark 现在正在实现统计信息处理的功能。TiDB 的统计信息是一个等高直方图。例如我们刚才说的两个索引，索引一在 studentId 上，索引二是在 school 上。查询用到了 studentId 和 school 这两个列相关的条件，配合索引，去等高直方图进行估算，直方图可以告诉你，经过谓词过滤大概会有多少条记录返回。假设说使用第一个索引能返回的记录是 1000 条，使用第二个能返回的记录是 800 条，看起来说应该选择 800 条的索引，因为他的选择度可能更好一点。但是实际上，因为聚簇索引访问代价会比较低，因为一次索引访问就能取到数据而 Secondary Index 则需要访问两次才能取到数据，所以实际上，反而可能 1000 条的聚簇索引访问是更好的一个选择。这个只是一个例子，并不是说永远是聚簇索引更好。 然后还有两个优化，一个优化是覆盖索引，也就是说索引是可以建多列的，这个索引不一定是只有 school 这个列，我可以把一个表里面很多列都建成索引，这样有一些查询可以直接用索引本身的信息计算，而不需要回表读取就可以完成。比如，select count(*) from student where school=’engineer’整个一条查询就只用到 school 这个列，如果我的索引键就是 school，此外并不需要其他东西。所以我只要读到索引之后，我就可以得到 count(*) 是多少。类似于这样的覆盖索引的东西，也有优化。TiSpark 比较特殊的是，下层接入的是一个完整的数据库而数据库把控了数据入口，我每个 Update 每个 Insert 都可以看到。这给我们带来什么方便，就是说每个更新带来的历史数据变更可以主动收集。基于代价优化的其他一些功能例如 Join Reorder 还只是计划中，现在并没有实现。刚才有跟 Hu Rong 老师有讨论，暂时 Spark 2.2 所做的 CBO，并不能接入一个外部的统计信息，我们暂时还没想好，这块应该这么样接。 接下来是聚合下推，聚合下推可能稍微特殊一点，因为一般来说，Spark 下面的数据引擎，就是说现在 Spark 的 Data Source API 并不会做聚合下推这种事情。还是刚才的 SQL 查询:这个例子稍微有一点特殊，因为他是计算平均值，为什么特殊，因为没有办法直接在 TiKV 做 AVG 平均值计算，然后直接在 Spark 再做直接聚合计算，因此这种情况会有一个改写，将 AVG 拆解成 SUM 和 COUNT，然后会把他们分别下推到 Coprocessor，最后在 Spark 继续做聚合计算。TiSpark 项目除了改写 Plan 之外，还要负责结合做类型转换和 Schema 转换。因为 TiKV 这个项目，本身并不是为了 TiSpark 来设计的，所以整个 Schema 和类型转化的规则都是不一样的。Coprocessor 部分聚合 (Partial Aggregation) 的结果，数据的类型和 Spark 是完全不一样的，因此这边还会做一次 Schema 的桥接。之后其他的就是跟前面一样了，会把请求发到对应的 Region。现在来讲 TiSpark 和 TiDB／TiKV，因为是整个一个产品的不同组件，所以说 TiSpark 的存储，也就是 TiDB 的存储，TiKV 会针对 TiSpark 这个项目来做一些 OLAP 相关定的 Feature。比如说在 OLTP 的模式下我们使用的是 SI 隔离级别，就是 Snapshot Isolation。在 OLTP 这边，需要面对一个 Lock Resolving 问题和开销。如果要看的话可以看一下 Percolator Transaction 的论文。为了避免 Lock Resolving 带来的开销，我们使用了一个 Read Committed 的模式。如果需要的话，后面再加 SI 也并不是非常难，只是现在这个版本并不会这样做。之后还有 OLTP 和 OLAP 混跑，大家可能会觉得有很大问题，就是资源怎么样隔离。现在资源隔离是这样的：对于比较大的查询，在 KV 那层会倾向于用更少的线程。当然是说你如果是在空跑，这台机器上没有其他人在跑的话，其实还是会用所有的资源，但如果你有跟其他 OLTP 查询对比的话，你会发现虽然我是请求了很多但你可能未必会拿到很多。用户也可以手动来降低优先级，例如，我明天就要给老板出一个报表，一个小时候之后就要拿结果，我可以手动提高一个分级。 所有刚刚讲的这些，基本上都是 TiSpark 本身提供了一些什么东西。现在说在一个类似于 Big Picture 的语境之下，怎么样去看这个项目。除了 Raw Spark 的功能之外，我们提供了什么多的东西。最不一样的地方就是 SQL-On-Hadoop，基本上来说，你可以认为它并不控制存储，存储并不在你这里，你灌的数据，可能是通过 Flume/Kafka 某一个 Sink 灌进来，或通过 Sqoop 导过来，整个不管是 Hive，还是 Spark SQL，他并不知道数据进来。对于一个数据库来说，每一条数据插入，全是经过数据库本身的，所以每一条数据怎么样进来，怎么样存，整个这个产品是可以知道的。另外就是说相对于 SQL-On-Hadoop，我们做一个数据库，肯定会提供 Update 和 Delete 这是不用多说的。因为 TiKV 本身会提供一些额外计算的功能，所以我们可以把一些复杂的查询进行下推。现在只是说了两个，一个是谓词相关的 下推，还有一个是刚才说的聚合下推，其他还有 Order，Limit 这些东西，其实也可以往下放。接下来就属于脑洞阶段了，除了刚才说的已经“高瞻远瞩”的东西之外，脑洞一下，接下来还可以做一些什么（当然现在还没有做），这个已经是 GA 还要再往后的东西了。首先说存储，TiKV 的存储是可以给你提供一个全局有序，这可能是跟很多的 SQL-On-Hadoop 的存储是不一样的。Global Order 有什么好处，你可以做 Merge Join，一个 Map Stage 可以做完，而不是要做 Shuffle 和 Sort。Index lookup join 是一个可以尝试去做的。Improved CBO，我们数据库团队现在正在开发实时收集统计信息。 其他一些传统数据库才可能的优化，我们也可以尝试。这里就不展开多说了。 整个系统，一个展望就是 Spark SQL 下层接数据库存储引擎 TiKV ，我可以希望说 Big Data 的那些平台是不是可以和传统的数据库就合在一起。因为本身 TiDB 加 TiKV 就是一个分布式的数据库。然后可以做 Online Transaction，类似于像 Spanner 提供的那些功能之外，我们加上 Spark 之后，是不是可以把一些 Spark 相关的 Workload 也搬上来。然后是 Use Case：首先一个平台，可以做两种不同的 Workload，Analytical 或者 Transactional 都可以在同一个平台上支持，最大的好处你可以想象：没有 ETL。比如说我现在有一个数据库，我可能通 Sqoop 每小时来同步一次，但是这样有一个延迟。而使用 TiSpark 的话，你查到的数据就是你刚才 Transaction 落地的数据而没有延迟。另外整个东西加在一起的话，就是有一个好处：只需要一套系统。要做数据仓库，或者做一些离线的分析，现在我并不需要把数据从一个平台导入数据分析平台。现在只要一套系统就可以，这样能降低你的维护成本。另外一个延伸的典型用法是，你可以用 TiDB 作为将多个数据库同步到一起的解决方案。这个方案可以实时接入变更记录，比如 Binlog，实时同步到 TiDB，再使用 TiSpark 做数据分析，也可以用它将 ETL 之后的结果写到 HDFS 数仓进行归档整理。需要说明的是，由于 TiDB / TiKV 整体是偏重 OLTP，暂时使用的是行存且有一定的事务和版本开销，因此批量读的速度会比 HDFS + 列存如 Parquet 要慢，并不是一个直接替代原本 Hadoop 上基于 Spark SQL / Hive 或者 Impala 这些的数仓解决方案。但是对于大数据场景下，如果你需要一个可变数据的存储，或者需要比较严格的一致性，那么它是一个合适的平台。后续我们将写一篇文章详细介绍 TiSpark 的 Use Case，对 TiSpark 感兴趣的小伙伴，欢迎发邮件到 info@pingcap.com 与 …"},
		{"url": "https://pingcap.com/blog/2017-08-30-prega/",
		"title": "TiDB Pre-GA Release Notes", 
		"content": " TiDB: The SQL query optimizer:  Adjust the cost model Use index scan to handle the where clause with the compare expression which has different types on each side Support the Greedy algorithm based Join Reorder  Many enhancements have been introduced to be more compatible with MySQL Support Natural Join Support the JSON type (Experimental), including the query, update and index of the JSON fields Prune the useless data to reduce the consumption of the executor memory Support configuring prioritization in the SQL statements and automatically set the prioritization for some of the statements according to the query type Completed the expression refactor and the speed is increased by about 30%  Placement Driver (PD): Support manually changing the leader of the PD cluster  TiKV: Use dedicated Rocksdb instance to store Raft log Use DeleteRange to speed up the deleting of replicas Coprocessor now supports more pushdown operators Improve the performance and stability  TiDB Connector for Spark Beta Release: Implement the predicates pushdown Implement the aggregation pushdown Implement range pruning Capable of running full set of TPC-H except for one query that needs view support   "},
		{"url": "https://pingcap.com/weekly/2017-08-28-tidb-weekly/",
		"title": "Weekly update (August 21 ~ August 27, 2017)", 
		"content": " Weekly update in TiDB Last week, we landed 55 PRs in the TiDB repositories.Added  Support the &amp;lsquo;SHOW PLUGINS&amp;rsquo; syntax with dummy implementation. Add a system variable to split to-be-deleted data into batches autmatically. Add the date literal. Add the framework of the X protocol, and commond line arguments.  Fixed  Fix a panic when the set statement meets a subquery. Set charset and collation for the union&amp;rsquo;s result. Fix show column comment, show create table auto-increment. Fix the bug in Date comparison. Fix the bug in index selection. Rewrite the index join plan generation to correct wrong index selection. Avoid &amp;lsquo;binary BINARY&amp;rsquo; for a special field type. Correct overflow check on the MINUS function. Fix a bug when casting JSON to other types. Concatenates string literals which placed each other. Fix an issue that the flags of the &amp;lsquo;IFNULL&amp;rsquo; Builtin function result is not consistent with MySQL.  Improved  Enlarge the batch size from 128M to 256M to reduce the network round-trip. Suport coalesce pushdown. Make auto analyze more conservative. Support isnull pushdown. Implement the MVCCStore interface using the leveldb backend. Calculate generated columns in CRUD. Refactor the following expression/builtin-function evaluation framework:  TIMESTAMP DAYNAME DATE UTC_TIME MONTHNAME WEEKDAY, FROM_DAYS, QURTER LAST_DAY DAYOFWEEK, DAYOFMONTH, DAYOFYEAR FIND_IN_SET DATEDIFF CURDATE, SYSDATE, YEARWEEK YEAR, MONTH WEEK, WEEKOFYEAR NOW, UTC_TIMESTAMP, UTC_DATE TIMESTAMPDIFF COALESCE   New Contributor (Thanks!)  David Ding xiaojian Cai bailaohe  Weekly update in TiKV Last week, We landed 21 PRs in the TiKV repositories.Added  Use dedicated Rocksdb instance to store Raft log. Add builtin_cast-II for DAG/expression. Add builtin scalar function operator for DAG. Add arithmetic operations for DAG. Add builtin conditions for DAG. Add more RocksDB metrics. Support negative operator for decimal.  Fixed  Fix a data race for PD. Fix a heartbeat stream bug for PD. Fix a bug with reverse scan.  Improved  Refactor the Coprocessor thread pool to pass contexts. Improve tests. Add timeout for RPC calls. Remove timeout for streaming calls. Make max tasks configurable for the Coprocessor. Move the heartbeat approximate size to PD worker.  "},
		{"url": "https://pingcap.com/meetup/meetup-2017-08-25/",
		"title": "【Infra Meetup NO.54】数据库计算存储分离架构分析（内附视频）", 
		"content": "  上周六，PingCAP Infra Meetup 第 54 期，我们邀请到了知乎大 V 李凯（知乎 ID：郁白）为大家分享了《数据库计算存储分离架构分析》。在活动现场，郁白老师跟小伙伴们有一番深度的交流与思想碰撞。长话短说，小编带你一起回顾精彩现场。 精彩视频 精彩现场 PingCAP Infra Meetup 第 54 期的活动现场十分火爆，活动签到时间未开始，小伙伴们就早早来到现场占位置，我想说早来的小伙伴们还是很明智的。因为&amp;hellip;&amp;hellip;后续到场的小伙伴只能酱婶儿滴扎堆在门口竖起耳朵听了，这场活动简直是一场郁白大神与粉丝的见面会。说了这么多，先上一张郁白老师的图吧~ 🙂技术干货节选 大数据下公有云面临的 5 个挑战谈到存储架构分离，为什么现在会有 Aurora 架构？包括前一阵阿里的 PolarDB 推出来以后，他们也在分析为什么要做这个东西。郁白老师认为单就公有云来说，现在云数据面临的挑战有以下 5 个： 跨 AZ 的可用性与数据安全性。 现在都提多 AZ 部署，亚马逊在全球有 40 多个 AZ， 16 个 Region，基本上每一个 Region 之内的那些关键服务都是跨 3 个 AZ。你要考虑整个 AZ 意外宕机或者计划内维护要怎么处理，数据迁移恢复速度怎么样。以传统的 MySQL 为例，比如说一个机器坏了，可能这个机器上存了几十 T、上百 T 的数据，那么即使在万兆网卡的情况下，也要拷个几分钟或者几十分钟都有可能。那么有没有可能加快这个速度。 还有一个就是服务恢复的速度。可能大家广为诟病就是基于 MySQL Binlog 复制。在主机压力非常大的情况下，是有可能在切换到备机以后，这个备机恢复可能需要几分钟甚至几十分钟。关键因素是回放 Binlog 的效率，MySQL 即使最新版本也只能做到 Group Commit 内的并发回放。这是数据库 RTO 指标，能不能在秒级、分钟级把这个服务恢复起来，这是一个在设计系统的时候要考虑的关键问题。 读写分离与弹性扩展。 一般来说我们讲云上数据库基本上都是集中化的，一写多读的，那这里会涉及到读写分离，把主库上一致性要求不高的读流量分给备库，这种情况下读写分离的备库能不能弹性扩展？我们知道 MySQL 可以通过 Binlog 复制来扩展备机，但是扩展的过程中就意味着复制一份完整数据，就像我们刚才提到的数据恢复一样，他要把整个数据全部复制过去然后把 Binlog 接上，这个时间可能你要真做的话几十分钟就过去了。如果说你的业务真撑不住说我赶快要加备机，那这个东西怎么去解决？ 资源的按需分配。 这点其实云计算上的云数据库一定程度上已经做到了，当然有些可能不一样，比如说有硬件独享的数据库就很难做到按需分配。像阿里可能会把 EBS 接到它的数据库虚拟机上，这样的话其实你接上了弹性化存储以后也基本上做到一个弹性的分配，要 1G 给 1G，再要 1G 再给 1G，不说一开始就把资源分配了，这是云上的一个弹性的东西。 高性能。 现在大家都要看跑分，除了跑分，还要看跑实际业务的时候到底行不行，有没有办法去优化。 生态兼容性。 比如说为什么 TiDB 一定要做 MySQL 的兼容？我觉得可能也是考虑这一点，现在开源领域最强的生态可能还是 MySQL，开源的数据库如果不做 MySQL 兼容，别人可能不一定会来用。  从以上这几点出发的话，我们就可以考虑一个云数据库到底怎么去发展。AWS Aurora 的架构特点及优势郁白老师选择 这几个具有代表性的数据库存储与计算架构做了重点讲解，他介绍到定义数据库服务器集群的架构决策的关键点在于集群共享发生的程度，它定义协调动作发生在什么层以及哪个层（ PE 计算层和 SE 存储层 ）将被复制或者共享。这不仅确定了系统在可扩展性和灵活性上的权衡，而且关系到每一种架构在现成的数据库服务器上的适用性。AWS Aurora 计算存储分离架构的优势主要体现在高可用、数据安全、弹性部署、性能方面。在介绍这个环节时，现场小伙伴提问不断，讨论非常激烈，强烈感受到思想碰撞在一起擦除的火花，郁白老师也针对大家的问题做了深度交流。计算存储分离架构的 4 个关键技术郁白老师总结出计算存储分离架构的四个关键技术与大家一起探讨，接下来将一一解析这些技术的关键点。关键技术一：跨 AZ 协同复制关键技术二：Cache Coherence 与一致性读关键技术三：统一 Log Structured Storage关键技术四：数据分片存储以上就是郁白老师带给大家的部分精彩分享，干货满满，特别感谢郁白老师精心准备的内容，意犹未尽的小伙伴们可以观看完整版演讲视频，慢慢 Enjoy~ 也敬请期待我们下一期的内容 :)点击下载 PPT"},
		{"url": "https://pingcap.com/blog-cn/linearizability/",
		"title": "Linearizability 一致性验证", 
		"content": " 上篇文章介绍了 TiDB 如何使用 Jepsen 来进行一致性验证，并且介绍了具体的测试案例，但是并没有对 Jepsen 背后的一致性验证算法做过多介绍。这篇文章将会深入 Jepsen 的核心库 knossos，介绍 knossos 库所涉及的 Linearizability（线性化）一致性验证算法。Linearizability 一致性模型 什么是一致性模型？ 一致性模型确定了编写系统的程序员与系统之间的某种协议，如果程序员遵守了这种协议，那么这个系统就能提供某种一致性。常见的一致性模型有： Strict Consistency Linearizability (Atomic Consistency) Sequential Consistency Casual Consistency Serializability ……  需要注意的是这里的系统指并发系统，分布式系统只是其中的一类。什么是 Linearizability？ 首先我们需要引入*历史*（history）的概念，*历史*是并发系统中由 invocation 事件和 response 事件组成的有限序列。  invocation: &amp;lt;x op(args*) A&amp;gt;，x 表示被执行对象的名称；op 表示操作名称，如读和写；args* 表示一系列参数值；A 表示进程的名称 response：&amp;lt;x term(res*) A&amp;gt;，term 表示结束（termination）状态；res* 表示一系列结果值 如果 invocation 和 response 的 x（对象）和 A（进程）相同，那么我们认为它们是对应操作，并且 complete（H）表示历史中的最多成对操作   当我们的历史 H 满足以下条件时我们把它称为*顺序化*（sequential）历史： H 中的第一个事件是 invocation 除了可能的最后一个事件外，每个 invocation 事件都紧跟着对应[^对应意味着对象和进程相同]的 response 事件；每个 response 事件都紧跟着对应的 invocation 事件    H|A 代表只含有进程A操作的子历史，H|x 代表只含有对象x操作的子历史 定义well-formed：如果每个进程子历史 H|A 都是顺序化的，那么这个历史 H 就是 well-formed。   如果一个*历史*不是顺序化的话那么就是并发的。历史 H 在操作上引出非自反的偏序关系$&amp;lt;_H$$e_0 &amp;lt;_H e_1$ if $res(e_0)$ precedes $inv(e_1)$ in $H$这里的 res 和 inv 分别对应 response 和 invocation。当历史 H 可以通过增加&amp;gt;=0个response 事件被延长时成为 H&amp;rsquo; 并且满足以下两个条件时，则这个*历史*是线性化（linearizable）的。  L1: complete(H&amp;rsquo;) 与某个合法的顺序化历史 S 相等 L2: $&amp;lt;_H ⊆ &amp;lt;_S$   complete(H&amp;rsquo;)表示进程以完整的操作进行交互，L2 表示如果 op1 在 H 中先于 op2 存在（注意这里的先于强调实时发生的顺序 real-time order），那么在 S 中也是这样。我们把 S 称为 H 的线性化点（linearization）。下面我们通过 3 个小例子来解释一下以上 2 个条件。q 代表 FIFO 队列，A、B 代表两个进程 q Enq(x) A q Deq() B q Ok(x) B 满足 linearizable，虽然 Enq(x)并没有返回 Ok，但是我们可以通过增加这条返回语句使得上述语句与某个合法的顺序化历史相等 q Enq(x) A q Ok() A q Enq(y) B q Ok() B q Deq() A q Ok(y) A 如果满足 linearizable 那必然 Enq(x)先于 Enq(y)，但是 Deq()是得到的却是 y，所以违反了 L2，因此这段历史不是线性化的 q Enq(y) A q Ok() A q Deq() A q Deq() B q Ok(y) A q Ok(y) B 不满足 linearizable 因为 Enq(y)只执行了一次，却被 Deq()了两次，不能与任何合法的顺序化历史相对应 Linearizability 的性质 局部性（Locality），当且仅当 H 中每个对象 x 都是线性化的，才能保证H是线性化的 非阻塞（Nonblocking），invocation 事件不用等待对应的 response 事件  验证 Linearizability 正确（correctness）的定义 一段历史 H 由两种对象组成，representation(REP)和 abstract(ABS)。abstract 是被实现的类型，而 representation 类型则是用于实现 ABS 的类型。这两种对象在以下条件下进行交互： 子历史 H|REP和H|ABS是well-formed 对于每个进程 P，在子历史 H|P 中，每一个 rep 操作都被 abs 操作所包含   对于某个实现中的所有历史 H 来说，如果 H|ABS 是线性化的，那么这个实现就是正确的。REP 值的子集中的合法表现由表达不变性（representation invariant）所表示：I: REP-&amp;gt; BOOL，一个合法表现的含义由抽象函数（abstract function）所表示：A: REP-&amp;gt;ABS。对于一个正确的实现 p 来说，存在一个表达不变性 I，以及一个抽象函数 A，并且无论何时 p 从一个合法的表达值 r 到达另一个表达值 r&amp;rsquo;，抽象操作 a 把抽象值 A&amp;reg;变成 A(r&amp;rsquo;)。我们从最简单的队列（FIFO queue）入手。struct queue { int back; element *elements; }; bool Enq(queue *q, element x) { int i = INC(&amp;amp;q-&amp;gt;back); // 原子自增并返回之前的值  STORE(&amp;amp;q-&amp;gt;elements[i], x); // 假设内存足够  return true; } element Deq(queue *q) { while (1) { int end = READ(&amp;amp;q-&amp;gt;back); // 原子读取  for (int i = 1; i &amp;lt; end; ++i) { element x = CAS(&amp;amp;q-&amp;gt;elements[i], NULL); // 返回 CAS 之前的值  if (x != NULL) return x; } } } Enq 和 Deq 可以看做是 abstract operation，而 Enq 和 Deq 中的每条语句可以看做是 representation operation。对线性化的历史的验证可以被转换为对顺序化历史的验证，对于给定的线性化历史，我们把最终线性化点的对象的值称为线性值。因为给定的历史可能有超过一个线性化点，所以这个对象可能会有多个线性值。我们用 Lin(H) 表示所有线性值的集合，可以把它们看作是系统外部的观察者所看到的值。对于以下几个队列操作，对应的线性值分别有以下几种。   History Linearized values      {[]}   Enq(x) A {[], [x]}   Enq(y) B {[], [x], [y], [x,y], [y,x]}   Ok() B {[y], [x,y], [y,x]}   Ok() A {[x,y], [y,x]}   Deq() C {[x], [y], [x,y], [y,x]}   Ok(x) C {[y]}    为了证明正确性，我们需要保证：For all $r$ in $Lin(H|REP)$, $I&amp;reg;$ holds and $A&amp;reg; ⊆ Lin(H|ABS)$其中 H|REP 和 H|ABS 都是线性化的，r 代表 H|REP 的线性值，并且$I&amp;reg; = (r.back ≥ 1)$ &amp;amp; $(∀ i. i ≥ r.back -&amp;gt; r.elements[i] = null)$ &amp;amp; $(lbound(r.elements) = 1)$其中 lbound 是最小的数组索引（队列从 1 开始）$A&amp;reg;$ = {$q | elements&amp;reg; = elements(q)$ &amp;amp; $&amp;lt;_r ⊆ &amp;lt;_q$}其中偏序关系$&amp;lt;_r$表示如果被插入元素 x 的赋值操作先于 y 的自增操作，则 $x &amp;lt;_r y，&amp;lt;_q$ 代表队列 q 的全序关系。换句话说，队列的表现值（representation value）就是队列中的元素，这些元素的排列顺序与 Enq 操作的顺序一致。下面这张图可以帮助你很好地理解上述公式的意思。第二列是线性化的表现值（linearized representation values），第三列是线性化的抽象值（linearized abstract values），可以看到每一行中第二列都是第三列的子集。Wing &amp;amp; Gong 线性化算法 介绍完了如何证明 linearizability，下面我们可以继续深入到 knossos 使用的两个核心算法之一——Wing &amp;amp; Gong Linearibility 算法（WGL）。 WGL 算法：对于给定的某个数据类型T，它的并发实现为 ConcObj，而它的顺序化要求为 SeqObj。对于给定的历史 H，我们在保证 H 的实时顺序 $&amp;lt;_H$ 的情况下尝试 H 的每一系列可能的顺序化操作，然后检查每个顺序化历史 $H_S$ 在 SeqObj 上执行时是否是线性化的。如果 H 的每一种可能都失败了，那么这个历史就不是线性化的。 我们定义历史是由一系列事件组成的：typedef struct ev { char item; char op; struct ev *match, *prev, *next; } event; 其中 iterm 是操作 op 的参数，name 是进行操作的进程的名字，prev 和 next 分别表示上一个和下一个事件，match 指向其对应的返回（res）事件。另外我们还需要区域以及 lift(unlift)这个概念。 区域（Section）：由触发（inv）事件，对应的返回事件，以及它们中间包含的所有事件。 虚线同时可以看作是也是 match 指针。 lift：将某对操作从历史中移出unlift：将移出的某对操作放回 这个算法的核心是一个搜索（Search）函数，如果历史H是线性化的，那么那么他返回一个线性化点（即顺序化历史 S）。搜索使用一个栈来保存历史中已经线性化的部分，这个栈及栈中的元素是这样定义的：typedef struct { event *pi, *pr, *inv, *resp; char item, op, result; } elt_stack; typedef struct { elt_stack value[STACK_LENGTH]; int in; } stacktype; 其中 pi 和 pr 分别表示子历史中第一个没有被检查的区域；inv 和 resp 表示子历史中第一对操作；item、op 和 result 记录这对操作的信息。一个完整的搜索函数是这样的： 初始化栈 通过 current 操作的 pi 和 pr 定位当前的区域，否则返回线性化点 从当前区域开始，选择一个操作并且将它的信息存储在 current 中 对选择的操作进行顺序化模拟，调用 op  5. - A：如果 op 返回真，意味着目前被检查的所有操作能够组成线性化的子历史，所以把这个操作推入栈中，并将这个操作从历史中移出，然后回到 2 - B: ① 如果当前区域内还有一些未被选择的触发（inv）事件没有排在任何返回（res）事件之后，那么选择一个然后回到 4；② 当前区域的所有操作已经被尝试但是失败了，所以我们需要将操作出栈然后尝试其他的顺序，如果栈是空的，那么意味着历史不是线性化的，函数返回；否则，将顶层元素出栈，这个元素包含了之前区域的所有信息，以及被选择的操作，然后 undo 之前的 op，unlift 这个操作，最后，设置 current 为之前区域的指针，然后回到 5B① 注：4 中 op 操作取决于具体模型，如果被测试的是一个寄存器的话，那 op 可以是 read、write 和 cas，如果 read 和 cas 时读到的值和预期值不一致，则操作无法进行。 这就是整个 WGL 算法。这个算法很简单也很好理解，但是有两个明显的缺点： 一旦操作数量上升，整个算法会运行地很缓慢，因为可能会出现涉及大量回溯的操作 这个算法只能验证是否线性化，一旦线性化不成立，并不能给出具体违反线性化的出错点  对此 knossos 库的第二个算法使用了 WGL 算法的改进版本，与 WGL 中的栈存放操作信息不同的是它使用了树遍历和图搜索两种方法来使算法更高效，同时存在“记忆”机制来避免对相同的操作进行重复验证，并且如果所验证的历史不满足一致性，会给出具体出错的点。篇幅有限，如果你对这个算法感兴趣的话，文末有链接。最后的思考 这篇文章介绍了什么是 Linearizability、Linearizability 正确性的验证及其算法。这些算法在分布式系统中的应用只是一个很小的方面，算法本身是独立的，它只需要一个历史 H，至于这个历史是随机生成的还是某个应用在实际中产生的并不重要。你可以使用这些算法对任何并发系统进行验证，小到一个无锁队列、Set，大到某个分布式系统。TiDB 作为一个分布式数据库却能被抽象化为一个队列、寄存器来被用作测试这本身就是一个很有意思的地方，同时也很好地展现了这些算法自身的魅力。参考 Consistency ModelKnossosSequential ConsistencyLinearizabilityLinearizability versus SerializabilityWGL算法Testing for Linearizability"},
		{"url": "https://pingcap.com/weekly/2017-08-21-tidb-weekly/",
		"title": "Weekly update (August 14 ~ August 20, 2017)", 
		"content": " Weekly update in TiDB Last week, we landed 57 PRs in the TiDB repositories.Added  Add the version information in diagnostic messages. Add the Close() method to RawKVClient. Add JSON in fieldTypeMergeRules. Add support to MySQL connector 6.06. Add Git branch name in tidb_version() and the starting log. Add the auto analyze feature for tables.  Fixed  DDL uses the correct method to check whether the context is done. Close the PD client in TiKV store and raw KV to prevent connection leak. Restore WrapCastAsReal and WrapCastAsDecimal if the argument is ClassInt. Notify the fetching goroutines to exit when IndexLookupExecutor closes. Fix the error in the return type for the ceil/floor function. Start GCWorker after the bootstrapping is finished. Add a limit to the number of multi-column index key parts. Disable the mysql.ClientMultiStatements capability Allow username to contain &amp;lsquo;@&amp;rsquo;.  Improved  Handle canceled error by Grpc remote. Remove all the stuff about the backgroud DDL worker. Use more effective terror comparison method. Union scan reuses PK when it is a handle. Use FieldType.Decimal to control the decimal length of double values to be shown in client. Support time constant pushdown in mocktikv. Return the unsupported error in the type infer stage. Refactor the following expression/builtin-function evaluation framework:  CRC32 TIME HOUR, MINUTE, SECOND, MICROSECOND RAND, POW, SIGN, SQRT DIV TRUNCATE ANY_VALUE DNF NULLIF TIMESTAMPADD TO_SECONDS TO_DAYS CASE RADIANS OCT IS_IPV4, IS_IPV6, IS_IPV4_COMPAT, IS_IPV4_MAPP INET_ATON, INET_NTOA, INET6_ATON, INET6_NTOA TIME_FORMAT   Weekly update in TiKV Last week, We landed 33 PRs in the TiKV repositories.Added  Add hex/escaped converting for tikv-ctl. Implement the basic structure of builtin_cast and some functions for DAG. Add compare to new expression calculate framework. Support leader resignation for PD. Add Git branch information to the starting log. Support the mysql-time constant. Add Constant and Column for coprocessor. Add RocksDB event metrics.  Fixed  Declare config for each CF explicitly. Use millisecond resolution for the coarse Instant. Only collect properties for DBEntryType::Put. Allow coarse error.  Improved  Check approximate size before split check. Get snapshot in batches for coprocessor. Get snapshot in batches for endpoint. Use separated thread to flush metrics. Use histogram coarse timer. Upgrade Rocksdb to 5.6.2. Deprecate v1 snapshot. Reorganize the server start up process for PD. Broadcast when there is pending conf change.  "},
		{"url": "https://pingcap.com/blog/2017-08-15-multi-raft/",
		"title": "The Design and Implementation of Multi-raft", 
		"content": " (Email: tl@pingcap.com) Placement Driver Raftstore  Region RocksDB / Keys Prefix Peer Storage Peer Multi-raft  Summary  Placement Driver Placement Driver (PD), the global central controller of TiKV, stores the metadata information of the entire TiKV cluster, generates Global IDs, and is responsible for the scheduling of TiKV and the global TSO time service.PD is a critical central node. With the integration of etcd, it automatically supports the distributed scaling and failover as well as solves the problem of single point of failure. We will write another article to thoroughly introduce PD.In TiKV, the interaction with PD is placed in the pd directory. You can interact with PD with your self-defined RPC and the protocol is quite simple. In pd/mod.rs, we provide the Client trait to interact with PD and have implemented the RPC Client.The Client trait of PD is easy to understand, most of which are the set/get operations towards the metadata information of the cluster. But you need to pay extra attention to the operations below:bootstrap_cluster: When we start a TiKV service, we should firstly find out whether the TiKV cluster has been bootstrapped through is_cluster_bootstrapped. If not, then create the first region on this TiKV service.region_heartbeat: Region reports its related information to PD regularly for the subsequent scheduling. For example, if the number of peers reported to PD is smaller than the predefined number of replica, then PD adds a new Peer replica to this Region.store_heartbeat: Store reports its related information to PD regularly for the subsequent scheduling. For example, Store informs PD of the current disk size and the free space. If PD considers it inadequate, it will not migrate other Peers to this Store.ask_split/report_split: When a Region needs to split, it will inform PD through ask_split and PD then generates the ID of the newly-split Region. After split successfully, Region informs PD through report_split.By the way, we will make PD support gRPC protocol in the future, so the ClientAPI will have some changes.Back to the topRaftstore The goal of TiKV is to support 100 TB+ data and it is impossible for one Raft group to make it, we need to use multiple Raft groups, which is Multi-raft. In TiKV, the implementation of Multi-raft is completed in Raftstore and you can find the code in the raftstore/store directory.Region To support Multi-raft, we perform data sharding and make each Raft store a portion of data.Hash and Range are commonly used for data sharding. TiKV uses Range and the main reason is that Range can better aggregate keys with the same prefix, which is convenient for operations like scan. Besides, Range outperforms in split/merge than Hash. Usually, it only involves metadata modification and there is no need to move data around.The problem of Range is that a Region may probably become a performance hotspot due to frequent operations. But we can use PD to schedule these Regions onto better machines.To sum up, we use Range for data sharding in TiKV and split them into multiple Raft Groups, each of which is called a Region.Below is the protocol definition of Region’s protbuf:message RegionEpoch { optional uint64 conf_ver = 1 [(gogoproto.nullable) = false]; optional uint64 version = 2 [(gogoproto.nullable) = false]; } message Region { optional uint64 id = 1 [(gogoproto.nullable) = false]; optional bytes start_key = 2; optional bytes end_key = 3; optional RegionEpoch region_epoch = 4; repeated Peer peers = 5; } message Peer { optional uint64 id = 1 [(gogoproto.nullable) = false]; optional uint64 store_id = 2 [(gogoproto.nullable) = false]; } region_epoch: When a Region adds or deletes Peer or splits, we think that this Region’s epoch has changed. RegionEpoch’s conf_ver increases during ConfChange while version increases during split/merge.id: Region’s only indication and PD allocates it in a globally unique way.start_key, end_key: Stand for the range of this Region [start_key, end_key). To the very first region, start and end key are both empty, and TiKV handles it in a special way internally.peers: The node information included in the current Region. To a Raft Group, we usually have three replicas, each of which is a Peer. Peer’s id is also globally allocated by PD and store_id indicates the Store of this Peer.Back to the topRocksDB / Keys Prefix In terms of actual data storage, whether it’s Raft Metadata, Log or the data in State Machine, we store them inside a RocksDB instance. More information about RocksDB, please refer tohttps://github.com/facebook/rocksdb.We use different prefixes to differentiate data of Raft and State Machine. For detailed information, please refer to raftstore/store/keys.rs. As for the actual data of State Machine, we add &amp;ldquo;z&amp;rdquo; as the prefix and for other metadata stored locally, including Raft, we use the 0x01 prefix.I want to highlight the Key format of some important metadata and I’ll skip the first 0x01 prefix. 0x01: To store StoreIdent. Before initializing this Store, we store information like its Cluster ID and Store ID into this key. 0x02: To store some information of Raft. 0x02 is followed by the ID of this Raft Region (8-byte big endian) and a Suffix to identify different subtypes. 0x01: Used to store Raft Log, followed by Log Index (8-byte big endian) 0x02: Used to store RaftLocalState 0x03: Used to store RaftApplyState 0x03：Used to store some local metadata of Region. 0x03 is followed by the Raft Region ID and a Suffix to represent different subtypes. 0x01: Used to store RegionLocalState  Types mentioned above are defined in protobuf:message RaftLocalState { eraftpb.HardState hard_state = 1; uint64 last_index = 2; } message RaftApplyState { uint64 applied_index = 1; RaftTruncatedState truncated_state = 2; } enum PeerState { Normal = 0; Applying = 1; Tombstone = 2; } message RegionLocalState { PeerState state = 1; metapb.Region region = 2; } RaftLocalState: Used to store HardState of the current Raft and the last Log Index.RaftApplyState: Used to store the last Log index that Raft applies and some truncated Log information.RegionLocalStaste: Used to store Region information and the corresponding Peer state on this Store. Normal indicates that this Peer is normal, Applying means this Peer hasn’t finished the apply snapshot operation and Tombstone shows that this Peer has been removed from Region and cannot join in Raft Group.Back to the topPeer Storage We use Raft through RawNode because one Region corresponds to one Raft Group. Peer in Region corresponds to one Raft replica. Therefore, we encapsulate operations towards RawNode in Peer.To use Raft, we need to define our storage and this can be implemented in the PeerStorage class of raftstore/store/peer_storage.rs.When creating PeerStorage, we need to get the previous RaftLocalStat, RaftApplyState and last_term of this Peer from RocksDB. These will be cached to memory for the subsequent quick access.Below requires extra attention:The value of both RAFT_INIT_LOG_TERM and RAFT_INIT_LOG_INDEX is 5 (as long as it&amp;rsquo;s larger than 1). In TiKV, there are several ways to create a Peer: Create actively: In general, for the first Peer replica of the first Region, we use this way and set its Log Term and Index as 5 during initialization. Create passively: When a Region adds a Peer replica and this ConfChange command has been applied, the Leader will send a Message to the Store of this newly-added Peer. When the Store receives this Message and confirms its legality, and finds that there is no corresponding Peer, it will create a corresponding Peer. However, at that time, this Peer is an uninitialized and any information of its Region is unknown to us, so we use 0 to initialize its Log Term and Index. Leader then will know this Follower has no data (there exists a Log notch from 0 to 5) and it will directly send snapshot to this Follower. Create when splitting: When a Region splits into two Regions, one of the Regions will …"},
		{"url": "https://pingcap.com/blog-cn/tidb-jepsen/",
		"title": "当 TiDB 遇上 Jepsen", 
		"content": " 本篇文章主要介绍 TiDB 是如何使用分布式一致性验证框架 Jepsen 进行一致性验证的。什么是 Jepsen Jepsen 是由 Kyle Kingsbury 采用函数式编程语言 Clojure 编写的验证分布式系统一致性的测试框架，作者使用它对许多著名的分布式系统（etcd, cockroachdb&amp;hellip;）进行了“攻击”（一致性验证），并且帮助其中的部分系统找到了 bug。这里一系列的博客展示了作者的验证过程以及对于一致性验证的许多思考。Jepsen 如何工作 Jepsen 验证系统由 6 个节点组成，一个控制节点（control node），五个被控制节点（默认为 n1, n2, n3, n4, n5），控制节点将所有指令发送到某些或全部被控制节点，这些指令包括底层的 shell 命令到上层的 SQL 语句等等。Jepsen 提供了几个核心 API 用于验证分布式系统： DBDB 封装了所验证的分布式系统下载、部署、启动和关闭命令，核心函数由 setup 和 teardown 组成，在 TiDB 的 Jepsen 测试中，setup 负责下载 TiDB 并且依次启动 Placement Driver、TiKV 和 TiDB；teardown 负责关闭整个 TiDB 系统并且删除日志。 ClientClient 封装了每一个测试所需要提供的客户，每个 client 提供两个接口：setup 和 invoke，setup 负责对 TiDB 进行连接，而 invoke 则包含了测试中 client 对 TiDB 调用的 sql 语句，具体语句依测试而定。 CheckerChecker 用于对测试生成的历史进行验证，判断测试结果是否符合预期，历史的格式如下图所示： NemesisNemesis 用于对系统引入故障，比如常见的网络分区、网络延时、节点宕机，在 TiDB 的测试中，有以下几种 nemesis：parts：网络分区 majority-ring：每个节点都看到不同的 majority start-stop：对某些节点进行 SIGSTOP start-kill：对某些节点进行 SIGKILL 下图展示了 parts nemesis 引入测试中后某些语句执行时出现了 time-out 的错误。 GeneratorGenerator 是 Jepsen 中的事件发生器，它将 Client 和 Nemesis 的操作交织在一起，为整个测试生成具体的执行语句。  TiDB 中的 Jepsen 测试 TiDB 中的 Jepsen 测试有 3 个，分别是 bank、set 和 register 测试。Bank Test 银行测试用于验证快照隔离。这个测试模拟了一个银行系统中的各种转账，每个银行系统的初始可以是这样的：[1 10] [2 10] [3 10] [4 10] [5 10] 1-5 分别代表账户名称，而 10 代表账户余额。测试会随机生成转账信息：[1 2 5] 代表将金额 5 从账户 1 转入账户 2 这个操作。与此同时，测试会随机读取所有账户的存款信息，例如某一时刻账户的存款信息可能是这样的：[8 14 2 11 15] 下面是测试进行中的某次截图：在快照隔离下，所有的转账都必须保证每一时刻所有账户的总金额是相同的。TiDB 在即使引入了各种 nemesis 的情况下仍旧顺利地通过了测试。Set Test 这个测试从不同节点并发的将不同的数插入一张表中，并且进行一次最终的表读取操作，用于验证所有返回成功的插入值一定会出现在表中，然后所有返回失败的插入值一定不在表中，同时，因为 nemesis 的引入，对于那些返回 time-out 的插入值，它们可能出现也可能不会出现在表中，这属于正常情况。下面是测试进行中的某次截图：同样，TiDB 通过了测试。Register Test 这个测试很好理解，建一个表，然后插入一条值，然后我们把这个值看做是一个寄存器，然后在测试中并发地从各个节点对其进行 read、write 和 cas 操作。然后利用 Jepsen 产生的一系列操作历史（如上图）进行 Linearizability 一致性验证。这个算法是 Jepsen 的核心，也是 Jepsen 被业界所熟知的原因之一，所以花时间去深入学习了下，我会在另一篇文章具体介绍这个算法。写在最后 每次 TiDB 更新代码，我们都会内部触发 CI 来执行 Jepsen，通过 Jepsen 来保证 TiDB 的数据一致性。如果你对分布式测试，一致性验证感兴趣，欢迎参与开发。TiDB Jepsen：https://github.com/pingcap/jepsen/tree/master/tidb"},
		{"url": "https://pingcap.com/weekly/2017-08-14-tidb-weekly/",
		"title": "Weekly update (August 07 ~ August 13, 2017)", 
		"content": " Weekly update in TiDB Last week, we landed 78 PRs in the TiDB repositories.Added  Enable pushing down the following operations to TiKV.  the JSON function ifnull minus and multiply  Use the Delete-in-Range feature to speed up the Drop Database/Table/Index operations. Support prioritizing statements.  Fixed  Fix the TimeDiff compatibility issue. Consider charset in Right/Left/Substr. Do not record metrics when running intenal SQL. Fix potential issue of schema validation check. Add desc order info in explain.  Improved  Refactor the row structure to reduce memory allocation. Refactor the following expressions/builtin-functions evaluation framework:  Plus Multiply Minus lpad/rpad bin from_base64 char reverse ifnull instr aes_encrypt/aes_decrypt is_true/is_false locate last_insert_id compress/uncompress/uncompress_length sleep conv char_length md5/sha1/sha2 charactor_length elt ifnull if round/abs random_bytes unaryplus  Adjust cost for the Sort operator. Change the row format from structure to Datum slice.  Weekly update in TiKV Last week, We landed 28 PRs in the TiKV repositories.Added  Add hex/escaped converting for tikv-ctl. Add custom instant for time utilities. Add rows properties. Add size properties. Add region approximate size. Support basic DAG expression evaluation for coprocessor. Support the delete range KV command. Add the diff command for tikv-ctl to check the difference between 2 databases.  Fixed  Fix a bug caused by using box_try for the coprocessor.  Improved  Speed up clearing meta. Reduce callback by batching RaftCmdRequest. Refactor the configuration. Refactor the DAG and old selection. Refactor the time utilities.  "},
		{"url": "https://pingcap.com/blog/2017-08-08-tidbforyuanfudao/",
		"title": "How TiDB tackles fast data growth and complex queries for yuanfudao.com", 
		"content": " TiDB use case Yuanfudao.com is an online tutoring service targeting the K-12 educational segment in China with the largest number of elementary and secondary school student users. It owns three applications, Yuantiku (猿题库), the online question bank, Xiaoyuansouti (小猿搜题), the application for question search by taking pictures, and yuanfudao.com, an online tutoring service.So far, the Yuanfudao APPs have more than 1.16 million paying users and provide live tutoring courses of English and Math Olympiad to the elementary users, as well as all the subjects for secondary school students. With yuanfudao.com, students from every corner of China can enjoy high-quality courses from top teachers at home.The enormous amount of data in the question bank, the audio and video learning materials, and all the user data and log call for a high level of storage and processing capacity of yuanfudao.com’s backend system.yuanfudao.com’s business scenario requires the following features from its backend system: The storage system should be able to scale out flexibly to serve the large data volume and rapid data growth. Be able to meet the complex queries and BI related requirements, and can perform real-time analysis based on indexes like cities and channels. The system must be highly available, can automatically failover and is easy to maintain.  In the early stage of solution evaluation and selection, yuanfudao.com considered the standalone MySQL solution but then gave up the idea because of the following reasons: They perceived that with the fast development of their business, the data storage capacity and concurrency stress would soon hit the processing bottleneck of a standalone database. If adding a sharding solution to MySQL, the sharding key must be specified, which would not support cross-shard distributed transactions. Not to mention that the proxy solution is intrusive to the business tier and developers must know clearly the partitioning rules, which makes it unable to achieve transparency. Sharding is difficult to implement cross-shard aggregate queries, such as correlated query, subquery and group-by aggregation of the whole table. In these business scenarios, the query complexity is passed on to the application developers. Even though some middleware can implement simple join support, there is still no way to guarantee the correctness of these queries. The broadcasting solution cannot scale and the overhead would be huge when the cluster becomes larger. For a business with a relatively large data volume, the problem of locking table for DDL on traditional RDBMS would be serious with a quite long lock time. If using some third-party tools like gh-ost to implement non-blocking DDL, the extra space overhead would be large and manual intervention would still be needed to guarantee the data consistency. What might make things worse is that the system might jitter during the switch process. It is safe to say that the maintenance complexity will increase exponentially with more and more machines while the scaling complexity is directly passed on to DBA.  In the end, the backend developers of yuanfudao.com decided to use a distributed storage solution and after researching quite a few community solutions, they found TiDB, a distributed relational database.TiDB is an open source distributed Hybrid Transactional/Analytical Processing (HTAP) database. It features horizontal scalability, strong consistency, and high availability. Users can regard TiDB as a standalone database with an infinite storage capacity. TiDB is nonintrusive to business and can elegantly replace the traditional sharding solutions such as database middleware and database sharding while at the same time maintaining the ACID properties of transactions. Instead of paying too much attention to the details of database scaling, developers are freed to focus on business development, which greatly improves the R&amp;amp;D productivity.As the complicated distributed transactions and data replication are supported by the underlying storage engine, developers just need to concentrate on the business logic and creating values.The following table outlines the difference between MySQL sharding solutions and TiDB:   MySQL Sharding TiDB   ACID Transaction No Yes   Online Scalability No Yes   Complex Query No Yes   Failover Manual Auto   MySQL Compatibility Low High   (Comparison between TiDB and traditional MySQL sharding solutions)TiDB cluster consists of three components: TiDB Server, TiKV Server, and PD Server.(the overall architecture of TiDB)TiDB Server is responsible for processing SQL request. When the business grows, adding more TiDB Server nodes can improve the entire processing capacity and offer a higher throughput.TiKV is responsible for storing data. When the data volume grows, deploying more TiKV Server nodes can directly increase the data storage capacity.PD schedules among the TiKV nodes in Regions and migrates a portion of data to the newly-added node. Therefore, in the early stage, users can deploy a few service instances and add more TiKV or TiDB instances if needed, depending on the data volume.For the deployment in a production environment, yuanfudao.com chose an architecture of 2 TiDB + 3 TiKV + 3 PD for the condition of 5 million rows of data volume per day, hundreds of millions of records in the routing database and the peak QPS is about 1000. It is noted that the architecture scales as the business data volume grows.The client end of yuanfudao.com collects data about the audio and video quality of live streaming, such as packet loss, latency, and quality grading. Then the client end sends these data to the server and the latter stores all data in TiDB.Guo Changzhen, the R&amp;amp;D Vice President of yuanfudao.com, expresses his appreciation towards TiDB: &amp;ldquo;TiDB is an ambitious project and solves the scaling problem of MySQL from scratch. It also has the OLAP capacity in many scenarios, saving the cost of building and learning a data warehouse, which is quite popular in the business tier.&amp;rdquo; As a next step, yuanfudao.com plans to synchronize through Syncer, then merge and perform statistical analysis to other sharding businesses.There are many other similar use cases like yuanfudao.com. With the rapid development of the Internet, plenty of businesses are booming and TiDB can meet their needs with its flexible scaling capacity."},
		{"url": "https://pingcap.com/weekly/2017-08-07-tidb-weekly/",
		"title": "Weekly update (July 31 ~ August 06, 2017)", 
		"content": " Weekly update in TiDB Last week, we landed 54 PRs in the TiDB repositories.Added  Support natural join. Add a switch for enabling cost-based optimizor. Assign low priority for SQL with full table scan and high priority for SQL with point get. Support N which is the shotcut of null. Support TIMESTAMP in the get_format function. Add a flag to enable TCP keep-alive. Support DISTINCTROW.  Fixed  Truncate the trailing spaces for &amp;ldquo;CHAR[(M)]&amp;rdquo; types Fix float point parsing with leading dot.  Improved  Refactor the row structure: to reduce memory allocation. Refactor the following expression/builtin-function evaluation framework:  trim rtrim  Check schema changing more precisely. Adjust the cost of index join.  Weekly update in TiKV Last week, We landed 21 PRs in the TiKV repositories.Added  Add priority for coprocessor thread pool. Add ceil_real function for coprocessor. Add remove function for JSON. Dynamically set label by PD API. Support delete member by id. Add more util functions.  Fixed  Stop the resolver thread explicitly.  Improved  Refactor the storage configuration field. Deny the http prefix for PD addresses. Refactor the properties collector. Improve the storage test. Use the MinOverlappingRatio compaction priority by default.  "},
		{"url": "https://pingcap.com/meetup/meetup-2017-08-05/",
		"title": "【Infra Meetup NO.53】知乎数据平台实践", 
		"content": " 今天的 Meetup，我们邀请到了知乎数据平台负责人王雨舟为大家做《知乎数据平台实践》的技术分享。 又是一个美好的周末，勤劳的小蜜蜂们早早出来参加活动了~🙂 今天的活动现场又是爆满~ 感觉要换地儿的节奏啊~今天 Meetup 的开场，我司联合创始人兼 CTO 黄东旭同学首先为大家分享了 TiDB 项目的最新进展。黄东旭同学好开心的样子，因为就在昨天，TiDB 正式发布 RC4 版 。开场过后，接下来由知乎数据平台负责人王雨舟（江湖人称宇宙哥）开始为大家做技术分享。宇宙哥真是 PingCAP 的真爱粉儿~ 穿着我司的文化衫亮相活动现场，超级有气场~以下是部分技术干货分享，Enjoy~宇宙哥在演讲开始先介绍了知乎大数据平台的整体架构情况并讲解了埋点流程及使用 Protobuf 做埋点标准化规范除此之外，宇宙哥还从以下几点来分析介绍 Druid在知乎的实践： 自定义多维分析功能和留存分析功能； 如何做到实时数据分析； 自定义指标、维度、报表、文件夹、Dashboard。  我是花边分割线这张 PPT 中有眼熟的部分哦😏宇宙哥用“丝般顺滑”总结了自己现在使用 TiDB 的感受，并表达了对 TiSpark 的期待✌️分享结束后，显然大家都还没有尽兴，接下来是一段时长堪比分享环节的 QA。激烈的讨论后现场小伙伴跟宇宙哥都嗨了，还没有嗨够的小伙伴我们下次见~"},
		{"url": "https://pingcap.com/blog/2017-08-04-rc4/",
		"title": "TiDB RC4 Release Notes", 
		"content": " Highlight:  For performance, the write performance is improved significantly, and the computing task scheduling supports prioritizing to avoid the impact of OLAP on OLTP. The optimizer is revised for a more accurate query cost estimating and for an automatic choice of the Join physical operator based on the cost. Many enhancements have been introduced to be more compatible with MySQL. TiSpark is now released to better support the OLAP business scenarios. You can now use Spark to access the data in TiKV.  Detailed updates: TiDB:  The SQL query optimizer refactoring: Better support for TopN queries Support the automatic choice of the of the Join physical operator based on the cost Improved Projection Elimination  The version check of schema is based on Table to avoid the impact of DDL on the ongoing transactions Support BatchIndexJoin Improve the Explain statement Improve the Index Scan performance Many enhancements have been introduced to be more compatible with MySQL Support the JSON type and operations Support the configuration of query prioritizing and isolation level  Placement Driver (PD):  Support using PD to set the TiKV location labels Optimize the scheduler PD is now supported to initialize the scheduling commands to TiKV. Accelerate the response speed of the region heartbeat. Optimize the balance algorithm  Optimize data loading to speed up failover  TiKV:  Support the configuration of query prioritizing Support the RC isolation level Improve Jepsen test results and the stability Support Document Store Coprocessor now supports more pushdown functions Improve the performance and stability  TiSpark Beta Release:  Implement the prediction pushdown Implement the aggregation pushdown Implement range pruning Capable of running full set of TPC-H except one query that needs view support  "},
		{"url": "https://pingcap.com/weekly/2017-07-31-tidb-weekly/",
		"title": "Weekly update (July 24 ~ July 30, 2017)", 
		"content": " Weekly update in TiDB Last week, we landed 50 PRs in the TiDB repositories.Added  Add a debugging tool for transaction inspection. Support two JSON syntactic sugars. Support renaming multiple tables in a single statement.  Fixed  Fix a bug in the update statement when doing the alter table after statement. Fix signed integer overflow in the minus unary scalar function. Fix the content in the information_schema for unsigned columns. Fix inserting zero length data into a column with zero field length. Check the field length limitation in the alter table statement. Add the field length limitation for the index columns. The password builtin function should return empty string when meeting null argument. Check the table name with white space in the end.  Improved  Add more DDL test cases. Refactor the explain statement. Refactor the following expression/builtin-function evaluation framework:  pi logicalAnd cot exp logicalXor bitAnd logicalOr bitOr bitXor leftShift rightShift bitNeg  Extract configuration related code to the config package. Improve the unit test coverage:  ast package  Fix a bug of using undefined user variable. Make go vet happy.  New Contributor (Thanks!)  Jiaxing Liang Hu Ming Wei Fu Liao Qiang  Weekly update in TiKV Last week, We landed 27 PRs in the TiKV repositories.Added  Add the transaction debugging gRPC API. Add the ceil, abs functions for coprocessor. Add the compaction priority for RocksDB. Handle thenull datum for coprocessor.  Fixed  Decrease the load limit to 10000. Fix a bug when using offset in expressions for coprocessor. Fix trace_size computing. Let Travis decide which compiler to use automatically. Fix a message delay bug. Avoid unnecessary clones when sending Raft messages.  Improved  Log more detailed error when transaction fails. Refactor RocksDB options. Implement serde for enum and readable types. Use the FIFO queue for coprocessor work queue. Decrease default leader-schedule-limit to 64. Update distinct score calculation for different stores. Enhance the PD join function.  New contributors  ariesdevil bailaohe dantin  "},
		{"url": "https://pingcap.com/blog/2017-07-28-raftintikv/",
		"title": "A TiKV Source Code Walkthrough - Raft in TiKV", 
		"content": " (Email: tl@pingcap.com)Table of content  Architecture Raft  Storage Config RawNode   Architecture Below is TiKV’s overall architecture:Placement Driver: Placement Driver (PD) is responsible for the management scheduling of the whole cluster.Node: Node can be regarded as an actual physical machine and each Node is responsible for one or more Store.Store: Store uses RocksDB to implement actual data storage and usually one Store corresponds to one disk.Region: Region is the smallest unit of data movement and it refers to the actual data extent in Store. Each Region has multiple replicas, each of which is placed in different Stores and these replicas make up a Raft group.Back to the topRaft TiKV uses the Raft algorithm to implement the strong consistency of data in a distributed environment. For detailed information about Raft, please refer to the paper In Search of an Understandable Consensus Algorithm and the official website. Simply put, Raft is a model of replication log + State Machine. We can only write through a Leader and the Leader will replicate the command to its Followers in the form of log. When the majority of nodes in the cluster receive this log, this log has been committed and can be applied into the State Machine.TiKV’s Raft mainly migrates etcd Raft and supports all functions of Raft, including: Leader election Log replicationLog compaction Membership changesLeader transfer Linearizable / Lease read  Note that how TiKV and etcd process membership change is different from what is in the Raft paper. TiKV’s membership change will take effect only when the log is applied. The main purpose is for a simple implementation. But it will be risky if we only have two nodes. Since we have to remove one node from inside and if a Follower has not received the log entry of ConfChange, the Leader will go down and be unrecoverable, then the whole cluster will be down. Therefore, it is recommended that users deploy 3 or more odd number of nodes.The Raft library is independent and users can directly embed it into their applications. What they need to do is to process storage and message sending. This article will briefly introduce how to use Raft and you can find the code under the directory of TiKV source code /src/raft.Storage First of all, we need to define our Storage, which is mainly used for storing relevant data of Raft. Below is the trait definition:pub trait Storage { fn initial_state(&amp;amp;self) -&amp;gt; Result&amp;lt;RaftState&amp;gt;; fn entries(&amp;amp;self, low: u64, high: u64, max_size: u64) -&amp;gt; Result&amp;lt;Vec&amp;lt;Entry&amp;gt;&amp;gt;; fn term(&amp;amp;self, idx: u64) -&amp;gt; Result&amp;lt;u64&amp;gt;; fn first_index(&amp;amp;self) -&amp;gt; Result&amp;lt;u64&amp;gt;; fn last_index(&amp;amp;self) -&amp;gt; Result&amp;lt;u64&amp;gt;; fn snapshot(&amp;amp;self) -&amp;gt; Result&amp;lt;Snapshot&amp;gt;; } We need to implement our Storage trait and I’ll elaborate on the implication of each interface:initial_state: Call this interface when initializing Raft Storage and it will return RaftState, whose definition is shown below:pub struct RaftState { pub hard_state: HardState, pub conf_state: ConfState, } HardState and ConfState is protobuf defined as follows:message HardState { optional unit64 term = 1; optional unit64 vote = 2; optional unit64 commit = 3; } message ConfState { repeated unit64 nodes = 1; } HardState stores the following information: the last saved term information of this Raft node which node was voted the log index that is already committed.  ConfState saves all node ID information of the Raft cluster.When calling relevant logic of Raft from outside, users need to handle the persistence of RaftState.entries: Get the Raft log entry of the [low, high) interval and controls the maximum number of the returned entries through max_size.term, first_index and last_index **refers to getting the current term, the smallest and the last log index respectively.snapshot: Get a snapshot of the current Storage. Sometimes, the amount of the current Storage is large and it takes time to create a snapshot. Then we have to asynchronously create it in another thread, so that the current Raft thread will not be clocked. At this time, the system can return SnapshotTemporarilyUnavailable error so that Raft will know snapshot is being prepared and will try again after a while.Note that the above Storage interface is just for Raft. But actually we also use this Storage to store data like Raft log and so we need to provide other interfaces, such as MemStorage in Raft storage.rs for testing. You can refer to MemStorage to implement your Storage.Back to the topConfig Before using Raft, we need to know some relevant configuration of Raft. Below are the items that need extra attention in Config:pub struct Config { pub id: u64, pub election_tick: usize, pub heartbeat_tick: usize, pub applied: u64, pub max_size_per_msg: u64, pub max_inflight_msgs: usize, } id: The unique identification of the Raft node. Within a Raft cluster, id has to be unique. Inside TiKV, the global uniqueness of id is guaranteed through PD.election_tick: When a Follower hasn’t received the message sent by its Leader after the election_tick time, then there will be a new election and TiKV uses 50 as the default.heartbeat_tick: The Leader sends a heartbeat message to its Follower every hearbeat_tick. The default value is 10.applied: It is the log index that was last applied.max_size_per_msg: Limit the maximum message size to be sent each time. The default value is 1MB.max_inflight_msgs: Limit the maximum number of in-flight message in replication. The default value is 256.Here is the detailed implication of tick: TiKV’s Raft is timing-driven. Assume that we call the Raft tick once every 100ms and when we call the tick times of headtbeat_tick, the Leader will send heartbeats to its Follower.Back to the topRawNode We use Raft through RawNode and below is its constructor:pub fn new(config: &amp;amp;Config, sotre: T, peers: &amp;amp;[peer]) -&amp;gt; Result&amp;lt;RawNode&amp;lt;T&amp;gt;&amp;gt; We need to define Raft’s Config and then pass an implemented Storage. The peers parameter is just used for testing and it will not be passed. After creating the RawNode object, we can use Raft. Below are some functions that we pay attention to:tick: We use the tick function to drive Raft regularly. In TiKV, we call tick once every 100ms.propose: The Leader writes the command sent by client to the Raft log through the propose command and replicates to other nodes.propose_conf_change: Like propose, this function is just used for handling the ConfChange command.step: When the node receives the message sent by other nodes, this function actively calls the driven Raft.has_ready: Used to determine whether a node is ready.ready: Get the ready state of the current node. Before that, we will use has_ready to determine whether a RawNode is ready.apply_conf_change: When a log of ConfChange is applied successfully, we need to actively call this driven Raft.advance: Tell Raft that ready has been processed and it’s time to start successive iterations.As for RawNode, we should emphasize the ready concept and below is its definition:pub struct Ready { pub ss: Option&amp;lt;SoftState&amp;gt;, pub hs: Option&amp;lt;HardState&amp;gt;, pub entries: Vec&amp;lt;Entry&amp;gt;, pub snapshot: Snapshot, pub committed_entries: Vec&amp;lt;Entry&amp;gt;, pub messages: Vec&amp;lt;Message&amp;gt;,	} ss: If SoftState has changes, such as adding or deleting a node, ss will not be empty.hs: If HardState has changes, such as re-voting or term increasing, hs will not be empty.entries: Needs to be stored in Storage before sending messages.snapshot: If snapshot is not empty, it needs to be stored in Storage.committed_entries: The Raft log that has been committed can be applied to State Machine.messages: Usually, the message sending to other nodes cannot be sent until entries are saved successfully. But to a Leader, it can send messages first before saving entries. This is the optimization method introduced in the Raft paper and is also what TiKV adopts.When the outside finds that a …"},
		{"url": "https://pingcap.com/blog-cn/tispark/",
		"title": "TiSpark (Beta) 用户指南", 
		"content": " TiSpark 是 PingCAP 推出的为了解决用户复杂 OLAP 需求的产品。借助 Spark 平台本身的优势，同时融合 TiKV 分布式集群的优势，和 TiDB 一起为用户一站式解决 HTAP （Hybrid Transactional/Analytical Processing）需求。 TiSpark 依赖 TiKV 集群和 PD 的存在。当然，TiSpark 也需要你搭建一个 Spark 集群。本文简单介绍如何部署和使用 TiSpark。本文假设你对 Spark 有基本认知。你可以参阅 Apache Spark 官网 了解 Spark 相关信息。一、概述 TiSpark 是将 Spark SQL 直接运行在 TiDB 存储引擎 TiKV 上的 OLAP 解决方案。TiSpark 架构图如下： TiSpark 深度整合了 Spark Catalyst 引擎, 可以对计算提供精确的控制，使 Spark 能够高效的读取 TiKV 中的数据，提供索引支持以实现高速的点查； 通过多种计算下推减少 Spark SQL 需要处理的数据大小，以加速查询；利用 TiDB 的内建的统计信息选择更优的查询计划。 从数据集群的角度看，TiSpark + TiDB 可以让用户无需进行脆弱和难以维护的 ETL，直接在同一个平台进行事务和分析两种工作，简化了系统架构和运维。 除此之外，用户借助 TiSpark 项目可以在 TiDB 上使用 Spark 生态圈提供的多种工具进行数据处理。例如使用 TiSpark 进行数据分析和 ETL；使用 TiKV 作为机器学习的数据源；借助调度系统产生定时报表等等。  二、环境准备 现有 TiSpark 版本支持 Spark 2.1，对于 Spark 2.0 及 Spark 2.2 还没有经过良好的测试验证。对于更低版本暂时无法支持。TiSpark 需要 JDK 1.8+ 以及 Scala 2.11（Spark2.0+ 默认 Scala 版本）。TiSpark 可以在 YARN，Mesos，Standalone 等任意 Spark 模式下运行。三 、推荐配置 3.1 部署 TiKV 和 TiSpark 集群 3.1.1 TiKV 集群部署配置 对于 TiKV 和 TiSpark 分开部署的场景，建议参考如下建议 硬件配置建议  普通场景可以参考 TiDB 和 TiKV 硬件配置建议，但是如果是偏重分析的场景，可以将 TiKV 节点增加到至少 64G 内存，如果是机械硬盘，则推荐 8 块。 TiKV 参数建议  [server] end-point-concurrency = 8 # 如果使用场景偏向分析，则可以考虑扩大这个参数 [raftstore] sync-log = false [rocksdb] max-background-compactions = 6 max-background-flushes = 2 [rocksdb.defaultcf] block-cache-size = &amp;#34;10GB&amp;#34; [rocksdb.writecf] block-cache-size = &amp;#34;4GB&amp;#34; [rocksdb.raftcf] block-cache-size = &amp;#34;1GB&amp;#34; [rocksdb.lockcf] block-cache-size = &amp;#34;1GB&amp;#34; [storage] scheduler-worker-pool-size = 4 3.1.2 Spark / TiSpark 集群独立部署配置 关于 Spark 的详细硬件推荐配置请参考官网，如下是根据 TiSpark 场景的简单阐述。Spark 推荐 32G 内存以上配额。请在配置中预留 25% 的内存给操作系统。Spark 推荐每台计算节点配备 CPU 累计 8 到 16 核以上。你可以初始设定分配所有 CPU 核给 Spark。Spark 的具体配置方式也请参考官方说明。下面给出的是根据 spark-env.sh 配置的范例：SPARK_EXECUTOR_MEMORY=32g SPARK_WORKER_MEMORY=32g SPARK_WORKER_CORES=8 3.1.3 TiSpark 与 TiKV 集群混合部署配置 对于 TiKV、TiSpark 混合部署场景，请在原有 TiKV 预留资源之外累加 Spark 所需部分并分配 25% 的内存作为系统本身占用。四、部署 TiSpark TiSpark 的 jar 包可以在这里下载。4.1 已有 Spark 集群的部署方式 在已有 Spark 集群上运行 TiSpark 无需重启集群。可以使用 Spark 的 &amp;ndash;jars 参数将 TiSpark 作为依赖引入:spark-shell --jars $PATH/tispark-0.1.0.jar如果想将 TiSpark 作为默认组件部署，只需要将 TiSpark 的 jar 包放进 Spark 集群每个节点的 jars 路径并重启 Spark 集群：${SPARK_INSTALL_PATH}/jars这样无论你是使用 Spark-Submit 还是 Spark-Shell 都可以直接使用 TiSpark。4.2 没有 Spark 集群的部署方式 如果你没有使用中的 Spark 集群，我们推荐 Spark Standalone 方式部署。我们在这里简单介绍下 Standalone 部署方式。如果遇到问题，你可以去官网寻找帮助；也欢迎在我们的 GitHub 上提 issue。4.2.1 下载安装包并安装 你可以在这里下载 Apache Spark。对于 Standalone 模式且无需 Hadoop 支持，请选择 Spark 2.1.x 且带有 Hadoop 依赖的 Pre-build with Apache Hadoop 2.x 任意版本。如你有需要配合使用的 Hadoop 集群，请选择对应的 Hadoop 版本号。你也可以选择从源代码自行构建以配合官方 Hadoop 2.6 之前的版本。请注意目前 TiSpark 仅支持 Spark 2.1.x 版本。假设你已经有了 Spark 二进制文件，并且当前 PATH 为 SPARKPATH。请将 TiSpark jar 包拷贝到 ${SPARKPATH}/jars 目录下。4.2.2 启动 Master 在选中的 Spark Master 节点执行如下命令：cd $SPARKPATH ./sbin/start-master.sh 在这步完成以后，屏幕上会打印出一个 log 文件。检查 log 文件确认 Spark-Master 是否启动成功。 你可以打开 http://spark-master-hostname:8080 查看集群信息（如果你没有改动 Spark-Master 默认 Port Numebr）。在启动 Spark-Slave 的时候，你也可以通过这个面板来确认 Slave 是否已经加入集群。4.2.3 启动 Slave 类似地，可以用如下命令启动 Spark-Slave节点：./sbin/start-slave.sh spark://spark-master-hostname:7077 命令返回以后，你就可以通过刚才的面板查看这个 Slave 是否已经正确的加入了 Spark 集群。 在所有 Slave 节点重复刚才的命令。在确认所有的 Slave 都可以正确连接 Master，这样之后你就拥有了一个 Standalone 模式的 Spark 集群。五、一个使用范例 假设你已经按照上述步骤成功启动了 TiSpark 集群， 下面简单介绍如何使用 Spark SQL 来做 OLAP 分析。这里我们用名为 tpch 数据库中的 lineitem 表作为范例。在 Spark-Shell 里输入下面的命令, 假设你的 PD 节点位于 192.168.1.100，端口 2379：import org.apache.spark.sql.TiContext val ti = new TiContext(spark, List(&amp;#34;192.168.1.100:2379&amp;#34;) ti.tidbMapDatabase(&amp;#34;tpch&amp;#34;) 之后你可以直接调用 Spark SQLspark.sql(&amp;#34;select count(*) from lineitem&amp;#34;).show 结果为：+-------------+ | count(1) | +-------------+ | 600000000 | +-------------+ 六、FAQ Q. 是独立部署还是和现有 Spark／Hadoop 集群共用资源？A. 你可以利用现有 Spark 集群无需单独部署，但是如果现有集群繁忙，TiSpark 将无法达到理想速度。Q. 是否可以和 TiKV 混合部署？A. 如果 TiDB 以及 TiKV 负载较高且运行关键的线上任务，请考虑单独部署 TiSpark；并且考虑使用不同的网卡保证 OLTP 的网络资源不被侵占而影响线上业务。如果线上业务要求不高或者机器负载不大，可以考虑与 TiKV 混合部署。"},
		{"url": "https://pingcap.com/blog/2017-07-24-tidbbestpractice/",
		"title": "TiDB Best Practices", 
		"content": " From Li SHEN: shenli@pingcap.comSee the following blogs (Data Storage, Computing, Scheduling) for TiDB&amp;rsquo;s principles.Table of Content  Preface Basic Concepts  Raft Distributed Transactions Data Sharding Load Balancing SQL on KV Secondary Indexes  Scenarios and Practices  Deployment Importing Data Write Query Monitoring and Log Documentation Best Scenarios for TiDB   Preface Database is a generic infrastructure system. It is important to, for one thing, consider various user scenarios during the development process, and for the other, modify the data parameters or the way to use according to actual situations in specific business scenarios.TiDB is a distributed database compatible with MySQL protocol and syntax. But with the internal implementation and supporting of distributed storage and transactions, the way of using TiDB is different from MySQL.Basic Concepts The best practices are closely related to its implementation principles. This article briefly introduces Raft, distributed transactions, data sharding, load balancing, the mapping solution from SQL to KV, implementation method of secondary indexing and distributed execution engine.Raft Raft is a consensus algorithm and ensures data replication with strong consistency. At the bottom layer, TiDB uses Raft to synchronize data. TiDB writes data to the majority of the replicas before returning the result of success. In this way, the system will definitely have the latest data even though a few replicas might get lost. For example, if there are three replicas, the system will not return the result of success until data has been written to two replicas. Whenever a replica is lost, at least one of the remaining two replicas have the latest data.To store three replicas, compared with the synchronization of Master-Slave, Raft is more efficient. The write latency of Raft depends on the two fastest replicas, instead of the slowest. Therefore, the implementation of geo-distributed and multiple active datacenters becomes possible by using the Raft synchronization. In the typical scenario of three datacenters distributing in two sites, to guarantee the data consistency, we just need to successfully write data into the local datacenter and the closer one, instead of writing to all three data-centers. However, this does not mean that cross-datacenter deployment can be implemented in any scenario. When the amount of data to be written is large, the bandwidth and latency between data-centers become the key factors. If the write speed exceeds the bandwidth or the latency is too high, the Raft synchronization mechanism still cannot work well.Back to the topDistributed Transactions TiDB provides complete distributed transactions and the model has some optimizations on the basis of Google Percolator. Here, I would just talk about two things: Optimistic LockTiDB’s transaction model uses the optimistic lock and will not detect conflicts until the commit phase. If there are conflicts, retry the transaction. But this model is inefficient if the conflict is severe because operations before retry are invalid and need to repeat. Assume that the database is used as a counter. High access concurrency might lead to severe conflicts, resulting in multiple retries or even timeouts. Therefore, in the scenario of severe conflicts, it is recommended to solve problems at the system architecture level, such as placing counter in Redis. Nonetheless, the optimistic lock model is efficient if the access conflict is not very severe. Transaction Size LimitsAs distributed transactions need to conduct two-phase commit and the bottom layer performs Raft replication, if a transaction is very large, the commit process would be quite slow and the following Raft replication flow is thus struck. To avoid this problem, we limit the transaction size: Each Key-Value entry is no more than 6MB The total number of Key-Value entry is no more than 300,000 rows The total size of Key-Value entry is no more than 100MB  There aresimilar limits on Google Cloud Spanner.  Back to the topData Sharding TiKV automatically shards bottom-layered data according to the Range of Key. Each Region is a range of Key, from a left-close-right-open interval, [StartKey, EndKey). When the amount of Key-Value in Region exceeds a certain value, it will automatically split.Load Balancing  PD will automatically balance the load of the cluster according to the state of the entire TiKV cluster. The unit of scheduling is Region and the logic is the strategy configured by PD.SQL on Key-Value TiDB automatically maps the SQL structure into Key-Value structure. For more information, please refer to Computing. Simply put, TiDB has done two things: A row of data is mapped to a Key-Value pair. Key is prefixed with TableID and suffixed with row ID. An index is mapped as a Key-Value pair. Key is prefixed with TableID+IndexID and suffixed with the index value.  As you can see, data and index in the same table have the same prefix, so that these Key-Values are at adjacent positions in the Key space of TiKV. Therefore, when the amount of data to be written is large and all is written to one table, the write hotspot is thus created. The situation gets worse when some index values of the continuous written data is also continuous (e.g. fields that increase with time, like update time), which will create a few write hotspots and become the bottleneck of the entire system. Likewise, if all data is read from a focused small range (e.g. the continuous tens or hundreds of thousands of rows of data), access hotspot of data will probably occur.Back to the topSecondary Indexes  TiDB supports the complete secondary indexes which are also global indexes. Many queries can be optimized by index. Lots of MySQL experience is also applicable to TiDB, it is noted that TiDB has its unique features. Below are a few notes when using secondary indexes in TiDB. The more secondary indexes, the better?Secondary indexes can speed up query, but adding an index has side effects. In the last section, we’ve introduced the storage model of index. For each additional index, there will be one more Key-Value when inserting a piece of data. Therefore, the more indexes, the slower the writing speed and the more space it takes up. In addition, too many indexes will influence the runtime of the optimizer. And inappropriate index will mislead the optimizer. Thus, the more secondary indexes is not necessarily the better. Which columns should create indexes?As mentioned before, index is important but the number of indexes should be proper. We need to create appropriate indexes according to the characteristics of business. In principle, we need to create indexes for the columns needed in the query, the purpose of which is to improve the performance. Below are the conditions that need to create indexes: For columns with a high degree of differentiation, the number of filtered rows is remarkably reduced though index. If there are multiple query criteria, you can choose composite indexes. Note to put the columns with equivalent condition before composite index. For example, for a commonly-used query is select * from t where c1 = 10 and c2 = 100 and c3 &amp;gt; 10, you can create a composite index Index cidx (c1, c2, c3). In this way, you can use the query criterion to create an index prefix and then Scan. The difference between query through indexes and directly scan Table   TiDB has implemented global indexes, so indexes and data of the Table are not necessarily on data sharding. When querying through indexes, it should firstly scan indexes to get the corresponding row ID and then use the row ID to get the data. Thus, this method involves two network requests and has a certain performance overhead.If the query involves lots of rows, scanning index proceeds concurrently. When the first batch of results is returned, getting the data of Table can then proceed. Therefore, this is a parallel + Pipeline model. Though the two accesses …"},
		{"url": "https://pingcap.com/weekly/2017-07-24-tidb-weekly/",
		"title": "Weekly update (July 17 ~ July 23, 2017)", 
		"content": " Weekly update in TiDB Last week, we landed 60 PRs in the TiDB repositories.Added  Add a tidb_version function to get the tidb-server version information. Support the READ COMMITTED isolation level. Support the ENCLOSED BY clause in the Load Data statement. Support creating index using type and comment.  Fixed  Fix a bug when in json_unquote. Fix field name with comment. Fix the wrong offset when using the Primary Key column as the handle.  Improved  Refactor the optimizer:  Refactoring projection elimination.  Refactor the builtin function evaluation framework:  substring_index log asin atan acos floor hex ceil unhex  Improve the unit test coverage:  table package schema.go kv package builtin_compare.go datum.go distsql package column.go expression.go scalar_function.go  Fix a bug of using undefined user variable.  Weekly update in TiKV Last week, We landed 19 PRs in the TiKV repositories.Added  Add DAG executor for coprocessor. Add json_object and json_array for coprocessor. Add the abs function for the coprocessor. Add the u64 support for JSON type. Add the max_compaction_bytes support for RocksDB. Add the TSO warning metrics for PD.  Fixed  Enlarge gRPC send message length. Remove the unix socket from PD test. Fix a bug when shrinking buffer.  Improved  Use collected properties to optimize GC. Get RocksDB snapshot lazily. Introduce rate limit to make QPS more stable. Send the apply proposals in batches. Stop supporting the old V1 snapshot format any more and support using V2 only.  "},
		{"url": "https://pingcap.com/meetup/meetup-2017-07-22/",
		"title": "【Infra Meetup NO.52】TiDB 自动化运维管理 —— TiDB-Operator", 
		"content": " 今天的 Meetup，由我司技术大拿邓栓同学为大家分享《TiDB 自动化运维管理 —— TiDB-Operator》。 今日的帝都带着一丝凉爽，如此好天气怎能辜负。小伙伴们一清早就来到互动现场，一起来吃“营养早午餐”。我司技术大拿邓栓同学激情满满的开始为大家做主题分享，主要从 TiDB-Operator 的功能介绍、整体架构、实现细节这几个纬度切入。邓栓同学开场介绍到：分布式系统由于自身的复杂性，其管理和运维通常是非常困难的事情，借助 TiDB-Operator 我们能够轻松地将 TiDB 集群部署到 Kubernetes 集群之上，并做到自动化运维管理，极大地降低了人力运维成本，现场小伙伴们听呆了～咦？what&amp;rsquo;wrong ? 黑灯瞎火嘛呢？ 其实是小伙伴们在一起很专注的看 demo 演示~活动最后，邓栓同学通过 demo 演示了 TiDB-operator bootstrap 一套完整的 TiDB 集群，然后在集群上面执行一个简单的操作就可以轻松实现扩容缩容，并且模拟物理节点挂掉时 TiDB-operator 对集群做自动恢复等各种自动化运维操作流程。以上为最新前方报道～ enjoy 😁邓栓，PingCAP SRE 工程师，Kubernetes 爱好者，目前主要负责 TiDB 与各种云平台整合。Rust 中国社区联合创始人。"},
		{"url": "https://pingcap.com/blog/2017-07-20-tidbinternal3/",
		"title": "TiDB Internal (III) - Scheduling", 
		"content": " From Li SHEN: shenli@pingcap.comTable of Content  Why scheduling The Requirements of Scheduling The Basic Operations of Scheduling Information Collecting The Policy of Scheduling The implementation of Scheduling Summary  Why scheduling? From the first blog of TiDB internal, we know that TiKV cluster is the distributed KV storage engine of TiDB database. Data is replicated and managed in Regions and each Region has multiple Replicas distributed on different TiKV nodes. Among these replicas, Leader is in charge of read/write and Follower synchronizes the raft log sent by Leader. Now, please think about the following questions: How to guarantee that multiple Replicas of the same Region are distributed on different nodes? Further, what happens if starting multiple TiKV instances on one machine? When TiKV cluster is performing multi-site deployment for disaster recovery, how to guarantee that multiple Replicas of a Raft Group will not get lost if there is outage in one datacenter? How to move data of other nodes in TiKV cluster onto the newly-added node? What happens if a node fails? What does the whole cluster need to do? How to handle if the node fails only temporarily (e.g. restarting a service)? What about a long-time failure (e.g. disk failure or data loss)? Assume that each Raft Group is required to have N replicas. A single Raft Group might have insufficient Replicas (e.g. node failure, loss of replica) or too much Replicas (e.g. the once failed node functions again and automatically add to the cluster). How to schedule the number? As read/write is performed by Leader, what happens to the cluster if all Leaders gather on a few nodes? There is no need to get access to all Regions and the hotspot probably resides in a few Regions. In this case, what should we do? The cluster needs to migrate data during the process of load balancing. Will this kind of data migration consume substantial network bandwidth, disk IO and CPU and influence the online service?  It is easy to solve the above questions one by one, but once mixed up, it becomes difficult. It seems that some questions just need to consider the internal situation of a single Raft Group, for example, whether to add replicas is determined by if the number is enough. But actually, where to add this replica needs a global view. The whole system is changing dynamically: situations like Region splitting, node joining, node failing and hotspot accessing changes occur constantly. The schedule system also needs to keep marching towards the best state. Without a component that can master, schedule and configure the global information, it is hard to meet these needs. Therefore, we need a central node to control and adjust the overall situation of the system. So here comes the Placement Driver (PD) module.Back to the topThe Requirements of Scheduling I want to categorize and sort out the previously listed questions. In general, there are two types: A distributed and highly available storage system must meet the following requirements: The right number of replicas. Replicas should be distributed on different machines. Replicas on other nodes can be migrated after adding nodes. When a node is offline, data on this node should be migrated.  A good distributed system needs to have the following optimizations: A balanced distribution of Leaders in the cluster. A balanced storage capacity in each node. A balanced distribution of hotspot accessing. Control the speed of balancing in order not to impact the online service. Manage the node state, including manually online/offline nodes and automatically offline faulty nodes.   If the first type of requirements are met, the system supports multi-replica disaster recovery, dynamic scalability, tolerance of node failure and automatic disaster recovery. If the second type of requirements are met, the load of the system becomes more balanced and easier to manage. To meet these needs, we need to, first of all, collect enough information, such as the state of each node, information of each Raft Group and the statistics of business access and operation. Then we should set some policies for PD to formulate a schedule plan to meet the previous requirements according to this information and the schedule policy.Back to the topThe Basic Operations of Scheduling The basic operations of schedule are the simplest. In other word, what we can do to meet the schedule policy. This is the essence of the whole scheduler. The previous scheduler requirements seem to be complicated, but can be generalized into 3 operations: Add a Replica. Delete a Replica. Transfer the role of Leader among different Replicas of a Raft Group.  The Raft protocol happens to meet these requirements: the AddReplica, RemoveReplica and TransferLeader commands support the three basic operations.Back to the topInformation Collecting Schedule depends on the information gathering of the whole cluster. Simply put, we need to know the state of each TiKV node and each Region. TiKV cluster reports two kinds of information to PD: Each TiKV node regularly reports the overall information of nodes to PD There are heartbeats between TiKV Store and PD. On the one hand, PD checks whether each Store is active or if there are newly-added Stores through heartbeats. On the other hand, heartbeats carry the state information of this Store, mainly including: total disk capacity free disk capacity the number of Regions data writing speed the number of sent/received Snapshot (Replicas synchronize data through Snapshots) whether it is overloaded label information (Label is a series of Tags that has hierarchical relationship)  Leader of each Raft Group reports to PD regularly Leader of each Raft Group and PD are connected with heartbeats, which report the state of this Region, including: the position of Leader the position of Followers the number of offline Replicas data reading/writing speed   Through these two kinds of heartbeats, PD gathers the information of the whole cluster and then makes decisions. What’s more, PD makes more accurate decisions by getting extra information through the management interface. For example, when the heartbeat of a Store is interrupted, PD has no idea whether it is temporarily or permanently. PD can only waits for a period of time (30 minutes by default); if there is still no heartbeat, PD considers that the Store has been offline and it needs to move all Regions on the Store away. However, if an Operations staff manually offline a machine, he needs to tell PD through its management interface that the Store is unavailable. In this case, PD will immediately move all Regions on the Store away.Back to the topThe Policy of Scheduling After gathering information, PD needs some policies to draw up a concrete schedule plan. The number of Replica in a Region should be correctWhen PD finds that the number of Replica for a Region doesn’t meet the requirement through the heartbeat of a Region Leader, it modifies the number through the Add/Remove Replica operations. This might occur when: a node drops and loses all data, leading to the lack of Replica in some Regions. a dropped node functions again and automatically joins in the cluster. In this case, there is a redundant Replica and needs to be removed. the administrator has modified the replica policy and the configuration of max-replicas.  Multiple Replicas of a Raft Group should not be in the same placePlease pay attention that it is the same place, not the same node. In general, PD can only guarantee that multiple Replicas would not be in the same node, so as to avoid the problem that many Replicas get lost when a node fails. In an actual deployment scenario, the following requirements may come out: Multiple nodes are deployed on the same physical machine. TiKV nodes distribute on multiple servers. It is expected that when a server powers down, the system is still available. TiKV nodes distribute on multiple IDCs. When a datacenter powers down, the system is …"},
		{"url": "https://pingcap.com/blog-cn/pax/",
		"title": "PAX：一个 Cache 友好高效的行列混存方案", 
		"content": " 今年，Spanner 终于发了另一篇 Paper 「Spanner: Becoming a SQL System」，里面提到 Spanner 使用了一种新的存储格式 - Ressi，用来支持 OLTP 和 OLAP。在 Ressi 里面，使用了 PAX 来组织数据。因为 TiDB 定位就是一个 HTAP 系统，所以我也一直在思考在 TiKV 这层如何更好的存储数据，用来满足 HTAP 的需要，既然 Spanner 使用了 PAX，那么就有研究的必要了。PAX 的论文可以看看 「Weaving Relations for Cache Performance」 或者 「Data Page Layouts for Relational Databases on Deep Memory Hierarchies」。NSM and DSM 在谈 PAX 之前，NSM 和 DSM 还是绕不开的话题，NSM 就是通常说的行存，对于现阶段很多偏重 OLTP 的数据，譬如 MySQL 等，都采用的这种方式存储的数据。而 DSM，则是通常的说的列存，几乎所有的 OLAP 系统，都采用的这种方式来存储的底层数据。NSM 会将 record 依次在磁盘 page 里面存放，每个 page 的末尾会存放 record 的 offset，便于快速的定位到实际的 record。如果我们每次需要得到一行 record，或者 scan 所有 records，这种格式非常的高效。但如果我们的查询，仅仅是要拿到 record 里面的一列数据，譬如 select name from R where age &amp;lt; 40，那么对于每次 age 的遍历，除了会将无用的其他数据一起读入，每次读取 record，都可能会引起 cache miss。不同于 NSM，DSM 将数据按照不同的 attributes 分别存放到不同的 page 里面。对于上面只需要单独根据某一个 attribute 进行查询的情况，我们会直接读出 page，遍历处理，这个对 cache 也是非常高效友好的。但是，如果一个查询会涉及到多个不同的 attributes，那么我们就可能需要多次 IO 来组合最终的 tuple。同时，对于写入，DSM 因为会将不同的 attributes 对应的数据写到不同的 page，也会造成较多的随机 IO。PAX 可以看到，NSM 和 DSM 都有各自的优劣，所以如何将它们和优点结合起来，就是现在很多 hybrid storage 包括 PAX 考虑的问题。PAX 全称是 Partition Attributes Across，它在 page 里面使用了一种 mini page 的方式，将 record 切到不同的 mini page 里面。假设有 n 个 attributes，PAX 就会将 page 分成 n 个 mini pages，然后将第一个 attribute 放在第一个 mini page 上面，第二个放在第二个 mini page，以此类推。在每个 page 的开头，会存放每个 mini page 的 offset，mini page 对于 Fixed-length attribute 的数据，会使用 F-minipage ，而对于 variable-length attribute 的数据，则会使用 V-minipage。对于 F-minipage 来说，最后会有一个 bit vector 来存放 null value。而对于 V-minipage 来说，最后会保存每个 value 在 mini page 里面的 offset。可以看到，PAX 的格式其实是 NSM 和 DSM 的一种折中，当要依据某一列进行 scan 的时候，我们可以方便的在 mini page 里面顺序扫描，充分利用 cache。而对于需要访问多 attributes 得到最终 tuple 的时候，我们也仅仅是需要在同一个 page 里面的 mini page 之间读取相关的数据。Data Manipulation Insert 当数据插入的时候，PAX 会首先生成一个新的 page，然后根据 attribute 的 value size 分配好不同的 mini page， 这里需要注意下 variable-length value，因为它们的长度是不固定的，PAX 会使用一些 hint 来得到一个平均的 size。插入一个 record 的时候，PAX 会将这个 record 里面的数据分别 copy 到不同的 mini page 上面。如果一个 record 还能插入到这个 page，但这个 record 里面某一个 attribute 的数据不能插入到对应的 mini page 了，PAX 会重新调整不同 mini page 的 boundary。如果一个 page 已经 full 了，那么 PAX 就会重新分配一个 page。Update 当数据更新的时候，PAX 会首先计算这个 record 需要更新的 attributes 在不同 mini page 里面的 offset，对于 variable-length value 来说，如果更新的数据大小超出了 mini page 可用空间，mini page 就会尝试向周围的 mini page 借一点空间。如果邻居也没有额外的空间了，那么这个 record 就会被移到新的 page 上面。Delete 当数据删除的时候，PAX 会在 page 最开始会维护一个 bitmap，用来标记删除的数据。当删除标记越来越多的时候，就可能会影响性能，因为会导致 mini page 里面出现很多 gap，并不能高效的利用 cache。所以 PAX 会定期去对文件重新组织。小结 PAX 其实是一个原理比较简单的东西，但它并没有成为一个业界主流的存储方案，应该有一些局限是我现在还不知道的。但既然 Spanner 敢用，证明在 HTAP 领域，PAX 也是一个可选择的方案，对我们后续 HTAP storage 的技术选型也有一定的指导作用。这里也就先记录一下，也希望能跟这方面有经验的同学多多交流下心得体会。"},
		{"url": "https://pingcap.com/blog-cn/grpc-rs/",
		"title": "gRPC-rs：从 C 到 Rust", 
		"content": " 介绍 在上篇文章中，我们讲到 TiKV 为了支持 gRPC，我们造了个轮子 gRPC-rs，这篇文章简要地介绍一下这个库。首先我们来聊聊什么是 gRPC。gRPC 是 Google 推出的基于 HTTP2 的开源 RPC 框架，希望通过它使得各种微服务之间拥有统一的 RPC 基础设施。它不仅支持常规的平台如 Linux，Windows，还支持移动设备和 IoT，现有十几种语言的实现，现在又多了一种语言 Rust。gRPC 之所以有如此多的语言支持，是因为它有一个 C 写的核心库(gRPC core)，因此只要某个语言兼容 C ABI，那么就可以通过封装，写一个该语言的 gRPC 库。Rust 对 C 有良好的支持，gRPC-rs 就是对 gRPC core ABI 的 Rust 封装。Core 能异步处理 RPC 请求，在考虑到 Rust 中已有较为成熟的异步框架 Futures，我们决定将 API 设计成 Future 模式。gRPC-rs 架构图我们将根据架构图从底向上地讲一下，在上一篇文章中已经讨论过传输层和协议，在这就不再赘述。gRPC Core Core 中有几个比较重要的对象： Call 以及 4 种类型 RPC： Call 代表了一次 RPC，可以派生出四种类型 RPC， Unary： 这是最简单的一种 RPC 模式，即一问一答，客户端发送一个请求，服务端返回一个回复，该轮 RPC 结束。 Client streaming： 这类的 RPC 会创建一个客户端到服务端的流，客户端可以通过这个流，向服务端发送多个请求，而服务端只会返回一个回复。 Server streaming： 与上面的类似，不过它会创建一个服务端到客户端的流，服务端可以发送多个回复， Bidirectional streaming： 如果说上面两类是单工，那么这类就是双工了，客户端和服务端可以同时向对方发送消息。   值得一提的是由于 gRPC 基于 HTTP2，它利用了 HTTP2 多路复用特性，使得一个 TCP 连接上可以同时进行多个 RPC，一次 RPC 即为 HTTP2 中的一个 Stream。 Channel： 它是对底层链接的抽象，具体来说一个 Channel 就是一条连着远程服务器的 TCP 链接。 Server： 顾名思义，它就是 gRPC 服务端封装，可以在上面注册我们的服务。 Completion queue: 它是 gRPC 完成事件队列，事件可以是收到新的回复，可以是新来的请求。  简要介绍一下 Core 库的实现，Core 中有一个 Combiner 的概念，Combiner 中一个函数指针或称组合子（Combinator）队列。每个组合子都有特定的功能，通过不同的组合可以实现不同的功能。下面的伪码大概说明了 Combiner 的工作方式。class combiner { mpscq q; // multi-producer single-consumer queue can be made non-blocking state s; // is it empty or executing run(f) { if (q.push(f)) { // q.push returns true if it&amp;#39;s the first thing while (q.pop(&amp;amp;f)) { // modulo some extra work to avoid races f(); } } } } Combiner 里面有一个 mpsc 的无锁队列 q，由于 q 只能有一个消费者，这就要求在同一时刻只能有一个线程去调用队列里面的各个函数。调用的入口是 run() 方法，在 run() 中各个函数会被序列地执行。当取完 q 时，该轮调用结束。假设一次 RPC 由六个函数组成，这样的设计使这组函数（RPC）可以在不同的线程上运行，这是异步化 RPC 的基础。Completion queue（以下简称 CQ）就是一个 Combiner，它暴露出了一个 next()借口，相当于 Combiner 的 run()。由于接口的简单，Core 内部不用开启额外线程，只要通过外部不断调用 next() 就能驱动整个 Core。所有的 HTTP2 处理，Client 的 RPC 请求和 Server 的 RPC 连接全是通过一个个组合子的不同组合而构成的。下面是一次 Unary 的代码。它由6个组合子组成，这些组合子作为一个 batch 再加上 Call 用于记录状态，两者构成了这次的 RPC。grpc_call_error grpcwarp_call_start_unary( grpc_call *call, grpcsharp_batch_context *tag) { grpc_op ops[6]; ops[0].op = GRPC_OP_SEND_INITIAL_METADATA; ... ops[1].op = GRPC_OP_SEND_MESSAGE; ... ops[2].op = GRPC_OP_SEND_CLOSE_FROM_CLIENT; ... ops[3].op = GRPC_OP_RECV_INITIAL_METADATA; ... ops[4].op = GRPC_OP_RECV_MESSAGE; ... ops[5].op = GRPC_OP_RECV_STATUS_ON_CLIENT; return grpcwrap_call_start_batch(call, ops, tag); } 用 Rust 封装 Core 介绍完 Core，现在说一下如何用 Rust 封装它。这一层封装并不会产生额外的开销，不像有的语言在调用 C 时会有类型的转换或者 runtime 会有较大开销，在 Rust 中开销微乎其微，这得益于 Rust 用 llvm 做编译器后端，它对 C 有良好的支持，Rust 调用 C ABI 就像调用一个普通的函数，可以做到 Zero-cost。同时用 Rust 封装 C ABI 是一件很简单的事情，简单到像黑魔法。比如封装 CQ next():C：grpc_event grpc_completion_queue_next(grpc_completion_queue *cq, gpr_timespec deadline, void *reserved); Rust：extern &amp;#34;C&amp;#34; { pub fn grpc_completion_queue_next(cq: *mut GrpcCompletionQueue, deadline: GprTimespec, reserved: *mut c_void) -&amp;gt; GrpcEvent; } 接着我们看看如何封装 C 的类型。继续以 next() 为例子：C：// CQ 指针 grpc_completion_queue *cq; // grpc_event 结构体 struct grpc_event { grpc_completion_type type; int success; void *tag; }; Rust：pub enum GrpcCompletionQueue {} #[repr(C)] pub struct GrpcEvent { pub event_type: GrpcCompletionType, pub success: c_int, pub tag: *mut c_void, } CQ 在 Core 的 ABI 中传递的形式是指针，Rust Wraper 无须知道 CQ 具体的内部结构。对于这种情况，Rust 推荐用无成员的枚举体表示，具体好处有两个，第一，由于没有成员，我们无法在 Rust 中构建该枚举体的实例，第二，Type safe，当传递了一个错误类型的指针时编译器会报错。#[repr(C)] 也是 Rust 的黑魔法之一。加上了这个标签的结构体，在内存中的布局和对齐就和 C 一样了，这样的结构体可以安全地传递给 C ABI。Futures in gRPC-rs 经过上一节的封装，我们已经得到了一个可用但是非常裸的 Rust gRPC 库了，grpc-sys。在实践中，我们不推荐直接用 grpc-sys，直接用它就像在 Rust 中写 C 一样，事倍功半，Rust 语言的诸多特性无法得到施展，例如泛型，Trait，Ownership 等，也无法融入 Rust 社区。上面说过 Core 能异步处理 RPC，那么如何用 Rust 来做更好的封装呢？ Futures！它是一个成熟的异步编程库，同时有一个活跃的社区。 Futures 非常适用于 RPC 等一些 IO 操作频繁的场景。Futures 中也有组合子概念，和 Core 中的类似，但是使用上更加方便，也更加好理解。举一个栗子：use futures::{future, Future}; fn double(i: i64) -&amp;gt; i64 { i * 2 } let ans = future::ok(1) .map(double) .and_then(|i| Ok(40 + i)); println!(&amp;#34;{:?}&amp;#34;, ans.wait().unwrap()); 你觉得输出的答案是多少呢？没错就是 42。在 Core 那节说过不同的组合子组织在一起可以干不同的事，在 Future 中我们可以这么理解，一件事可以分成多个步骤，每个步骤由一个组合子完成。比如上例，map 完成了翻倍的动作，and_then 将输入加上 40。 现在来看看 gRPC-rs 封装的 API。// helloworld.proto service Greeter { // An unary RPC, sends a greeting  rpc SayHello (HelloRequest) returns (HelloReply) {} } impl GreeterClient { pub fn say_hello_async(&amp;amp;self, req: HelloRequest) -&amp;gt; ClientUnaryReceiver&amp;lt;HelloReply&amp;gt; { self.client.unary_call_async(&amp;amp;METHOD_GREETER_SAY_HELLO, req, CallOption::default()) } ... } 以 helloworld.proto 为例，GreeterClient::say_hello_async() 向远程 Server 发送一个请求 (HelloRequest)，Server 返回给一个结果 (HelloReply)。由于是异步操作，这个函数会立即返回，返回的 ClientUnaryReceiver 实现了 Future，当它完成时就会得到 HelloReply。在一般的异步编程中都会有 Callback，用于处理异步的返回值，在这个 RPC 中就是 HelloReply，在 Future 中可以用组合子来写，比如 and_then，再举一个栗子，现有一次完整的 RPC 逻辑，拿到回复后打印到日志。下面就是 gRPC-rs 的具体用法。// 同步 let resp = client.say_hello(req); println!(&amp;#34;{:?}&amp;#34;, resp); // 异步 let f = client.say_hello_async(req) .and_then(|resp| { println！(&amp;#34;{:?}&amp;#34;, resp); Ok(()) }); executer.spawn(f); // 类似 Combiner,  // 用于异步执行 Future，  // 常用的有 tokio-core。 Unary RPC gRPC-rs 根据 service 在 proto 文件中的定义生成对应的代码，包括 RPC 方法的定义（Method）、客户端和服务端代码，生成的代码中会使用 gRPC-rs 的 API。那么具体是怎么做的呢？这节还是以 helloworld.proto 为例，来讲讲客户端 Unary RPC 具体的实现。首先，SayHello 的 Method 记录了 RPC 类型，全称以及序列化反序列化函数。为什么要序列化反序列化函数呢？因为 Core 本身不涉及消息的序列化，这一部分交由封装层解决。在生成的客户端中可以会调用 gRPC-rs 的 API，根据 Method 的定义发起 RPC。// 生成的代码 const METHOD_GREETER_SAY_HELLO: Method&amp;lt;HelloRequest, HelloReply&amp;gt; = Method { ty: MethodType::Unary, name: &amp;#34;/helloworld.Greeter/SayHello&amp;#34;, req_mar: Marshaller { ser: pb_ser, de: pb_de }, resp_mar: Marshaller { ser: pb_ser, de: pb_de }, }; impl GreeterClient { // An unary RPC, sends a greeting  pub fn say_hello_async(&amp;amp;self, req: HelloRequest) -&amp;gt; ClientUnaryReceiver&amp;lt;HelloReply&amp;gt; { self.client.unary_call_async(&amp;amp;METHOD_GREETER_SAY_HELLO, req) } ... } // gRPC-rs 的 API。该函数立即返回，不会等待 RPC 完成。省略部分代码。 pub fn unary_async&amp;lt;P, Q&amp;gt;(channel: &amp;amp;Channel, method: &amp;amp;Method&amp;lt;P, Q&amp;gt;, req: P) -&amp;gt; ClientUnaryReceiver&amp;lt;Q&amp;gt; { let mut payload = vec![]; (method.req_ser())(&amp;amp;req, &amp;amp;mut payload); // 序列化消息  let call = channel.create_call(method, &amp;amp;opt); // 新建 Call  let cq_f = unsafe { grpc_sys::grpcwrap_call_start_unary(call.call, // 发起 RPC  payload, tag) }; ClientUnaryReceiver::new(call, cq_f, method.resp_de()) // 收到回复后再反序列化 } 写在最后 这篇简单介绍了 gRPC Core 的实现和 gRPC-rs 的封装，详细的用法，在这就不做过多介绍了，大家如果感兴趣可以查看 examples。 gRPC-rs 深入使用了 Future，里面有很多神奇的用法，比如 Futures in gRPC-rs 那节最后的 executer， gRPC-rs 利用 CQ 实现了一个能并发执行 Future 的 executer（类似 furtures-rs 中的 Executer），大幅减少 context switch，性能得到了显著提升。如果你对 gRPC 和 rust 都很感兴趣，欢迎参与开发，目前还有一些工作没完成，详情请点击 https://github.com/pingcap/grpc-rs参考资料：gRPC open-source universal RPC frameworkThe rust language implementation of gRPCHypertext Transfer Protocol Version 2 (HTTP/2) Zero-cost Futures in Rust[深入了解 gRPC：协议][上一篇文章]gRPC, Combiner ExplanationRust, Representing opaque structsRust repr(), alternative representationsgRPC - A solution for RPCs by GoogleTokio, A platform for writing fast networking code with Rust."},
		{"url": "https://pingcap.com/weekly/2017-07-17-tidb-weekly/",
		"title": "Weekly update (July 10 ~ July 16, 2017)", 
		"content": " Weekly update in TiDB Last week, we landed 51 PRs in the TiDB repositories.Added  Support setting variables with ON and OFF. Support show stats_histgrams and show stats_buckets for debugging. Support the Scan API for the raw KV interface. Support the Show Charset statement. Support user name without quotes such as root@127.0.0.1.  Fixed  Fix a bug when explicitly inserting the null value into columns with the timestamp type. Do not check privileges for the information_schema database. Correct the error message for the authentication error. Fix a bug in the cast function with an unsigned type.  Improved  Refactor optimizer:  Select Join operator by CBO framework automatically: #3623, #3737, #3761 Use statsProfile to estimate count and cardinality.  Refactor the builtin function evaluation framework:  left &amp;amp; right lower &amp;amp; upper concat_ws to_base64 md5 space replace substring/substr compare uuid bit_length log10 Move the type inference work from typeinfer to expression rewritter: aggregate expression, column &amp;amp; constant expression  Close the connection after sending ERR_Packet. Improve the unit test coverage for parser packages.  Weekly update in TiKV Last week, We landed 23 PRs in the TiKV repositories.Added  Add json_modify, math function evaluators for the coprocessor. Pass the compression type to generate snapshot SST. Set 256 MB as the default region size. Add the raw Scan API.  Fixed  Ignore the command that removes the leader itself. Fix diff hint overflow. Rotate PD log by default. Fix the leap second problem in Go 1.9. Clean up stale metadata when start.  Improved  Reduce the lock scope for region heartbeat. Send the raft messages in batch. Refactor the coprocessor code directory. Enable the pipelined write. Remove unnecessary hash group keys.  "},
		{"url": "https://pingcap.com/blog-cn/reconstruct-built-in-function-report/",
		"title": "十分钟成为 Contributor 系列 | 重构内建函数进度报告", 
		"content": " 6 月 22 日，TiDB 发布了一篇如何十分钟成为 TiDB Contributor 系列的第二篇文章，向大家介绍如何为 TiDB 重构 built-in 函数。截止到目前，得到了来自社区的积极支持与热情反馈，TiDB 参考社区 contributors 的建议，对计算框架进行了部分修改以降低社区同学参与的难度。本文完成以下2 项工作，希望帮助社区更好的参与进 TiDB 的项目中来: 对尚未重写的 built-in 函数进行陈列 对继上篇文章后，计算框架所进行的修改，进行详细介绍  一. 尚未重写的 built-in 函数陈列如下： 共计 165 个 在 expression 目录下运行 grep -rn &amp;quot;^tbaseBuiltinFunc$&amp;quot; -B 1 * | grep &amp;quot;Sig struct {&amp;quot; | awk -F &amp;quot;Sig&amp;quot; &#39;{print $1}&#39; | awk -F &amp;quot;builtin&amp;quot; &#39;{print $3}&#39; &amp;gt; ~/Desktop/func.txt 命令可以获得所有未实现的 built-in 函数   0 1 2 3 4     Coalesce Uncompress Log10 Default UnaryOp   Greatest UncompressedLength Rand InetAton IsNull   Least ValidatePasswordStrength Pow InetNtoa In   Interval Database Round Inet6Aton Row   CaseWhen FoundRows Conv Inet6Ntoa SetVar   If CurrentUser CRC32 IsFreeLock GetVar   IfNull User Sqrt IsIPv4 Values   NullIf ConnectionID Arithmetic IsIPv4Prefixed BitCount   AesDecrypt LastInsertID Acos IsIPv6 Reverse   AesEncrypt Version Asin IsUsedLock Convert   Compress Benchmark Atan MasterPosWait Substring   Decode Charset Cot NameConst SubstringIndex   DesDecrypt Coercibility Exp ReleaseAllLocks Locate   DesEncrypt Collation PI UUID Hex   Encode RowCount Radians UUIDShort UnHex   Encrypt Regexp Truncate AndAnd Trim   OldPassword Abs Sleep OrOr LTrim   RandomBytes Ceil Lock LogicXor RTrim   SHA1 Floor ReleaseLock BitOp Rpad   SHA2 Log AnyValue IsTrueOp BitLength   Char Format FromDays DayOfWeek Timestamp   CharLength FromBase64 Hour DayOfYear AddTime   FindInSet InsertFunc Minute Week ConvertTz   Field Instr Second WeekDay MakeTime   MakeSet LoadFile MicroSecond WeekOfYear PeriodAdd   Oct Lpad Month Year PeriodDiff   Quote Date MonthName YearWeek Quarter   Bin DateDiff Now FromUnixTime SecToTime   Elt TimeDiff DayName GetFormat SubTime   ExportSet DateFormat DayOfMonth StrToDate TimeFormat   UTCTim ToSeconds TimestampDiff DateArith Extract   UnixTimestamp UTCTimestamp UTCDate Time CurrentTime   ToDays TimestampAdd TimeToSec CurrentDate SysDate    二. 计算框架进行的修改: 此处依然使用 Length 函数( expression/builtin_string.go )为例进行说明，与前文采取相同目录结构:1. expression/builtin_string.go（1）lengthFunctionClass.getFunction() 方法: 简化类型推导实现getFunction 方法用来生成 built-in 函数对应的函数签名，在构造 ScalarFunction 时被调用func (c *lengthFunctionClass) getFunction(args []Expression, ctx context.Context) (builtinFunc, error) { // 此处简化类型推导过程，对 newBaseBuiltinFuncWithTp() 实现进行修改，新的实现中，传入 Length 返回值类型 tpInt 表示返回值类型为 int，参数类型 tpString 表示返回值类型为 string  bf, err := newBaseBuiltinFuncWithTp(args, ctx, tpInt, tpString) if err != nil { return nil, errors.Trace(err) } // 此处参考 MySQL 实现，设置返回值长度为 10(character length)  // 对于 int/double/decimal/time/duration 类型返回值，已在 newBaseBuiltinFuncWithTp() 中默认调用 types.setBinChsClnFlag() 方法，此处无需再进行设置  bf.tp.Flen = 10 sig := &amp;amp;builtinLengthSig{baseIntBuiltinFunc{bf}} return sig.setSelf(sig), errors.Trace(c.verifyArgs(args)) } 注： 对于返回值类型为 string 的函数，需要，注意参考 MySQL 行为设置bf.tp.[charset | collate | flag] 查看 MySQL 行为可以通过在终端启动$ mysql -uroot --column-type-info，这样对于每一个查询语句，可以查看每一列详细的 metadata 对于返回值类型为 string 的函数，以 concat 为例，当存在类型为 string 且包含 binary flag 的参数时，其返回值也应设置 binary flag 对于返回值类型为 Time 的函数，需要注意，根据函数行为，设置bf.tp.Tp = [ TypeDate | TypeDatetime | TypeTimestamp ] ， 若为 TypeDate/ TypeDatetime，还需注意推导 bf.tp.Decimal (即小数位数) 不确定性的函数：     0 1 2 3 4 5     Rand ConnectionID CurrentUser User Database RowCount   Schema FoundRows LastInsertId Version Sleep UUID   GetVar SetVar Values SessionUser SystemUser     （2）实现 builtinLengthSig.evalInt() 方法：保持不变，此处请注意修改该函数的注释 (s/ eval/ evalXXX)2. expression/builtin_string_test.gofunc (s *testEvaluatorSuite) TestLength(c *C) { defer testleak.AfterTest(c)() cases := []struct { args interface{} expected int64 isNil bool getErr bool }{ ...... } for _, t := range cases { f, err := newFunctionForTest(s.ctx, ast.Length, primitiveValsToConstants([]interface{}{t.args})...) c.Assert(err, IsNil) d, err := f.Eval(nil) // 注意此处不再对 LENGTH 函数的返回值类型进行测试，相应测试被移动到 plan/typeinfer_test.go/TestInferType 函数中，(注意不是expression/typeinferer_test.go）  if t.getErr { c.Assert(err, NotNil) } else { c.Assert(err, IsNil) if t.isNil { c.Assert(d.Kind(), Equals, types.KindNull) } else { c.Assert(d.GetInt64(), Equals, t.expected) } } } // 测试函数是否具有确定性  // 在 review 社区的 PRs 过程中发现，这个测试经常会被遗漏，烦请留意  f, err := funcs[ast.Length].getFunction([]Expression{Zero}, s.ctx) c.Assert(err, IsNil) c.Assert(f.isDeterministic(), IsTrue) } 3. executor/executor_test.go与上一篇文章保持不变，需要注意的是，为了保证可读性， TestStringBuiltin() 方法仅对 expression/builtin_string.go 文件中的 built-in 函数进行测试。如果 executor_test.go 文件中不存在对应的 TestXXXBuiltin() 方法，可以新建一个对应的测试函数。4. plan/typeinfer_test.gofunc (s *testPlanSuite) TestInferType(c *C) { .... tests := []struct { sql string tp byte chs string flag byte flen int decimal int }{ ... // 此处添加对 length 函数返回值类型的测试  // 此处注意，对于返回值类型、长度等受参数影响的函数，此处测试请尽量覆盖全面  {&amp;#34;length(c_char, c_char)&amp;#34;, mysql.TypeLonglong, charset.CharsetBin, mysql.BinaryFlag, 10, 0}, ... } for _, tt := range tests { ... } } 注：当有多个 PR 同时在该文件中添加测试时，若有别的 contributor 的 PR 先于自己的 PR merge 进 master，有可能会发生冲突，此时在本地 merge 一下 master 分支，解决一下再 push 一下即可。成为 New Contributor 赠送限量版马克杯的活动还在继续中，任何一个新加入集体的小伙伴都将收到我们充满了诚意的礼物，很荣幸能够认识你，也很高兴能和你一起坚定地走得更远。成为 New Contributor 获赠限量版马克杯，马克杯获取流程如下：  提交 PR PR提交之后，请耐心等待维护者进行 Review。 目前一般在一到两个工作日内都会进行 Review，如果当前的 PR 堆积数量较多可能回复会比较慢。 代码提交后 CI 会执行我们内部的测试，你需要保证所有的单元测试是可以通过的。期间可能有其它的提交会与当前 PR 冲突，这时需要修复冲突。 维护者在 Review 过程中可能会提出一些修改意见。修改完成之后如果 reviewer 认为没问题了，你会收到 LGTM(looks good to me) 的回复。当收到两个及以上的 LGTM 后，该 PR 将会被合并。 合并 PR 后自动成为 Contributor，会收到来自 PingCAP Team 的感谢邮件，请查收邮件并填写领取表单 表单填写地址：http://cn.mikecrm.com/01wE8tX  后台 AI 核查 GitHub ID 及资料信息，确认无误后随即便快递寄出属于你的限量版马克杯 期待你分享自己参与开源项目的感想和经验，TiDB Contributor Club 将和你一起分享开源的力量  了解更多关于 TiDB 的资料请登陆我们的官方网站：https://pingcap.com加入 TiDB Contributor Club 请添加我们的 AI 微信："},
		{"url": "https://pingcap.com/blog/2017-07-11-tidbinternal1/",
		"title": "TiDB Internal (I) - Data Storage", 
		"content": " From Li SHEN: shenli@pingcap.com Table of Content  Foreword Storing data Key-Value RocksDB Raft Region MVCC Transaction Miscellaneous  Foreword: Database, operating system and compiler are known as the three big systems and regarded as the footstone of the whole computer software. Among them, database supports the businesses and is closer to the application layer. After decades of development, progress keeps emerging in this field.Many people must have used databases of different kinds, but few have the experience of developing one, especially a distributed database. Knowing the principle and detail of implementing a database helps to advance one’s skill level, which is good for building other systems, and is also helpful to make better use of database.I believe the best way to work on a technology is to dive deeply into an open source project of the field. Database is no exception. There are many good open source projects in the field of a standalone database. Among them, MySQL and PostgreSQL are the most famous and many people must have read their source code. However, in terms of distributed database, there are not many good open source projects and TiDB is one of the few. Many people, especially technophiles, hope to participate in this project. However, due to the complexity of distributed database, lots of people find it hard to understand the whole project. Therefore, I plan to write several articles to illustrate the technical principle of TiDB, including the technique that users can see as well as numerous invisible ones behind the SQL interface.This is the first of our series of articles.Back to the topStoring data I’d like to begin with the most fundamental function of a database &amp;ndash; storing data. There are lots of ways to store data and the easiest one is building a data structure in the memory to store data sent by users. For example, use an array to store data and add a new entry to the array when receiving a piece of data. This solution is simple, meets the basic needs and has good performance. But its drawback outweighs the advantages. The biggest problem is that as all data is stored in the memory, if the server stops or restarts, data would get lost.To achieve data persistence, we can store data in the non-volatile storage medium, disk for example. We create a file on disk and append a new record to the file when receiving data. This is a durable storage solution. But this is not enough. What if the disk is broken? To avoid the bad track of a disk, we can use RAID (Redundant Array of Independent Disks) for standalone redundant storage. However, what if the entire machine goes down? What if there is an outbreak of fire? RAID is no safe house. Another solution is to store data in the network or use hardware or software for storage and replication. But the problem is how to guarantee the consistency between replicas. Securing the intactness and correctness of data is the basic requirement, the following problems are far more demanding: Does the database support disaster recovery of multi-datacenter? Is the write speed fast enough? Is it convenient to read data when data is stored? How to update the stored data? How does it deal with the concurrent revision? How to revise multiple records atomically?  All these problems are difficult to solve. But an excellent database storage system must be able to deal with each and every one of them.For this, we have developed TiKV. Now, I want to share with you the design philosophy and basic concept of TiKV.As we are talking about TiKV, I hope you can forget any concept about SQL and focus on how to implement TiKV, a huge distributed ordered Map that is of high performance and reliability.Back to the topKey-Value A data storage system should, first and foremost, determine the store model of data. In other words, in which format should the data be stored. TiKV chooses the Key-Value model and offers a solution to traverse orderly. To put it simply: you can see TiKV as a huge Map where Key and Value are the original Byte array. In this Map, Key is arranged in a comparison order according to the raw binary bit of the byte array.The following points need to be kept in mind: This is a huge Map of Key-Value pairs. In this Map, Key-Value pairs are ordered according to the Key’s binary sequence. We can Seek the position of a Key, and use the Next method to other Key-Value pairs, and these Key-Value pairs are all bigger than this one.  You might wonder the relation between the storage model that I’m talking about and the table in SQL. Here, I want to highlight: they are irrelevant.RocksDB Any durable storage engine stores data on disk and TiKV is no exception. But TiKV doesn’t write data to disk directly. Instead, it stores data in RocksDB and then RocksDB is responsible for the data storage. The reason is that it costs a lot to develop a standalone storage engine, especially a high-performance standalone engine. You need to do all kinds of detailed optimization. Fortunately, we found that RocksDB is an excellent open source standalone storage engine that meets all of our requirements. Besides, as the Facebook team keeps optimizing it, we can enjoy a powerful and advancing standalone engine without investing much effort. But of course we contribute a few lines of code to RocksDB and we hope that this project would get better. In a word, you can regard RocksDB as a standalone Key-Value Map.Back to the topRaft Finding an effective, reliable and local storage solution is the important first step of this complex project. Now we are facing with a more difficult thing: how to secure the intactness and correctness of data when a single machine fails? A good way is to replicate data to multiple machines. Then, when one machine crashes, we have replicas on other machines. But it is noted that the replicate solution should be reliable, effective and can deal with the situation of an invalid replica. It sounds difficult but Raft makes it possible. Raft is a consensus algorithm and an equivalent to Paxos while Raft is easier to understand. Those who are interested in Raft can refer to this paper for more details. I want to point out that the Raft paper only presents a basic solution and the performance would be bad if strictly follow the paper. We have made numerous optimizations to implement Raft and for more detail, please refer to this blog written by our Chief Architect, Tang Liu.Raft is a consensus algorithm and offers three important functions: Leader election Membership change Log replication  TiKV uses Raft to replicate data and each data change will be recorded as a Raft log. Through the log replication function of Raft, data is safely and reliably synchronized to multiple nodes of the Raft group.In summary, through the standalone RocksDB, we can store data on a disk rapidly; through Raft, we can replicate data to multiple machines in case of machine failure. Data is written through the interface of Raft instead of to RocksDB. Thanks to the implementation of Raft, we have a distributed Key-Value and no longer need to worry about machine failure.Back to the topRegion In this section, I want to introduce a very important concept: Region. It is the foundation to comprehend a series of mechanism.Before we begin, let’s forget about Raft and try to picture that all the data only has one replica. As I mentioned earlier, TiKV is seen as a huge but ordered Key-Value Map. To implement the horizontal scalability of storage, we need to distribute data among multiple machines. For a Key-Value system, there are two typical solutions to distribute data among multiple machines. One is to create Hash and select the corresponding storage node according to the Hash value; the other is to use Range and store a segment of serial Key in a storage node. TiKV chose the second solution and divided the whole Key-Value space into many segments. Each segment consists of a series of adjacent Key and we call such segment “Region”. …"},
		{"url": "https://pingcap.com/blog/2017-07-11-tidbinternal2/",
		"title": "TiDB Internal (II) - Computing", 
		"content": " From Li SHEN: shenli@pingcap.comTable of Content  Mapping the Relational Model to the Key-Value Model Metadata Management Architecture of SQL on Key-Value SQL Computing Distributed SQL Operation Architecture of the SQL Layer Summary  My last blog introduces the way that TiDB stores data, which is also the basic concepts of TiKV. In this article, I’ll elaborate on how TiDB uses the bottom layer Key-Value to store data, maps the relational model to the Key-Value model and performs SQL computing.Mapping the Relational Model to the Key-Value Model Let’s simplify the Relational Model to be just about Table and the SQL statements. What we need to think about is how to store Table and run the SQL statements on top of the Key-Value structure.Assuming we have a Table as follows:CREATE TABLE User { ID int, Name varchar(20), Role varchar(20), Age int, PRIMARY KEY (ID)， Key idxAge (age) }; Given the huge differences between the SQL and Key-Value structures, how to map SQL to Key-Value conveniently and efficiently becomes vital. The article will, first of all, go through the requirements and characteristics of data computing, which is essential to determine whether a mapping solution is good or not.A Table contains three parts of data: Metadata about the table Rows in the Table Index data  Note that metadata will not be discussed in this article.Data can be stored either in rows or in columns, both have their advantages and disadvantages. The primary goal for TiDB is online transactional processing (OLTP), which supports read, save, update, and delete a row of data quickly. Therefore, row store seems better.TiDB supports both the Primary Index and Secondary Index. The function of Index is for faster query, higher query performance, and the Constraints. There are two forms of query: Point query, which uses equivalent conditions such as Primary Key or Unique Key for a query, e.g. select name from user where id=1;, locating a certain row of data through index. Range query, e.g. select name from user where age &amp;gt; 30 and age &amp;lt; 35;, querying the data that the age is between 20 and 30 through idxAge. There are two types of Indexes: Unique Index and Non-unique Index, both of which are supported by TiDB.  After analyzing the characteristics of the data to be stored, let’s move on to what we need to do to manipulate the data, including the Insert/Update/Delete/Select statements. The Insert statement writes Row into Key-Value and creates the index data. The Update statement updates Row and index data if necessary. The Delete statement deletes both Row and index. Among the four, the Select statement deals with the most complicated situation:  Reading a row of data easily and quickly. In this case, each Row needs to have an ID (explicit or implicit). Continuously reading multiple rows of data, such as Select * from user;. Reading data through index, leveraging the index either in Point query or Range query.   A globally ordered and distributed Key-Value engine satisfies the above needs. The feature of being globally ordered helps us solve quite a few problems. Take the following as two examples: To get a row of data quickly. Assume that we can create a certain or some Keys, when locating this row, we’d be able to use the Seek method provided by TiKV to quickly locate this row of data. To scan the whole table. If the table can be mapped to the Range of Key, then all data can be got by scanning from StartKey to EndKey. The way to manipulate Index data is similar.  Back to the topNow let’s see how this works in TiDB.TiDB allocates a TableID to each table, an IndexID to each index, and a RowID to each row. If the table has an integer Primary Key, then the value will be used as the RowID. The TableID is unique in the whole cluster while the IndexID/RowID unique in the table. All of these ID are int64. Each row of data is encoded into a Key-Value pair according to the following rule:Key: tablePrefix_rowPrefix_tableID_rowID Value: [col1, col2, col3, col4] The tablePrefix/rowPrefix of the Key are specific string constants and used to differentiate other data in the Key-Value space. Index data is encoded into a Key-Value pair according to the following rule: Key: tablePrefix_idxPrefix_tableID_indexID_indexColumnsValue Value: rowIDThe above encoding rule applies to Unique Index while it cannot create a unique Key for Non-unique Index. The reason is that the tablePrefix_idxPrefix_tableID_indexID_ of an Index is the same. It’s possible that the ColumnsValue of multiple rows is also the same. Therefore, we’ve made some changes to encode the Non-unique Index:Key: tablePrefix_idxPrefix_tableID_indexID_ColumnsValue_rowID Value：null In this way, the unique Key of each row of data can be created. In the above rules, all xxPrefix of the Keys are string constants with the function of differentiating the namespace to avoid the conflict between different types of data.var( tablePrefix = []byte{&amp;#39;t&amp;#39;} recordPrefixSep = []byte(&amp;#34;_r&amp;#34;) indexPrefixSep = []byte(&amp;#34;_i&amp;#34;) ) Note that the Key encoding solution of either Row or Index has the same prefix. Specifically speaking, all Rows in a Table has the same prefix, so does data of Index. These data with the same prefix is arranged together in the Key space of TiKV. In other words, we just need to carefully design the encoding solution of the suffix, ensuing the comparison relation remains unchanged, then Row or Index data can be stored in TiKV orderly. The solution of maintaining the relation unchanged before and after encoding is called Memcomparable. As for any type of value, the comparison result of two objects before encoding is consistent with that of the byte array after encoding (Note: both Key and Value of TiKV are the primitive byte array). For more detailed information, please refer to the codec package of TiDB. When adopting this encoding solution, all Row data of a table will be arranged in the Key space of TiKV according to the RowID order. So will the data of a certain Index, according to the ColumnValue order of Index.Back to the topNow we take the previous requirements and TiDB’s mapping solution into consideration and verify the feasibility of the solution. Firstly, we transform Row and Index data into Key-Value data through the mapping solution and make sure that each row and each piece of index data has a unique Key. Secondly, we can easily create the corresponding Key of some row or some piece of index by using this mapping solution as it is good for both Point query and Range query. Lastly, when ensuring some Constraints in the table, we can create and check the existence of a certain Key to determine whether the corresponding Constraint has been satisfied.  Up to now, we’ve already covered how to map Table onto Key-Value. Here is one more case with the same table structure. Assume that the table has three rows of data:1, &amp;#34;TiDB&amp;#34;, &amp;#34;SQL Layer&amp;#34;, 10 2, &amp;#34;TiKV&amp;#34;, &amp;#34;KV Engine&amp;#34;, 20 3, &amp;#34;PD&amp;#34;, &amp;#34;Manager&amp;#34;, 30 First, each row of data will be mapped as a Key-Value pair. As this table has an Int Primary Key, the value of RowID is the value of this Primary Key. Assume that the TableID of this table is 10, its Row data is:t_r_10_1 --&amp;gt; [&amp;#34;TiDB&amp;#34;, &amp;#34;SQL Layer&amp;#34;, 10] t_r_10_2 --&amp;gt; [&amp;#34;TiKV&amp;#34;, &amp;#34;KV Engine&amp;#34;, 20] t_r_10_3 --&amp;gt; [&amp;#34;PD&amp;#34;, &amp;#34;Manager&amp;#34;, 30] In addition to Primary Key, this table also has an Index. Assume that the ID of Index is 1, its data is:t_i_10_1_10_1 —&amp;gt; null t_i_10_1_20_2 --&amp;gt; null t_i_10_1_30_3 --&amp;gt; null The previous encoding rules help you to understand the above example. We hope that you can realize the reason why we chose this mapping solution and the purpose of doing so.Back to the topMetadata Management After explaining how data and index of a table is mapped to Key-Value, this section introduces the storage of metadata.Both Database and Table have metadata, which refers to its definition and various attributes. …"},
		{"url": "https://pingcap.com/blog-cn/tidb-best-practice/",
		"title": "TiDB Best Practice", 
		"content": " 本文档用于总结在使用 TiDB 时候的一些最佳实践，主要涉及 SQL 使用、OLAP/OLTP 优化技巧，特别是一些 TiDB 专有的优化开关。 建议先阅读讲解 TiDB 原理的三篇文章(讲存储，说计算，谈调度)，再来看这篇文章。前言 数据库是一个通用的基础组件，在开发过程中会考虑到多种目标场景，在具体的业务场景中，需要根据业务的实际情况对数据的参数或者使用方式进行调整。TiDB 是一个兼容 MySQL 协议和语法的分布式数据库，但是由于其内部实现，特别是支持分布式存储以及分布式事务，使得一些使用方法和 MySQL 有所区别。基本概念 TiDB 的最佳实践与其实现原理密切相关，建议读者先了解一些基本的实现机制，包括 Raft、分布式事务、数据分片、负载均衡、SQL 到 KV 的映射方案、二级索引的实现方法、分布式执行引擎。下面会做一点简单的介绍，更详细的信息可以参考 PingCAP 公众号以及知乎专栏的一些文章。Raft Raft 是一种一致性协议，能提供强一致的数据复制保证，TiDB 最底层用 Raft 来同步数据。每次写入都要写入多数副本，才能对外返回成功，这样即使丢掉少数副本，也能保证系统中还有最新的数据。比如最大 3 副本的话，每次写入 2 副本才算成功，任何时候，只丢失一个副本的情况下，存活的两个副本中至少有一个具有最新的数据。相比 Master-Slave 方式的同步，同样是保存三副本，Raft 的方式更为高效，写入的延迟取决于最快的两个副本，而不是最慢的那个副本。所以使用 Raft 同步的情况下，异地多活成为可能。在典型的两地三中心场景下，每次写入只需要本数据中心以及离得近的一个数据中心写入成功就能保证数据的一致性，而并不需要三个数据中心都写成功。但是这并不意味着在任何场景都能构建跨机房部署的业务，当写入量比较大时候，机房之间的带宽和延迟成为关键因素，如果写入速度超过机房之间的带宽，或者是机房之间延迟过大，整个 Raft 同步机制依然无法很好的运转。分布式事务 TiDB 提供完整的分布式事务，事务模型是在 Google Percolator 的基础上做了一些优化。具体的实现大家可以参考这篇文章。这里只说两点： 乐观锁TiDB 的事务模型采用乐观锁，只有在真正提交的时候，才会做冲突检测，如果有冲突，则需要重试。这种模型在冲突严重的场景下，会比较低效，因为重试之前的操作都是无效的，需要重复做。举一个比较极端的例子，就是把数据库当做计数器用，如果访问的并发度比较高，那么一定会有严重的冲突，导致大量的重试甚至是超时。但是如果访问冲突并不十分严重，那么乐观锁模型具备较高的效率。所以在冲突严重的场景下，推荐在系统架构层面解决问题，比如将计数器放在 Redis 中。 事务大小限制由于分布式事务要做两阶段提交，并且底层还需要做 Raft 复制，如果一个事务非常大，会使得提交过程非常慢，并且会卡住下面的 Raft 复制流程。为了避免系统出现被卡住的情况，我们对事务的大小做了限制： 单条 KV entry 不超过 6MB KV entry 的总条数不超过 30w KV entry 的总大小不超过 100MB  在 Google 的 Cloud Spanner 上面，也有类似的限制。  数据分片 TiKV 自动将底层数据按照 Key 的 Range 进行分片。每个 Region 是一个 Key 的范围，从 StartKey 到 EndKey 的左闭右开区间。Region 中的 Key-Value 总量超过一定值，就会自动分裂。这部分用户不需要担心。负载均衡 PD 会根据整个 TiKV 集群的状态，对集群的负载进行调度。调度是以 Region 为单位，以 PD 配置的策略为调度逻辑，自动完成。SQL on KV TiDB 自动将 SQL 结构映射为 KV 结构。具体的可以参考这篇文档。简单来说，TiDB 做了两件事： 一行数据映射为一个 KV，Key 以 TableID 构造前缀，以行 ID 为后缀 一条索引映射为一个 KV，Key 以 TableID+IndexID 构造前缀，以索引值构造后缀  可以看到，对于一个表中的数据或者索引，会具有相同的前缀，这样在 TiKV 的 Key 空间内，这些 Key-Value 会在相邻的位置。那么当写入量很大，并且集中在一个表上面时，就会造成写入的热点，特别是连续写入的数据中某些索引值也是连续的(比如 update time 这种按时间递增的字段)，会再很少的几个 Region 上形成写入热点，成为整个系统的瓶颈。同样，如果所有的数据读取操作也都集中在很小的一个范围内 (比如在连续的几万或者十几万行数据上)，那么可能造成数据的访问热点。Secondary Index TiDB 支持完整的二级索引，并且是全局索引，很多查询可以通过索引来优化。如果利用好二级索引，对业务非常重要，很多 MySQL 上的经验在 TiDB 这里依然适用，不过 TiDB 还有一些自己的特点，需要注意，这一节主要讨论在 TiDB 上使用二级索引的一些注意事项。 二级索引是否越多越好  二级索引能加速查询，但是要注意新增一个索引是有副作用的，在上一节中我们介绍了索引的存储模型，那么每增加一个索引，在插入一条数据的时候，就要新增一个 Key-Value，所以索引越多，写入越慢，并且空间占用越大。另外过多的索引也会影响优化器运行时间，并且不合适的索引会误导优化器。所以索引并不是越多越好。 对哪些列建索引比较合适上面提到，索引很重要但不是越多越好，我们需要根据具体的业务特点创建合适的索引。原则上我们需要对查询中需要用到的列创建索引，目的是提高性能。下面几种情况适合创建索引： 区分度比较大的列，通过索引能显著地减少过滤后的行数 有多个查询条件时，可以选择组合索引，注意需要把等值条件的列放在组合索引的前面  这里举一个例子，假设常用的查询是 select * from t where c1 = 10 and c2 = 100 and c3 &amp;gt; 10, 那么可以考虑建立组合索引 Index cidx (c1, c2, c3)，这样可以用查询条件构造出一个索引前缀进行 Scan。 通过索引查询和直接扫描 Table 的区别TiDB 实现了全局索引，所以索引和 Table 中的数据并不一定在一个数据分片上，通过索引查询的时候，需要先扫描索引，得到对应的行 ID，然后通过行 ID 去取数据，所以可能会涉及到两次网络请求，会有一定的性能开销。如果查询涉及到大量的行，那么扫描索引是并发进行，只要第一批结果已经返回，就可以开始去取 Table 的数据，所以这里是一个并行 + Pipeline 的模式，虽然有两次访问的开销，但是延迟并不会很大。有两种情况不会涉及到两次访问的问题： 索引中的列已经满足了查询需求。比如 Table t 上面的列 c 有索引，查询是 select c from t where c &amp;gt; 10; 这个时候，只需要访问索引，就可以拿到所需要的全部数据。这种情况我们称之为覆盖索引(Covering Index)。所以如果很关注查询性能，可以将部分不需要过滤但是需要再查询结果中返回的列放入索引中，构造成组合索引，比如这个例子： select c1, c2 from t where c1 &amp;gt; 10; 要优化这个查询可以创建组合索引 Index c12 (c1, c2)。 表的 Primary Key 是整数类型。在这种情况下，TiDB 会将 Primary Key 的值当做行 ID，所以如果查询条件是在 PK 上面，那么可以直接构造出行 ID 的范围，直接扫描 Table 数据，获取结果。  查询并发度数据分散在很多 Region 上，所以 TiDB 在做查询的时候会并发进行，默认的并发度比较保守，因为过高的并发度会消耗大量的系统资源，且对于 OLTP 类型的查询，往往不会涉及到大量的数据，较低的并发度已经可以满足需求。对于 OLAP 类型的 Query，往往需要较高的并发度。所以 TiDB 支持通过 System Variable 来调整查询并发度。 tidb_distsql_scan_concurrency在进行扫描数据的时候的并发度，这里包括扫描 Table 以及索引数据。 tidb_index_lookup_size如果是需要访问索引获取行 ID 之后再访问 Table 数据，那么每次会把一批行 ID 作为一次请求去访问 Table 数据，这个参数可以设置 Batch 的大小，较大的 Batch 会使得延迟增加，较小的 Batch 可能会造成更多的查询次数。这个参数的合适大小与查询涉及的数据量有关。一般不需要调整。 tidb_index_lookup_concurrency如果是需要访问索引获取行 ID 之后再访问 Table 数据，每次通过行 ID 获取数据时候的并发度通过这个参数调节。  通过索引保证结果顺序索引除了可以用来过滤数据之外，还能用来对数据排序，首先按照索引的顺序获取行 ID，然后再按照行 ID 的返回顺序返回行的内容，这样可以保证返回结果按照索引列有序。前面提到了扫索引和获取 Row 之间是并行 + Pipeline 模式，如果要求按照索引的顺序返回 Row，那么这两次查询之间的并发度设置的太高并不会降低延迟，所以默认的并发度比较保守。可以通过 tidb_index_serial_scan_concurrency 变量进行并发度调整。 逆序索引目前 TiDB 支持对索引进行逆序 Scan，但是速度要比顺序 Scan 慢 5 倍左右，所以尽量避免对索引的逆序 Scan。  场景与实践 上一节我们讨论了一些 TiDB 基本的实现机制及其对使用带来的影响，本节我们从具体的使用场景出发，谈一些更为具体的操作实践。我们以从部署到支撑业务这条链路为序，进行讨论。部署 在部署之前请务必阅读 TiDB 部署建议以及对硬件的需求。推荐通过 TiDB-Ansible 部署 TiDB 集群，这个工具可以部署、停止、销毁、升级整个集群，非常方便易用。具体的使用文档在这里。非常不推荐手动部署，后期的维护和升级会很麻烦。导入数据 如果有 Unique Key 并且业务端可以保证数据中没有冲突，可以在 Session 内打开这个开关： SET @@session.tidb_skip_constraint_check=1;另外为了提高写入性能，可以对 TiKV 的参数进行调优，具体的文档在这里。请特别注意这个参数：[raftstore] # 默认为 true，表示强制将数据刷到磁盘上。如果是非金融安全级别的业务场景，建议设置成 false， # 以便获得更高的性能。 sync-log = true 写入 上面提到了 TiDB 对单个事务的大小有限制，这层限制是在 KV 层面，反映在 SQL 层面的话，简单来说一行数据会映射为一个 KV entry，每多一个索引，也会增加一个 KV entry，所以这个限制反映在 SQL 层面是： 单行数据不大于 6MB 总的行数*(1 + 索引个数) &amp;lt; 30w 一次提交的全部数据小于 100MB  另外注意，无论是大小限制还是行数限制，还要考虑 TiDB 做编码以及事务额外 Key 的开销，在使用的时候，建议每个事务的行数不要超过 1w 行，否则有可能会超过限制，或者是性能不佳。建议无论是 Insert，Update 还是 Delete 语句，都通过分 Batch 或者是加 Limit 的方式限制。在删除大量数据的时候，建议使用 Delete * from t where xx limit 5000; 这样的方案，通过循环来删除，用 Affected Rows == 0 作为循环结束条件，这样避免遇到事务大小的限制。如果一次删除的数据量非常大，这种循环的方式会越来越慢，因为每次删除都是从前向后遍历，前面的删除之后，短时间内会残留不少删除标记(后续会被 gc 掉)，影响后面的 Delete 语句。如果有可能，建议把 Where 条件细化。举个例子，假设要删除 2017-05-26 当天的所有数据，那么可以这样做：for i from 0 to 23: while affected_rows &amp;gt; 0: delete * from t where insert_time &amp;gt;= i:00:00 and insert_time &amp;lt; (i+1):00:00 limit 5000; affected_rows = select affected_rows() 上面是一段伪代码，意思就是要把大块的数据拆成小块删除，以避免删除过程中前面的 Delete 语句影响后面的 Delete 语句。查询 看业务的查询需求以及具体的语句，可以参考这篇文档 可以通过 SET 语句控制 SQL 执行的并发度，另外通过 Hint 控制 Join 物理算子选择。另外 MySQL 标准的索引选择 Hint 语法，也可以用，通过 Use Index/Ignore Index hint 控制优化器选择索引。如果是个 OLTP 和 OLAP 混合类型的业务，可以把 TP 请求和 AP 请求发送到不同的 tidb-server 上，这样能够减小 AP 业务对于 TP 业务的影响。 承载 AP 业务的 tidb-server 推荐使用高配的机器，比如 CPU 核数比较多，内存比较大。监控 &amp;amp; 日志 Metrics 系统是了解系统状态的最佳方法，建议所有的用户都部署监控系统。TiDB 使用 Grafana+Prometheus 监控系统状态，如果使用 TiDB-Ansible 部署集群，那么会自动部署和配置监控系统。监控系统中的监控项很多，大部分是给 TiDB 开发者查看的内容，如果没有对源代码比较深入的了解，并没有必要了解这些监控项。我们会精简出一些和业务相关或者是系统关键组件状态相关的监控项，放在一个独立的面板中，供用户使用。除了监控之外，查看日志也是了解系统状态的常用方法。TiDB 的三个组件 tidb-server/tikv-server/pd-server 都有一个 --log-file 的参数，如果启动的时候设置了这个参数，那么日志会保存着参数所设置的文件的位置，另外会自动的按天对 Log 文件做归档。如果没有设置 --log-file 参数，日志会输出在 stderr 中。文档 了解一个系统或者解决使用中的问题最好的方法是阅读文档，明白实现原理，TiDB 有大量的官方文档，希望大家在遇到问题的时候能先尝试通过文档或者搜索 Issue list 寻找解决方案。官方文档在这里。如果希望阅读英文文档，可以看这里。其中的 FAQ 和故障诊断章节建议大家仔细阅读。另外 TiDB 还有一些不错的工具，也有配套的文档，具体的见各项工具的 GitHub 页面。除了文档之外，还有很多不错的文章介绍 TiDB 的各项技术细节内幕，大家可以关注下面这些文章发布渠道： 公众号：微信搜索 PingCAP 知乎专栏：TiDB 的后花园 官方博客  TiDB 的最佳适用场景 简单来说，TiDB 适合具备下面这些特点的场景： 数据量大，单机保存不下 不希望做 Sharding 或者懒得做 Sharding 访问模式上没有明显的热点 需要事务、需要强一致、需要灾备  "},
		{"url": "https://pingcap.com/weekly/2017-07-04-tidb-weekly/",
		"title": "Weekly update (June 26 ~ July 02, 2017)", 
		"content": " Weekly update in TiDB Last week, we landed 33 PRs in the TiDB repositories.Added  Support priority options in the select statement. Add the GetOwnerID interface in OwnerManager. Add some columns in mysql.db: Make it compatible with MySQL. Add the IsolationLevel option in the coprocessor request to TiKV. Add the priority option in Key-Value request to TiKV. Support the RenameTable statement without the To keyword.  Fixed  Fix a bug in the update statement like update T set a = 8, b = a. Consider the timezone information when using current_timestamp as the default value. Break the campaigning owner loop when closing a DDL worker. Check the validity of table name in the CreateTable statement. Handle the null argument in substr function. Disable concurrent pre-write when meeting the SkipConstrainCheck option. Handle binary data in the HEX function and the unhex function. Return ErrNoDB (No database selected) when no database is used and the table name is without database qualifier.  Improved  Refactor optimiser:  Prune ordered column paths. Prepare for using the new CBO framework.  Flatten the row operations. Log critical statements. Code clean up.  New contributor sempr JackDrogon Zhang JianWeekly update in TiKV Last week, We landed 12 PRs in the TiKV repositories.Added  Use the bi-directional stream for PD region heartbeat. Add the selection executor for coprocessor. Support the read committed isolation level for transactions. Add the modify function for JSON document.  Fixed  Use Equal to check parsed timestamp. Fix the heartbeat tests.  Improved  Generate snapshot concurrently. Skip handling read only commands in the Raft apply worker. Upgrade Rust Protobuf to the 1.4 version.  "},
		{"url": "https://pingcap.com/meetup/meetup-2017-07-01/",
		"title": "【Infra Meetup NO.51】百度统一分布式计算框架 Bigflow", 
		"content": " 今天的 Meetup，我们邀请到了滴滴地图事业部专家工程师王聪老师，为大家分享《百度统一分布式计算框架 Bigflow 》。 王聪，滴滴地图事业部专家工程师，前百度基础架构部工程师，主要工作方向为分布式计算与流式计算，在百度负责计算表示层 Bigflow 与流式计算引擎 Flink。活动现场听得很专注的小伙伴们，桑拿天也阻止不了大家的学习热情。王聪老师首先展示了分布式计算在百度的发展例程，他介绍百度在 2003 年建立了自己的分布式搜索系统。08 年引入 hadoop，09 年底搭建了大规模的机器学习平台，当时用的是 MPI。10 年百度自研了两套流式计算引擎，主要用来完成点击流与展现流的 join。基于多引擎并存、跨引擎成本高、升级困难这几个痛点，最终开发了一款叫做 Bigflow 的计算框架，Bigflow 希望用户使用我们提供的 API 写代码，Bigflow 将作业进行计划的优化和翻译，并提交到计算引擎之上。对于这样的思路，有一种说法“计算机领域的任何问题，都可以通过增加一个额外的中间层来解决”。在这里 Bigflow 就是架在用户与引擎之间的中间层。以下是新鲜出炉的 PPT 节选，尽情享用~点击下载 PPT"},
		{"url": "https://pingcap.com/blog/2017-06-27-refactor-builtin/",
		"title": "Refactoring the Built-in Functions in TiDB", 
		"content": "  In order to accelerate expression evaluation, we recently refactored its framework. This tutorial will show you how to use the new computational framework to rewrite or add a built-in function in TiDB.Table of Content  The overall process Example  Take a look at builtin_string.go Refine the existing TestLength() method Test the implementation of LENGTH at the SQL level   Before refactoring&amp;hellip; After refactoring&amp;hellip; Appendix  The overall process   Select any function to your interest from the expression directory, assuming the function is named XX. Override the XXFunctionClass.getFunction() method:This method refers to MySQL rules, inferring the return value type according to the parameter of the built-in function.Different function signatures will be generated based on the number &amp;amp; type of the parameters, and the return value type of the function. See detailed description of the function signature in the appendix at the end of this article. Implement the evalYY() method on all the function signatures corresponding to the built-in function. YY represents the return value type of the function signature. Add tests in the expression directory, refine tests about the implementation of the given function in the TestXX() method. In the executor directory, add tests at the SQL level. Run make dev and ensure that all the test cases pass.  Example: Let&amp;rsquo;s look at the PR of overriding the LENGTH () function for detailed explanation:Take a look at expression/builtin_string.go： First, let&amp;rsquo;s take a look at the expression/builtin_string.go file: Implement the lengthFunctionClass.getFunction() method. This method mainly accomplishes two tasks:1). Infer the return value type of the LEGNTH function according to MySQL rules.2). Generate function signature based on the number &amp;amp; type of parameters, and return value type of the LENGTH function. Because the LENGTH function only has one number &amp;amp; type of parameters, and return value type, we don’t need to define a type for the new function signature. Instead, we modified the existing builtinLengthSig, so that it could be composite with baseIntBuiltinFunc, which means that the return value type in the given function signature is int.type builtinLengthSig struct { baseIntBuiltinFunc } func (c *lengthFunctionClass) getFunction(args []Expression, ctx context.Context) (builtinFunc, error) { //Infer the return value type of `LEGNTH` function according to MySQL rules tp := types.NewFieldType(mysql.TypeLonglong) tp.Flen = 10 types.SetBinChsClnFlag(tp) //Generate function signature based on the number &amp;amp; type of parameters, and return value type. Note that after refactoring, instead of the `newBaseBuiltinFunc` method, the `newBaseBuiltinFuncWithTp` method is used here. //In the `newBaseBuiltinFuncWithTp` function declaration, `args` represents the function&amp;#39;s parameters, `tp` represents the return value type of the function, and `argsTp` represents the correct types of all parameters in the function signature. // The number of parameters for `LENGTH` is 1, the parameter type is string, and the return value type is int. Therefore, `tp` here stands for the return value type of the function and `tpString` is used to identify the correct type of parameter. For a function with multiple parameters, when calling `newBaseBuiltinFuncWithTp`, we need to input the correct types of all parameters. bf, err := newBaseBuiltinFuncWithTp(args, tp, ctx, tpString) if err != nil { return nil, errors.Trace(err) } sig := &amp;amp;builtinLengthSig{baseIntBuiltinFunc{bf}} return sig.setSelf(sig), errors.Trace(c.verifyArgs(args)) } Implement the builtinLengthSig.evalInt() method:func (b *builtinLengthSig) evalInt(row []types.Datum) (int64, bool, error) { // For the `builtinLengthSig` function signature, the parameter type is decided as string, so we can directly call the `b.args[0].EvalString()` method to calculate the parameter:  val, isNull, err := b.args[0].EvalString(row, b.ctx.GetSessionVars().StmtCtx) if isNull || err != nil { return 0, isNull, errors.Trace(err) } return int64(len([]byte(val))), false, nil }  Back to the topRefine the existing TestLength() method: Moving on to expression/builtin_string_test.go, let’s refine the existing TestLength() method:func (s *testEvaluatorSuite) TestLength(c *C) { defer testleak.AfterTest(c)() // This line is used to monitor goroutine leak.  // You can use the following cases to test the `LENGTH` function  // Note: in addition to normal cases, it is best to add some abnormal cases, such as the input args is nil, or the function has different types of parameters.  cases := []struct { args interface{} expected int64 isNil bool getErr bool }{ {&amp;#34;abc&amp;#34;, 3, false, false}, {&amp;#34;你好&amp;#34;, 6, false, false}, {1, 1, false, false}, ... } for _, t := range cases { f, err := newFunctionForTest(s.ctx, ast.Length, primitiveValsToConstants([]interface{}{t.args})...) c.Assert(err, IsNil) // The following lines test the return value type of the `LENGTH` function:  tp := f.GetType() c.Assert(tp.Tp, Equals, mysql.TypeLonglong) c.Assert(tp.Charset, Equals, charset.CharsetBin) c.Assert(tp.Collate, Equals, charset.CollationBin) c.Assert(tp.Flag, Equals, uint(mysql.BinaryFlag)) c.Assert(tp.Flen, Equals, 10) // The following lines test the evaluation result of LENGTH function:  d, err := f.Eval(nil) if t.getErr { c.Assert(err, NotNil) } else { c.Assert(err, IsNil) if t.isNil { c.Assert(d.Kind(), Equals, types.KindNull) } else { c.Assert(d.GetInt64(), Equals, t.expected) } } } // The following lines test whether the function has determinacy:  f, err := funcs[ast.Length].getFunction([]Expression{Zero}, s.ctx) c.Assert(err, IsNil) c.Assert(f.isDeterministic(), IsTrue) } Back to the topTest the implementation of LENGTH at the SQL level  Finally let’s look at executor/executor_test.go and test the implementation of LENGTH at the SQL level:// Tests for string built-in functions can be added in the following method: func (s *testSuite) TestStringBuiltin(c *C) { defer func() { s.cleanEnv(c) testleak.AfterTest(c)() }() tk := testkit.NewTestKit(c, s.store) tk.MustExec(&amp;#34;use test&amp;#34;) // for length  // It’s best that these tests can also cover different scenarios:  tk.MustExec(&amp;#34;drop table if exists t&amp;#34;) tk.MustExec(&amp;#34;create table t(a int, b double, c datetime, d time, e char(20), f bit(10))&amp;#34;) tk.MustExec(`insert into t values(1, 1.1, &amp;#34;2017-01-01 12:01:01&amp;#34;, &amp;#34;12:01:01&amp;#34;, &amp;#34;abcdef&amp;#34;, 0b10101)`) result := tk.MustQuery(&amp;#34;select length(a), length(b), length(c), length(d), length(e), length(f), length(null) from t&amp;#34;) result.Check(testkit.Rows(&amp;#34;1 3 19 8 6 2 &amp;lt;nil&amp;gt;&amp;#34;)) } Before refactoring&amp;hellip; TiDB abstracts the expression through the Expression interface (defined in the expression/expression.go file) and defines the eval method to evaluate the expression:type Expression interface{ ... eval(row []types.Datum) (types.Datum, error) ... } Expressions that implement the Expression interface include: Scalar Function Column Constant  The case below shows the framework of expression evaluation before refactoring:create table t ( c1 int, c2 varchar(20), c3 double ) select * from t where c1 + CONCAT( c2, c3 &amp;lt; “1.1” ) About the where condition in the select statement shown above:In the compiling phase. TiDB will build an expression tree as shown in the graph below:In the executing phase, the eval method of the root node is called, and the expression is evaluated by the following traversal expression tree.The evaluate the &amp;lt; expression, take the types of the two parameters into account, and convert the values of the two parameters into required data types according to certain rules. In the expression tree above, the parameter types are double and varchar. According to the evaluation rules of MySQL, these two parameters need to be compared using the float type. Therefore, &amp;ldquo;1.1&amp;rdquo; should be converted to double type, and then be evaluated.Similarly, for …"},
		{"url": "https://pingcap.com/weekly/2017-06-26-tidb-weekly/",
		"title": "Weekly update (June 19 ~ June 25, 2017)", 
		"content": " Weekly update in TiDB Last week, we landed 35 PRs in the TiDB repositories.Added  JSON type:  Tiny cleanup. Fix a bug in JSON path_expr. Fix a bug in unquote. Make json_unquote compatiable with MySQL Code cleanup.   Fixed  Fix failed test cases on the ppc64le platform Fix a bug when naming table with the auto_increment column. Use UTC time to compose index key for timestamp column. Consider fsp for MySQLDuration type in some scenarios. Reset transaction when error occurs during retrying. Fix a bug about adding index after adding column with default value. Suport time format &amp;lsquo;YY-MM-DD HH:MM&amp;rsquo;. Fix the issue of parsing datetime 00-00-00.  Improved  Refactor the expression evaluation:  Rewrite concat with new expression framework. Remove the inferType inerface of functionClass. Rewrite the length built-in function with the new expression framework. Wrap arguments when building new built-in functions. Rewrite the makedate built-in function with the new expression framework.  Change the result format of the aggregation operator. Refactor range related code: #3489, #3496 Code style cleanup:#3502, #3505 Handle out-of-range data in the load local file command. Remove the TiDBSkipDDLWait variable.  New contributor dreamqusterWeekly update in TiKV Last week, We landed 24 PRs in the TiKV repositories.Added  Use individual thread to handle high priority command. Support JSON type. Support the json_type, unquote, extract, json_merge functions for JSON. Add the TOPN, limit executors for coprocessor.  Fixed  Handling GC command in tombstone tests. Print readable message if the RocksDB configuration parsing fails.  Improved  Split different JSON modules into independent files. Revise leader lease time log. Remove sever thread and directly send Raft messages to the gRPC thread. Remove unnecessary read_quorum and uuid fields.  "},
		{"url": "https://pingcap.com/blog-cn/tangliu-tool-2/",
		"title": "工欲性能调优，必先利其器（2）- 火焰图", 
		"content": " 在前一篇文章，我们简单提到了 perf，实际 perf 能做的事情远远不止这么少，这里就要好好介绍一下，我们在 TiKV 性能调优上面用的最多的工具 - 火焰图。火焰图，也就是 FlameGraph，是超级大牛 Brendan Gregg 捣鼓出来的东西，主要就是将 profile 工具生成的数据进行可视化处理，方便开发人员查看。我第一次知道火焰图，应该是来自 OpenResty 的章亦春介绍，大家可以详细去看看这篇文章动态追踪技术漫谈。之前，我的所有工作在很长一段时间几乎都是基于 Go 的，而 Go 原生提供了很多相关的 profile 工具，以及可视化方法，所以我没怎么用过火焰图。但开始用 Rust 开发 TiKV 之后，我就立刻傻眼了，Rust 可没有官方的工具来做这些事情，怎么搞？自然，我们就开始使用火焰图了。使用火焰图非常的简单，我们仅仅需要将代码 clone 下来就可以了，我通常喜欢将相关脚本扔到 /opt/FlameGraph 下面，后面也会用这个目录举例说明。一个简单安装的例子：wget https://github.com/brendangregg/FlameGraph/archive/master.zip unzip master.zip sudo mv FlameGraph-master/ /opt/FlameGraph CPU 对于 TiKV 来说，性能问题最开始关注的就是 CPU，毕竟这个是一个非常直观的东西。当我们发现 TiKV CPU 压力很大的时候，通常会对 TiKV 进行 perf，如下：perf record -F 99 -p tikv_pid -g -- sleep 60 perf script &amp;gt; out.perf 上面，我们对一个 TiKV 使用 99 HZ 的频繁采样 60 s，然后生成对应的采样文件。然后我们生成火焰图：/opt/FlameGraph/stackcollapse-perf.pl out.perf &amp;gt; out.folded /opt/FlameGraph/flamegraph.pl out.folded &amp;gt; cpu.svg 上面就是生成的一个 TiKV 火焰图，我们会发现 gRPC 线程主要开销在 c gRPC core 上面，而这个也是现在 c gRPC core 大家普遍反映的一个问题，就是太费 CPU，但我相信凭借 Google gRPC team 的实力，这问题应该能够搞定。另外，在 gRPC 线程上面，我们可以发现，protobuf 的编解码也占用了很多 CPU，这个也是现阶段 rust protobuf 库的一个问题，性能比较差，但幸好后面的办法有一个优化，我们准备马上采用。另外，还需要注意，raftstore 线程主要的开销在于 RocksDB 的 Get 和 Write，对于 TiKV 来说，如果 raftstore 线程出现了瓶颈，那么整个 Raft 流程都会被拖慢，所以自然这个线程就是我们的重点优化对象。可以看到，Get 的开销其实就是我们拿到 Raft 的 committed entries，然后扔给 apply Raft log 线程去异步 apply，所以自然这一步 Get 自然能扔到 apply worker 去处理。另外，对于 Write，鉴于 Raft log 的格式，我们可以非常方便的使用 RocksDB 一个 insert_with_hint 特性来优化，或者将 Write 也放到另一个线程去 async 处理。可以看到，我们通过火焰图，能够非常方便的发现 CPU 大部分时间开销都消耗在哪里，也就知道如何优化了。这里在说一下，大家通常喜欢将目光定在 CPU 消耗大头的地方，但有时候一些小的不起眼的地方，也需要引起大家的注意。这里并不是这些小地方会占用多少 CPU，而是要问为啥会在火焰图里面出现，因为按照正常逻辑是不可能的。我们通过观察 CPU 火焰图这些不起眼的小地方，至少发现了几处代码 bug。Memory 通常大家用的最多的是 CPU 火焰图，毕竟这个最直观，但火焰图可不仅仅只有 CPU 的。我们还需要关注除了 CPU 之外的其他指标。有一段时间，我对 TiKV 的内存持续上涨问题一直很头疼，虽然 TiKV 有 OOM，但总没有很好的办法来定位到底是哪里出了问题。于是也就研究了一下 memory 火焰图。要 profile TiKV 的 memory 火焰图，其实我们就需要监控 TiKV 的 malloc 分配，只要有 malloc，就表明这时候 TiKV 在进行内存分配。因为 TiKV 是自己内部使用了 jemalloc，并没有用系统的 malloc，所以我们不能直接用 perf 来探查系统的 malloc 函数。幸运的是，perf 能支持动态添加探针，我们将 TiKV 的 malloc 加入：perf probe -x /deploy/bin/tikv-server -a malloc 然后采样生成火焰图:perf record -e probe_tikv:malloc -F 99 -p tikv_pid -g -- sleep 10 perf script &amp;gt; out.perf /opt/FlameGraph/stackcollapse-perf.pl out.perf &amp;gt; out.folded /opt/FlameGraph/flamegraph.pl --colors=mem out.folded &amp;gt; mem.svg 上面是生成的一个 malloc 火焰图，我们可以看到，大部分的内存开销仍然是在 RocksDB 上面。通过 malloc 火焰图，我们曾发现过 RocksDB 的 ReadOption 在非常频繁的调用分配，后面准备考虑直接在 stack 上面分配，不过这个其实对性能到没啥太大影响 😓 。除了 malloc，我们也可以 probe minor page fault 和 major page fault，因为用 pidstat 观察发现 TiKV 很少有 major page fault，所以我们只 probe 了 minor，如下：perf record -e minor-faults -F 99 -p $1 -g -- sleep 10 perf script &amp;gt; out.perf /opt/FlameGraph/stackcollapse-perf.pl out.perf &amp;gt; out.folded /opt/FlameGraph/flamegraph.pl --colors=mem out.folded &amp;gt; minflt.svg Off CPU 有时候，我们还会面临一个问题。系统的性能上不去，但 CPU 也很闲，这种的很大可能都是在等 IO ，或者 lock 这些的了，所以我们需要看到底 CPU 等在什么地方。对于 perf 来说，我们可以使用如下方式采样 off CPU。perf record -e sched:sched_stat_sleep -e sched:sched_switch  -e sched:sched_process_exit -p tikv_pid -g -o perf.data.raw sleep 10 perf inject -v -s -i perf.data.raw -o perf.data 但不幸的是，上面的代码在 Ubuntu 或者 CentOS 上面通常都会失败，主要是现在最新的系统为了性能考虑，并没有支持 sched statistics。 对于 Ubuntu，貌似只能重新编译内核，而对于 CentOS，只需要安装 kernel debuginfo，然后在打开 sched statistics 就可以了，如下:dnf install kernel-debug kernel-debug-devel kernel-debug-debuginfo echo 1 | sudo tee /proc/sys/kernel/sched_schedstats 然后生成 off cpu 火焰图:perf script -F comm,pid,tid,cpu,time,period,event,ip,sym,dso,trace | awk &amp;#39; NF &amp;gt; 4 { exec = $1; period_ms = int($5 / 1000000) } NF &amp;gt; 1 &amp;amp;&amp;amp; NF &amp;lt;= 4 &amp;amp;&amp;amp; period_ms &amp;gt; 0 { print $2 } NF &amp;lt; 2 &amp;amp;&amp;amp; period_ms &amp;gt; 0 { printf &amp;#34;%sn%dnn&amp;#34;, exec, period_ms }&amp;#39; |  /opt/FlameGraph/stackcollapse.pl |  /opt/FlameGraph/flamegraph.pl --countname=ms --title=&amp;#34;Off-CPU Time Flame Graph&amp;#34; --colors=io &amp;gt; offcpu.svg 上面就是 TiKV 一次 off CPU 的火焰图，可以发现只要是 server event loop 和 time monitor 两个线程 off CPU 比较长，server event loop 是等待外部的网络请求，因为我在 perf 的时候并没有进行压力测试，所以 wait 是正常的。而 time monitor 则是 sleep 一段时间，然后检查时间是不是出现了 jump back，因为有长时间的 sleep，所以也是正常的。上面我说到，对于 Ubuntu 用户，貌似只能重新编译内核，打开 sched statistics，如果不想折腾，我们可以通过 systemtap 来搞定。systemtap 是另外一种 profile 工具，其实应该算另一门语言了。我们可以直接使用 OpenResty 的 systemtap 工具，来生成 off CPU 火焰图，如下：wget https://raw.githubusercontent.com/openresty/openresty-systemtap-toolkit/master/sample-bt-off-cpu chmod +x sample-bt-off-cpu ./sample-bt-off-cpu -t 10 -p 13491 -u &amp;gt; out.stap /opt/FlameGraph/stackcollapse-stap.pl out.stap &amp;gt; out.folded /opt/FlameGraph/flamegraph.pl --colors=io out.folded &amp;gt; offcpu.svg 可以看到，使用 systemptap 的方式跟 perf 没啥不一样，但 systemtap 更加复杂，毕竟它可以算是一门语言。而 FlameGraph 里面也自带了 systemaptap 相关的火焰图生成工具。Diff 火焰图 除了通常的几种火焰图，我们其实还可以将两个火焰图进行 diff，生成一个 diff 火焰图，如下：/opt/difffolded.pl out.folded1 out.folded2 | ./flamegraph.pl &amp;gt; diff2.svg 但现在我仅仅只会生成，还没有详细对其研究过，这里就不做过多说明了。总结 上面简单介绍了我们在 TiKV 里面如何使用火焰图来排查问题，现阶段主要还是通过 CPU 火焰图发现了不少问题，但我相信对于其他火焰图的使用研究，后续还是会很有帮助的。"},
		{"url": "https://pingcap.com/blog-cn/reconstruct-built-in-function/",
		"title": "十分钟成为 Contributor 系列 | 为 TiDB 重构 built-in 函数", 
		"content": " 这是十分钟成为 TiDB Contributor 系列的第二篇文章，让大家可以无门槛参与大型开源项目，感谢社区为 TiDB 带来的贡献，也希望参与 TiDB Community 能为你的生活带来更多有意义的时刻。为了加速表达式计算速度，最近我们对表达式的计算框架进行了重构，这篇教程为大家分享如何利用新的计算框架为 TiDB 重写或新增 built-in 函数。对于部分背景知识请参考这篇文章，本文将首先介绍利用新的表达式计算框架重构 built-in 函数实现的流程，然后以一个函数作为示例进行详细说明，最后介绍重构前后表达式计算框架的区别。重构 built-in 函数整体流程  在 TiDB 源码 expression 目录下选择任一感兴趣的函数，假设函数名为 XX 重写 XXFunctionClass.getFunction() 方法 该方法参照 MySQL 规则，根据 built-in 函数的参数类型推导函数的返回值类型 根据参数的个数、类型、以及函数的返回值类型生成不同的函数签名，关于函数签名的详细介绍见文末附录  实现该 built-in 函数对应的所有函数签名的 evalYY() 方法，此处 YY 表示该函数签名的返回值类型 添加测试： 在 expression 目录下，完善已有的 TestXX() 方法中关于该函数实现的测试 在 executor 目录下，添加 SQL 层面的测试  运行 make dev，确保所有的 test cast 都能跑过  示例 这里以重写 LENGTH() 函数的 PR 为例，进行详细说明首先看 expression/builtin_string.go:（1）实现 lengthFunctionClass.getFunction() 方法该方法主要完成两方面工作： 参照 MySQL 规则推导 LEGNTH 的返回值类型 根据 LENGTH 函数的参数个数、类型及返回值类型生成函数签名。由于 LENGTH 的参数个数、类型及返回值类型只存在确定的一种情况，因此此处没有定义新的函数签名类型，而是修改已有的 builtinLengthSig，使其组合了 baseIntBuiltinFunc（表示该函数签名返回值类型为 int）  type builtinLengthSig struct { baseIntBuiltinFunc } func (c *lengthFunctionClass) getFunction(args []Expression, ctx context.Context) (builtinFunc, error) { // 参照 MySQL 规则，对 LENGTH 函数返回值类型进行推导 	tp := types.NewFieldType(mysql.TypeLonglong) tp.Flen = 10 types.SetBinChsClnFlag(tp) // 根据参数个数、类型及返回值类型生成对应的函数签名，注意此处与重构前不同，使用的是 newBaseBuiltinFuncWithTp 方法，而非 newBaseBuiltinFunc 方法 	// newBaseBuiltinFuncWithTp 的函数声明中，args 表示函数的参数，tp 表示函数的返回值类型，argsTp 表示该函数签名中所有参数对应的正确类型 	// 因为 LENGTH 的参数个数为1，参数类型为 string，返回值类型为 int，因此此处传入 tp 表示函数的返回值类型，传入 tpString 用来标识参数的正确类型。对于多个参数的函数，调用 newBaseBuiltinFuncWithTp 时，需要传入所有参数的正确类型 	bf, err := newBaseBuiltinFuncWithTp(args, tp, ctx, tpString) if err != nil { return nil, errors.Trace(err) } sig := &amp;amp;builtinLengthSig{baseIntBuiltinFunc{bf}} return sig.setSelf(sig), errors.Trace(c.verifyArgs(args)) } (2) 实现 builtinLengthSig.evalInt() 方法func (b *builtinLengthSig) evalInt(row []types.Datum) (int64, bool, error) { // 对于函数签名 builtinLengthSig，其参数类型已确定为 string 类型，因此直接调用 b.args[0].EvalString() 方法计算参数 	val, isNull, err := b.args[0].EvalString(row, b.ctx.GetSessionVars().StmtCtx) if isNull || err != nil { return 0, isNull, errors.Trace(err) } return int64(len([]byte(val))), false, nil } 然后看 expression/builtin_string_test.go，对已有的 TestLength() 方法进行完善：func (s *testEvaluatorSuite) TestLength(c *C) { defer testleak.AfterTest(c)() // 监测 goroutine 泄漏的工具，可以直接照搬  // cases 的测试用例对 length 方法实现进行测试 	// 此处注意，除了正常 case 之外，最好能添加一些异常的 case，如输入值为 nil，或者是多种类型的参数 	cases := []struct { args interface{} expected int64 isNil bool getErr bool }{ {&amp;#34;abc&amp;#34;, 3, false, false}, {&amp;#34;你好&amp;#34;, 6, false, false}, {1, 1, false, false}, ... } for _, t := range cases { f, err := newFunctionForTest(s.ctx, ast.Length, primitiveValsToConstants([]interface{}{t.args})...) c.Assert(err, IsNil) // 以下对 LENGTH 函数的返回值类型进行测试 	tp := f.GetType() c.Assert(tp.Tp, Equals, mysql.TypeLonglong) c.Assert(tp.Charset, Equals, charset.CharsetBin) c.Assert(tp.Collate, Equals, charset.CollationBin) c.Assert(tp.Flag, Equals, uint(mysql.BinaryFlag)) c.Assert(tp.Flen, Equals, 10) // 以下对 LENGTH 函数的计算结果进行测试 	d, err := f.Eval(nil) if t.getErr { c.Assert(err, NotNil) } else { c.Assert(err, IsNil) if t.isNil { c.Assert(d.Kind(), Equals, types.KindNull) } else { c.Assert(d.GetInt64(), Equals, t.expected) } } } // 以下测试函数是否是具有确定性 	f, err := funcs[ast.Length].getFunction([]Expression{Zero}, s.ctx) c.Assert(err, IsNil) c.Assert(f.isDeterministic(), IsTrue) } 最后看 executor/executor_test.go，对 LENGTH 的实现进行 SQL 层面的测试：// 关于 string built-in 函数的测试可以在这个方法中添加 func (s *testSuite) TestStringBuiltin(c *C) { defer func() { s.cleanEnv(c) testleak.AfterTest(c)() }() tk := testkit.NewTestKit(c, s.store) tk.MustExec(&amp;#34;use test&amp;#34;) // for length 	// 此处的测试最好也能覆盖多种不同的情况 	tk.MustExec(&amp;#34;drop table if exists t&amp;#34;) tk.MustExec(&amp;#34;create table t(a int, b double, c datetime, d time, e char(20), f bit(10))&amp;#34;) tk.MustExec(`insert into t values(1, 1.1, &amp;#34;2017-01-01 12:01:01&amp;#34;, &amp;#34;12:01:01&amp;#34;, &amp;#34;abcdef&amp;#34;, 0b10101)`) result := tk.MustQuery(&amp;#34;select length(a), length(b), length(c), length(d), length(e), length(f), length(null) from t&amp;#34;) result.Check(testkit.Rows(&amp;#34;1 3 19 8 6 2 &amp;lt;nil&amp;gt;&amp;#34;)) } 重构前的表达式计算框架 TiDB 通过 Expression 接口(在 expression/expression.go 文件中定义)对表达式进行抽象，并定义 eval 方法对表达式进行计算：type Expression interface{ ... eval(row []types.Datum) (types.Datum, error) ... } 实现 Expression 接口的表达式包括： Scalar Function：标量函数表达式 Column：列表达式 Constant：常量表达式  下面以一个例子说明重构前的表达式计算框架。例如：create table t ( c1 int, c2 varchar(20), c3 double ) select * from t where c1 + CONCAT( c2, c3 &amp;lt; “1.1” ) 对于上述 select 语句 where 条件中的表达式： 在编译阶段，TiDB 将构建出如下图所示的表达式树:在执行阶段，调用根节点的 eval 方法，通过后续遍历表达式树对表达式进行计算。对于表达式 ‘&amp;lt;’，计算时需要考虑两个参数的类型，并根据一定的规则，将两个参数的值转化为所需的数据类型后进行计算。上图表达式树中的 ‘&amp;lt;’，其参数类型分别为 double 和 varchar，根据 MySQL 的计算规则，此时需要使用浮点类型的计算规则对两个参数进行比较，因此需要将参数 “1.1” 转化为 double 类型，而后再进行计算。同样的，对于上图表达式树中的表达式 CONCAT，计算前需要将其参数分别转化为 string 类型；对于表达式 ‘+’，计算前需要将其参数分别转化为 double 类型。因此，在重构前的表达式计算框架中，对于参与运算的每一组数据，计算时都需要大量的判断分支重复地对参数的数据类型进行判断，若参数类型不符合表达式的运算规则，则需要将其转换为对应的数据类型。此外，由 Expression.eval() 方法定义可知，在运算过程中，需要通过 Datum 结构不断地对中间结果进行包装和解包，由此也会带来一定的时间和空间开销。为了解决这两点问题，我们对表达式计算框架进行重构。##重构后的表达式计算框架 重构后的表达式计算框架，一方面，在编译阶段利用已有的表达式类型信息，生成参数类型“符合运算规则”的表达式，从而保证在运算阶段中无需再对类型增加分支判断；另一方面，运算过程中只涉及原始类型数据，从而避免 Datum 带来的时间和空间开销。继续以上文提到的查询为例，在编译阶段，生成的表达式树如下图所示，对于不符合函数参数类型的表达式，为其加上一层 cast 函数进行类型转换；这样，在执行阶段，对于每一个 ScalarFunction，可以保证其所有的参数类型一定是符合该表达式运算规则的数据类型，无需在执行过程中再对参数类型进行检查和转换。附录  对于一个 built-in 函数，由于其参数个数、类型以及返回值类型的不同，可能会生成多个函数签名分别用来处理不同的情况。对于大多数 built-in 函数，其每个参数类型及返回值类型均确定，此时只需要生成一个函数签名。 对于较为复杂的返回值类型推导规则，可以参考 CONCAT 函数的实现和测试。可以利用 MySQLWorkbench 工具运行查询语句 select funcName(arg0, arg1, ...) 观察 MySQL 的 built-in 函数在传入不同参数时的返回值数据类型。 在 TiDB 表达式的运算过程中，只涉及 6 种运算类型(目前正在实现对 JSON 类型的支持)，分别是   int (int64) real (float64) decimal string Time Duration  通过 WrapWithCastAsXX() 方法可以将一个表达式转换为对应的类型。 对于一个函数签名，其返回值类型已经确定，所以定义时需要组合与该类型对应的 baseXXBuiltinFunc，并实现 evalXX() 方法。(XX 不超过上述 6 种类型的范围)  &amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;- 我是 AI 的分割线 &amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;-回顾三月启动的《十分钟成为 TiDB Contributor 系列 | 添加內建函数》活动，在短短的时间内，我们收到了来自社区贡献的超过 200 条新建內建函数，这之中有很多是来自大型互联网公司的资深数据库工程师，也不乏在学校或是刚毕业在刻苦钻研分布式系统和分布式数据库的学生。TiDB Contributor Club 将大家聚集起来，我们互相分享、讨论，一起成长。感谢你的参与和贡献，在开源的道路上我们将义无反顾地走下去，和你一起。成为 New Contributor 赠送限量版马克杯的活动还在继续中，任何一个新加入集体的小伙伴都将收到我们充满了诚意的礼物，很荣幸能够认识你，也很高兴能和你一起坚定地走得更远。成为 New Contributor 获赠限量版马克杯，马克杯获取流程如下：  提交 PR PR提交之后，请耐心等待维护者进行 Review。 目前一般在一到两个工作日内都会进行 Review，如果当前的 PR 堆积数量较多可能回复会比较慢。 代码提交后 CI 会执行我们内部的测试，你需要保证所有的单元测试是可以通过的。期间可能有其它的提交会与当前 PR 冲突，这时需要修复冲突。 维护者在 Review 过程中可能会提出一些修改意见。修改完成之后如果 reviewer 认为没问题了，你会收到 LGTM(looks good to me) 的回复。当收到两个及以上的 LGTM 后，该 PR 将会被合并。 合并 PR 后自动成为 Contributor，会收到来自 PingCAP Team 的感谢邮件，请查收邮件并填写领取表单 表单填写地址：http://cn.mikecrm.com/01wE8tX  后台 AI 核查 GitHub ID 及资料信息，确认无误后随即便快递寄出属于你的限量版马克杯 期待你分享自己参与开源项目的感想和经验，TiDB Contributor Club 将和你一起分享开源的力量  了解更多关于 TiDB 的资料请登陆我们的官方网站：https://pingcap.com加入 TiDB Contributor Club 请添加我们的 AI 微信："},
		{"url": "https://pingcap.com/blog/2017-06-20-rc3/",
		"title": "TiDB RC3 Release Notes", 
		"content": " Highlight:  The privilege management is refined to enable users to manage the data access privileges using the same way as in MySQL. DDL is accelerated. The load balancing policy and process are optimized for performance. TiDB-Ansible is open sourced. By using TiDB-Ansilbe, you can deploy, upgrade, start and shutdown a TiDB cluster with one click.  Detailed updates: TiDB:  The following features are added or improved in the SQL query optimizer: Support incremental statistics Support the Merge Sort Join operator Support the Index Lookup Join operator Support the Optimizer Hint Syntax Optimize the memory consumption of the Scan, Join, Aggregation operators Optimize the Cost Based Optimizer (CBO) framework Refactor Expression  Support more complete privilege management DDL acceleration Support using HTTP API to get the data distribution information of tables Support using system variables to control the query concurrency Add more MySQL built-in functions Support using system variables to automatically split a big transaction into smaller ones to commit  Placement Driver (PD):  Support gRPC Provide the Disaster Recovery Toolkit Use Garbage Collection to clear stale data automatically Support more efficient data balance Support hot Region scheduling to enable load balancing and speed up the data importing Performance Accelerate getting Client TSO Improve the efficiency of Region Heartbeat processing  Improve the pd-ctl function Update the Replica configuration dynamically Get the Timestamp Oracle (TSO) Use ID to get the Region information   TiKV:  Support gRPC Support the Sorted String Table (SST) format snapshot to improve the load balancing speed of a cluster Support using the Heap Profile to uncover memory leaks Support Streaming SIMD Extensions (SSE) and speed up the CRC32 calculation Accelerate transferring leader for faster load balancing Use Batch Apply to reduce CPU usage and improve the write performance Support parallel Prewrite to improve the transaction write speed Optimize the scheduling of the coprocessor thread pool to reduce the impact of big queries on point get The new Loader supports data importing at the table level, as well as splitting a big table into smaller logical blocks to import concurrently to improve the data importing speed.  "},
		{"url": "https://pingcap.com/weekly/2017-06-20-tidb-weekly/",
		"title": "Weekly update (June 12 ~ June 18, 2017)", 
		"content": " Weekly update in TiDB Last week, we landed 30 PRs in the TiDB repositories.Added  Refactor the optimizer:  Refactor ranger/statistic: calculate the range and row count of non pk column.  JSON type:  Support generated column:](https://github.com/pingcap/tidb/pull/3431) Support the JSON type in the cast expression.  Generated column:  Support basic generated column. Prevent modifying generated column. Add GeneratedExpr in ColumnInfo.  Support using clause in join statement. Use GetTSAsync api from pd-client. Support SubTime.  Fixed  Fix a bug in alter table statement. Fix a bug in cast. Fix a bug about parsing Duration literal. Fix a divide zero bug in statistic. Fix a protobuf unmarshal related bug in coprocessor. Fix a bug about auto_increment column conflict after renaming table.  Improved  Add some columns in mysql.user: compatiable with MySQL. Use multiple grpc connnections for each store: improve performance. Refactor expression evaluation:  Consider unsigned flag in cast expression.  Speed up DDL process:  Revoke the etcd-session when ddl worker is closed.   Weekly update in TiKV Last week, We landed 14 PRs in the TiKV repositories.Added  Coprocessor supports index scan executor. Add MySQL JSON type. Support comparison for mySQL JSON type. *  Fixed  Fix a bug caused by GC with stale command.  Improved  Increase initialized window size to 2MB for gRPC stream. Using prefix_same_as_start to optimize seek for Write CF. Use multiple connections to speed up Raft message sending. Update Dockerfile, see 1915 and 1919.  New contributor  AkihiroSuda  "},
		{"url": "https://pingcap.com/blog-cn/grpc/",
		"title": "深入了解 gRPC：协议", 
		"content": " 经过很长一段时间的开发，TiDB 终于发了 RC3。RC3 版本对于 TiKV 来说最重要的功能就是支持了 gRPC，也就意味着后面大家可以非常方便的使用自己喜欢的语言对接 TiKV 了。gRPC 是基于 HTTP/2 协议的，要深刻理解 gRPC，理解下 HTTP/2 是必要的，这里先简单介绍一下 HTTP/2 相关的知识，然后在介绍下 gRPC 是如何基于 HTTP/2 构建的。HTTP/1.x HTTP 协议可以算是现阶段 Web 上面最通用的协议了，在之前很长一段时间，很多应用都是基于 HTTP/1.x 协议，HTTP/1.x 协议是一个文本协议，可读性非常好，但其实并不高效，笔者主要碰到过几个问题：Parser 如果要解析一个完整的 HTTP 请求，首先我们需要能正确的读出 HTTP header。HTTP header 各个 fields 使用 rn 分隔，然后跟 body 之间使用 rnrn 分隔。解析完 header 之后，我们才能从 header 里面的 content-length 拿到 body 的 size，从而读取 body。这套流程其实并不高效，因为我们需要读取多次，才能将一个完整的 HTTP 请求给解析出来，虽然在代码实现上面，有很多优化方式，譬如： 一次将一大块数据读取到 buffer 里面避免多次 IO read 读取的时候直接匹配 rn 的方式流式解析  但上面的方式对于高性能服务来说，终归还是会有开销。其实最主要的问题在于，HTTP/1.x 的协议是 文本协议，是给人看的，对机器不友好，如果要对机器友好，二进制协议才是更好的选择。如果大家对解析 HTTP/1.x 很感兴趣，可以研究下 http-parser，一个非常高效小巧的 C library，见过不少框架都是集成了这个库来处理 HTTP/1.x 的。Request/Response HTTP/1.x 另一个问题就在于它的交互模式，一个连接每次只能一问一答，也就是client 发送了 request 之后，必须等到 response，才能继续发送下一次请求。这套机制是非常简单，但会造成网络连接利用率不高。如果需要同时进行大量的交互，client 需要跟 server 建立多条连接，但连接的建立也是有开销的，所以为了性能，通常这些连接都是长连接一直保活的，虽然对于 server 来说同时处理百万连接也没啥太大的挑战，但终归效率不高。Push 用 HTTP/1.x 做过推送的同学，大概就知道有多么的痛苦，因为 HTTP/1.x 并没有推送机制。所以通常两种做法： Long polling 方式，也就是直接给 server 挂一个连接，等待一段时间（譬如 1 分钟），如果 server 有返回或者超时，则再次重新 poll。 Web-socket，通过 upgrade 机制显示的将这条 HTTP 连接变成裸的 TCP，进行双向交互。  相比 Long polling，笔者还是更喜欢 web-socket 一点，毕竟更加高效，只是 web-socket 后面的交互并不是传统意义上面的 HTTP 了。Hello HTTP/2 虽然 HTTP/1.x 协议可能仍然是当今互联网运用最广泛的协议，但随着 Web 服务规模的不断扩大，HTTP/1.x 越发显得捉襟见肘，我们急需另一套更好的协议来构建我们的服务,于是就有了 HTTP/2。HTTP/2 是一个二进制协议，这也就意味着它的可读性几乎为 0，但幸运的是，我们还是有很多工具，譬如 Wireshark， 能够将其解析出来。在了解 HTTP/2 之前，需要知道一些通用术语： Stream： 一个双向流，一条连接可以有多个 streams。 Message： 也就是逻辑上面的 request，response。 Frame:：数据传输的最小单位。每个 Frame 都属于一个特定的 stream 或者整个连接。一个 message 可能有多个 frame 组成。  Frame Format Frame 是 HTTP/2 里面最小的数据传输单位，一个 Frame 定义如下（直接从官网 copy 的）：+-----------------------------------------------+ | Length (24) | +---------------+---------------+---------------+ | Type (8) | Flags (8) | +-+-------------+---------------+-------------------------------+ |R| Stream Identifier (31) | +=+=============================================================+ | Frame Payload (0...) ... +---------------------------------------------------------------+ Length：也就是 Frame 的长度，默认最大长度是 16KB，如果要发送更大的 Frame，需要显示的设置 max frame size。 Type：Frame 的类型，譬如有 DATA，HEADERS，PRIORITY 等。 Flag 和 R：保留位，可以先不管。 Stream Identifier：标识所属的 stream，如果为 0，则表示这个 frame 属于整条连接。 Frame Payload：根据不同 Type 有不同的格式。可以看到，Frame 的格式定义还是非常的简单，按照官方协议，赞成可以非常方便的写一个出来。Multiplexing HTTP/2 通过 stream 支持了连接的多路复用，提高了连接的利用率。Stream 有很多重要特性： 一条连接可以包含多个 streams，多个 streams 发送的数据互相不影响。 Stream 可以被 client 和 server 单方面或者共享使用。 Stream 可以被任意一段关闭。 Stream 会确定好发送 frame 的顺序，另一端会按照接受到的顺序来处理。 Stream 用一个唯一 ID 来标识。  这里在说一下 Stream ID，如果是 client 创建的 stream，ID 就是奇数，如果是 server 创建的，ID 就是偶数。ID 0x00 和 0x01 都有特定的使用场景，不会用到。Stream ID 不可能被重复使用，如果一条连接上面 ID 分配完了，client 会新建一条连接。而 server 则会给 client 发送一个 GOAWAY frame 强制让 client 新建一条连接。为了更大的提高一条连接上面的 stream 并发，可以考虑调大 SETTINGS_MAX_CONCURRENT_STREAMS，在 TiKV 里面，我们就遇到过这个值比较小，整体吞吐上不去的问题。这里还需要注意，虽然一条连接上面能够处理更多的请求了，但一条连接远远是不够的。一条连接通常只有一个线程来处理，所以并不能充分利用服务器多核的优势。同时，每个请求编解码还是有开销的，所以用一条连接还是会出现瓶颈。在 TiKV 有一个版本中，我们就过分相信一条连接跑多 streams 这种方式没有问题，就让 client 只用一条连接跟 TiKV 交互，结果发现性能完全没法用，不光处理连接的线程 CPU 跑满，整体的性能也上不去，后来我们换成了多条连接，情况才好转。Priority 因为一条连接允许多个 streams 在上面发送 frame，那么在一些场景下面，我们还是希望 stream 有优先级，方便对端为不同的请求分配不同的资源。譬如对于一个 Web 站点来说，优先加载重要的资源，而对于一些不那么重要的图片啥的，则使用低的优先级。我们还可以设置 Stream Dependencies，形成一棵 streams priority tree。假设 Stream A 是 parent，Stream B 和 C 都是它的孩子，B 的 weight 是 4，C 的 weight 是 12，假设现在 A 能分配到所有的资源，那么后面 B 能分配到的资源只有 C 的 1/3。Flow Control HTTP/2 也支持流控，如果 sender 端发送数据太快，receiver 端可能因为太忙，或者压力太大，或者只想给特定的 stream 分配资源，receiver 端就可能不想处理这些数据。譬如，如果 client 给 server 请求了一个视屏，但这时候用户暂停观看了，client 就可能告诉 server 别在发送数据了。虽然 TCP 也有 flow control，但它仅仅只对一个连接有效果。HTTP/2 在一条连接上面会有多个 streams，有时候，我们仅仅只想对一些 stream 进行控制，所以 HTTP/2 单独提供了流控机制。Flow control 有如下特性： Flow control 是单向的。Receiver 可以选择给 stream 或者整个连接设置 window size。 Flow control 是基于信任的。Receiver 只是会给 sender 建议它的初始连接和 stream 的 flow control window size。 Flow control 不可能被禁止掉。当 HTTP/2 连接建立起来之后，client 和 server 会交换 SETTINGS frames，用来设置 flow control window size。 Flow control 是 hop-by-hop，并不是 end-to-end 的，也就是我们可以用一个中间人来进行 flow control。  这里需要注意，HTTP/2 默认的 window size 是 64 KB，实际这个值太小了，在 TiKV 里面我们直接设置成 1 GB。HPACK 在一个 HTTP 请求里面，我们通常在 header 上面携带很多改请求的元信息，用来描述要传输的资源以及它的相关属性。在 HTTP/1.x 时代，我们采用纯文本协议，并且使用 rn 来分隔，如果我们要传输的元数据很多，就会导致 header 非常的庞大。另外，多数时候，在一条连接上面的多数请求，其实 header 差不了多少，譬如我们第一个请求可能 GET /a.txt，后面紧接着是 GET /b.txt，两个请求唯一的区别就是 URL path 不一样，但我们仍然要将其他所有的 fields 完全发一遍。HTTP/2 为了结果这个问题，使用了 HPACK。虽然 HPACK 的 RFC 文档 看起来比较恐怖，但其实原理非常的简单易懂。HPACK 提供了一个静态和动态的 table，静态 table 定义了通用的 HTTP header fields，譬如 method，path 等。发送请求的时候，只要指定 field 在静态 table 里面的索引，双方就知道要发送的 field 是什么了。对于动态 table，初始化为空，如果两边交互之后，发现有新的 field，就添加到动态 table 上面，这样后面的请求就可以跟静态 table 一样，只需要带上相关的 index 就可以了。同时，为了减少数据传输的大小，使用 Huffman 进行编码。这里就不再详细说明 HPACK 和 Huffman 如何编码了。小结 上面只是大概列举了一些 HTTP/2 的特性，还有一些，譬如 push，以及不同的 frame 定义等都没有提及，大家感兴趣，可以自行参考 HTTP/2 RFC 文档。Hello gRPC gRPC 是 Google 基于 HTTP/2 以及 protobuf 的，要了解 gRPC 协议，只需要知道 gRPC 是如何在 HTTP/2 上面传输就可以了。gRPC 通常有四种模式，unary，client streaming，server streaming 以及 bidirectional streaming，对于底层 HTTP/2 来说，它们都是 stream，并且仍然是一套 request + response 模型。Request gRPC 的 request 通常包含 Request-Headers, 0 或者多个 Length-Prefixed-Message 以及 EOS。Request-Headers 直接使用的 HTTP/2 headers，在 HEADERS 和 CONTINUATION frame 里面派发。定义的 header 主要有 Call-Definition 以及 Custom-Metadata。Call-Definition 里面包括 Method（其实就是用的 HTTP/2 的 POST），Content-Type 等。而 Custom-Metadata 则是应用层自定义的任意 key-value，key 不建议使用 grpc- 开头，因为这是为 gRPC 后续自己保留的。Length-Prefixed-Message 主要在 DATA frame 里面派发，它有一个 Compressed flag 用来表示改 message 是否压缩，如果为 1，表示该 message 采用了压缩，而压缩算法定义在 header 里面的 Message-Encoding 里面。然后后面跟着四字节的 message length 以及实际的 message。EOS（end-of-stream） 会在最后的 DATA frame 里面带上了 END_STREAM 这个 flag。用来表示 stream 不会在发送任何数据，可以关闭了。Response Response 主要包含 Response-Headers，0 或者多个 Length-Prefixed-Message 以及 Trailers。如果遇到了错误，也可以直接返回 Trailers-Only。Response-Headers 主要包括 HTTP-Status，Content-Type 以及 Custom-Metadata 等。Trailers-Only 也有 HTTP-Status ，Content-Type 和 Trailers。Trailers 包括了 Status 以及 0 或者多个 Custom-Metadata。HTTP-Status 就是我们通常的 HTTP 200，301，400 这些，很通用就不再解释。Status 也就是 gRPC 的 status， 而 Status-Message 则是 gRPC 的 message。Status-Message 采用了 Percent-Encoded 的编码方式，具体参考这里。如果在最后收到的 HEADERS frame 里面，带上了 Trailers，并且有 END_STREAM 这个 flag，那么就意味着 response 的 EOS。Protobuf gRPC 的 service 接口是基于 protobuf 定义的，我们可以非常方便的将 service 与 HTTP/2 关联起来。 Path : /Service-Name/{method name} Service-Name : ?( {proto package name} &amp;quot;.&amp;quot; ) {service name} Message-Type : {fully qualified proto message name} Content-Type : &amp;ldquo;application/grpc+proto&amp;rdquo;  后记 上面只是对 gRPC 协议的简单理解，可以看到，gRPC 的基石就是 HTTP/2，然后在上面使用 protobuf 协议定义好 service RPC。虽然看起来很简单，但如果一门语言没有 HTTP/2，protobuf 等支持，要支持 gRPC 就是一件非常困难的事情了。悲催的是，Rust 刚好没有 HTTP/2 支持，也仅仅有一个可用的 protobuf 实现。为了支持 gRPC，我们 team 付出了很大的努力，也走了很多弯路，从最初使用纯 Rust 的 rust-grpc 项目，到后来自己基于 c-grpc 封装了 grpc-rs，还是有很多可以说的，后面在慢慢道来。如果你对 gRPC 和 rust 都很感兴趣，欢迎参与开发。gRPC-rs: https://github.com/pingcap/grpc-rs"},
		{"url": "https://pingcap.com/blog-cn/series-B-funding/",
		"title": "来自 PingCAP CEO 的信：说在 B 轮融资完成之际", 
		"content": "平时技术说得多，今天说点走心的。从决定出来创业到现在，刚好两年多一点，如果把 PingCAP 比喻成一个孩子的话， 刚是过了蹒跚学步的时期，前方有更大更美好的世界等我们去探索。这两年时间，在一片质疑声之中 ，TiDB 还算顽强的从无到有成长了起来。其实这一切的初心也很简单，最开始只不过是几个不愿妥协的分布式系统工程师对心目中&amp;rsquo;完美&amp;rsquo;的数据库的探索。很欣喜的看到 TiDB 的日渐成熟，周边工具和社区渐渐壮大，我感到由衷的自豪，在这个过程中，也一次又一次的挑战着技术和各自能力的边界，很庆幸能和自己的产品一起成长。坚持做正确的事，哪怕这看起来是一条更困难的路。TiDB 从诞生的第一天起便决定开源，虽然更多的是商业上的考量，不过里面也有一点点读书人兼济天下的情怀和对传统 Hacker 精神的贯彻。在我们之前，很多人认为分布式 OLTP 和 OLAP 融合几乎是不可能的事情，也有无数的人，其中不乏亲朋好友，劝我们说在国内做这个事情几乎难于登天，而且没有成功的先例。不过我们还是相信一个朴素道理，有价值的技术一定会有它的舞台，另外，任何事情如果没有尝试就打退堂鼓也不是我们的风格。如果没有成功的先例，那就一起来创造先例，做开创者是我们每个人的梦想。说实话，从技术上来说，这个领域是一个非常前沿的领域，大多数时候我们面前是无人区，也很幸运，目前看来技术上和预想的没有出现大的偏差，整个产品和团队也在稳步的前进。整个团队也从一开始的 3 个人，到今天 63 个志同道合的伙伴结伴前行，又一次很幸运，能凑齐这么一个具有很强战斗力和国际视野的团队，挑战计算机领域最困难和最前沿的课题之一，前方还有无数个迷人的问题等待着被解决，有时候也只能摸索着前进，不过这正是这个事情有意思的地方。谢谢你们，和你们一同工作，是我的荣幸。到今天，我们很自豪的宣布，已经有数十家客户将 TiDB 使用在各自的生产环境中解决问题，感谢我们早期的铁杆用户和可爱的社区开发者，是你们让 TiDB 一点点的变得更加稳定成熟，随着社区的不断变大，TiDB 正以惊人的速度正向迭代，这就是开源的力量。最后，PingCAP 也刚顺利的完成了 1500 万美金的 B 轮融资，感谢这轮的领投方华创资本，以及跟投方经纬中国，云启资本，峰瑞资本，险峰华兴。我们的征途是星辰大海，感谢有你们的一路支持。刘奇、黄东旭、崔秋PingCAP2017-6-13"},
		{"url": "https://pingcap.com/weekly/2017-06-12-tidb-weekly/",
		"title": "Weekly update (June 06 ~ June 11, 2017)", 
		"content": " Weekly update in TiDB Last week, we landed 52 PRs in the TiDB repositories.Added  Refactor the optimizer: Add an statsProfile interface to propagate statistic information among plans. Implement the statsProfile interface for the Join plan.  JSON type: Suport json_set, json_insert, json_replace and json_merge. Support the JSON type in the cast expression.  Use gRPC between TiDB and TiKV. Refactor the Analyze executor. Add the SubDuration function in util package. Support the generated column grammar.  Fixed  Create a session when the current session with etcd expires. Fix a bug about padding zeros for binary data. Fix a bug about type inference. Fix a but about data access race. Avoid allocating too much memory in the top-n operator. Add timeout when getting TimeStamp. Make the behavior of read-only transactions using the SelectForUpdate statement consistent with MySQL.  Improved  Refactor expression evaluation: Add builtinAddTimeSig Correct cast function implementation. Clean up code and prepare to add test cases. Implement wrap expression with the cast expression.  Speed up DDL process: Extract interfaces for OwnerManger and SchemaSyncer. Add mock implementation for unit test. Add TTL for DDL related keys on etcd.  Code style clean up.  Weekly update in TiKV Last week, We landed 25 PRs in the TiKV repositories.Added  Add the bloom filter for Lock CF meltable. Introduce gRPC, see 1892, 1894, 1896, 1903 Use SST based bloom filter by default. Remove the bloom filter for Raft CF. Provide the GetTSAsync API.  Fixed  Delete idle snapshot only on GC. Notify the blocked goroutines when get stream error. Fix a bug in get_txn_commit_info for concurrent prewrite. Check the leader again when fail to create TSO stream. Fix a data race problem in PD client.  Improved  Optimize the Raft log entry fetching. Capture the SIGHUP and close gracefully. Avoid allocating lots of memory in topn function. Use SST snapshot format by default to speed up Raft snapshot applying.  "},
		{"url": "https://pingcap.com/blog-cn/deployment-by-ansible/",
		"title": "使用 Ansible 安装部署 TiDB", 
		"content": " 背景知识 TiDB 作为一个分布式数据库，在多个节点分别配置安装服务会相当繁琐，为了简化操作以及方便管理，使用自动化工具来批量部署成为了一个很好的选择。Ansible 是基于 Python 研发的自动化运维工具，糅合了众多老牌运维工具的优点实现了批量操作系统配置、批量程序的部署、批量运行命令等功能，而且使用简单，仅需在管理工作站上安装 Ansible 程序配置被管控主机的 IP 信息，被管控的主机无客户端。基于以上原因，我们选用自动化工具 Ansible 来批量的安装配置以及部署 TiDB。下面我们来介绍如何使用 Ansible 来部署 TiDB。TiDB 安装环境配置如下 操作系统使用 CentOS7.2 或者更高版本，文件系统使用 EXT4。 说明：低版本的操作系统(例如 CentOS6.6 )和 XFS 文件系统会有一些内核 Bug，会影响性能，我们不推荐使用。    IP Services     192.168.1.101 PD Prometheus Grafana Pushgateway Node_exporter   192.168.1.102 PD TiDB Node_exporter   192.168.1.103 PD TiDB Node_exporter   192.168.1.104 TiKV Node_exporter   192.168.1.105 Tikv Node_exporter   192.168.1.106 TiKV Node_exporter    我们选择使用 3 个 PD、2 个 TiDB、3 个 TiKV，这里简单说一下为什么这样部署。 对于 PD 。PD 本身是一个分布式系统，由多个节点构成一个整体，并且同时有且只有一个主节点对外提供服务。各个节点之间通过选举算法来确定主节点，选举算法要求节点个数是奇数个 (2n+1) ，1 个节点的风险比较高，所以我们选择使用 3 个节点。 对于 TiKV 。TiDB 底层使用分布式存储，我们推荐使用奇数 (2n+1) 个备份，挂掉 n 个备份之后数据仍然可用。使用 1 备份或者 2 备份的话，有一个节点挂掉就会造成一部分数据不可用，所以我们选择使用 3 个节点、设置 3 个备份 (默认值)。 对于 TiDB 。我们的 TiDB 是无状态的，现有集群的 TiDB 服务压力大的话，可以在其他节点直接增加 TiDB 服务，无需多余的配置。我们选择使用两个 TiDB，可以做 HA 和负载均衡。 当然如果只是测试集群的话，完全可以使用一个 PD 、一个 TiDB 、三个 TiKV (少于三个的话需要修改备份数量)  下载 TiDB 安装包并解压 #创建目录用来存放 ansible 安装包 mkdir /root/workspace #切换目录 cd /root/workspace #下载安装包 wget https://github.com/pingcap/tidb-ansible/archive/master.zip #解压压缩包到当前目录下 unzip master.zip #查看安装包结构，主要内容说明如下 cd tidb-ansible-master &amp;amp;&amp;amp; ls 部分内容含义ansible.cfg: ansible 配置文件 inventory.ini: 组和主机的相关配置 conf: TiDB 相关配置模版 group_vars: 相关变量配置 scripts: grafana 监控 json 模版 local_prepare.yml: 用来下载相关安装包 bootstrap.yml: 初始化集群各个节点 deploy.yml: 在各个节点安装 TiDB 相应服务 roles: ansible tasks 的集合 start.yml: 启动所有服务 stop.yml: 停止所有服务 unsafe_cleanup_data.yml: 清除数据 unsafe_cleanup.yml: 销毁集群 修改配置文件 主要配置集群节点的分布情况，以及安装路径。会在 tidb_servers 组中的机器上安装 TiDB 服务(其他类似)，默认会将所有服务安装到变量 deploy_dir 路径下。#将要安装 TiDB 服务的节点 [tidb_servers] 192.168.1.102 192.168.1.103 #将要安装 TiKV 服务的节点 [tikv_servers] 192.168.1.104 192.168.1.105 192.168.1.106 #将要安装 PD 服务的节点 [pd_servers] 192.168.1.101 192.168.1.102 192.168.1.103 #将要安装 Promethues 服务的节点 # Monitoring Part [monitoring_servers] 192.168.1.101 #将要安装 Grafana 服务的节点 [grafana_servers] 192.168.1.101 #将要安装 Node_exporter 服务的节点 [monitored_servers:children] tidb_servers tikv_servers pd_servers [all:vars] #服务安装路径，每个节点均相同，根据实际情况配置 deploy_dir = /home/tidb/deploy ## Connection #方式一：使用 root 用户安装 # ssh via root: # ansible_user = root # ansible_become = true # ansible_become_user = tidb #方式二：使用普通用户安装(需要有 sudo 权限) # ssh via normal user ansible_user = tidb #集群的名称，自定义即可 cluster_name = test-cluster # misc enable_elk = False enable_firewalld = False enable_ntpd = False # binlog trigger #是否开启 pump，pump 生成 TiDB 的 binlog #如果有从此 TiDB 集群同步数据的需求，可以改为 True 开启 enable_binlog = False 安装过程可以分为 root 用户安装和普通用户安装两种方式。有 root 用户当然是最好的，修改系统参数、创建目录等不会涉及到权限不够的问题，能够直接安装完成。 但是有些环境不会直接给 root 权限，这种场景就需要通过普通用户来安装。为了配置简便，我们建议所有节点都使用相同的普通用户；为了满足权限要求，我们还需要给这个普通用户 sudo 权限。 下面介绍两种安装方式的详细过程，安装完成之后需要手动启动服务。1. 使用 root 用户安装  下载 Binary 包到 downloads 目录下，并解压拷贝到 resources/bin 下，之后的安装过程就是使用的 resources/bin 下的二进制程序  ansible-playbook -i inventory.ini local_prepare.yml  初始化集群各个节点。会检查 inventory.ini 配置文件、Python 版本、网络状态、操作系统版本等，并修改一些内核参数，创建相应的目录。 修改配置文件如下  ## Connection # ssh via root: ansible_user = root # ansible_become = true ansible_become_user = tidb # ssh via normal user # ansible_user = tidb  执行初始化命令  ansible-playbook -i inventory.ini bootstrap.yml -k #ansible-playboo命令说明请见附录 安装服务。该步骤会在服务器上安装相应的服务，并自动设置好配置文件和所需脚本。 修改配置文件如下  ## Connection # ssh via root: ansible_user = root ansible_become = true ansible_become_user = tidb # ssh via normal user # ansible_user = tidb  执行安装命令  ansible-playbook -i inventory.ini deploy.yml -k  2. 使用普通用户安装  下载 Binary 包到中控机  ansible-playbook -i inventory.ini local_prepare.yml  初始化集群各个节点。 修改配置文件如下  ## Connection # ssh via root: # ansible_user = root # ansible_become = true # ansible_become_user = tidb # ssh via normal user ansible_user = tidb  执行初始化命令  ansible-playbook -i inventory.ini bootstrap.yml -k -K 安装服务  ansible-playbook -i inventory.ini deploy.yml -k -K 启停服务  启动所有服务  ansible-playbook -i inventory.ini start.yml -k  停止所有服务  ansible-playbook -i inventory.ini stop.yml 附录 ansible-playbook -i inventory.ini xxx.yml -k -K-k 执行之后需要输入 ssh 连接用户的密码，如果做了中控机到所有节点的互信，则不需要此参数-K 执行之后需要输入 sudo 所需的密码，如果使用 root 用户或者 sudo 无需密码，则不需要此参数 "},
		{"url": "https://pingcap.com/blog-cn/tidb-internal-3/",
		"title": "三篇文章了解 TiDB 技术内幕 - 谈调度", 
		"content": " 为什么要进行调度 先回忆一下第一篇文章提到的一些信息，TiKV 集群是 TiDB 数据库的分布式 KV 存储引擎，数据以 Region 为单位进行复制和管理，每个 Region 会有多个 Replica（副本），这些 Replica 会分布在不同的 TiKV 节点上，其中 Leader 负责读/写，Follower 负责同步 Leader 发来的 raft log。了解了这些信息后，请思考下面这些问题： 如何保证同一个 Region 的多个 Replica 分布在不同的节点上？更进一步，如果在一台机器上启动多个 TiKV 实例，会有什么问题？ TiKV 集群进行跨机房部署用于容灾的时候，如何保证一个机房掉线，不会丢失 Raft Group 的多个 Replica？ 添加一个节点进入 TiKV 集群之后，如何将集群中其他节点上的数据搬过来? 当一个节点掉线时，会出现什么问题？整个集群需要做什么事情？如果节点只是短暂掉线（重启服务），那么如何处理？如果节点是长时间掉线（磁盘故障，数据全部丢失），需要如何处理？ 假设集群需要每个 Raft Group 有 N 个副本，那么对于单个 Raft Group 来说，Replica 数量可能会不够多（例如节点掉线，失去副本），也可能会 过于多（例如掉线的节点又回复正常，自动加入集群）。那么如何调节 Replica 个数？ 读/写都是通过 Leader 进行，如果 Leader 只集中在少量节点上，会对集群有什么影响？ 并不是所有的 Region 都被频繁的访问，可能访问热点只在少数几个 Region，这个时候我们需要做什么？ 集群在做负载均衡的时候，往往需要搬迁数据，这种数据的迁移会不会占用大量的网络带宽、磁盘 IO 以及 CPU？进而影响在线服务？  这些问题单独拿出可能都能找到简单的解决方案，但是混杂在一起，就不太好解决。有的问题貌似只需要考虑单个 Raft Group 内部的情况，比如根据副本数量是否足够多来决定是否需要添加副本。但是实际上这个副本添加在哪里，是需要考虑全局的信息。整个系统也是在动态变化，Region 分裂、节点加入、节点失效、访问热点变化等情况会不断发生，整个调度系统也需要在动态中不断向最优状态前进，如果没有一个掌握全局信息，可以对全局进行调度，并且可以配置的组件，就很难满足这些需求。因此我们需要一个中心节点，来对系统的整体状况进行把控和调整，所以有了 PD 这个模块。调度的需求 上面罗列了一大堆问题，我们先进行分类和整理。总体来看，问题有两大类：作为一个分布式高可用存储系统，必须满足的需求，包括四种： 副本数量不能多也不能少 副本需要分布在不同的机器上 新加节点后，可以将其他节点上的副本迁移过来 节点下线后，需要将该节点的数据迁移走  作为一个良好的分布式系统，需要优化的地方，包括： 维持整个集群的 Leader 分布均匀 维持每个节点的储存容量均匀 维持访问热点分布均匀 控制 Balance 的速度，避免影响在线服务 管理节点状态，包括手动上线/下线节点，以及自动下线失效节点  满足第一类需求后，整个系统将具备多副本容错、动态扩容/缩容、容忍节点掉线以及自动错误恢复的功能。满足第二类需求后，可以使得整体系统的负载更加均匀、且可以方便的管理。为了满足这些需求，首先我们需要收集足够的信息，比如每个节点的状态、每个 Raft Group 的信息、业务访问操作的统计等；其次需要设置一些策略，PD 根据这些信息以及调度的策略，制定出尽量满足前面所述需求的调度计划；最后需要一些基本的操作，来完成调度计划。调度的基本操作 我们先来介绍最简单的一点，也就是调度的基本操作，也就是为了满足调度的策略，我们有哪些功能可以用。这是整个调度的基础，了解了手里有什么样的锤子，才知道用什么样的姿势去砸钉子。上述调度需求看似复杂，但是整理下来最终落地的无非是下面三件事： 增加一个 Replica 删除一个 Replica 将 Leader 角色在一个 Raft Group 的不同 Replica 之间 transfer  刚好 Raft 协议能够满足这三种需求，通过 AddReplica、RemoveReplica、TransferLeader 这三个命令，可以支撑上述三种基本操作。信息收集 调度依赖于整个集群信息的收集，简单来说，我们需要知道每个 TiKV 节点的状态以及每个 Region 的状态。TiKV 集群会向 PD 汇报两类消息：每个 TiKV 节点会定期向 PD 汇报节点的整体信息TiKV 节点（Store）与 PD 之间存在心跳包，一方面 PD 通过心跳包检测每个 Store 是否存活，以及是否有新加入的 Store；另一方面，心跳包中也会携带这个 Store 的状态信息，主要包括： 总磁盘容量 可用磁盘容量 承载的 Region 数量 数据写入速度 发送/接受的 Snapshot 数量（Replica 之间可能会通过 Snapshot 同步数据） 是否过载 标签信息（标签是具备层级关系的一系列 Tag）  每个 Raft Group 的 Leader 会定期向 PD 汇报信息每个 Raft Group 的 Leader 和 PD 之间存在心跳包，用于汇报这个 Region 的状态，主要包括下面几点信息： Leader 的位置 Followers 的位置 掉线 Replica 的个数 数据写入/读取的速度  PD 不断的通过这两类心跳消息收集整个集群的信息，再以这些信息作为决策的依据。除此之外，PD 还可以通过管理接口接受额外的信息，用来做更准确的决策。比如当某个 Store 的心跳包中断的时候，PD 并不能判断这个节点是临时失效还是永久失效，只能经过一段时间的等待（默认是 30 分钟），如果一直没有心跳包，就认为是 Store 已经下线，再决定需要将这个 Store 上面的 Region 都调度走。但是有的时候，是运维人员主动将某台机器下线，这个时候，可以通过 PD 的管理接口通知 PD 该 Store 不可用，PD 就可以马上判断需要将这个 Store 上面的 Region 都调度走。调度的策略 PD 收集了这些信息后，还需要一些策略来制定具体的调度计划。一个 Region 的 Replica 数量正确当 PD 通过某个 Region Leader 的心跳包发现这个 Region 的 Replica 数量不满足要求时，需要通过 Add/Remove Replica 操作调整 Replica 数量。出现这种情况的可能原因是： 某个节点掉线，上面的数据全部丢失，导致一些 Region 的 Replica 数量不足 某个掉线节点又恢复服务，自动接入集群，这样之前已经补足了 Replica 的 Region 的 Replica 数量多过，需要删除某个 Replica 管理员调整了副本策略，修改了 max-replicas 的配置  一个 Raft Group 中的多个 Replica 不在同一个位置注意第二点，『一个 Raft Group 中的多个 Replica 不在同一个位置』，这里用的是『同一个位置』而不是『同一个节点』。在一般情况下，PD 只会保证多个 Replica 不落在一个节点上，以避免单个节点失效导致多个 Replica 丢失。在实际部署中，还可能出现下面这些需求： 多个节点部署在同一台物理机器上 TiKV 节点分布在多个机架上，希望单个机架掉电时，也能保证系统可用性 TiKV 节点分布在多个 IDC 中，希望单个机房掉电时，也能保证系统可用  这些需求本质上都是某一个节点具备共同的位置属性，构成一个最小的容错单元，我们希望这个单元内部不会存在一个 Region 的多个 Replica。这个时候，可以给节点配置 lables 并且通过在 PD 上配置 location-labels 来指明哪些 lable 是位置标识，需要在 Replica 分配的时候尽量保证不会有一个 Region 的多个 Replica 所在结点有相同的位置标识。副本在 Store 之间的分布均匀分配前面说过，每个副本中存储的数据容量上限是固定的，所以我们维持每个节点上面，副本数量的均衡，会使得总体的负载更均衡。Leader 数量在 Store 之间均匀分配Raft 协议要读取和写入都通过 Leader 进行，所以计算的负载主要在 Leader 上面，PD 会尽可能将 Leader 在节点间分散开。访问热点数量在 Store 之间均匀分配每个 Store 以及 Region Leader 在上报信息时携带了当前访问负载的信息，比如 Key 的读取/写入速度。PD 会检测出访问热点，且将其在节点之间分散开。各个 Store 的存储空间占用大致相等每个 Store 启动的时候都会指定一个 Capacity 参数，表明这个 Store 的存储空间上限，PD 在做调度的时候，会考虑节点的存储空间剩余量。控制调度速度，避免影响在线服务调度操作需要耗费 CPU、内存、磁盘 IO 以及网络带宽，我们需要避免对线上服务造成太大影响。PD 会对当前正在进行的操作数量进行控制，默认的速度控制是比较保守的，如果希望加快调度(比如已经停服务升级，增加新节点，希望尽快调度)，那么可以通过 pd-ctl 手动加快调度速度。支持手动下线节点当通过 pd-ctl 手动下线节点后，PD 会在一定的速率控制下，将节点上的数据调度走。当调度完成后，就会将这个节点置为下线状态。调度的实现 了解了上面这些信息后，接下来我们看一下整个调度的流程。PD 不断的通过 Store 或者 Leader 的心跳包收集信息，获得整个集群的详细数据，并且根据这些信息以及调度策略生成调度操作序列，每次收到 Region Leader 发来的心跳包时，PD 都会检查是否有对这个 Region 待进行的操作，通过心跳包的回复消息，将需要进行的操作返回给 Region Leader，并在后面的心跳包中监测执行结果。注意这里的操作只是给 Region Leader 的建议，并不保证一定能得到执行，具体是否会执行以及什么时候执行，由 Region Leader 自己根据当前自身状态来定。总结 本篇文章讲的东西，大家可能平时很少会在其他文章中看到，每一个设计都有背后的考量，希望大家能了解到一个分布式存储系统在做调度的时候，需要考虑哪些东西，如何将策略、实现进行解耦，更灵活的支持策略的扩展。至此三篇文章已经讲完，希望大家能够对整个 TiDB 的基本概念和实现原理有了解，后续我们还会写更多的文章，从架构以及代码级别介绍 TiDB 的更多内幕。如果大家有问题，欢迎发邮件到 shenli@pingcap.com 进行交流。"},
		{"url": "https://pingcap.com/weekly/2017-06-05-tidb-weekly/",
		"title": "Weekly update (May 22 ~ June 05, 2017)", 
		"content": " Weekly update in TiDB Last two weeks, we landed 53 PRs in the TiDB repositories.Added  Support using After and First to specify column position in the Alter Table Statement. Support the password builtin function. Support the inet6_ntoa builtin function. Support the Extract and Unquote function for Json. Support json_{set/insert/replace} and json_merge for Json. Support batched Index Lookup Join.  Fixed  Fix a goroutine leak problem. Fix a bug in double read executor. Fix a bug for type inferrer of Bit literal value. Fix a bug about context cancellation.  Improved  Refactor expression evaluation: Refactor the cast function. Add the GetTypeClass() interface in Expression.  Update stored password in mysql.user table  to make it consistent with MySQL. Speed up the DDL process: Use etcd to synchronise schema version instead of waiting two leases. Enable DDL speed up.  Add the references_priv column in mysql.user. Close DDL worker gracefully when shutting down the TiDB server.  Weekly update in TiKV Last two weeks, We landed 33 PRs in the TiKV repositories.Added  Introduce gRPC, see 1850, 1865, 1868, 1879. Report the written keys to PD for better scheduler. Balance the hot regions by peer and leader together. Record the cluster bootstrap time. Add the state for operators. Add the metrics for balance. Add the disaster recovery tool for PD. Output the allocation statistics when receiving SIGUSR1. Add the direct IO configuration for RocksDB.  Fixed  Check the cluster ID when re-connecting to PD. Reuse the timer to avoid spawning thread frequently. Remove the realtime signal handler. Prevent accessing nil store when recovering cluster. Check the environment TZ setting. Use the forward mode for scanning data in GC command. Check the last apply index to determine handing snapshot.  Improved  Increase the region size from 64 MB to 96 MB. Split total region peer cache. Use FlatMap instead of HashMap to improve performance. Modify the default compression in RocksDB.  "},
		{"url": "https://pingcap.com/meetup/meetup-2017-06-03/",
		"title": "【Infra Meetup No.49】TiDB Best Practice", 
		"content": "  今日的 Meetup，我司 Engineering VP 申砾同学亲自上阵，为大家分享了《TiDB Best Practice 》，好多使用经验及背后技术实现原理都是首次揭秘（当然，包括彩蛋）。 本期讲师：申砾，PingCAP Engineering VP，前网易有道词典服务器端核心开发，前奇虎 360 新闻推荐系统 / 地图基础数据与检索系统 Tech Lead。TiDB 是一个分布式数据库，支持 MySQL 协议以及语法，在一些场景中都可以无缝替换 MySQL，以获得分布式的好处。但是分布式数据库有其自身的特点，想要在业务中用好需要遵循一些实践原则。本次分享申砾同学首先介绍了 TiDB 的一些关键部分的实现原理，理解这些内部实现有利于理解 TiDB 的外在表现。然后与大家讨论了应用数据库时的典型操作的最佳实践以及要注意的事项，并对 TiDB 的适用场景进行了讲解。PPT 很干，一点水都挤不出来&amp;hellip;随便放几张你们感受下┑(￣Д ￣)┍最后，申砾同学还分享了 TiDB 最近的一些项目进展，并首次公开披露 PingCAP 最新动向：独立研发的 TiDB 专用的 Spark Connector 即将上线。Spark 是当下最流行的大数据分析系统，拥有活跃的社区。PingCAP 希望能够将 TiDB 与 Spark 相结合，通过 Spark 对 TiDB 中存储的数据做实时分析，以融入这个生态。为了保证这个连接过程尽可能的高效，所以除了基本的 JDBC Connector 之外，便有了 TiDB 专用的 Spark Connector。彩蛋来啦Demo of Spark on TiDB 即将上线，敬请期待～"},
		{"url": "https://pingcap.com/blog-cn/tangliu-tool-1/",
		"title": "工欲性能调优，必先利其器（1）", 
		"content": " 使用 iostat 定位磁盘问题 在一个性能测试集群，我们选择了 AWS c3.4xlarge 机型，主要是为了在一台机器的两块盘上面分别跑 TiKV。在测试一段时间之后，我们发现有一台 TiKV 响应很慢，但是 RocksDB 并没有相关的 Stall 日志，而且慢查询也没有。于是我登上 AWS 机器，使用 iostat -d -x -m 5 命令查看，得到如下输出：Device: rrqm/s wrqm/s r/s w/s rMB/s wMB/s avgrq-sz avgqu-sz await r_await w_await svctm %util xvda 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 xvdb 8.00 12898.00 543.00 579.00 31.66 70.15 185.84 51.93 54.39 7.03 98.79 0.60 66.80 xvdc 0.00 0.00 206.00 1190.00 10.58 148.62 233.56 106.67 70.90 13.83 80.78 0.56 78.40 上面发现，两个盘 xvdb 和 xvdc 在 wrqm/s 上面差距太大，当然后面一些指标也有明显的差距，这里就不在详细的解释 iostat 的输出。只是需要注意，大家通常将目光注意到 util 上面，但有时候光有 util 是反应不了问题的。于是我继续用 fio 进行测试，fio -ioengine=libaio -bs=4k -direct=1 -thread -rw=write -size=10G -filename=test -name=&amp;quot;PingCAP max throughput&amp;quot; -iodepth=4 -runtime=60发现两个盘的写入有 2 倍的差距，xvdb 的写入竟然只有不到 70 MB，而 xvdc 有 150 MB，所以自然两个 TiKV 一个快，一个慢了。对于磁盘来说，通常我们使用的就是 iostat 来进行排查，另外也可以考虑使用 pidstat，iotop 等工具。使用 perf 定位性能问题 RC3 最重要的一个功能就是引入 gRPC，但这个对于 rust 来说难度太大。最开始，我们使用的是 rust-grpc 库，但这个库并没有经过生产环境的验证，我们还是胆大的引入了，只是事后证明，这个冒险的决定还是傻逼了，一些试用的用户跟我们反映 TiKV 时不时 coredump，所以我们立刻决定直接封装 c gRPC。因为现在大部分语言 gRPC 实现都是基于 c gRPC 的，所以我们完全不用担心这个库的稳定性。在第一个版本的实现中，我们发现，rust 封装的 c gRPC 比 C Plus Plus 的版本差了几倍的性能，于是我用 perf stat 来分别跑 C Plus Plus 和 rust 的benchmark，得到类似如下的输出：Performance counter stats for &amp;#39;python2.7 tools/run_tests/run_performance_tests.py -r generic_async_streaming_ping_pong -l c++&amp;#39;: 216989.551636 task-clock (msec) # 2.004 CPUs utilized 3,659,896 context-switches # 0.017 M/sec 5,078 cpu-migrations # 0.023 K/sec 4,104,965 page-faults # 0.019 M/sec 729,530,805,665 cycles # 3.362 GHz &amp;lt;not supported&amp;gt; stalled-cycles-frontend &amp;lt;not supported&amp;gt; stalled-cycles-backend 557,766,492,733 instructions # 0.76 insns per cycle 121,205,705,283 branches # 558.579 M/sec 3,095,509,087 branch-misses # 2.55% of all branches 108.267282719 seconds time elapsed 上面是 C Plus Plus 的结果，然后在 rust 测试的时候，我们发现 context-switch 是 C Plus Plus 的 10 倍，也就是我们进行了太多次的线程切换。刚好我们第一个版本的实现是用 rust futures 的 park 和 unpark task 机制，不停的在 gRPC 自己的 Event Loop 线程和逻辑线程之前切换，但 C Plus Plus 则是直接在一个 Event Loop 线程处理的。于是我们立刻改成类似 C Plus Plus 架构，没有了 task 的开销，然后性能一下子跟 C Plus Plus 的不相伯仲了。当然，perf 能做到的还远远不仅于此，我们通常会使用火焰图工具，关于火焰图，网上已经有太多的介绍，我们也通过它来发现了很多性能问题，这个后面可以专门来说一下。使用 strace 动态追踪 因为我们有一个记录线程 CPU 的统计，通常在 Grafana 展示的时候都是按照线程名字来 group 的，并没有按照线程 ID。但我们也可以强制发送 SIGUSR1 信号给 TiKV 在 log 里面 dump 相关的统计信息。在测试 TiKV 的时候，我发现 pd worker 这个 thread 出现了很多不同线程 ID 的 label，也就是说，这个线程在不停的创建和删除。要动态追踪到这个情况，使用 strace -f 是一个不错的方式，我同时观察 TiKV 自己的输出 log，发现当 TiKV 在处理分裂逻辑，给 PD worker 发送 message 的时候，就有一个新的线程创建出来。然后在查找对应的代码，发现我们每次在发消息的时候都创建了一个 tokio-timer，而这个每次都会新创建一个线程。有时候，也可以使用 strace -c 来动态的追踪一段时间的系统调用。在第一版本的 rust gRPC 中，我们为了解决 future task 导致的频繁线程切换，使用 gRPC 自己的 alarm 来唤醒 Event Loop，但发现这种实现会产生大量的信号调用，因为 gRPC 的 alarm 会发送一个实时信号用来唤醒 epoll，后面通过火焰图也发现了 Event Loop 很多 CPU 消耗在 alarm 这边，所以也在开始改进。这里需要注意，strace 对性能影响比较大，但对于内部性能测试影响还不大，不到万不得已，不建议长时间用于生产环境。小结 上面仅仅是三个最近用工具发现的问题，当然还远远不止于此，后续也会慢慢补上。其实对于性能调优来说，工具只是一个辅助工具，最重要的是要有一颗对问题敏锐的心，不然即使工具发现了问题，因为不敏锐直接就忽略了。我之前就是不敏锐栽过太多的坑，所以现在为了刻意提升自己这块的能力，直接给自己下了死规定，就是怀疑一切能能怀疑的东西，认为所有东西都是有问题的。即使真的是正常的，也需要找到充足的理由去验证。"},
		{"url": "https://pingcap.com/blog/2017-05-27-rust-in-tikv/",
		"title": "Rust in TiKV", 
		"content": " This is the speech Siddon Tang gave at the 1st Rust Meetup in Beijing on April 16, 2017.(Email: tl@pingcap.com)Table of Content  What&amp;rsquo;s TiKV We need a language with&amp;hellip; Why not C++? Why not Go? So we turned to Rust&amp;hellip; TiKV Timeline TiKV Architecture Multi-Raft Scale out A simple write flow Key technologies Future plan  Hello everyone, today I will talk about how we use Rust in TiKV.Before we begin, let me introduce myself. My name is Siddon Tang, the Chief Architect of PingCAP. Before I joined PingCAP, I had worked at Kingsoft and Tencent. I love open source and have developed some projects like LedisDB, go-mysql, etc…At first, I will explain the reason why we chose Rust to develop TiKV, then show you the architecture of TiKV briefly and the key technologies. In the end, I will introduce what we plan to do in the future.What’s TiKV? All right, let’s begin. First, what is TiKV. TiKV is a distributed Key-Value database with the following features: Geo-replication: We use Raft and Placement Driver to replicate data geographically to guarantee data safety. Horizontal scalability: We can add some nodes directly if we find that the rapidly growing data will soon exceed the system capacity. Consistent distributed transaction: We use an optimized, two phase commit protocol, based on Google Percolator, to support distributed transactions. You can use &amp;ldquo;begin&amp;rdquo; to start a transaction, then do something, then use “commit” or “rollback” to finish the transaction. Coprocessor for distributed computing: Just like HBase, we support a coprocessor framework to let user do computing in TiKV directly. Working with TiDB like Spanner with F1: Using TiKV as a backend storage engine of TiDB, we can provide the best distributed relational database.  Back to the topWe need a language with&amp;hellip;  As you see, TiKV has many powerful features. To develop these features, we also need a powerful programming language. The language should have: Fast speed: We take the performance of TiKV very seriously, so we need a language which runs very fast at runtime. Memory safety: As a program that is going to run for a long time, we don’t want to meet any memory problem, such as dangling pointer, memory leak, etc… Thread safety: We must guarantee data consistency all the time, so any data race problem must be avoided. Binding C efficiency: We depend on RocksDB heavily, so we must be able to call the RocksDB API as fast as we can, without any performance reduction.  Why not C++?  To develop a high performance service, C++ may be the best choice in most cases, but we didn’t choose it. We figured we might spend too much time avoiding the memory problem or the data race problem. Moreover, C++ has no official package manager and that makes the maintaining and compiling third dependences very troublesome and difficult, resulting in a long development cycle.Why not Go?  At first, we considered using Go, but then gave up this idea. Go has GC which fixes many memory problems, but it might stop the running process sometimes. No matter how little time the stop takes, we can’t afford it. Go doesn’t solve the data race problem either. Even we can use double dash race in test or at runtime, this isn’t enough.Besides, although we can use Goroutine to write the concurrent logic easily, we still can’t neglect the runtime expenses of the scheduler. We met a problem a few days ago: we used multi goroutines to select the same context but found that the performance was terrible, so we had to use one sub context for one goroutine, then the performance became better.More seriously, CGO has heavy expenses, but we need to call RocksDB API without delay. For the above reasons, we didn’t choose Go even this is the favorite language in our team.Back to the topSo we turned to Rust&amp;hellip; But Rust&amp;hellip; Rust is a system programming language, maintained by Mozilla. It is a very powerful language, however, you can see the curve, the learning curve is very very steep.I have been using many programming languages, like C++, Go, python, lua, etc. and Rust is the hardest language for me to master. In PingCAP, we will let the new colleague spend at least one month to learn Rust, to struggle with the compiling errors, and then to rise above it. This would never happen for Go.Besides, the compiling time is very long, even longer than C++. Each time when I type cargo build to start building TiKV, I can even do some pushups.Although Rust is around for a long time, it still lacks of libraries and tools, and some third projects have not been verified in production yet. These are all the risks for us. Most seriously, it is hard for us to find Rust programmer because only few know it in China, so we are always shorthanded.Then, Why Rust? Although Rust has the above disadvantages, its advantages are attractive for us too. Rust is memory safe, so we don’t need to worry about memory leak, or dangling pointer any more.Rust is thread safe, so there won’t be any data race problem. All the safety are guaranteed by compiler. So in most cases, when the compiling passes, we are sure that we can run the program safely.Rust has no GC expenses, so we won’t meet the &amp;ldquo;stop the world&amp;rdquo; problem. Calling C through FFI is very fast, so we don’t worry the performance reduction when calling the RocksDB API. At last, Rust has an official package manager, Cargo, we can find many libraries and use them directly.We made a hard but great decision: Use Rust! Back to the topTiKV Timeline  Here you can see the TiKV timeline. We first began to develop TiKV January 1st, 2016, and made it open source on April 1st, 2016, and this is not a joke like Gmail at All April Fool&amp;rsquo;s Day. TiKV was first used in production in October, 2016, when we had not even released a beta version. In November, 2016, we released the first beta version; then RC1 in December, 2016, RC2 in February, this year. Later we plan to release RC3 in April and the first GA version in June.As you can see, the development of TiKV is very fast and the released versions of TiKV are stable. Choosing Rust has already been proved a correct decision. Thanks, Rust.TiKV Architecture  Now let’s go deep into TiKV. You can see from the TiKV architecture that the hierarchy of TiKV is clear and easy to understand.At the bottom layer, TiKV uses RocksDB, a high performance, persistent Key-Value store, as the backend storage engine.The next layer is Raft KV. TiKV uses the Raft to replicate data geographically. TiKV is designed to store tons of data which one Raft group can’t hold. So we split the data with ranges and use each range as an individual Raft group. We name this approach: Multi-Raft groups.TiKV provides a simple Key-Value API including SET, GET, DELETE to let user use it just as any distributed Key-Value storage. The upper layer also uses these to support advanced functions.Above the Raft layer, it is MVCC. All the keys saved in TiKV must contain a globally unique timestamp, which is allocated by Placement Driver. TiKV uses it to support distributed transactions.On the top layer, it is the KV and coprocessor API layer for handling client requests.Back to the topMulti-Raft  Here is an example of Multi-Raft.You can see that there are four TiKV nodes. Within each store, we have several regions. Region is the basic unit of data movement and is replicated by Raft. Each region is replicated to three nodes. These three replicas of one Region make a Raft group.Scale Out Scale-out (initial state) Here is an example of horizontal scalability. At first, we have four nodes, Node A has three regions, others have two regions.Of course, Node A is busier than other nodes, and we want to reduce its stress.Scale-out (add new node) So we add a new Node E, and begin to move the region 1 in Node A to Node E. But here we find that the leader of region 1 is in Node A, so we will first transfer the leader from Node A to Node B.Scale-out (balancing) After that, the leader of region 1 …"},
		{"url": "https://pingcap.com/blog-cn/tidb-internal-2/",
		"title": "三篇文章了解 TiDB 技术内幕 - 说计算", 
		"content": " 关系模型到 Key-Value 模型的映射 在这我们将关系模型简单理解为 Table 和 SQL 语句，那么问题变为如何在 KV 结构上保存 Table 以及如何在 KV 结构上运行 SQL 语句。 假设我们有这样一个表的定义：CREATE TABLE User { ID int, Name varchar(20), Role varchar(20), Age int, PRIMARY KEY (ID)， Key idxAge (age) }; SQL 和 KV 结构之间存在巨大的区别，那么如何能够方便高效地进行映射，就成为一个很重要的问题。一个好的映射方案必须有利于对数据操作的需求。那么我们先看一下对数据的操作有哪些需求，分别有哪些特点。对于一个 Table 来说，需要存储的数据包括三部分： 表的元信息 Table 中的 Row 索引数据  表的元信息我们暂时不讨论，会有专门的章节来介绍。 对于 Row，可以选择行存或者列存，这两种各有优缺点。TiDB 面向的首要目标是 OLTP 业务，这类业务需要支持快速地读取、保存、修改、删除一行数据，所以采用行存是比较合适的。对于 Index，TiDB 不止需要支持 Primary Index，还需要支持 Secondary Index。Index 的作用的辅助查询，提升查询性能，以及保证某些 Constraint。查询的时候有两种模式，一种是点查，比如通过 Primary Key 或者 Unique Key 的等值条件进行查询，如 select name from user where id=1; ，这种需要通过索引快速定位到某一行数据；另一种是 Range 查询，如 select name from user where age &amp;gt; 30 and age &amp;lt; 35;，这个时候需要通过idxAge索引查询 age 在 20 和 30 之间的那些数据。Index 还分为 Unique Index 和 非 Unique Index，这两种都需要支持。分析完需要存储的数据的特点，我们再看看对这些数据的操作需求，主要考虑 Insert/Update/Delete/Select 这四种语句。对于 Insert 语句，需要将 Row 写入 KV，并且建立好索引数据。对于 Update 语句，需要将 Row 更新的同时，更新索引数据（如果有必要）。对于 Delete 语句，需要在删除 Row 的同时，将索引也删除。上面三个语句处理起来都很简单。对于 Select 语句，情况会复杂一些。首先我们需要能够简单快速地读取一行数据，所以每个 Row 需要有一个 ID （显示或隐式的 ID）。其次可能会读取连续多行数据，比如 Select * from user;。最后还有通过索引读取数据的需求，对索引的使用可能是点查或者是范围查询。大致的需求已经分析完了，现在让我们看看手里有什么可以用的：一个全局有序的分布式 Key-Value 引擎。全局有序这一点重要，可以帮助我们解决不少问题。比如对于快速获取一行数据，假设我们能够构造出某一个或者某几个 Key，定位到这一行，我们就能利用 TiKV 提供的 Seek 方法快速定位到这一行数据所在位置。再比如对于扫描全表的需求，如果能够映射为一个 Key 的 Range，从 StartKey 扫描到 EndKey，那么就可以简单的通过这种方式获得全表数据。操作 Index 数据也是类似的思路。接下来让我们看看 TiDB 是如何做的。TiDB 对每个表分配一个 TableID，每一个索引都会分配一个 IndexID，每一行分配一个 RowID（如果表有整数型的 Primary Key，那么会用 Primary Key 的值当做 RowID），其中 TableID 在整个集群内唯一，IndexID/RowID 在表内唯一，这些 ID 都是 int64 类型。 每行数据按照如下规则进行编码成 Key-Value pair：Key： tablePrefix_rowPrefix_tableID_rowID Value: [col1, col2, col3, col4] 其中 Key 的 tablePrefix/rowPrefix 都是特定的字符串常量，用于在 KV 空间内区分其他数据。 对于 Index 数据，会按照如下规则编码成 Key-Value pair：Key: tablePrefix_idxPrefix_tableID_indexID_indexColumnsValue Value: rowID Index 数据还需要考虑 Unique Index 和非 Unique Index 两种情况，对于 Unique Index，可以按照上述编码规则。但是对于非 Unique Index，通过这种编码并不能构造出唯一的 Key，因为同一个 Index 的 tablePrefix_idxPrefix_tableID_indexID_  都一样，可能有多行数据的 ColumnsValue  是一样的，所以对于非 Unique Index 的编码做了一点调整：Key: tablePrefix_idxPrefix_tableID_indexID_ColumnsValue_rowID Value：null 这样能够对索引中的每行数据构造出唯一的 Key。 注意上述编码规则中的 Key 里面的各种 xxPrefix 都是字符串常量，作用都是区分命名空间，以免不同类型的数据之间相互冲突，定义如下：var( tablePrefix = []byte{&amp;#39;t&amp;#39;} recordPrefixSep = []byte(&amp;#34;_r&amp;#34;) indexPrefixSep = []byte(&amp;#34;_i&amp;#34;) ) 另外请大家注意，上述方案中，无论是 Row 还是 Index 的 Key 编码方案，一个 Table 内部所有的 Row 都有相同的前缀，一个 Index 的数据也都有相同的前缀。这样具体相同的前缀的数据，在 TiKV 的 Key 空间内，是排列在一起。同时只要我们小心地设计后缀部分的编码方案，保证编码前和编码后的比较关系不变，那么就可以将 Row 或者 Index 数据有序地保存在 TiKV 中。这种保证编码前和编码后的比较关系不变 的方案我们称为 Memcomparable，对于任何类型的值，两个对象编码前的原始类型比较结果，和编码成 byte 数组后（注意，TiKV 中的 Key 和 Value 都是原始的 byte 数组）的比较结果保持一致。具体的编码方案参见 TiDB 的 codec 包。采用这种编码后，一个表的所有 Row 数据就会按照 RowID 的顺序排列在 TiKV 的 Key 空间中，某一个 Index 的数据也会按照 Index 的 ColumnValue 顺序排列在 Key 空间内。现在我们结合开始提到的需求以及 TiDB 的映射方案来看一下，这个方案是否能满足需求。首先我们通过这个映射方案，将 Row 和 Index 数据都转换为 Key-Value 数据，且每一行、每一条索引数据都是有唯一的 Key。其次，这种映射方案对于点查、范围查询都很友好，我们可以很容易地构造出某行、某条索引所对应的 Key，或者是某一块相邻的行、相邻的索引值所对应的 Key 范围。最后，在保证表中的一些 Constraint 的时候，可以通过构造并检查某个 Key 是否存在来判断是否能够满足相应的 Constraint。至此我们已经聊完了如何将 Table 映射到 KV 上面，这里再举个简单的例子，便于大家理解，还是以上面的表结构为例。假设表中有 3 行数据： &amp;ldquo;TiDB&amp;rdquo;, &amp;ldquo;SQL Layer&amp;rdquo;, 10 &amp;ldquo;TiKV&amp;rdquo;, &amp;ldquo;KV Engine&amp;rdquo;, 20 &amp;ldquo;PD&amp;rdquo;, &amp;ldquo;Manager&amp;rdquo;, 30  那么首先每行数据都会映射为一个 Key-Value pair，注意这个表有一个 Int 类型的 Primary Key，所以 RowID 的值即为这个 Primary Key 的值。假设这个表的 Table ID 为 10，其 Row 的数据为：t_r_10_1 --&amp;gt; [&amp;#34;TiDB&amp;#34;, &amp;#34;SQL Layer&amp;#34;, 10] t_r_10_2 --&amp;gt; [&amp;#34;TiKV&amp;#34;, &amp;#34;KV Engine&amp;#34;, 20] t_r_10_3 --&amp;gt; [&amp;#34;PD&amp;#34;, &amp;#34;Manager&amp;#34;, 30] 除了 Primary Key 之外，这个表还有一个 Index，假设这个 Index 的 ID 为 1，则其数据为：t_i_10_1_10_1 --&amp;gt; null t_i_10_1_20_2 --&amp;gt; null t_i_10_1_30_3 --&amp;gt; null 大家可以结合上面的编码规则来理解这个例子，希望大家能理解我们为什么选择了这个映射方案，这样做的目的是什么。元信息管理 上节介绍了表中的数据和索引是如何映射为 KV，本节介绍一下元信息的存储。Database/Table 都有元信息，也就是其定义以及各项属性，这些信息也需要持久化，我们也将这些信息存储在 TiKV 中。每个 Database/Table 都被分配了一个唯一的 ID，这个 ID 作为唯一标识，并且在编码为 Key-Value 时，这个 ID 都会编码到 Key 中，再加上 m_ 前缀。这样可以构造出一个 Key，Value 中存储的是序列化后的元信息。 除此之外，还有一个专门的 Key-Value 存储当前 Schema 信息的版本。TiDB 使用 Google F1 的 Online Schema 变更算法，有一个后台线程在不断的检查 TiKV 上面存储的 Schema 版本是否发生变化，并且保证在一定时间内一定能够获取版本的变化（如果确实发生了变化）。这部分的具体实现参见 TiDB 的异步 schema 变更实现一文。SQL on KV 架构 TiDB 的整体架构如下图所示TiKV Cluster 主要作用是作为 KV 引擎存储数据，上篇文章已经介绍过了细节，这里不再敷述。本篇文章主要介绍 SQL 层，也就是 TiDB Servers 这一层，这一层的节点都是无状态的节点，本身并不存储数据，节点之间完全对等。TiDB Server 这一层最重要的工作是处理用户请求，执行 SQL 运算逻辑，接下来我们做一些简单的介绍。SQL 运算 理解了 SQL 到 KV 的映射方案之后，我们可以理解关系数据是如何保存的，接下来我们要理解如何使用这些数据来满足用户的查询需求，也就是一个查询语句是如何操作底层存储的数据。 能想到的最简单的方案就是通过上一节所述的映射方案，将 SQL 查询映射为对 KV 的查询，再通过 KV 接口获取对应的数据，最后执行各种计算。 比如 Select count(*) from user where name=&amp;quot;TiDB&amp;quot;; 这样一个语句，我们需要读取表中所有的数据，然后检查 Name 字段是否是 TiDB，如果是的话，则返回这一行。这样一个操作流程转换为 KV 操作流程： 构造出 Key Range：一个表中所有的 RowID 都在 [0, MaxInt64) 这个范围内，那么我们用 0 和 MaxInt64 根据 Row 的 Key 编码规则，就能构造出一个 [StartKey, EndKey) 的左闭右开区间 扫描 Key Range：根据上面构造出的 Key Range，读取 TiKV 中的数据 过滤数据：对于读到的每一行数据，计算 name=&amp;quot;TiDB&amp;quot; 这个表达式，如果为真，则向上返回这一行，否则丢弃这一行数据 计算 Count：对符合要求的每一行，累计到 Count 值上面 这个方案肯定是可以 Work 的，但是并不能 Work 的很好，原因是显而易见的：   在扫描数据的时候，每一行都要通过 KV 操作同 TiKV 中读取出来，至少有一次 RPC 开销，如果需要扫描的数据很多，那么这个开销会非常大 并不是所有的行都有用，如果不满足条件，其实可以不读取出来 符合要求的行的值并没有什么意义，实际上这里只需要有几行数据这个信息就行  分布式 SQL 运算 如何避免上述缺陷也是显而易见的，首先我们需要将计算尽量靠近存储节点，以避免大量的 RPC 调用。其次，我们需要将 Filter 也下推到存储节点进行计算，这样只需要返回有效的行，避免无意义的网络传输。最后，我们可以将聚合函数、GroupBy 也下推到存储节点，进行预聚合，每个节点只需要返回一个 Count 值即可，再由 tidb-server 将 Count 值 Sum 起来。 这里有一个数据逐层返回的示意图：这里有一篇文章详细描述了 TiDB 是如何让 SQL 语句跑的更快，大家可以参考一下。SQL 层架构 上面几节简要介绍了 SQL 层的一些功能，希望大家对 SQL 语句的处理有一个基本的了解。实际上 TiDB 的 SQL 层要复杂的多，模块以及层次非常多，下面这个图列出了重要的模块以及调用关系：用户的 SQL 请求会直接或者通过 Load Balancer 发送到 tidb-server，tidb-server 会解析 MySQL Protocol Packet，获取请求内容，然后做语法解析、查询计划制定和优化、执行查询计划获取和处理数据。数据全部存储在 TiKV 集群中，所以在这个过程中 tidb-server 需要和 tikv-server 交互，获取数据。最后 tidb-server 需要将查询结果返回给用户。小结 到这里，我们已经从 SQL 的角度了解了数据是如何存储，如何用于计算。SQL 层更详细的介绍会在今后的文章中给出，比如优化器的工作原理，分布式执行框架的细节。 下一篇文章我们将会介绍一些关于 PD 的信息，这部分会比较有意思，里面的很多东西是在使用 TiDB 过程中看不到，但是对整体集群又非常重要。主要会涉及到集群的管理和调度。"},
		{"url": "https://pingcap.com/blog/2017-05-23-perconalive17/",
		"title": "A Brief Introduction of TiDB", 
		"content": " This is the speech Edward Huang gave at Percona Live - Open Source Database Conference 2017.The slides are here. Speaker introduction What would you do when… TiDB Project - Goal Sofware Stack Safe Split Scale Out ACID Transaction Distributed SQL TiDB SQL Layer Overview What Happens behind a query Distributed Join (HashJoin) Tools Matter Use Cases Sysbench Roadmap  Speaker introduction  As one of the three co-founders of PingCAP, I feel honored that PingCAP was once again invited to the Percona Live Conference.Last year, our CEO Max Liu has introduced TiDB and TiKV to the public. He mainly focused on how we build TiDB and also formulated a future plan of our projects. This time, I’ll draw a detailed picture of TiDB to help you understand how it works.First of all, I’d like to introduce myself. My name is Edward Huang, an infrastructure software engineer and the CTO of PingCAP.Up to now, I have worked on three projects, Codis, a proxy-based redis cluster solution which is very popular in China , TiDB and TiKV, a NewSQL database, our topic today. All of them are open source and many people benefit from them, especially in China. And I prefer languages such as Golang, Python, and Rust. By the way, we are using Golang and Rust in our projects (TiDB is written in Go and TiKV uses Rust).What would you do when… And first of all I want to ask a question: what would you do when your RDBMS is becoming the bottleneck of your application? Maybe most of you guys may have experienced the following situations. In the old days, all you can do is to either refactor your application or use database middleware, something like mysql proxy. But once you decide to use the sharding solution, you will never get rid of sharding key and say goodbye to complex query as it’s a one-way path.So how to scale your relational database is a pain point of the entire industry.Back to the topTiDB goal And there comes TiDB, when we were designing TiDB, we want to achieve the following goals: Make sharding and data movement transparent to users so that developers can focus on application development. 100% OLTP and 80% OLAP support. TiDB aims to be a hybrid database that supports both OLTP and OLAP. This is feasible because TiDB supports transactions and has our own full featured distributed SQL engine (including parser, optimizer and query executor). TiDB has to be compatible with the MySQL protocol, by implementing MySQL grammars and the network protocol. In this way, our users can reuse many MySQL tools and greatly reduce the migration costs. Twenty-four/Seven availability, even in case of datacenter outages. Thanks to the Raft consensus algorithm, TiDB can ensure the data consistency and availability all the time. Open source, of course.  During the first section, I’ll talk about the technical overview of TiDB and TiKV project, including the storage layer, a brief walk through our distributed sql engine, and some tools for community users to migrate from MySQL to TiDB and vice versa. Secondly, I’ll introduce some real world cases and benchmarks. We got several users in China, which have already used TiDB in production for over 3 months. And in the end, I’ll do a quick demo of setting up a TiDB-cluster and have some queries on it.Architecture Below shows the TiDB architecture.In this diagram, there are three components: the SQL layer, which is TiDB; the distributed storage layer, which is TiKV; and Placement Driver, aka PD.These three components communicate with each other through gRPC. TiDB server is stateless. It doesn’t store data and it is for computing only. It translates user’s SQL statement and generates the query plan, which presents as the rpc calls of TiKV. TiKV is a distributed key value database, acting as the underlying storage layer of TiDB and it’s the place where data is actually stored. This layer uses Raft consensus algorithm to replicate data and guarantee data safety. And TiKV also implements a distributed computing mechanism so that the sql layer would be able to do something like predicate push down or aggregate push down. Placement Driver is the managing component of the entire cluster and it stores the metadata, handles timestamp allocation request for ACID transaction, just like the TrueTime for Spanner, but we don’t have the hardware. What’s more, it’s controlling the data movement for dynamic workload balance and failover.  Back to the topStorage stack  Let’s dive deep into the storage stack of TiKV.As mentioned earlier, TiKV is the underlying storage layer where data is actually stored. More specifically, data is stored in RocksDB locally, which is the bottom layer of the TiKV architecture as you can see from this diagram. On top of RocksDB, we build a Raft layer.So what is Raft? Raft is a consensus algorithm that equals to Multi-Paxos in fault-tolerance and performance. It has several key features such as leader election, auto failover and membership changes. And Raft ensures that data is safely replicated. We have exposed the Raw Key Value API at this layer. If you want a scalable, highly available kv database, and don’t care about cross-row ACID transaction, you can use the Raw Key Value API for higher performance.The middle layer is MVCC, Multiversion concurrency control. The top two layers are transaction and grpc API. The API here is the transactional KV API.TiKV is written in Rust and the reason is that the storage layer is performance-critical and stability is first-class citizen of course. We only got c/c++ in the past, and now we have rust. Rust is great for infrastructure system software like database, operation system… Without any extra cost for GC, runtime and high performance. Another great thing is that Rust does a lot of innovation works to prevent memory leaks and data race, which means a lot to us.Now we know that the actual data is stored in RocksDB. But how exactly is data organized inside of the RocksDB instances? The answer is by Regions.Region is a set of continuous key-value pairs in byte-order.Back to the topSafe split Let’s take a look at the diagram here: The data is split into a set of continuous key-value pairs which we name them from a to z. Region 1 stores &amp;ldquo;a&amp;rdquo; to “e”, Region 2 “f” to “j”, Region 3 “k” to “o”, etc. As region is a logical concept, all the regions in a physical node share the same rocksdb instance.In each RocksDB instance, as I just mentioned, there are several regions and each region is replicated to other instances by Raft. The replicas of the same Region, Region 4 for example, make a Raft group.The metadata of the raft groups is stored in Placement Driver, and of course, placement driver is a cluster, replicates the metadata by Raft, too.In TiKV, we adopt a multi-raft model. What’s multi-raft? It’s a way to split and merge regions dynamically, and of course, safely. We name this approach &amp;ldquo;safe split/merge&amp;rdquo;.For example, Region 1 from &amp;ldquo;a&amp;rdquo; to “e” is safely split into Region 1.1 “a” to “c” and Region 1.2 “d” to “e”, we need to guarantee no data is lost during the split.This explains how one Region is split, but how about its replicas on other nodes? Let me show you the process.This is the initial state for Region 1. You can see there is a Raft group with three TiKV nodes. Region 1 on TiKV1 is the Leader and the other two replicas are the followers.However, there comes a situation that there are too much data in Region 1 and it needs to be split.It’s easy if there is only one Region 1, but in this case, we have three replicas. How can all the replicas be split safely? The answer is also Raft. Let’s see how it works.The split is initiated by the Leader, which means Region 1 is split to Region 1.1 and Region 1.2 firstly in the Leader as you can see from the diagram.When the split-log is written to WAL in the Leader, it is replicated by Raft and sent to the followers. Then, the followers apply the split log, just like any other normal raft log.And finally, once the split-log is …"},
		{"url": "https://pingcap.com/blog/2017-05-22-Comparison-between-MySQL-and-TiDB-with-tens-of-millions-of-data-per-day/",
		"title": "Migration from MySQL to TiDB to handle tens of millions of rows of data per day", 
		"content": " TiDB use case Table of content  Background MySQL, our first choice Look for new solutions TiDB, give it a go Feedbacks from TiDB  Background GAEA is a mobile gaming provider and aims to develop high-quality games for international players. GAEA uses its GaeaAD system to support the cross-platform real-time advertising system. GaeaAD performs a real-time match between the advertising data and the information reported by the game SDK. In other words, GaeaAD conducts a real-time analysis based on the data of the advertisements on different advertising channels and the amount of players brought by the corresponding channels, with the purpose of displaying and optimizing the conversion effects of advertising within minutes.MySQL, our first choice Considering the amount of data and for a simplified implementation, GAEA chose the highly-available MySQL RDS storage solution at the very beginning of designing GaeaAD. At that time, we mainly used SQL Syntax to implement the matching logic, including many join table queries and aggregation operations. The system worked well and responded within one minute with tens of millions of rows of data. Look for new solutions However, with the growth of business, GaeaAD receives more than tens of millions of rows of data per day and the amount multiplies during peak hours. Obviously, database has become a bottleneck. And at the moment, GAEA’s entire technical framework encounters three problems: The time needed for a single match has increased to over 2 minutes from about 10 seconds. The slowest aggregation query, even takes 20 minutes to complete. This imposes a serious challenge on timeliness. What’s worse, one of the drawbacks of MySQL is the query time increases with the amount of data. Therefore, the larger the data volume, the slower the query. With the accumulation of historical data, the amount of data stored in a single table soon reaches a hundred million rows. At that time, the read/write capabilities of the single table are close to its limit. Due to the query performance mentioned above and the capacity limit of a standalone database, we have to delete data regularly. Thus, it is not possible to query the business data from a long time ago.  According to the data volume growth, we thought that distributed database would be a good solution. Vertically and horizontally splitting business, the middleware solutions based on MySQL and some predominant NoSQL solutions were all taken into consideration.After a thorough assessment, we denied the solution of horizontally splitting business. Since the business logic contains lots of related queries and subqueries, it is impossible to achieve transparent compatibility if tables have been split. Besides, it is a core business system, we are not allowed to refactor it given the limited time and energy. The middleware solutions face similar problems with database sharding: even though it enables mass storage and real-time writing, it has a limited query flexibility. Moreover, the maintenance cost of multiple MySQL instances requires a second thought.Then we continued to analyze the second choice, NoSQL. Since this system needs to support the concurrent real-time writing and query from the business-end, it is not suitable to use systems like Greenplum, Hive or SparkSQL, which are not designed for real-time writing. As for MongoDB, its document query access interface is a challenge for our business. Besides, we were not sure whether MongoDB could perform the efficient aggregate analysis under the condition of such a large amount of data.In conclusion, what we want is a database that is as easy to use as MySQL, eliminating the trouble of modifying any business, and it meets the needs of distributed storage and a high performance of complex query.We studied many distributed database solutions in the community and came upon TiDB. As the protocol layer of TiDB is compatible with MySQL and it supports complex query, we can power our applications without changing a single line of code. Besides, there is hardly any migration cost.TiDB, give it a go In the process of test deployment, we used the Syncer tool, provided by TiDB, to deploy TiDB as a MySQL Slave to the MySQL master of the original business, testing the compatibility and stability of read/write. After a while, the system was proved to perform well in read/write so we decided to move the read request of the business layer to TiDB. Later, we also switch the write business to the TiDB cluster, making the system online smoothly.The GaeaAD system works well for more than half a year since it has come online in October, 2016. Based on the hands-on experience, we summarized the following benefits brought by TiDB: We replaced the highly-available MySQL RDS with the 3-node TiDB cluster. The average time needed for a single match reduces to about 30 seconds from over 2 minutes, and it even reaches to 10 seconds or so with the continuous optimization of TiDB’s engineers. In addition, we found that TiDB has superior advantages and outperforms MySQL especially when the data volume is large. We guess this owes to the existence of TiDB’s self-developed distributed SQL Optimizer. But when it comes to a small amount of data, this advantage is not that prominent because of the internal communication cost. (A comparison between the query time of TiDB and MySQL in cases of different amounts of data) TiDB supports automatic Sharding. The business side doesn’t need to split tables and TiDB no longer sets the Sharding key or partition table as a traditional database middleware product. Storage of the bottom layer automatically spreads across clusters according to the data distribution. The capacity and performance can be scaled horizontally through adding more nodes, greatly reducing the maintenance cost. TiDB supports ongoing rolling upgrades. Up to now, we have about 10 online upgrades and never experienced one interrupt, which shows TiDB’s excellent availability. TiDB supports the mutual backup with MySQL. This function solves the transition problem in business migration.  Currently, we are replacing MongoDB with TiDB as MongoDB is not easy to use, expensive to maintain and its query manner is not as flexible as traditional SQL. MongoDB once served as our data storage system of the real-time computing business of the BI system in the storm cluster. We are also planning to migrate the business that requires high real-time performance, large storage capacity and long storage cycle to TiDB, which seems to be a suitable scenario.Feedbacks from TiDB TiDB helps GAEA in the following aspects: 1. TiDB supports many push-down expressions and makes full use of the computing resources of TiKV’s multiple instances and therefore, accelerates the computing speed. At the same time, TiDB filters as much unnecessary data as possible and reduces the network overhead. TiDB supports HashJoin by default and tries hard to parallelize operators, making full use of the computing resources of the entire cluster. TiDB reads data in a linear way and has optimized the IndexScan operator, shortening the startup time of the whole process.  About the author LIU Xuan, the senior development engineer of GAEA’s data platform, is responsible for the real-time data business and the data flow field. Graduated from College of Computer Science and Electronic Engineering of Hunan University, LIU was Baidu’s senior operation &amp;amp; maintenance engineer and was responsible for the database creating and maintenance of Search Service Group (SSG)."},
		{"url": "https://pingcap.com/weekly/2017-05-22-tidb-weekly/",
		"title": "Weekly update (May 15 ~ May 21, 2017)", 
		"content": " Weekly update in TiDB Last week, we landed 31 PRs in the TiDB repositories.Added  Use etcd to elect DDL job leader. Support Load Data with specified column list. Add the following builtin functions: umcompress and uncompressdlength convert_tz period-diff  Support the Json type and Json data encoding/decoding. Add a Jenkins CI in Pull Request. Add the EvalDuration and EvalTime interface for expressions. Support Index Lookup Join in new planner.  Fixed  Add default value for the Password Validation system variables. Fix the issue of retrying with no limitation when committing primary key failed. Fix the issue of context cancellation triggering onSendFail and dropping cache: context cancel doesn&amp;rsquo;t mean cache out of data. Fix a bug in Sort Merge Join. Correct comment mistakes. Reprocess SQL statement when meeting the infoschema change error. Update an error code to MySQL standard error code. Consider schema changing when retrying prepared statement.  Improved  Refator range calculation related code. Consider task type when building the physical plan: to make cost more precise. Support more SQL grammar for Lock Options. Rename SupportRequestType to IsRequestTypeSupported to improve readability. Support more SQL grammar for Compress Options in the AlterTable statement. Change the schema of statistic table.  Weekly update in TiKV Last week, We landed 16 PRs in the TiKV repositories.Added  Support table scan with DAG in coprocessor. Add Jenkins support, see 643, 644, 1835, 1836. Write Raft log synchronously to ensure data safety.  Fixed  Use IEC size for size output. Enlarge max duration for scheduler histogram metrics.  Improved  Use SmallGroupFirstQueue in endpoint scheduler. Fetch snapshot lazily to reduce the cost of creating unnecessary snapshot. Create WriteBatch with capacity to avoid reallocating.  "},
		{"url": "https://pingcap.com/weekly/2017-05-15-tidb-weekly/",
		"title": "Weekly update (May 08 ~ May 14, 2017)", 
		"content": " Weekly update in TiDB Last week, we landed 28 PRs in the TiDB repositories.Added  Add builtin function uncompress and uncompressdlength, convert_tz, period-diff Fill data into information_schema.key_column_usage. Add Open interface for Executor. Show warnings for Load Data statement. Support Json type and functions in parser. Support top-n operator in new planner.  Fixed  Consider session variable time_zone for timestamp datum. Fix data race problem in IndexLookup Executor. Fix a bug in HashJoin Executor encoding/decoding. Return right value for @@version.  Improved  Refator range calculation related code. Refactor expression evaluation framework: Add self attribute in builtin function.Return panic when calling wrong function on baseXBuiltinFunc.  Weekly update in TiKV Last week, We landed 17 PRs in the TiKV repositories.Added  Use clap to parse command line options. Show TiKV instance downtime state in pd-ctl. Show scheduling operation history in pd-ctl. Show cluster ID in pd-ctl. Introduce SmallGroupFirstQueue to speedup point get later. Use zstd compression type. Add process metrics in Prometheus.  Fixed  Fix dial wrong listening address bug. Remove the ￼offline peer￼ directly. Mark peer as pending_remove to avoid following operations.  Improved  Add the reason of cluster ID mismatch in error log. Add log to apply delegate register and deregister.  "},
		{"url": "https://pingcap.com/blog-cn/tidb-internal-1/",
		"title": "三篇文章了解 TiDB 技术内幕 - 说存储", 
		"content": " 引言 数据库、操作系统和编译器并称为三大系统，可以说是整个计算机软件的基石。其中数据库更靠近应用层，是很多业务的支撑。这一领域经过了几十年的发展，不断的有新的进展。很多人用过数据库，但是很少有人实现过一个数据库，特别是实现一个分布式数据库。了解数据库的实现原理和细节，一方面可以提高个人技术，对构建其他系统有帮助，另一方面也有利于用好数据库。研究一门技术最好的方法是研究其中一个开源项目，数据库也不例外。单机数据库领域有很多很好的开源项目，其中 MySQL 和 PostgreSQL 是其中知名度最高的两个，不少同学都看过这两个项目的代码。但是分布式数据库方面，好的开源项目并不多。 TiDB 目前获得了广泛的关注，特别是一些技术爱好者，希望能够参与这个项目。由于分布式数据库自身的复杂性，很多人并不能很好的理解整个项目，所以我希望能写一些文章，自顶向上，由浅入深，讲述 TiDB 的一些技术原理，包括用户可见的技术以及大量隐藏在 SQL 界面后用户不可见的技术点。保存数据 数据库最根本的功能是能把数据存下来，所以我们从这里开始。保存数据的方法很多，最简单的方法是直接在内存中建一个数据结构，保存用户发来的数据。比如用一个数组，每当收到一条数据就向数组中追加一条记录。这个方案十分简单，能满足最基本，并且性能肯定会很好，但是除此之外却是漏洞百出，其中最大的问题是数据完全在内存中，一旦停机或者是服务重启，数据就会永久丢失。为了解决数据丢失问题，我们可以把数据放在非易失存储介质（比如硬盘）中。改进的方案是在磁盘上创建一个文件，收到一条数据，就在文件中 Append 一行。OK，我们现在有了一个能持久化存储数据的方案。但是还不够好，假设这块磁盘出现了坏道呢？我们可以做 RAID （Redundant Array of Independent Disks），提供单机冗余存储。如果整台机器都挂了呢？比如出现了火灾，RAID 也保不住这些数据。我们还可以将存储改用网络存储，或者是通过硬件或者软件进行存储复制。到这里似乎我们已经解决了数据安全问题，可以松一口气了。But，做复制过程中是否能保证副本之间的一致性？也就是在保证数据不丢的前提下，还要保证数据不错。保证数据不丢不错只是一项最基本的要求，还有更多令人头疼的问题等待解决： 能否支持跨数据中心的容灾？ 写入速度是否够快？ 数据保存下来后，是否方便读取？ 保存的数据如何修改？如何支持并发的修改？ 如何原子地修改多条记录？  这些问题每一项都非常难，但是要做一个优秀的数据存储系统，必须要解决上述的每一个难题。 为了解决数据存储问题，我们开发了 TiKV 这个项目。接下来我向大家介绍一下 TiKV 的一些设计思想和基本概念。Key-Value 作为保存数据的系统，首先要决定的是数据的存储模型，也就是数据以什么样的形式保存下来。TiKV 的选择是 Key-Value 模型，并且提供有序遍历方法。简单来讲，可以将 TiKV 看做一个巨大的 Map，其中 Key 和 Value 都是原始的 Byte 数组，在这个 Map 中，Key 按照 Byte 数组总的原始二进制比特位比较顺序排列。 大家这里需要对 TiKV 记住两点： 这是一个巨大的 Map，也就是存储的是 Key-Value pair 这个 Map 中的 Key-Value pair 按照 Key 的二进制顺序有序，也就是我们可以 Seek 到某一个 Key 的位置，然后不断的调用 Next 方法以递增的顺序获取比这个 Key 大的 Key-Value  讲了这么多，有人可能会问了，这里讲的存储模型和 SQL 中表是什么关系？在这里有一件重要的事情要说四遍：这里的存储模型和 SQL 中的 Table 无关！ 这里的存储模型和 SQL 中的 Table 无关！ 这里的存储模型和 SQL 中的 Table 无关！ 这里的存储模型和 SQL 中的 Table 无关！现在让我们忘记 SQL 中的任何概念，专注于讨论如何实现 TiKV 这样一个高性能高可靠性的巨大的（分布式的） Map。RocksDB 任何持久化的存储引擎，数据终归要保存在磁盘上，TiKV 也不例外。但是 TiKV 没有选择直接向磁盘上写数据，而是把数据保存在 RocksDB 中，具体的数据落地由 RocksDB 负责。这个选择的原因是开发一个单机存储引擎工作量很大，特别是要做一个高性能的单机引擎，需要做各种细致的优化，而 RocksDB 是一个非常优秀的开源的单机存储引擎，可以满足我们对单机引擎的各种要求，而且还有 Facebook 的团队在做持续的优化，这样我们只投入很少的精力，就能享受到一个十分强大且在不断进步的单机引擎。当然，我们也为 RocksDB 贡献了一些代码，希望这个项目能越做越好。这里可以简单的认为 RocksDB 是一个单机的 Key-Value Map。Raft 好了，万里长征第一步已经迈出去了，我们已经为数据找到一个高效可靠的本地存储方案。俗话说，万事开头难，然后中间难，最后结尾难。接下来我们面临一件更难的事情：如何保证单机失效的情况下，数据不丢失，不出错？简单来说，我们需要想办法把数据复制到多台机器上，这样一台机器挂了，我们还有其他的机器上的副本；复杂来说，我们还需要这个复制方案是可靠、高效并且能处理副本失效的情况。听上去比较难，但是好在我们有 Raft 协议。Raft 是一个一致性算法，它和 Paxos 等价，但是更加易于理解。这里是 Raft 的论文，感兴趣的可以看一下。本文只会对 Raft 做一个简要的介绍，细节问题可以参考论文。另外提一点，Raft 论文只是一个基本方案，严格按照论文实现，性能会很差，我们对 Raft 协议的实现做了大量的优化，具体的优化细节可参考我司首席架构师 tangliu 同学的这篇文章。Raft 是一个一致性协议，提供几个重要的功能： Leader 选举 成员变更 日志复制  TiKV 利用 Raft 来做数据复制，每个数据变更都会落地为一条 Raft 日志，通过 Raft 的日志复制功能，将数据安全可靠地同步到 Group 的多数节点中。到这里我们总结一下，通过单机的 RocksDB，我们可以将数据快速地存储在磁盘上；通过 Raft，我们可以将数据复制到多台机器上，以防单机失效。数据的写入是通过 Raft 这一层的接口写入，而不是直接写 RocksDB。通过实现 Raft，我们拥有了一个分布式的 KV，现在再也不用担心某台机器挂掉了。Region 讲到这里，我们可以提到一个 非常重要的概念：Region。这个概念是理解后续一系列机制的基础，请仔细阅读这一节。前面提到，我们将 TiKV 看做一个巨大的有序的 KV Map，那么为了实现存储的水平扩展，我们需要将数据分散在多台机器上。这里提到的数据分散在多台机器上和 Raft 的数据复制不是一个概念，在这一节我们先忘记 Raft，假设所有的数据都只有一个副本，这样更容易理解。对于一个 KV 系统，将数据分散在多台机器上有两种比较典型的方案：一种是按照 Key 做 Hash，根据 Hash 值选择对应的存储节点；另一种是分 Range，某一段连续的 Key 都保存在一个存储节点上。TiKV 选择了第二种方式，将整个 Key-Value 空间分成很多段，每一段是一系列连续的 Key，我们将每一段叫做一个 Region，并且我们会尽量保持每个 Region 中保存的数据不超过一定的大小(这个大小可以配置，目前默认是 64mb)。每一个 Region 都可以用 StartKey 到 EndKey 这样一个左闭右开区间来描述。注意，这里的 Region 还是和 SQL 中的表没什么关系！ 请各位继续忘记 SQL，只谈 KV。 将数据划分成 Region 后，我们将会做 两件重要的事情： 以 Region 为单位，将数据分散在集群中所有的节点上，并且尽量保证每个节点上服务的 Region 数量差不多 以 Region 为单位做 Raft 的复制和成员管理  这两点非常重要，我们一点一点来说。先看第一点，数据按照 Key 切分成很多 Region，每个 Region 的数据只会保存在一个节点上面。我们的系统会有一个组件来负责将 Region 尽可能均匀的散布在集群中所有的节点上，这样一方面实现了存储容量的水平扩展（增加新的结点后，会自动将其他节点上的 Region 调度过来），另一方面也实现了负载均衡（不会出现某个节点有很多数据，其他节点上没什么数据的情况）。同时为了保证上层客户端能够访问所需要的数据，我们的系统中也会有一个组件记录 Region 在节点上面的分布情况，也就是通过任意一个 Key 就能查询到这个 Key 在哪个 Region 中，以及这个 Region 目前在哪个节点上。至于是哪个组件负责这两项工作，会在后续介绍。对于第二点，TiKV 是以 Region 为单位做数据的复制，也就是一个 Region 的数据会保存多个副本，我们将每一个副本叫做一个 Replica。Repica 之间是通过 Raft 来保持数据的一致（终于提到了 Raft），一个 Region 的多个 Replica 会保存在不同的节点上，构成一个 Raft Group。其中一个 Replica 会作为这个 Group 的 Leader，其他的 Replica 作为 Follower。所有的读和写都是通过 Leader 进行，再由 Leader 复制给 Follower。 大家理解了 Region 之后，应该可以理解下面这张图：我们以 Region 为单位做数据的分散和复制，就有了一个分布式的具备一定容灾能力的 KeyValue 系统，不用再担心数据存不下，或者是磁盘故障丢失数据的问题。这已经很 Cool，但是还不够完美，我们需要更多的功能。MVCC 很多数据库都会实现多版本控制（MVCC），TiKV 也不例外。设想这样的场景，两个 Client 同时去修改一个 Key 的 Value，如果没有 MVCC，就需要对数据上锁，在分布式场景下，可能会带来性能以及死锁问题。 TiKV 的 MVCC 实现是通过在 Key 后面添加 Version 来实现，简单来说，没有 MVCC 之前，可以把 TiKV 看做这样的：Key1 -&amp;gt; Value Key2 -&amp;gt; Value …… KeyN -&amp;gt; Value 有了 MVCC 之后，TiKV 的 Key 排列是这样的：Key1-Version3 -&amp;gt; Value Key1-Version2 -&amp;gt; Value Key1-Version1 -&amp;gt; Value …… Key2-Version4 -&amp;gt; Value Key2-Version3 -&amp;gt; Value Key2-Version2 -&amp;gt; Value Key2-Version1 -&amp;gt; Value …… KeyN-Version2 -&amp;gt; Value KeyN-Version1 -&amp;gt; Value …… 注意，对于同一个 Key 的多个版本，我们把版本号较大的放在前面，版本号小的放在后面（回忆一下 Key-Value 一节我们介绍过的 Key 是有序的排列），这样当用户通过一个 Key + Version 来获取 Value 的时候，可以将 Key 和 Version 构造出 MVCC 的 Key，也就是 Key-Version。然后可以直接 Seek(Key-Version)，定位到第一个大于等于这个 Key-Version 的位置。事务 TiKV 的事务采用的是 Percolator 模型，并且做了大量的优化。事务的细节这里不详述，大家可以参考论文以及我们的其他文章。这里只提一点，TiKV 的事务采用乐观锁，事务的执行过程中，不会检测写写冲突，只有在提交过程中，才会做冲突检测，冲突的双方中比较早完成提交的会写入成功，另一方会尝试重新执行整个事务。当业务的写入冲突不严重的情况下，这种模型性能会很好，比如随机更新表中某一行的数据，并且表很大。但是如果业务的写入冲突严重，性能就会很差，举一个极端的例子，就是计数器，多个客户端同时修改少量行，导致冲突严重的，造成大量的无效重试。其他 到这里，我们已经了解了 TiKV 的基本概念和一些细节，理解了这个分布式带事务的 KV 引擎的分层结构以及如何实现多副本容错。下一节会介绍如何在 KV 的存储模型之上，构建 SQL 层。"},
		{"url": "https://pingcap.com/blog-cn/tile-row-store/",
		"title": "基于 Tile 连接 Row-Store 和 Column-Store", 
		"content": " 在之前的 Kudu 的文章里面，我已经提到过，行列混存是一个非常有意思的研究方向，因为不同的存储方式有不同的针对应用场景，但作为技术人员，折腾是天性，所以大家都在研究如何融合行存和列存，让一个服务能尽量满足大部分应用需求，而这也是 TiDB 在努力的方向。在 Kudu Paper 里面说到，Kudu 首先在 mem 里面使用行存，但刷到硬盘之后，则使用的是列存，这当然是一个可以尝试的方式，但我觉得应该还有更多种的解决方式，于是找到了 CMU 的 Peloton 以及相关的 Paper，觉得有必要研究记录一下。Storage Model 很多时候，我喜欢用行存和列存，但看 Paper 的时候，发现都喜欢使用 NSM 和 DSM 来说明，这里就简单说明一下。NSM NSM 是 N-ary storage model 的简称，当然就是通常的行存了。NSM 主要针对 OLTP 场景，因为需要高性能的随机写入，NSM 的存储方式如下：NSM 不适用需要读取大量数据，并分析特定 column 的场景，因为 NSM 需要把整个 record 给读出来，在拿到对应的 column 数据分析，数据数据量很大，整个开销会很大。DSM DSM 是 decomposition storage model 的简称，也就是列存。DSM 主要针对 OLAP 场景，因为需要对一些特定的 column 进行快速扫描分析，DSM 的存储方式如下：DSM 当然就不适用与需要频繁随机更新的情况，因为任何写入，DSM 需要将 record 分开写入到不同的地方，写开销会很大。FSM 为了解决这个问题，就有了一个 FSM flexible storage model 来融合 NSM 和 DSM，在 Peloton 里面，它把这套系统叫做 HTAP (Hybrid Transactional/Analytical Processing)，不同于 NSM 按照每行存放，以及 DSM 按照每列存放，FSM 将数据分成了多个区块，Peloton 里面叫做 Tile，上面的图显示的是两个 Tile，一个 Tile 包含了 ID，Image ID 以及 Name，而另一个 Tile 里面则是包含了 Price 和 Data。各个 Tile 里面数据是连续存放的。就是说，我们使用 Tile 来模拟了 DSM，在 Tile 里面则是 NSM。Tile-Based Architecture Peloton 使用 tiles 来抽象了 storage 这一层，在上面的 FSM 例子我们可以看到，Tile 可以将一个 table 进行垂直和水平切分。Peloton 使用 physical tile 来处理实际的 storage，然后用另一个 logical tile 来隐藏了 physical tile 的实现，让外面更容易使用。Physical Tile Physical tile 的最小存储单位是 tile tuple，一批 tile tuple 形成了一个 physical tile。而一批 physical tile 则组成一个 tile group。一个 table 则是有多个 tile group 组成。在上面的例子中，table 被分成了三个 tile group (A, B, C)，每个 group 都有不同的 physical tiles。譬如 group A 就是由 tile A-1 和 A-2 组成，tile A-1 包含 table 前面三个 column ID，IMAGE-ID，和 NAME，而 tile A-2 则包含了 PRICE 和 DATA。使用 tile group，我们可以非常方便的支持 FSM。对于新插入的数据，通常是热点数据，我们会直接放到一个 OLTP 友好的 group 中，也就是 group 里面只有一个 tile（NSM）。当数据变冷之后，我们就将当前的 tile group 转成更 OLAP 优化的布局，也就是 group 里面可能有几个 tile 了。当 group 里面每个 tile 都只有一个 column 的时候，这其实就变成了 DSM 了。Logical Tile 使用 physical tile 的好处在于我们可以根据不同的情况将数据切分成不同的布局，但是这对于查询并不友好，因为数据在不同的 tile 里面。为了解决这个问题，Peloton 引入了 logical tile。Logical tile 隐藏了 physical tile 的具体实现，logical tile 的每个 column 可以指向一个或者多个 physical tiles 的 columns，每个 logical tile column 里面存储的是 tuple 在 physical tiles 里面的偏移位置。在上面的例子中，logical tile X 指向了两个 physical tiles A-1 和 A-2。 X 的第一个 column 指向了 physical tile A-1 的 ATTR-1 和 ATTR-2。而第一个 column 里面存放的 1，2，3 则是对应的 tuple 在 tile A-1 里面的偏移。譬如 1 就对应的是 (101, 201)。一个 logical tile column 也可以映射不同的 physical tile 的 columns。譬如上面 X 的第二个 column，就是对应的 tile A-1 的 ATTR-3 和 A-2 的 ATTR-1。当然一些 physical tile column 也可能不会有任何映射，譬如上面的 A-2 的 ATTR-2。使用 logical tile 的好处是很明显的，主要包括： Layout Transparency：logical tile 隐藏底层底层实际的存储实现，所以我们可以用不同的 engine 来满足不同的场景。 Vectorized Processing：使用 logical tile，我们可以一次进行一批向量处理，在一些场景下能极大的提升 CPU 性能。 Flexible Materialization：我们可以提前或者推迟的物化。在执行 query plan tree 的时候，甚至都能够动态去选择一个物化策略。 Caching Behavior：我们可以根据不同的维度去创建不同的 tile group，放到 cache 了，用来处理后续的查询。  Logical Tile Algebra Peloton 提供 algebra operators 来让外面更方便的使用。Operators 主要包括： Bridge：Bridge operators 连接的 logical tile 和 physical tile。譬如我们可以使用 sequential scan 和 index scan operators 去访问实际的数据，然后生成对应的 logical tile。而 materialize operator 则是将实际的 logical tile 转成实际的 physical tile。 Metadata：Logical tile 的 metadata 存放的一些关于底层 physical tile 的信息，以及一些 bitmap 来表明哪些 rows 在执行的时候必须被检查。Metadata 的 operators 只会改变 logical tile 的 metadata，并不会改变底层的 physical tile 的数据。譬如 projection operator 如果发现上层的 query plan 并不需要一些 attributes 了，就可以在 logical tile 里面移除。 Mutators ：Mutator operators 会改变 table 的实际存储数据。譬如 insert operator 首先会重新构建 logical tile 的 tuple，然后在插入到对应的 table 里面，而 delete operator 则是删除 table 里面的数据，update operator 则是先在 logical tile 里面删除，在通过之前的 tuple 重新构建一个新版本的 tuple，在插入到 table。Mutators 同时也会控制 tuple 在 transaction 里面的可见性。 Pipeline Breakers：当我们给一个 query plan 生成对应的 query plan tree 之后，在 tree 上层的 operators 需要等待 children 的操作完成返回了，才能继续进行。譬如 join operator 需要处理多个 logical tiles，并且在这些 tiles 上面执行 predicate。首先，join operator 会构建一个 output logical tile，它的 schema 是根据输入的 logical tile 来构建的。然后 join operator 会遍历 input logical tile，如果发现满足 predicate，就将结果放到 output logical tile，下面是 join 的一个例子：  Layout reorganization 虽然基于 Tile-Based Architecture 看起来很美好，但如果对于不同的 query，如果没有好的 tile 与其对应，那么其实根本就没啥用。 Peloton 采用的方法是定期对每个 table 计算出最优化的 storage layout，然后在根据这个 layout 重新组织 table。Peloton 使用一个轻量级的 monitor 来记录每个 query 访问的 attributes，用来确定哪一些 attributes 应该在新的 physical tile 里面放在一起。通常 Peloton 会收集 query 里面的 SELECT 和 WHERE 上面的 attributes。为了减少监控带来的开销，monitor 会随机的对 query 进行采样统计，并且 monitor 还需要保证不能只关注频繁的 transactional query，还需要关注一些不频繁的 analytical query，所以 Peloton 会也会记录 query 的 plan cost，并通过这些来给 analytical query 生成特定的 storage layout。Peloton 使用增量的方式进行 data layout 的 reorganization。对于一个给定的 tile group，Peloton 首先会将 data 拷贝到一个新的 layout 上面，同时会原子地用一个新的 tile group 来替换。任何并发的 delete 或者 update 操作都只会更新metadata 信息。新的 tile group 会有旧的 tile group metadata 的引用。如果一个 physical tile 没有被任何 logical tile 引用，那么 Peloton 就会将其回收。对于一个热点 tile group 来说，因为很可能一直在被 OLTP 的事务持续访问，所以 Peloton 并不会对这种 tile group 做 reorganization，只会等到数据变冷之后才会进行。因为 Peloton 使用的是通用的 MVCC 模式，所以一段时间之后，老版本的数据一定会变成冷数据，那么就可以开始 reorganization 了。Reorganization 通常就是将 layout 从 OLTP 友好的，逐渐变成 OLAP 友好的。小结 上面仅仅是介绍 Peloton 的一些实现 FSM 的机制，这里并没有介绍 MVCC，transaction 这些，因为这些对于数据库产品来说都几乎是标配的东西，原理上面差不多。这里仅仅是关注的是 Peloton 是如何做到行列混存的。简单来说，Peloton 使用 physical tile 将数据切分成了不同的单元，同时用 logical tile 来隐藏了相关的实现，并提供 algebra 让上层更方便的操作。在后台，统计 query 等信息，并定期增量的对 layout 进行 reorganization。不得不说，整个设计思路还是很巧妙的。TiKV 后面也会考虑行列混存，我们一直想同时搞定 OLTP + OLAP，但究竟采用哪些方案，还需要我们慢慢研究和思考，也欢迎对这块感兴趣的同学加入。"},
		{"url": "https://pingcap.com/meetup/meetup-2017-05-13/",
		"title": "【Infra Meetup No.48】分布式对象存储面临的挑战", 
		"content": " 今天的 Meetup，我们请到了来自白山云的张炎泼老师，为大家分享《分布式对象存储面临的挑战》。 讲师介绍：张炎泼 (xp)，30 年软件开发经验，物理系背叛者，设计师眼中的美工，bug maker，vim 死饭，悬疑片脑残粉。曾就职新浪，美团。现在白山云，不是白云山。在本次分享中，张炎泼老师从：海量小文件如何存储、如何节省存储成本、如何实现数据的自动恢复，三个方面，为大家进行了详细讲解。以下是本期 PPT 节选点击 下载本期 PPT"},
		{"url": "https://pingcap.com/blog-cn/kudu/",
		"title": "Kudu - 一个融合低延迟写入和高性能分析的存储系统", 
		"content": " Kudu 是一个基于 Raft 的分布式存储系统，它致力于融合低延迟写入和高性能分析这两种场景，并且能很好的嵌入到 Hadoop 生态系统里面，跟其他系统譬如 Cloudera Impala，Apache Spark 等对接。Kudu 很类似 TiDB。最开始，TiDB 是为了 OLTP 系统设计的，但后来发现我们 OLAP 的功能也越来越强大，所以就有了融合 OLTP 和 OLAP 的想法，当然这条路并不是那么容易，我们还有很多工作要做。因为 Kudu 的理念跟我们类似，所以我也很有兴趣去研究一下它，这里主要是依据 Kudu 在 2015 发布的 paper，因为 Kudu 是开源的，并且在不断的更新，所以现在代码里面一些实现可能还跟 paper 不一样了，但这里仅仅先说一下我对 paper 的理解，实际的代码我后续研究了在详细说明。为什么需要 Kudu？ 结构化数据存储系统在 Hadoop 生态系统里面，通常分为两类： 静态数据，数据通常都是使用二进制格式存放到 HDFS 上面，譬如 Apache Avro，Apache Parquet。但无论是 HDFS 还是相关的系统，都是为高吞吐连续访问数据这些场景设计的，都没有很好的支持单独 record 的更新，或者是提供好的随机访问的能力。 动态数据，数据通常都是使用半结构化的方式存储，譬如 Apache HBase，Apache Cassandra。这些系统都能低延迟的读写单独的 record，但是对于一些像 SQL 分析这样需要连续大量读取数据的场景，显得有点捉紧见拙。  上面的两种系统，各有自己的侧重点，一类是低延迟的随机访问特定数据，而另一类就是高吞吐的分析大量数据。之前，我们并没有这样的系统可以融合上面两种情况，所以通常的做法就是使用 pipeline，譬如我们非常熟悉的 Kafka，通常我们会将数据快速写到 HBase 等系统里面，然后通过 pipeline，在导出给其它分析系统。虽然我们在一定层面上面，我们其实通过 pipeline 来对整个系统进行了解耦，但总归要维护多套系统。而且数据更新之后，并不能直接实时的进行分析处理，有延迟的开销。所以在某些层面上面，并不是一个很好的解决方案。Kudu 致力于解决上面的问题，它提供了简单的来处理数据的插入，更新和删除，同时提供了 table scan 来处理数据分析。通常如果一个系统要融合两个特性，很有可能就会陷入两边都做，两边都没做好的窘境，但 Kudu 很好的在融合上面取得了平衡，那么它是如何做到的呢？Keyword Tables 和 schemas Kudu 提供了 table 的概念。用户可以建立多个 table，每个 table 都有一个预先定义好的 schema。Schema 里面定义了这个 table 多个 column，每个 column 都有名字，类型，是否允许 null 等。一些 columns 组成了 primary key。可以看到，Kudu 的数据模型非常类似关系数据库，在使用之前，用户必须首先建立一个 table，访问不存在的 table 或者 column 都会报错。用户可以使用 DDL 语句添加或者删除 column，但不能删除包含 primary key 的 column。但在 Paper 里面说到 Kudu 不支持二级索引以及除了 primary key 之外的唯一索引，这个后续可以通过更新的代码来确定下。其实我这里非常关注的是 Kudu 的 Online DDL 是如何做的，只是 Paper 里面貌似没有提及，后面只能看代码了。API Kudu 提供了 Insert，Update 和 Delete 的 write API。不支持多行事务 API，这个不知道最新的能支持了没有，因为仅仅能对单行数据操作，还远远不够。Kudu 提供了 Scan read API 让用户去读取数据。用户可以指定一些特定的条件来过滤结果，譬如用一个常量跟一个 column 里面的值比较，或者一段 primary key 的范围等条件。提供 API 的好处在于实现简单，但对于用户来说，其实更好的使用方式仍然是 SQL，一些复杂的查询最好能通过 SQL 搞定，而不是让用户自己去 scan 数据，然后自己组装。一致性模型 Kudu 提供两种一致性模型：snapshot consistency 和 external consistency。默认 Kudu 提供 Snapshot consistency， 它具有更好的读性能，但可能会有 write skew 问题。而 External consistency 则能够完全保证整个系统的 linearizability，也就是当写入一条数据之后，后面的任何读取都一定能读到最新的数据。为了实现 External consistency，Kudu 提供了几种方法： 在 clients 之间显示的传递时间戳。当写入一条数据之后，用户用要求 client 去拿一个时间戳作为 token，然后通过一个 external channel 的方式传递给另一个 client。然后另一个 client 就可以通过这个 token 去读取数据，这样就一定能保证读取到最新的数据了。不过这个方法实在是有点复杂。 提供类似 Spanner 的 commit-wait 机制。当写入一条数据之后，client 需要等待一段时间来确定写入成功。Kudu 并没有采用 Spanner TrueTime 的方案，而是使用了 HybridTime 的方案。HybridTime 依赖 NTP，这个可能导致 wait 的时间很长，但 Kudu 认为未来随着 read-time clock 的完善，这应该不是问题了。  Kudu 是我已知的第二个采用 HybridTime 来解决 External consistency 的产品，第一个当然就是 CockroachDB 了。TiDB 跟他们不一样，我们采用的是全局授时的方案，这个会简单很多，但其实也有跟 PD 交互的网络开销。后续TiDB 可能使用类似 Spanner 的 GPS + 原子钟，现阶段相关硬件的制造方式 Google 并没有说明，但其实难度不大。因为已经有很多硬件厂商主动找我们希望一起合作提供，只是比较贵，而现阶段我们大多数客户并没有跨全球事务这种场景。Kudu 的一致性模型依赖时间戳，这应该是现在所有分布式系统通用的做法。Kudu 并没有给用户保留时间戳的概念，主要是觉得用户很可能会困惑，毕竟不是所有的用户都能很好的理解 MVCC 这些概念。当然，对于 read API，还是允许用户指定特定的一个时间戳，这样就能读取到历史数据。这个 TiDB 也是类似的做法，用户不知道时间戳，只是我们额外提供了一个设置 snapshot 的操作，让用户指定生成某个时间点的快照，读取那个时间点的数据。这个功能已经帮很多公司恢复了因为错误操作写坏的数据了。架构 上面说了一些 Kudu 的 keyword， 现在来说说 Kudu 的整体架构。Kudu 类似 GFS，提供了一个单独的 Master 服务，用来管理整个集群的元信息，同时有多个 Tablet 服务，用来存储实际的数据。分区 Kudu 支持对数据按照 Range 以及 Hash 的方式进行分区。 每个大的 table 都可以通过这种方式将数据分不到不同的 Tablet 上面。当用户创建一个表的时候，同时也可以指定特定的 partition schema，partition schema 会将 primary key 映射成对应的 partition key。每个 Tablet 上面会覆盖一段或者多段 partition keys 的range。当 client 需要操作数据的时候，它可以很方便的就知道这个数据在哪一个 Tablet 上面。一个 partition schema 可以包括 0 或者多个 hash-partitioning 规则和最多一个 range-partitioning 规则。用户可以根据自己实际的场景来设置不同的 partition 规则。譬如有一行数据是 (host, metric, time, value)，time 是单调递增的，如果我们将 time 按照 hash 的方式分区，虽然能保证数据分散到不同的 Tablets 上面，但如果我们想查询某一段时间区间的数据，就得需要全部扫描所有的 Tablets 了。所以通常对于 time，我们都是采用 range 的分区方式。但 range 的方式会有 hot range 的问题，也就是同一个时间会有大量的数据写到一个 range 上面，而这个 hot range 是没法通过 scale out 来缓解的，所以我们可以将 (host, metric) 按照 hash 分区，这样就在 write 和 read 之间提供了一个平衡。通过多个 partition 规则组合，能很好的应对一些场景，但同时这个这对用户的要求比较高，他们必须更加了解 Kudu，了解自己的整个系统数据会如何的写入以及查询。现在 TiDB 还只是单纯的支持 range 的分区方式，但未来不排除也引入 hash。Raft Kudu 使用 Raft 算法来保证分布式环境下面数据一致性，这里就不再详细的说明 Raft 算法了，因为有太多的资料了。Kudu 的 heartbeat 是 500 毫秒，election timeout 是 1500 毫秒，这个时间其实很频繁，如果 Raft group 到了一定量级，网络开销会比较大。另外，Kudu 稍微做了一些 Raft 的改动： 使用了 exponential back-off 算法来处理 leader re-election 问题。 当一个新的 leader 跟 follower 进行交互的时候，Raft 会尝试先找到这两个节点的 log 分叉点，然后 leader 再从这个点去发送 log。Kudu 直接是通过 committedIndex 这个点来发送。  对于 membership change，Kudu 采用的是 one-by-one 算法，也就是每次只对一个节点进行变更。这个算法的好处是不像 joint consensus 那样复杂，容易实现，但其实还是会有一些在极端情况下面的 corner case 问题。当添加一个新的节点之后，Kudu 首先要走一个 remote bootstrap 流程。 将新的节点加入到 Raft 的 configuration 里面 Leader 发送 StartEmoteBootstrap RPC，新的 follower 开始拉去 snapshot 和之后的 log Follower 接受完所有数据并 apply 成功之后，开始响应 Raft RPC  可以看到，这个流程跟 TiKV 的做法类似，这个其实有一个缺陷的。假设我们有三个节点，加入第四个之后，如果新的节点还没 apply 完 snapshot，这时候挂掉了一个节点，那么整个集群其实是没法工作的。为了解决这个问题，Kudu 引入了 PRR_VOTER 概念。当新的节点加入的时候，它是 PRE_VOTE 状态，这个节点不会参与到 Raft Vote 里面，只有当这个节点接受成功 snapshot 之后，才会变成 VOTER。当删除一个节点的时候，Leader 直接提交一个新的 configuration，删除这个节点，当这个 log 被 committed 之后，这个节点就把删除了。被删除的节点有可能不知道自己已经被删除了，如果它长时间没有收到其他的节点发过来的消息，就会问下 Master 自己还在不在，如果不在了，就自己干掉自己。这个做法跟 TiKV 也是类似的。Master Kudu 的 Master 是整个集群最核心的东西，类似于 TiKV 里面的 PD。在分布式系统里面，一些系统采用了无中心化的架构设计方案，但我个人觉得，有一个中心化的单点，能更好的用全局视角来控制和调度整个系统，而且实现起来很简单。在 Kudu 里面，Master 自己也是一个单一的 Tablet table，只是对用户不可见。它保存了整个集群的元信息，并且为了性能，会将其全部缓存到内存上面。因为对于集群来说，元信息的量其实并不大，所以在很长一段时间，Master 都不会有 scale 的风险。同时 Master 也是采用 Raft 机制复制，来保证单点问题。这个设计其实跟 PD 是一样的，PD 也将所有的元信息放到内存。同时，PD 内部集成 etcd，来保证整个系统的可用性。跟 Kudu Master 不一样的地方在于，PD 是一个独立的组件，而 Kudu 的 Master 其实还是集成在 Kudu 集群里面的。Kudu 的 Master 主要负责以下几个事情：Catalog manager Master 的 catalog table 会管理所有 table 的一些元信息，譬如当前 table schema 的版本，table 的 state（creating，running，deleting 等），以及这个 table 在哪些 Tables 上面。当用户要创建一个 table 的时候，首先 Master 在 catalog table 上面写入需要创建 table 的记录，table 的 state 为 CREATING。然后异步的去选择 Tablet servers 去创建相关的元信息。如果中间 Master 挂掉了，table 记录里面的 CREATING state 会表明这个 table 还在创建中，新的 Master leader 会继续这个流程。Cluster coordinator 当 Tablet server 启动之后，会给 Master 注册，并且持续的给 Master 进行心跳汇报消后续的状态变化。虽然 Master 是整个系统的中心，但它其实是一个观察者，它的很多信息都需要依赖 Tablet server 的上报，因为只有 Tablet server 自己知道当前自己有哪一些 tablet 在进行 Raft 复制，Raft 的操作是否执行成功，当前 tablet 的版本等。因为 Tablet 的状态变更依赖 Raft，每一次变更其实就在 Raft log 上面有一个对应的 index，所以上报给 Master 的消息一定是幂等的，因为 Master 自己会比较 tablet 上报的 log index 跟当前自己保存的 index，如果上报的 log index 是旧的，那么会直接丢弃。这个设计的好处在于极大的简化了整个系统的设计，如果要 Master 自己去负责管理整个集群的状态变更，譬如 Master 给一个 tablet 发送增加副本的命令，然后等待这个操作完成，在继续处理后面的流程。整个系统光异常处理，都会变得特别复杂，譬如我们需要关注网络是不是断开了，超时了到底是成功了还是失败了，要不要再去 tablet 上面查一下？相反，如果 Master 只是给 tablet 发送一个添加副本的命令，然后不管了，剩下的事情就是一段时间后让 tablet 自己上报回来，如果成功了继续后面的处理，不成功则尝试在加一次。虽然依赖 tablet 的上报会有延迟（通常情况，只要有变动，tablet 会及时的上报通知，所以这个延迟其实挺小的），整个架构简单了很多。其实看到这里的时候，我觉得非常的熟悉，因为我们也是采用的这一套架构方案。最开始设计 PD 的时候，我们还设想的是 PD 主动去控制 TiKV，也就是我上面说的那套复杂的发命令流程。但后来发现实在是太复杂了，于是改成 TiKV 主动上报，这样 PD 其实就是一个无状态的服务了，无状态的服务好处就是如果挂了，新启动的 PD 能立刻恢复（当然，实际还是要做一些很多优化工作的）。Tablet directory 因为 Master 知道集群所有的信息，所以当 client 需要读写数据的时候，它一定要先跟 Master 问一下对应的数据在哪一个 Tablet server 的 tablet 上面，然后才能发送对应的命令。如果每次操作都从 Master 获取信息，那么 Master 铁定会成为一个性能瓶颈，鉴于 tablet 的变更不是特别的频繁，所以很多时候，client 会缓存访问的 tablet 信息，这样下次再访问的时候就不用从 Master 再次获取。因为 tablet 也可能会变化，譬如 leader 跑到了另一个 server 上面，或者 tablet 已经不在当前 server 上面，client 会收到相关的错误，这时候，client 就重新再去 Master 获取一下最新的路由信息。这个跟我们的做法仍然是一样的，client 缓存最近的路由信息，当路由失效的时候，重新去 PD 获取一下。当然，如果只是单纯的 leader 变更，其实返回的错误里面通常就会带上新的 leader 信息，这时候 client 直接刷新缓存，在直接访问了。Tablet storage Tablet server 是 Kudu 用来存放实际数据的服务，为了更好的性能，Kudu 自己实现了一套 tablet storage，而没有用现有的开源解决方案。Tablet storage 目标主要包括： 快速的按照 Column 扫描数据 低延迟的随机更新 一致的性能  RowSets Tablets 在 Kudu 里面被切分成更小的单元，叫做 RowSets。一些 RowSets 只存在于内存，叫做 MemRowSets，而另一些则是使用 disk 和 memory 共享存放，叫做 DiskRowSets。任何一行数据只存在一个 RowSets 里面。在任何时候，一个 tablet 仅有一个单独的 MemRowSet 用来保存最近插入的数据。后台有一个线程会定期的将 这些 MemRowSets 刷到 disk 上面。当一个 MemRowSet 被刷到 disk 之后，一个新的空的 MemRowSet 被创建出来。之前的 MemRowSet 在刷到 disk 之后，就变成了 DiskRowSet。当刷的同时，如果有新的写入，仍然会写到这个正在刷的 MemRowSet 上面，Kudu 有一套机制能够保证新写入的数据也能一起被刷到 disk 上面。MemRowSet MemRowSet 是一个支持并发，提供锁优化的 B-tree，主要基于 MassTree，也有一些不同： 因为 Kudu 使用的是 MVCC，所以任何的删除其实也是插入，所以这个 tree 没有删除操作。 不支持任意的 in-place 数据变更操作，除非这次操作不会改变 value 的大小。 将 Leaf link 起来，类似 B+-tree，这样对于 scan 会有明显的性能提升。 并没有完全实现 trie of trees，是只是使用了一个单一 tree，因为 Kudu 并没有太多高频随机访问的场景。  DiskRowSet 当 MemRowSets 被刷到 disk 之后，就变成了 DiskRowSets。 …"},
		{"url": "https://pingcap.com/weekly/2017-05-08-tidb-weekly/",
		"title": "Weekly update (May 01 ~ May 07, 2017)", 
		"content": " Weekly update in TiDB Last week, we landed 33 PRs in the TiDB repositories.Added  Add builtin function is_ipv4_mapped, makedate, utc_time Enable privilege checking by default. You could also disable privilege checking through --privilege=false. Support Analyze Index statement: after adding an index, we could run this statement to analyze the newly added index. Add Trigger_priv column in mysql.user system table. Add coveralls in CI.  Fixed  Fix incompatible issue in found_rows(). Fix a few panic when infering type for some functions without argument. Fix a data race in Join operator. Fix parse time_zone -6:00. Fix a bug in checking duplicate column. Check ignored errors. Fix comment warning in executor package. Fix a bug about converting string to bit. Recognize uuid() as a dynamic function. Fix a bug in rand() with seed.  Improved  Enlarge the stack size to save the runtime.morestack cost. Parallelly prewrite primary and secondary lock. New planner framework: SortMergeJoin, IndexReader Refactor expression evaluation framework: Rewrite compare operator. Implement the new Eval interface for ColumnExpr and ConstExpr. Decide the return type of ColumnExpr and ConstExpr during planning.  Weekly update in TiKV Last week, We landed 13 PRs in the TiKV repositories.Added  Apply the Raft logs of multiply regions in batches. Add hotspot command in pd-ctl: detect and show hotspot regions. Add sub-compaction, base-background-compactions and writable-file-max-buffer-size for RocksDB. Let manual compaction run concurrently with background compaction.  Fixed  Create column family orderly. Remove CRC32 checksum when initializing snapshot in RaftStore thread. Get the correct position in the pending vote queue. Remove old operator limiter when adding new PriorityKind operator.  Improved  Use channel to reduce lock contention when pushing task to thread pool. Make scheduler more smooth when starting PD.  "},
		{"url": "https://pingcap.com/meetup/meetup-2017-05-06/",
		"title": "【Infra Meetup No.47】分布式定时任务中间件架构 Elastic-Job 的两种实现", 
		"content": " 今天的 Meetup ，我们请到了当当架构部负责人张亮，大家分享了《分布式定时任务中间件架构 Elastic-Job 的两种实现》。 本期讲师：张亮当当架构部负责人，主要负责分布式中间件以及私有云平台的搭建。致力于开源，目前主导两个开源项目 elastic-job 和 sharding-jdbc。擅长以 java 为主分布式架构以及以 Mesos 为主的云平台方向，推崇优雅代码，对如何写出具有展现力的代码有较多研究。今日帝都依然大风，但小伙伴们学习的热情丝毫未减哦~在本次分享中，张亮老师从分布式定时任务中间件的适用场景，轻量级去中心化架构方案以及基于 Mesos 的中心化架构方案，三个方面为大家进行了详细讲解。在互联网应用中，各式各样的定时任务存于系统的各个角落，我们希望由一个平台统一将这些作业管理起来。然而，一旦平台中运行大量的作业，发现异常作业并手动处理难免会感到繁琐，同时人工处理还会带来很多其他的额外成本。如何最大限度的减少人工干预？高可用可以让作业在被系统发现宕机之后能自动切换。而弹性化可以认为是高可用的进阶版本，在高可用的同时还能够提升效率和充分利用资源。对于动态的扩容和缩容，通常采用分片的方式实现。去中心化架构是指所有的作业节点都是对等的，优点是轻量级，部署成本低；缺点则是，如果各作业服务器时钟不一致会产生同一作业的不同分片运行有先有后，缺乏统一调度，并且不能跨语言。中心化架构将系统分为调度节点和执行节点，可以解决服务器时间差以及跨语言的问题；缺点是部署和运维稍复杂。Elastic-Job 最初的版本分离于当当内部的应用框架 ddframe，是一个纯 Java 实现的分布式方案，参照 dubbo 的方式，提供无中心化解决方案。如今，Elastic-Job 已开源近 2 年，截止目前已更新发布18 次，GitHub Star 数近 2000，成绩出色。更有多个开源产品衍生自 Elastic-Job。应小伙伴们的强烈要求，张亮老师临时加场 Demo 演示。 最后，还有超多第一手爆料，是属于现场听讲小伙伴们的专属福利 ✌️ 很心动？下周六，老时间，老地点，PingCAP 第 48 期 Infra Meetup 等你呦！"},
		{"url": "https://pingcap.com/weekly/2017-05-02-tidb-weekly/",
		"title": "Weekly update (April 24 ~ April 30, 2017)", 
		"content": " Weekly update in TiDB Last week, we landed 49 PRs in the TiDB repositories.Added  Add builtin function truncate, makedate, utc_time Add pre-commit hook. Add a plan for Analyze. Add a check list about the things that contributors should think about before sending a PR. Support ENGINE option with partition option in table definition. Add code review guide.  Fixed  Fix bug in converting string to hex/bit. Fix a few panic when infering type for some functions without argument. Fix a data race in Join operator.  Improved  Add countReliable attribute in physicalPlanInfo: In CBO phrase, when row count is estimated by real statistics or limit clause, the cost is reliable. Refactor TypeInferer: Add a few interfaces for evaluate expression, Refine EvalBool, Refactor planner: Union, Hash Aggregate, Union Scan, Join, Apply, Refine code Refactor executor: Table Scan Improve contribute guide. Make goword happy in the following package: ddl, util, tablecodec, expression, bootstrap, model, table, ast, perfschema Add a few empty memory tables in information_schema: Make DTS happy. When where expresion can be converted to false, use dual table instead of a read table.  New Contributors Thank you guys! lkk2003rty Ce Gao Zejun Li  Weekly update in TiKV Last week, We landed 16 PRs in the TiKV repositories.Added  Let the right split region derive the parent region information. Add a hot region scheduler. Introduce structure log and support JSON format. Use --data-dir instead of --store for TiKV storage path.  Fixed  Support idempotent bootstrap for TiKV. Detect snapshot in all nodes. Use non-system port to avoid test failed.  Improved  Throttle big query to reduce the influence on the point query. Add test for applying entries. Use a better mechanism when re-connecting PD. Ignore unnecessary message size computing.  "},
		{"url": "https://pingcap.com/meetup/meetup-2017-04-22/",
		"title": "【Infra Meetup No.46】MySQL 5.7 的特性及实践", 
		"content": "  今天的 Meetup，我们邀请到了熊猫直播 DBA 杨尚刚老师，为大家分享《MySQL 5.7 的特性及实践》~ 本期讲师：杨尚刚熊猫直播高级 DBA，负责后端数据库平台建设和架构设计。前新浪高级数据库工程师，负责新浪微博核心数据库架构改造优化，以及数据库相关的服务器存储选型设计。2015 年最重磅的当属 MySQL 5.7 GA 的发布，号称 160 万只读 QPS，大有赶超 NoSQ L趋势。不过官方的硬件测试环境是很高的，所以这个 160 万 QPS 对于大家测试来说，可能还比较遥远，所以实际测试的结果可能会失望。但是，至少我们看到了基于同样测试环境，MySQL 5.7 在性能上的改进，对于多核利用的改善。本次分享中，杨老师讲解了 MySQL 5.7 在运维、优化器 Server 层、InnoDB 层等方面的优化，以及 MySQL 未来的发展趋势。运维方面  动态修改 Buffer Pool MySQL redo log大小 innodb_file_per_table query cache SQL_Mode binlog_rows_query_log_events max_execution_time replication info in tables innodb_numa_interleave 动态修改 replication filter  优化器 Server 层改进 优化器主要还是基于 cost model 层面和给用户更多自主优化。可配置cost based optimizer、mysql.server_cost 和mysql.engine_cost。 New JSON 数据类型和函数支持。当然 JSON 也可以存在 Text 或 VARCHAR 里用内置 json，更容易访问，方便修改。支持生成列（虚拟列），以及虚拟列上索引。5.7 还对 explain 做了增强，对于当前正在运行查询 explain。InnoDB 层优化 InnoDB 层核心还是拆分各种锁，提高并发。只读事务优化就是其中一个例子。atomic write，disable double write支持 spatial index 空间索引。Transparent page compressionperformance_schema 改进新增加的 sys 数据库。Replication 改进。最大的亮点 GTID 增强，支持在线调整 GTID。使用 mysqlbinlog 作为伪 slave 是个不错方案。并行复制优化，Database 5.6 默认并行复制，logical-clock 5.7 引入。5.7 引入的 group replication 也是为了提高可用性。多主复制，多点写入，内部检测冲突，保证一致性，自动探测。支持 GTID，共享 UUID，只支持 InnoDB，不支持并发 DDL。讲师总结从整体来说，MySQL 5.7 做的改进还是非常有吸引力的，不论是从运维角度还是性能优化上，当然真正在生产环境上遇到问题时在所难免的，要做好踩坑的准备。"},
		{"url": "https://pingcap.com/blog-cn/talk-cloud-native/",
		"title": "演讲实录|黄东旭：Cloud-Native 的分布式数据库架构与实践", 
		"content": " 4 月 19 日，我司 CTO 黄东旭同学在全球云计算开源大会上，发表了《Cloud-Native 的分布式数据库架构与实践》主题演讲，以下为演讲实录。实录 大家好，今天我的题目是 Cloud-Native 与分布式数据库的实践。先简单的介绍一下自己，我是 PingCAP 的联合创始人和 CTO，过去一直在做基础软件领域的工程师，基本上做的所有的东西都是开源。在分享之前我想说一下为什么现在各行各业或者整个技术软件社区一直在重复的再造数据库，现在数据库到底怎么了，为什么这么百花齐放？ 首先随着业务的多种多样，还有不管是传统行业还是互联网行业，业务的迭代速度越来越互联网化，使得整个数据量其实是一直在往上走的； 第二就是随着 IOT 的设备还有包括像手机、移动互联网蓬勃的发展，终端其实也不再仅仅是传统的 PC 客户端的数据的接入； 第三方面随着现在 AI 或者大数据分析一些模型或者理论上的突破，使得在大数据上进行计算的手段越来越多样，还有在物理上一些硬件的新的带有保护的内存，各种各样新的物理的设备，越来越多的硬件或者物理上的存储成本持续的降低，使得我们的数据库需要要面对更多的挑战。  关联数据库理论是上世纪七十年代做出来的东西，现在四十年过去不管是物理的环境还是计算模型都是完全不一样的阶段，还抱着过去这种观念可能并不是一个面向未来的设计。而且今天我的题目是 Cloud-Native，有一个比较大胆的假设，大家在过去三十年的计算平台基本都是在一台 PC 或者一个服务器或者一个手机这样的独立的计算平台，但是未来我觉得一切的服务都应该是分布式的。因为我觉得摩尔定律已经失效了，所以未来的操作系统会是一个大规模分布式的操作系统，在上面跑的任何的进程，任何的服务都应该是分布式的，在这个假设下怎么去做设计，云其实是这个假设最好的载体。怎么在这个假设上去设计面向云的技术软件，其实是最近我一直在思考的一个问题。其实在这个时代包括面向云的软件，对业务开发来说尽量还是不要太多的改变过去的开发习惯。你看最近大数据的发展趋势，从最传统的关系数据库到过去十年相比，整个改变了用户的编程模型，但是改变到底是好的还是不好的，我个人觉得其实并不是太好。最近这两年大家会看到整个学术圈各种各样的论文都在回归，包括 DB 新时代的软件都会把扩展性和分布式放在第一个要素。大家可能听到主题会有点蒙，叫 Cloud-Native，Cloud-Native 是什么？其实很早的过去也不是没有人做过这种分布式系统的尝试，最早是 IBM 提出面向服务的软件架构设计，最近热门的 SOA、Micro Service 把自己的服务拆分成小的服务，到现在谷歌一直对外输出一个观点就是 Cloud-Native，就是未来大家的业务看上去的分布式会变成一个更加透明的概念，就是你怎么让分布式的复杂性消失在云的基础设施后，这是 Cloud-Native 更加关心的事情。这个图是 CNCF 的一个基金会，也是谷歌支持的基金会上扒过来的图。这里面有一个简单的定义，就是 SCALE 作为一等公民，面向 Cloud-Native 的业务必须是弹性伸缩的，不仅能伸也得能缩；第二就是在对于这种 Cloud-Native 业务来说是面向 Micro service 友好；第三就是部署更加的去人工化。最近大家可能也看到很多各种各样容器化的方案，背后代表的意义是什么？就是整个运维和部署脱离人工，大家可以想象过去十几二十年来，一直以来运维的手段是什么样的。我找了一个运维，去买服务器，买服务器装系统，在上面部署业务。但是现在 Cloud-Native 出现变得非常的自动化，就相当于把人的功能变得更低，这是很有意义的，因为理想中的世界或者未来的世界应该怎么样，一个业务可能会有成百上千的物理节点，如果是人工的去做运维和部署是根本不可能做得到的，所以其实构建整个 Cloud-Native 的基础设施的两个条件：第一个就是存储本身的云化；第二就是运维要和部署的方式必须是云化的。我就从这两个点说一下我们 TiDB 在上面的一些工作和一些我的思考。存储本身的云化有几个基本条件，大家过去认为是高可用，主要停留在双活。其实仔细去思考的话，主备的方案是很难保证数据在完全不需要人工的介入情况下数据的一致性可用性的，所以大家会发现最近这几年出来的分布式存储系统的可用性的协议跟复制协议基本都会用类似 Raft/Paxos 基于选取的一致性算法，不会像过去做这种老的复制的方案。第二就是整个分片的策略，作为分布式系统数据一定是会分片的，数据分片是来做分布式存储唯一的思路，自动分片一定会取代传统的人工分片来去支撑业务。比如传统分片，当你的数据量越来越大，你只能做分库分表或者用中间件，不管你分库分表还是中间件都必须制订自己人工的分辨规则，但是其实在一个真正面向 Cloud 的数据库设计里，任何一种人的介入的东西都是不对的。第三，接入层的去状态化，因为你需要面对 Cloud-Native 做设计，你的接入层就不能带有状态，你可以相当于是前端的接入层是一层无状态的或者连接到任何一个服务的接入的入口，对你来说应该都是一样的，就是说你不能整个系统因为一个关键组件的损坏或者说磁盘坏掉或者机器的宕机，整个系统就不能服务了，整个测试系统应该具有自我愈合和自我维护的能力。 其实我本身是架构师，所以整个我们数据库的设计都是围绕这些点来做思考的，就是一个数据库怎么能让他自我的生长，自我的维护，哪怕出现任何的灾难性的物理故障（有物理故障是非常正常的，每天在一个比较大的集群的规模下，网络中断、磁盘损坏甚至是很多很诡异的问题都是很正常的），所以怎么设计这种数据库。我们的项目是 TiDB，我不知道在座的有没有听说过这个项目，它其实是一个开源实现。当你的业务已经用了数据库，数据量一大就有可能遇到一些问题，一个是单点的可靠性的问题，还有一个数据容量怎么去弹性伸缩的问题。TiDB 就能解决这个问题，它本身对外暴露了一个接口，你可以用客户端去连接，你可以认为你下面面对的是一个弹性动态伸缩完全没有容量限制，同时还可以在上面支持复杂的子查询的东西。底层存储的话，相当于这一层无状态的会把这个请求发到底层的节点上，这个存储里面的数据都是通过协议做高可用和复制的，这个数据在底层会不停的分裂和移动，你可以认为这个数据库是一个活的数据库，你任何一个节点的宕机都不会影响整个数据库对业务层的使用，包括你可以跨数据中心部署，甚至你在跨数据中心的时候，整个数据中心网络被切断，机房着火，业务层都几乎完全是透明的，而且这个数据比如副本丢失会自己去修复，自己去管理或者移动数据。上图右边是一个集群的调度模块，你可以认为它是整个集群的大脑，是一个 7×24 小时不眠的 DBA，你任何的业务可以接上来。NewSQL 这个词大家听的都烂了，但是我还是想提，首先它是一个面向扩展的数据库，同时它还结合的传统管理数据库的强一致事务，必须得是 SSI 隔离级别的，并不是非常弱根本没法用的隔离级别，而是需要提供最强一致性的隔离级别的数据库。扩展性其实是通过在 TiKV 层面做完全自动分片，可用性是通过 Raft 为保证，我们整个数据库没有主从的概念，每一个节点既可以是主又可以是从，然后整个数据复制通过 Raft 为保证，对外的是一个强一致性的事务层，其实背后的算法是用两阶段提交的算法，比如一个最简单的例子，我一百个并发过来很快，每个平均十毫秒访问数据，一百个并发，一百万个并发，你还能保证每一个请求都是十毫秒返回数据吗？那我可以简单的通过添加机器把系统的吞吐加上去，每一个吞吐的响应延迟会更大一些，在论文里提到过，谷歌数据库写的每一个平均延迟是 150 到 100 毫秒，可以做到线性扩展。所以终于有一个数据库可以 scable。刚才说的是存储层面的云化，刚才提到了 Cloud-Native 还有一个重要的点就是部署和运维方式的云化，我们其实选的这个 Kubernetes 就是用来承载背后数据库大规模集群上的部署，其实大家都知道这个是什么东西，这是最出名的开源项目之一，谷歌开源的，大家也知道 borg，就是他们用了这么多年的集群调度服务，主要做的事情就是服务的编排、部署、自动化的运维，一台物理机挂掉了，他会很好的让这个业务不中断，像集群调度器，它就是整个数据中心的操作系统，但是面临最大的问题就是所有的业务一旦是有状态的，那你这个就非常恶心。举个案例，比如我在跑一个数据库，如果你这台物理机宕机，按照 Kubernetes 会开一个服务器在那边展开，但是这一台老的宕机服务器数据是存在上面的，你不能只是把进程在新的服务器那启起来而数据不带过去，相当于这种业务是带状态的，这在 Kubernetes 过去是一个老大难的问题，其实整个应用层，因为它的特性被分成了四个大的阵营，如果想上可以看一下自己的业务到底属于哪一个阵营。第一个就是完全无状态的业务，比如这是一个简单的 CRUD 业务层的逻辑，其实是无状态的应用层。第二种就是单点的带状态的 applications，还有任何单机的数据库其实都有属于 applications。第三个就是 Static distributed，比如高可用的分布式服务，大家也不会做扩容什么的，这种是静态的分布式应用。还有最难的一个问题，就是这种集群型的 applications，clustered 是没有办法很好的调度服务的。这是刚才说的，其实对于 Single point，其实是很好解决的，对于静态的其实也是用 Static distributed 来解决带状态的持续化的方案。刚才我说最难的那个问题，我们整个架构中引入了 Operational，调度一套有状态服务的方案，其实它思路也很简单，就是把运维分布人员系统的领域知识给嵌入到了系统中，Knowledge 发布调度任务之前都会相当于被 CoreOS 提供的接口以后再调度业务，这套方案依赖了 HtirdPartyResources 调度，因为这个里面我的状态我自己才知道，这个时候你需要通过把你的系统领域知识放到 operator 里。很简单，就是 Create 用 TiDB Operator 来实现，还有 Rollingupdate 和 Scale Out 还有 Failover 等。使用 Kubemetes 很重要的原因就是它只用到了很毛的一层 API，内部大量的逻辑是在 Operator 内部，而且像 PV/Statefulset 中并不是一个很好的方案，比如 PV 实现你可以用分布式文件系统或者快存储作为你持久化的这一层，但是数据库的业务，我对我底层的 IO 或者硬件有很强的掌控力的，我在我的业务层自己做了三副本和高可用，我其实就不需要在存储层面上比如我在一个裸盘上跑的性能能比其他上快十倍，这个时候你告诉我不好意思下面只支持 statefulset 那这是不适合跑数据库的，也就是数据库集群是跟它的普通的云盘或者云存储的业务是分开的。分布式数据库也不是百分之百所有的业务场景都适合，特别是大规模的分布式数据库来说，比如自增 ID，虽然我们支持，但是自增 IT 高压力写入的时候它的压力会集中在表的末尾，其实是我不管怎么调度总会有一块热点，当然你可以先分表，最后我们聚合分析也没有问题，像秒杀或者特别小的表，其实是完全不适合在这种分布式数据库上做的，比较好的一些实践业务的读写比较平均，数据量比较大，同时还有并发的事务的支持，需要有事务的支持，但并不是冲突特别大的场景，并不是秒杀的场景，同时你又可以需要在上面做比较复杂的分析，比如现在我们的一些线上的用户特别好玩，过去它的业务全都是跑在 MySQL 上，一主多从，他发现我一个个 MySQL 成为了信息的孤岛，他并没有办法做聚合地分析，过去传统的方案就是我把 MySQL 或者数据通过一个 ETL 流程导到数据仓库里，但是开发者不会用各种琳琅满目大数据的东西，他只会用 MySQL，一般他做的数据分析都会把 MySQL 倒腾到一个库上再做分析，数据量一大堆分析不出来，这个时候他把所有他自己的 MySQL 总库连到了一个 TiDB上，过去是一主多从，先多主多从，把所有的从都打通，做 MySQL 的分析，所以我觉得 TiDB 打造了新的模型，可以读写高并发的事务，所以未来的数据库会是一个什么样子的，我觉得可能是至少我们觉得沿着这条路走下去应该会有一条新的路，谢谢大家。"},
		{"url": "https://pingcap.com/weekly/2017-04-17-tidb-weekly/",
		"title": "Weekly update (April 10 ~ April 16, 2017)", 
		"content": " Weekly update in TiDB Last week, we landed 26 PRs in the TiDB repositories.Added  Update etcd client in vendor. Add builtin function time-to-sec Add definition for builtin functions: bit_count, to_base64, right  Fixed  Fix the error message for data overflow. Fix a panic. Fix a bug in top-n pushdown. Fix a data race bug in session context. Fix a bug in name resolver for GroupBy clause.  Improved  Refactor optimizer: Implement new planner interface for projection and sort. Refactor statistic module.#3014, #3019, #3029, #3031, #3044, #3048, #3053 Refactor coprocessor architecture: #3009, #3027 Use etcd for privilege update notification among tidb-servers. Enlarge the limitation of write flow to TiKV. Reduce memory usage in HashAggregate operator. Reduce memory usage in Distinct operator. Code cleanup in type infer.  New Contributors Thank you guys! Xuanwo fudali113  Weekly update in TiKV Last week, We landed 18 PRs in the TiKV repositories.Added  Support memory profiling. Add block cache monitoring. Add log for critical operations. Use tokio-core to support asynchronous PD client, see 1745, 1752, 1753.  Fixed  Always update PD client to avoid the problem that PD client can not reconnect to PD server after network error. Fix the bug when decode empty data to a StringSlice. Fix a deadlock bug when PD client reconnects server.  Improved  Add lots of test cases to improve stability, see 1737 , 1738, 1742.  "},
		{"url": "https://pingcap.com/meetup/meetup-2017-04-16/",
		"title": "今天这里有一场国内最大的 Rust 爱好者聚会～✌️", 
		"content": " 今天这里有一场国内最大的 Rust 爱好者聚会～✌️ 原创2017-04-16 PingCAP PingCAP PingCAPPingCAP ![]() 微信号pingcap2015功能介绍PingCAP 专注于新型"},
		{"url": "https://pingcap.com/weekly/2017-04-10-tidb-weekly/",
		"title": "Weekly update (March 27 ~ April 09, 2017)", 
		"content": " Weekly update in TiDB Last two weeks, we landed 74 PRs in the TiDB repositories.Added  Support Index Nest Lookup Join operator.#2834, #2945 Add many builtin functions: quote, is_ipv4, compress, inet_aton, format, bin, random-bytes, sin, inet_ntoa, cos, from_base64, tan, cot, to_days, timestampadd Check privileges for show databases/tables statement. Calculate distinct information in statistic module.#2947, #2966 Add a switcher to split large inset transaction into multiple small transactions automatically. Add memory table INFORMATION_SCHEMA.USER_PRIVILEGES. Add memory table INFORMATION_SCHEMA.ENGINES. Support Super_priv. Support Top-N operator push down in optimizer.  Fixed  Fix case-when expression and coalesce expression type inference. Fix a goroutine leak in tikv client. Fix a bug in the date_format builtin function. Fix goroutine leak in tikv client. Reset affected rows counter when retrying SQL statement. Recognize number literal as decimal when meet int64 overflow error.  Improved  Refactor optimizer: Introduce TaskProfile to represent a group of physical plans. Add base phyiscal plans. Refactor statistic module.#2913, #2952, #2956, #2993 Refactor coprocessor architecture: table scan and index scan operator, aggregation operator, limit operator, top-n operator Add sorted information for SortMergeJoin operator. Refactor distsql package. Reduce memory usage in HashJoin operator.  New Contributors Thank you guys! Blame cosmos Zhao Yi Jun Bin Liu Michael Belenchenko Van jinhelin  Weekly update in TiKV Last two weeks, We landed 46 PRs in the TiKV repositories.Added  Add rate limiter for RocksDB compaction. Add gRPC support for PD, see 493, 590, 1591, 1712, 1725. Add RocksDB SST format for snapshot. Show replication configuration in pd-ctl. Make RocksDB info log path configurable. Slow down balance interval increasing speed. Report disk space usage to PD. Report region write-flow rate to PD. Show more information for region in HTTP API. Output statistics regularly for pd-tso-bench. Avoid selecting on the same context in different Goroutines.  Fixed  Avoid removing the leader peer directly. Avoid open RocksDB many times. Parse ALREADY_BOOTSTRAPPED error correctly, see 593, 1720. Use pool￼ to store TSO request to reduce memory allocation and GC pressure.  Improved  Abort Prewrite command if the transaction is rolled back before. Make PD scheduler more reliable. Make Raft ReadIndex more stable. Add many unit tests to improve the system stability.  "},
		{"url": "https://pingcap.com/meetup/meetup-2017-04-08/",
		"title": "COISF 专场|PingCAP 第 44 期 Meetup", 
		"content": " COISF 专场|PingCAP 第 44 期 Meetup 2017-04-08 徐磊 PingCAP PingCAPPingCAP ![]() 微信号pingcap2015功能介绍PingCAP 专注于新型"},
		{"url": "https://pingcap.com/weekly/2017-03-27-tidb-weekly/",
		"title": "Weekly update (March 20 ~ March 26, 2017)", 
		"content": " Weekly update in TiDB Last week, we landed 40 PRs in the TiDB repositories.Added  Support Sort Merge Join operator. Add many builtin functions: degree, insert, instr, any_value, elt, uuid, ord, sin, inet_ntoa, maketime, sha2, quarter Add a command line flag to start TiDB without authentication function. Support use special comment /*+ */ for optimizer hint. Make index serial scan concurrency configurable with system variable.  Fixed  Replace standard context package with golang.org/x/net/context. Fix a bug in the floor builtin function. Fix a bug in the date_format builtin function. Fix goroutine leak in tikv client.  Improved  Build statistics with multiple goroutines. Refactor coprocessor architecture: Add DAG physical plan protobuf. Refactor TiDB specific system variable: Make code cleaner.  New Contributors Thank you guys! Ma Xiaoyu Zhang Yuning zs634134578 Arthur Yang qhsong Sheng Tang hiwjd  Weekly update in TiKV Last week, We landed 18 PRs in the TiKV repositories.Added  Add RocksDB WAL options to support in memory storage.  Fixed  Fix fetching stale TSO when leader switches back to the previous node. Make sure that commit TS is always after the start TS. Speed up the process of transfer leader.  Improved  Make Lock CF configurable. Make schedule and replication options persistent. Optimize balance speed with calculating leader/region count from cache. Prepare for introducing mio 0.6, see 1691 1692 1695.  New Contributor (Thanks!)  aya  "},
		{"url": "https://pingcap.com/blog-cn/how-to-contribute/",
		"title": "如何从零开始参与大型开源项目", 
		"content": " 写在前面的话 上世纪 70 年代，IBM 发明了关系型数据库。但是随着现在移动互联网的发展，接入设备越来越多，数据量越来越大，业务越来越复杂，传统的数据库显然已经不能满足海量数据存储的需求。虽然目前市场上也不乏分布式数据库模型，但没有品位的文艺青年不是好工程师，我们觉得，不，这些方案都不是我们想要的，它们不够美，鲜少能够把分布式事务与弹性扩展做到完美。受 Google Spanner/F1 的启发，一款从一开始就选择了开源道路的 TiDB 诞生了。 它是一款代表未来的新型分布式 NewSQL 数据库，它可以随着数据增长而无缝水平扩展，只需要通过增加更多的机器来满足业务增长需求，应用层可以不用关心存储的容量和吞吐，用东旭的话说就是「他自己会生长」。在开源的世界里，TiDB 和 TiKV 吸引了更多的具有极客气质的开发者，目前已经拥有超过 9000 个 star 和 100 个 contributor，这已然是一个世界顶级开源项目的水准。而成就了这一切的，则是来自社区的力量。最近我们收到了很多封这样的邮件和留言，大家说： 谢谢你们，使得旁人也能接触大型开源项目。本身自己是DBA，对数据库方面较干兴趣，也希望自己能逐步深入数据库领域，深入TiDB，为 TiDB 社区贡献更多、更有价值的力量。 我是一个在校学生，刚刚收到邮件说我成为了 TiDB 的 Contributor，这让我觉得当初没听父母的话坚持了自己喜欢的计算机技术，是个正确的选择，但我还需要更多的历练，直到能完整地展现、表达我的思维。  这让我感触颇多，因为，应该是我们感谢你们才是啊，没有社区，一个开源项目就成不了一股清泉甚至一汪海洋。 公司的小姑娘说，她觉得还有很多的人想要参与进来的，可工程师团队欠缺平易近人的表达，这个得改。于是便有了这篇文章以及未来的多篇文章和活动，我们欢迎所有的具有气质的开发者能和 TiDB 一起成长，一起见证数据库领域的革新，改变世界这事儿有时候也不那么难。我要重点感谢今天这篇文章的作者，来自社区的朱武（GitHub ID:viile ）、小卢（GitHub ID:lwhhhh ）和杨文（GitHub ID: yangwenmai），当在 TiDB Contributor Club 里提到想要做这件事的时候，是他们踊跃地加入了 TiDB Tech Writer 的队伍，高效又专业地完成了下文的编辑，谢谢你们。一个典型的开源项目是由什么组成的 The Community（社区）  一个项目经常会有一个围绕着它的社区，这个社区由各个承担不同角色的用户组成。 项目的拥有者：在他们账号中创建项目并拥有它的用户或者组织。 维护者和合作者：主要做项目相关的工作和推动项目发展，通常情况下拥有者和维护者是同一个人，他们拥有仓库的写入权限。 贡献者：发起拉取请求 (pull request) 并且被合并到项目里面的人。 社区成员：对项目非常关心，并且在关于项目的特性以及 pull requests 的讨论中非常活跃的人。  The Docs（文档） 项目中经常出现的文件有： Readme：几乎所有的 Github 项目都包含一个 README.md 文件，readme 文件提供了一些项目的详细信息，包括如何使用，如何构建。有时候也会告诉你如何成为贡献者。 TiDB Readme https://github.com/pingcap/tidb/blob/master/README.md  Contributing：项目以及项目的维护者各式各样，所以参与贡献的最佳方式也不尽相同。如果你想成为贡献者的话，那么你要先阅读那些有 CONTRIBUTING 标签的文档。Contributing 文档会详细介绍了项目的维护者希望得到哪些补丁或者是新增的特性。 文件里也可以包含需要写哪些测试，代码风格，或者是哪些地方需要增加补丁之类的内容。 TiDB Contributing 文档 https://github.com/pingcap/tidb/blob/master/CONTRIBUTING.md  License：LICENSE 文件就是这个开源项目的许可证。一个开源项目会告知用户他们可以做什么，不可做什么(比如：使用，修改，重新分发)，以及贡献者允许其他人做哪些事。开源许可证有多种，你可以在认识各种开源协议及其关系了解更多关于开源许可证的信息。 TiDB 遵循 Apache-2.0 Lincense https://github.com/pingcap/tidb/blob/master/LICENSE TiKV 遵循 Apache-2.0 Lincense https://github.com/pingcap/tikv/blob/master/LICENSE  Documentation：许多大型项目不会只通过自述文件去引导用户如何使用。在这些项目中你经常可以找到通往其他文件的超链接，或者是在仓库中找到一个叫做 docs 的文件夹. TiDB Docs https://github.com/pingcap/tidb/tree/master/docs    齐步走成为 Contributor Create an Issue 如果你在使用项目中发现了一个 bug，而且你不知道怎么解决这个 bug。或者使用文档时遇到了麻烦。或者有关于这个项目的问题。你可以创建一个 issue。 不管你有什么 bug，你提出 bug 后，会对那些和你有同样 bug 的人提供帮助。 更多关于 issue 如何工作的信息，请点击 Issues guide。Issues Pro Tips  检查你的问题是否已经存在 重复的问题会浪费大家的时间，所以请先搜索打开和已经关闭的问题，来确认你的问题是否已经提交过了。 清楚描述你的问题  TiDB Issue 模版如下 TiKV Issue 模版如下  给出你的代码链接 使用像 JSFiddle 或者 CodePen 等工具，贴出你的代码，好帮助别人复现你的问题 详细的系统环境介绍 例如使用什么版本的浏览器，什么版本的库，什么版本的操作系统等其他你运行环境的介绍。 go 版本： go version Linux 版本： uname -a 详细的错误输出或者日志 使用 Gist 贴出你的错误日志。如果你在 issue 中附带错误日志，请使用`来标记你的日志。以便更好的显示。  Pull Request 如果你能解决这个 bug，或者你能够添加其他的功能。并且知道如何成为贡献者，理解 license，已经签过Contributor Licence Agreement (CLA) 后，请发起 Pull Request。这样维护人员可以将你的分支与现有分支进行比较，来决定是否合并你的更改。Pull Request Pro Tips  Fork 代码并且 clone 到你本地 通过将项目的地址添加为一个 remote，并且经常从 remote 合并更改来保持你的代码最新，以便在提交你的 pull 请求时，尽可能少的发生冲突。详情请参阅这里 创建 branch 来修改你的代码，目前 TiDB 相关的项目默认的 branch 命名规则是 user/name。例如 disksing/grpc，简单明确，一目了然。 描述清楚你的问题 方便其他人能够复现。或者说明你添加的功能有什么作用，并且清楚描述你做了哪些更改。 注意测试 如果项目中包含逻辑修改，那么必须包含相应的测试，在 CI 中会包含测试覆盖率的检测，如果测试覆盖率下降，那么是不可以合并到 master 的。 包含截图 如果您的更改包含 HTML/CSS 中的差异，请添加前后的屏幕截图。将图像拖放到您的 pull request 的正文中。 保持良好的代码风格这意味着使用与你自己的代码风格中不同的缩进，分号或注释，但是使维护者更容易合并，其他人将来更容易理解和维护。目前 TiDB 项目的 CI 检测包含代码风格的检查，如果代码风格不符合要求，那么是不可以合并到 master 的。  Open Pull Requests 一旦你新增一个 pull request，讨论将围绕你的更改开始。其他贡献者和用户可能会进入讨论，但最终决定是由维护者决定的。你可能会被要求对你的 pull request 进行一些更改，如果是这样，请向你的 branch 添加更多代码并推送它们，它们将自动进入现有的 pull request。如果你的 pull request 被合并，这会非常棒。如果没有被合并，不要灰心。也许你的更改不是项目维护者需要的。或者更改已经存在了。发生这种情况时，我们建议你根据收到的任何反馈来修改代码，并再次提出 pull request。或创建自己的开源项目。TiDB 合并流程 PR 提交之后，请耐心等待维护者进行 Review。 目前一般在一到两个工作日内都会进行 Review，如果当前的 PR 堆积数量较多可能回复会比较慢。 代码提交后 CI 会执行我们内部的测试，你需要保证所有的单元测试是可以通过的。期间可能有其它的提交会与当前 PR 冲突，这时需要修复冲突。 维护者在 Review 过程中可能会提出一些修改意见。修改完成之后如果 reviewer 认为没问题了，你会收到 LGTM(looks good to me) 的回复。当收到两个及以上的 LGTM 后，该 PR 将会被合并。标注：本文「一个典型的开源项目是由什么组成的」及「齐步走为 Contributor」参考自英文 GitHub Guide，由社区成员朱武（GitHub ID: viile）、小卢（GitHub ID:lwhhhh）着手翻译并替换部分原文中的截图。GitHub Guides：如何参与一个 GitHub 开源项目英文原文地址： https://guides.github.com/activities/contributing-to-open-source/加入 TiDB Contributor Club为更好地促进 Contributor 间的交流，便于随时提出好的想法和反馈，我们创建了一个 Contributor Club 微信群，对成为 TiDB Contributor 有兴趣的同学可以添加 TiDB Robot 微信号，它会在后台和你打招呼，并积极招募你成为开源社区的一员。"},
		{"url": "https://pingcap.com/meetup/meetup-2017-03-25/",
		"title": "RocksDB 专场分享|PingCAP 第 43 期 NewSQL Meetup", 
		"content": " RocksDB 专场分享|PingCAP 第 43 期 NewSQL Meetup 2017-03-25 宋昭&amp;amp;赵安安 PingCAP PingCAPPingCAP ![]() 微信号pingcap2015功能介绍PingCAP 专注于新型"},
		{"url": "https://pingcap.com/weekly/2017-03-20-tidb-weekly/",
		"title": "Weekly update (March 13 ~ March 19, 2017)", 
		"content": " Weekly update in TiDB Last week, we landed 33 PRs in the TiDB repositories.Added  Add system table mysql.stats_meta: used for storing statistic information. Add a Restful API to get region info from pd. Add a system variable to control the behavior of unfolding subquery in in expression. Add many builtin functions: acos, asin, atan, make_set, oct, pi, lpad, radians, exp, ip_v6  Fixed  Check truncated UTF8 string when casting value. Fix data race. Stop the service when meet critical error. Keep the return value of builtin function conv consistent with MySQL when meet invalid input.  Improved  Refactor tikv coprocessor client: make code cleaner and fix memory leak. Refactor code about aggregation pushdown and aggregation prunning: make code cleaner.  New Contributors Thank you guys! Huxley Hu Zhao Yi Jun silenceper Rick Yu Qiannan Tao Meng yangwenmai Hongyuan Wang 8cbx alston111111 Du Chuan  Weekly update in TiKV Last week, We landed 10 PRs in the TiKV repositories.Added  Use SSE support by default. Handle [CTRL + D] in pd-ctl (https://github.com/pingcap/pd/pull/570)  Fixed  Avoid panic when get the tombstone store information. Check epoch in ReadIndex to avoid stale read. Clean ReadIndex callback when peer is destroyed to avoid deadlock.  Improved  Queue vote to prevent followers from discarding it if region hasn’t been split. Improve random picking region from cache.  "},
		{"url": "https://pingcap.com/meetup/meetup-2017-03-18/",
		"title": "COISF 专场|PingCAP 第 42 期 NewSQL Meetup", 
		"content": " COISF 专场|PingCAP 第 42 期 NewSQL Meetup 2017-03-18 梁堰波 PingCAP PingCAPPingCAP ![]() 微信号pingcap2015功能介绍PingCAP 专注于新型"},
		{"url": "https://pingcap.com/blog-cn/add-a-built-in-function/",
		"title": "十分钟成为 TiDB Contributor 系列 | 添加內建函数", 
		"content": " 背景知识 SQL 语句发送到 TiDB 后首先会经过 parser，从文本 parse 成为 AST（抽象语法树），通过 Query Optimizer 生成执行计划，得到一个可以执行的 plan，通过执行这个 plan 即可得到结果，这期间会涉及到如何获取 table 中的数据，如何对数据进行过滤、计算、排序、聚合、滤重以及如何对表达式进行求值。 对于一个 builtin 函数，比较重要的是进行语法解析以及如何求值。其中语法解析部分需要了解如何写 yacc 以及如何修改 TiDB 的词法解析器，较为繁琐，我们已经将这部分工作提前做好，大多数 builtin 函数的语法解析工作已经做完。 对 builtin 函数的求值需要在 TiDB 的表达式求值框架下完成，每个 builtin 函数被认为是一个表达式，用一个 ScalarFunction 来表示，每个 builtin 函数通过其函数名以及参数，获取对应的函数类型以及函数签名，然后通过函数签名进行求值。 总体而言，上述流程对于不熟悉 TiDB 的朋友而言比较复杂，我们对这部分做了些工作，将一些流程性、较为繁琐的工作做了统一处理，目前已经将大多数未实现的 buitlin 函数的语法解析以及寻找函数签名的工作完成，但是函数实现部分留空。换句话说，只要找到留空的函数实现，将其补充完整，即可作为一个 PR。添加 builtin 函数整体流程  找到未实现的函数 在 TiDB 源码中的 expression 目录下搜索 errFunctionNotExists，即可找到所有未实现的函数，从中选择一个感兴趣的函数，比如 SHA2 函数：func (b *builtinSHA2Sig) eval(row []types.Datum) (d types.Datum, err error) { return d, errFunctionNotExists.GenByArgs(&amp;#34;SHA2&amp;#34;) } 实现函数签名 接下来要做的事情就是实现 eval 方法，函数的功能请参考 MySQL 文档，具体的实现方法可以参考目前已经实现函数。 在 typeinferer 中添加类型推导信息 在 plan/typeinferer.go 中的 handleFuncCallExpr() 里面添加这个函数的返回结果类型，请保持和 MySQL 的结果一致。全部类型定义参见 MySQL Const。 ``` 注意大多数函数除了需要填写返回值类型之外，还需要获取返回值的长度。 ```  写单元测试 在 expression 目录下，为函数的实现增加单元测试，同时也要在 plan/typeinferer_test.go 文件中添加 typeinferer 的单元测试 运行 make dev，确保所有的 test case 都能跑过  示例 这里以新增 SHA1() 函数的 PR 为例，进行详细说明 首先看 expression/builtin_encryption.go： 将 SHA1() 的求值方法补充完整func (b *builtinSHA1Sig) eval(row []types.Datum) (d types.Datum, err error) { // 首先对参数进行求值，这块一般不用修改 args, err := b.evalArgs(row) if err != nil { return types.Datum{}, errors.Trace(err) } // 每个参数的意义请参考 MySQL 文档 // SHA/SHA1 function only accept 1 parameter arg := args[0] if arg.IsNull() { return d, nil } // 这里对参数值做了一个类型转换，函数的实现请参考 util/types/datum.go bin, err := arg.ToBytes() if err != nil { return d, errors.Trace(err) } hasher := sha1.New() hasher.Write(bin) data := fmt.Sprintf(&amp;#34;%x&amp;#34;, hasher.Sum(nil)) // 设置返回值 d.SetString(data) return d, nil } 接下来给函数实现添加单元测试，参见 expression/builtin_encryption_test.go：var shaCases = []struct { origin interface{} crypt string }{ {&amp;#34;test&amp;#34;, &amp;#34;a94a8fe5ccb19ba61c4c0873d391e987982fbbd3&amp;#34;}, {&amp;#34;c4pt0r&amp;#34;, &amp;#34;034923dcabf099fc4c8917c0ab91ffcd4c2578a6&amp;#34;}, {&amp;#34;pingcap&amp;#34;, &amp;#34;73bf9ef43a44f42e2ea2894d62f0917af149a006&amp;#34;}, {&amp;#34;foobar&amp;#34;, &amp;#34;8843d7f92416211de9ebb963ff4ce28125932878&amp;#34;}, {1024, &amp;#34;128351137a9c47206c4507dcf2e6fbeeca3a9079&amp;#34;}, {123.45, &amp;#34;22f8b438ad7e89300b51d88684f3f0b9fa1d7a32&amp;#34;}, } func (s *testEvaluatorSuite) TestShaEncrypt(c *C) { defer testleak.AfterTest(c)() // 监测 goroutine 泄漏的工具，可以直接照搬 fc := funcs[ast.SHA] for _, test := range shaCases { in := types.NewDatum(test.origin) f, _ := fc.getFunction(datumsToConstants([]types.Datum{in}), s.ctx) crypt, err := f.eval(nil) c.Assert(err, IsNil) res, err := crypt.ToString() c.Assert(err, IsNil) c.Assert(res, Equals, test.crypt) } // test NULL input for sha var argNull types.Datum f, _ := fc.getFunction(datumsToConstants([]types.Datum{argNull}), s.ctx) crypt, err := f.eval(nil) c.Assert(err, IsNil) c.Assert(crypt.IsNull(), IsTrue) } * 注意，除了正常 case 之外，最好能添加一些异常的case，如输入值为 nil，或者是多种类型的参数 最后还需要添加类型推导信息以及 test case，参见 plan/typeinferer.go，plan/typeinferer_test.go：case ast.SHA, ast.SHA1: tp = types.NewFieldType(mysql.TypeVarString) chs = v.defaultCharset tp.Flen = 40{`sha1(123)`, mysql.TypeVarString, &amp;#34;utf8&amp;#34;}, {`sha(123)`, mysql.TypeVarString, &amp;#34;utf8&amp;#34;}, 编辑按：添加 TiDB Robot 微信，加入 TiDB Contributor Club，无门槛参与开源项目，改变世界从这里开始吧（萌萌哒）。"},
		{"url": "https://pingcap.com/weekly/2017-03-13-tidb-weekly/",
		"title": "Weekly update (March 06 ~ March 12, 2017)", 
		"content": " Weekly update in TiDB Last week, we landed 30 PRs in the TiDB repositories.Added  Add context in exector: prepare for canceling execution. Support the KILL statement. Support string literal with in the national character set N&#39;literal&#39;. Check privilege when running create user statement. Add a session variable to prevent eager aggregation.  Fixed  Clean up pending task when close executor: fix memory leak. Make the type of coalesce function the same with MySQL. Clean up process info when finish running statement. Fix a bug in optimizer about pushing down limit across filter. Fix a bug in optimizer about column prunning on union statement. Use QUOTE as an identifier.  Improved  Create user automatically when grant privilege for unexist user. Speed up Add Column statement. Update grpc version to v1.0.4 in vendor. Avoid useless schema version update when running DDL. Add metrics for potential time-consuming queries.  Weekly update in TiKV Last week, We landed 13 PRs in the TiKV repositories.Added  Add metric for RocksDB compaction. Collect scanned key count to detect region access hotspot. Support GetRegionByID for pd-ctl. Enable AutoCompactionRetention to reduce PD space occupation.  Fixed  Reduce manual compaction for Lock CF to avoid too many WAL logs. Only check scheduler busy error for Write command. Fix data race for updating replication configuration. Prohibit joining the PD cluster with empty name.  Improved  Beautify region log. Cleanup iterator creation. Adjust default leader scheduler limit to speed up leader balance.  "},
		{"url": "https://pingcap.com/meetup/meetup-2017-03-11/",
		"title": "COISF 专场|PingCAP 第 41 期 NewSQL Meetup", 
		"content": " COISF 专场|PingCAP 第 41 期 NewSQL Meetup 2017-03-11 杨建军&amp;amp;张金鹏 PingCAP PingCAPPingCAP ![]() 微信号pingcap2015功能介绍PingCAP 专注于新型"},
		{"url": "https://pingcap.com/blog-cn/optimizing-raft-in-tikv/",
		"title": "TiKV 源码解析系列 - Raft 的优化", 
		"content": " 在分布式领域，为了保证数据的一致性，通常都会使用 Paxos 或者 Raft 来实现。但 Paxos 以其复杂难懂著称，相反 Raft 则是非常简单易懂，所以现在很多新兴的数据库都采用 Raft 作为其底层一致性算法，包括我们的 TiKV。当然，Raft 虽然简单，但如果单纯的按照 Paper 的方式去实现，性能是不够的。所以还需要做很多的优化措施。本文假定用户已经熟悉并了解过 Raft 算法，所以对 Raft 不会做过多说明。Simple Request Flow 这里首先介绍一下一次简单的 Raft 流程： Leader 收到 client 发送的 request。 Leader 将 request append 到自己的 log。 Leader 将对应的 log entry 发送给其他的 follower。 Leader 等待 follower 的结果，如果大多数节点提交了这个 log，则 apply。 Leader 将结果返回给 client。 Leader 继续处理下一次 request。  可以看到，上面的流程是一个典型的顺序操作，如果真的按照这样的方式来写，那性能是完全不行的。Batch and Pipeline 首先可以做的就是 batch，大家知道，在很多情况下面，使用 batch 能明显提升性能，譬如对于 RocksDB 的写入来说，我们通常不会每次写入一个值，而是会用一个 WriteBatch 缓存一批修改，然后在整个写入。 对于 Raft 来说，Leader 可以一次收集多个 requests，然后一批发送给 Follower。当然，我们也需要有一个最大发送 size 来限制每次最多可以发送多少数据。如果只是用 batch，Leader 还是需要等待 Follower 返回才能继续后面的流程，我们这里还可以使用 Pipeline 来进行加速。大家知道，Leader 会维护一个 NextIndex 的变量来表示下一个给 Follower 发送的 log 位置，通常情况下面，只要 Leader 跟 Follower 建立起了连接，我们都会认为网络是稳定互通的。所以当 Leader 给 Follower 发送了一批 log 之后，它可以直接更新 NextIndex，并且立刻发送后面的 log，不需要等待 Follower 的返回。如果网络出现了错误，或者 Follower 返回一些错误，Leader 就需要重新调整 NextIndex，然后重新发送 log 了。Append Log Parallelly 对于上面提到的一次 request 简易 Raft 流程来说，我们可以将 2 和 3 并行处理，也就是 Leader 可以先并行的将 log 发送给 Followers，然后再将 log append。为什么可以这么做，主要是因为在 Raft 里面，如果一个 log 被大多数的节点append，我们就可以认为这个 log 是被 committed 了，所以即使 Leader 再给 Follower 发送 log 之后，自己 append log 失败 panic 了，只要 N / 2 + 1 个 Follower 能接收到这个 log 并成功 append，我们仍然可以认为这个 log 是被 committed 了，被 committed 的 log 后续就一定能被成功 apply。那为什么我们要这么做呢？主要是因为 append log 会涉及到落盘，有开销，所以我们完全可以在 Leader 落盘的同时让 Follower 也尽快的收到 log 并 append。这里我们还需要注意，虽然 Leader 能在 append log 之前给 Follower 发 log，但是 Follower 却不能在 append log 之前告诉 Leader 已经成功 append 这个 log。如果 Follower 提前告诉 Leader 说已经成功 append，但实际后面 append log 的时候失败了，Leader 仍然会认为这个 log 是被 committed 了，这样系统就有丢失数据的风险了。Asynchronous Apply 上面提到，当一个 log 被大部分节点 append 之后，我们就可以认为这个 log 被 committed 了，被 committed 的 log 在什么时候被 apply 都不会再影响数据的一致性。所以当一个 log 被 committed 之后，我们可以用另一个线程去异步的 apply 这个 log。所以整个 Raft 流程就可以变成： Leader 接受一个 client 发送的 request。 Leader 将对应的 log 发送给其他 follower 并本地 append。 Leader 继续接受其他 client 的 requests，持续进行步骤 2。 Leader 发现 log 已经被 committed，在另一个线程 apply。 Leader 异步 apply log 之后，返回结果给对应的 client。  使用 asychronous apply 的好处在于我们现在可以完全的并行处理 append log 和 apply log，虽然对于一个 client 来说，它的一次 request 仍然要走完完整的 Raft 流程，但对于多个 clients 来说，整体的并发和吞吐量是上去了。Now Doing… SST Snapshot 在 Raft 里面，如果 Follower 落后 Leader 太多，Leader 就可能会给 Follower 直接发送 snapshot。在 TiKV，PD 也有时候会直接将一个 Raft Group 里面的一些副本调度到其他机器上面。上面这些都会涉及到 Snapshot 的处理。在现在的实现中，一个 Snapshot 流程是这样的： Leader scan 一个 region 的所有数据，生成一个 snapshot file Leader 发送 snapshot file 给 Follower Follower 接受到 snapshot file，读取，并且分批次的写入到 RocksDB  如果一个节点上面同时有多个 Raft Group 的 Follower 在处理 snapshot file，RocksDB 的写入压力会非常的大，然后极易引起 RocksDB 因为 compaction 处理不过来导致的整体写入 slow 或者 stall。幸运的是，RocksDB 提供了 SST 机制，我们可以直接生成一个 SST 的 snapshot file，然后 Follower 通过 injest 接口直接将 SST file load 进入 RocksDB。Asynchronous Lease Read 在之前的 Lease Read 文章中，我提到过 TiKV 使用 ReadIndex 和 Lease Read 优化了 Raft Read 操作，但这两个操作现在仍然是在 Raft 自己线程里面处理的，也就是跟 Raft 的 append log 流程在一个线程。无论 append log 写入 RocksDB 有多么的快，这个流程仍然会 delay Lease Read 操作。所以现阶段我们正在做的一个比较大的优化就是在另一个线程异步实现 Lease Read。也就是我们会将 Leader Lease 的判断移到另一个线程异步进行，Raft 这边的线程会定期的通过消息去更新 Lease，这样我们就能保证 Raft 的 write 流程不会影响到 read。总结 虽然外面有声音说 Raft 性能不好，但既然我们选择了 Raft，所以就需要对它持续的进行优化。而且现阶段看起来，成果还是很不错的。相比于 RC1，最近发布的 RC2 无论在读写性能上面，性能都有了极大的提升。但我们知道，后面还有很多困难和挑战在等着我们，同时我们也急需在性能优化上面有经验的大牛过来帮助我们一起改进，如果你对我们做的东西感兴趣，想让 Raft 快的飞起，欢迎联系我： 邮箱：tl@pingcap.com 微信：siddontang  "},
		{"url": "https://pingcap.com/weekly/2017-03-06-tidb-weekly/",
		"title": "Weekly update (February 27 ~ March 05, 2017)", 
		"content": " Weekly update in TiDB Last week, we landed 21 PRs in the TiDB repositories.Added  Add metrics for related region count for a transaction. Record sql_mode in session context. Support Show Processlist. Add an empty Triggers table in information_schema database. Support ANSI_QUOTES sql_mode in parser. Support use wildcard as user hostname in grant statement. Support the MD5 builtin function. Support the SHA/SHA1 builtin function.  Fixed  Use PI as identifier Fix a panic in prepared statement.  Improved  Use unique key to eliminate aggregation. Tuning tikv client max timeout parameter. Adjust log level in tikv client.  Weekly update in TiKV Last week, We landed 8 PRs in the TiKV repositories.Added  Use ReadIndex to serve safe Raft read. Add tso command for pd-ctl. Add ServerIsBusy metric.  Fixed  Fix a data race when update configuration.  Improved  Notify PD immediately when leader changes. Make scheduler TooBusy threshold configurable.  "},
		{"url": "https://pingcap.com/meetup/meetup-2017-03-04/",
		"title": "COISF 专场|PingCAP 第 40 期 NewSQL Meetup", 
		"content": " COISF 专场|PingCAP 第 40 期 NewSQL Meetup 2017-03-04 吴晓飞&amp;amp;韩飞 PingCAP PingCAPPingCAP ![]() 微信号pingcap2015功能介绍PingCAP 专注于新型"},
		{"url": "https://pingcap.com/blog-cn/how-to-use-tidb/",
		"title": "TiDB 的正确使用姿势", 
		"content": "  最近这几个月，特别是 TiDB RC1 发布后，越来越多的用户已经开始测试起来，也有很多朋友已经在生产环境中使用，我们这边也陆续的收到了很多用户的测试和使用反馈。非常感谢各位小伙伴和早期用户的厚爱，而且看了这么多场景后，也总结出了一些 TiDB 的使用实践 (其实 Spanner 的最佳实践大部分在 TiDB 中也是适用的，MySQL 最佳实践也是），也是借着 Google Cloud Spanner 发布的东风，看了一下 Spanner 官方的一些最佳实践文档，写篇文章讲讲 TiDB 以及分布式关系型数据库的一些正确的使用姿势，当然，时代也在一直发展，TiDB 也在不停的进化，这篇文章基本上只代表近期的一些观察。 首先谈谈 Schema 设计的一些比较好的经验。由于 TiDB 是一个分布式的数据库，可能在表结构设计的时候需要考虑的事情和传统的单机数据库不太一样，需要开发者能够带着「这个表的数据会分散在不同的机器上」这个前提，才能做更好的设计。和 Spanner 一样，TiDB 中的一张表的行（Rows）是按照主键的字节序排序的（整数类型的主键我们会使用特定的编码使其字节序和按大小排序一致），即使在 CREATE TABLE 语句中不显式的创建主键，TiDB 也会分配一个隐式的。 有四点需要记住： 1. 按照字节序的顺序扫描的效率是比较高的； 2. 连续的行大概率会存储在同一台机器的邻近位置，每次批量的读取和写入的效率会高； 3. 索引是有序的（主键也是一种索引），一行的每一列的索引都会占用一个 KV Pair，比如，某个表除了主键有 3 个索引，那么在这个表中插入一行，对应在底层存储就是 4 个 KV Pairs 的写入：数据行以及 3 个索引行。 4. 一行的数据都是存在一个 KV Pair 中，不会被切分，这点和类 BigTable 的列式存储很不一样。表的数据在 TiDB 内部会被底层存储 TiKV 切分成很多 64M 的 Region（对应 Spanner 的 Splits 的概念），每个 Region 里面存储的都是连续的行，Region 是 TiDB 进行数据调度的单位，随着一个 Region 的数据量越来越大和时间的推移，Region 会分裂/合并，或者移动到集群中不同的物理机上，使得整个集群能够水平扩展。 建议：  尽可能批量写入，但是一次写入总大小不要超过 Region 的分裂阈值（64M），另外 TiDB 也对单个事务有大小的限制。 存储超宽表是比较不合适的，特别是一行的列非常多，同时不是太稀疏，一个经验是最好单行的总数据大小不要超过 64K，越小越好。大的数据最好拆到多张表中。 对于高并发且访问频繁的数据，尽可能一次访问只命中一个 Region，这个也很好理解，比如一个模糊查询或者一个没有索引的表扫描操作，可能会发生在多个物理节点上，一来会有更大的网络开销，二来访问的 Region 越多，遇到 stale region 然后重试的概率也越大（可以理解为 TiDB 会经常做 Region 的移动，客户端的路由信息可能更新不那么及时），这些可能会影响 .99 延迟；另一方面，小事务（在一个 Region 的范围内）的写入的延迟会更低，TiDB 针对同一个 Region 内的跨行事务是有优化的。另外 TiDB 对通过主键精准的点查询（结果集只有一条）效率更高。   关于索引 除了使用主键查询外，TiDB 允许用户创建二级索引以加速访问，就像上面提到过的，在 TiKV 的层面，TiDB 这边的表里面的行数据和索引的数据看起来都是 TiKV 中的 KV Pair，所以很多适用于表数据的原则也适用于索引。和 Spanner 有点不一样的是，TiDB 只支持全局索引，也就是 Spanner 中默认的 Non-interleaved indexes。全局索引的好处是对使用者没有限制，可以 scale 到任意大小，不过这意味着，索引信息*不一定*和实际的数据在一个 Region 内。 建议： 对于大海捞针式的查询来说 (海量数据中精准定位某条或者某几条)，务必通过索引。 当然也不要盲目的创建索引，创建太多索引会影响写入的性能。  反模式 (最好别这么干！) 其实 Spanner 的白皮书已经写得很清楚了，我再赘述一下：第一种，过度依赖单调递增的主键，AUTO INCREMENT ID 在传统的关系型数据库中，开发者经常会依赖自增 ID 来作为 PRIMARY KEY，但是其实大多数场景大家想要的只是一个不重复的 ID 而已，至于是不是自增其实无所谓，但是这个对于分布式数据库来说是不推荐的，随着插入的压力增大，会在这张表的尾部 Region 形成热点，而且这个热点并没有办法分散到多台机器。TiDB 在 GA 的版本中会对非自增 ID 主键进行优化，让 insert workload 尽可能分散。 建议： 如果业务没有必要使用单调递增 ID 作为主键，就别用，使用真正有意义的列作为主键（一般来说，例如：邮箱、用户名等） 使用随机的 UUID 或者对单调递增的 ID 进行 bit-reverse （位反转）  第二种，单调递增的索引 (比如时间戳) 很多日志类型的业务，因为经常需要按照时间的维度查询，所以很自然需要对 timestamp 创建索引，但是这类索引的问题本质上和单调递增主键是一样的，因为在 TiDB 的内部实现里，索引也是一堆连续的 KV Pairs，不断的插入单调递增的时间戳会造成索引尾部的 Region 形成热点，导致写入的吞吐受到影响。 建议： 因为不可避免的，很多用户在使用 TiDB 存储日志，毕竟 TiDB 的弹性伸缩能力和 MySQL 兼容的查询特性是很适合这类业务的。另一方面，如果发现写入的压力实在扛不住，但是又非常想用 TiDB 来存储这种类型的数据，可以像 Spanner 建议的那样做 Application 层面的 Sharding，以存储日志为例，原来的可能在 TiDB 上创建一个 log 表，更好的模式是可以创建多个 log 表，如：log_1, log_2 … log_N，然后业务层插入的时候根据时间戳进行 hash ，随机分配到 1..N 这几个分片表中的一个。  相应的，查询的时候需要将查询请求分发到各个分片上，最后在业务层汇总结果。查询优化 TiDB 的优化分为基于规则的优化（Rule Based Optimization）和基于代价的优化（Cost Based Optimization）, 本质上 TiDB 的 SQL 引擎更像是一个分布式计算框架，对于大表的数据因为本身 TiDB 会将数据分散到多个存储节点上，能将查询逻辑下推，会大大的提升查询的效率。TiDB 基于规则的优化有： 谓词下推 谓词下推会将 where/on/having 条件推到离数据表尽可能近的地方，比如：select * from t join s on t.id = s.id where t.c1 &amp;lt; 10可以被 TiDB 自动改写成select * from (select * from t where t.c1 &amp;lt; 10) as t join s on t.id = s.id关联子查询消除关联子查询可能被 TiDB 改写成 Join，例如：select * from t where t.id in (select id from s where s.c1 &amp;lt; 10 and s.name = t.name)可以被改写成：select * from t semi join s on t.id = s.id and s.name = t.name and s.c1 &amp;lt; 10聚合下推 聚合函数可以被推过 Join，所以类似带等值连接的 Join 的效率会比较高，例如：select count(s.id) from t join s on t.id = s.t_id可以被改写成：select sum(agg0) from t join (select count(id) as agg0, t_id from s group by t_id) as s on t.id = s.t_id基于规则的优化有时可以组合以产生意想不到的效果，例如：select s.c2 from s where 0 = (select count(id) from t where t.s_id = s.id)在TiDB中，这个语句会先通过关联子查询消除的优化，变成：select s.c2 from s left outer join t on t.s_id = s.id group by s.id where 0 = count(t.id)然后这个语句会通过聚合下推的优化，变成：select s.c2 from s left outer join (select count(t.id) as agg0 from t group by t.s_id) t on t.s_id = s.id group by s.id where 0 = sum(agg0)再经过聚合消除的判断，语句可以优化成：select s.c2 from s left outer join (select count(t.id) as agg0 from t group by t.s_id) t on t.s_id = s.id where 0 = agg0基于代价的优化有：读取表时，如果有多条索引可以选择，我们可以通过统计信息选择最优的索引。例如:select * from t where age = 30 and name in ( ‘小明’, ‘小强’) 对于包含 Join 的操作，我们可以区分大小表，TiDB 的对于一个大表和一个小表的 Join 会有特殊的优化。 例如 select * from t join s on s.id = t.id 优化器会通过对表大小的估计来选择 Join 的算法：即选择把较小的表装入内存中。 对于多种方案，利用动态规划算法选择最优者，例如:(select * from t where c1 &amp;lt; 10) union all (select * from s where c2 &amp;lt; 10) order by c3 limit 10t 和 s 可以根据索引的数据分布来确定选择索引 c3 还是 c2。总之正确使用 TiDB 的姿势，或者说 TiDB 的典型的应用场景是：大数据量下，MySQL 复杂查询很慢；大数据量下，数据增长很快，接近单机处理的极限，不想分库分表或者使用数据库中间件等对业务侵入性较大，架构反过来约束业务的 Sharding 方案；大数据量下，有高并发实时写入、实时查询、实时统计分析的需求；有分布式事务、多数据中心的数据 100% 强一致性、auto-failover 的高可用的需求。如果整篇文章你只想记住一句话，那就是数据条数少于 5000w 的场景下通常用不到 TiDB，TiDB 是为大规模的数据场景设计的。如果还想记住一句话，那就是单机 MySQL 能满足的场景也用不到 TiDB。"},
		{"url": "https://pingcap.com/blog/2017-03-01-rc2/",
		"title": "TiDB RC2 Release Notes", 
		"content": " We have great efforts to improve the compatibility with MySQL, SQL query optimizer, system stability and performance in this version. What’s more, a new + of permission management is added and users can control data access rights in the same way of MySQL privilege management system.TiDB:  Query optimizer Collect column/index statistics and use them in the query optimizer Optimize the correlated subquery Optimize the Cost Based Optimizer (CBO) framework Eliminate aggregation using unique key information Refactor expression evaluation framework Convert Distinct to GroupBy Support the topn operation push-down  Support basic privilege management Add lots of MySQL built-in functions Improve the Alter Table statement and support the modification of table name, default value and comment Support the Create Table Like statement Support the Show Warnings statement Support the Rename Table statement Restrict the size of a single transaction to avoid the cluster blocking of large transactions Automatically split data in the process of Load Data Optimize the performance of the AddIndex and Delete statement Support &amp;ldquo;ANSI_QUOTES&amp;rdquo; sql_mode Improve the monitoring system Fix Bugs Solve the problem of memory leak  PD:  Support location aware replica scheduling Conduct fast scheduling based on the number of region pd-ctl support more features Add or delete PD Obtain Region information with Key Add or delete scheduler and operator Obtain cluster label information   TiKV:  Support Async Apply to improve the entire write performance Use prefix seek to improve the read performance of Write CF Use memory hint prefix to improve the insert performance of Raft CF Optimize the single read transaction performance Support more push-down expressions Improve the monitoring system Fix Bugs  "},
		{"url": "https://pingcap.com/weekly/2017-02-27-tidb-weekly/",
		"title": "Weekly update (February 19 ~ February 26, 2017)", 
		"content": " Weekly update in TiDB Last week, we landed 39 PRs in the TiDB repositories.Added  Support Revoke statement. Add rules in parser and empty implementation for unsupported builtin functions: #2667, #2677, #2679 Support wildcard chars in username or host in Grant statement. Add a context arggument for distsql/kv interface: We will use the context to cancel running jobs. Support Create Table ... Like statement. Support ON UPDATE CURRENT_TIMESTAMP with precision Support Show Warnings statement.  Fixed  Fix memory leak when IndexScanExecutor meet error. Add missing field length information for show databases; Add missing field length information for Show statement. Fix compatibility problme about string truncate error. Ignore malformated data in MySQL packet. Fix a bug about topn operator pushdown. Fix a bug about add column with default value and not null flag.  Improved  Prune correlated columns in groupby item list. Imporve the performance of point get query using primary key or unique key by 3%. Decorrelated for aggregation.  Weekly update in TiKV Last week, We landed 25 PRs in the TiKV repositories.Added  Detect system CPU/memory to adjust config automatically. Add RocksDB statistics. Balance store according to region count. Add label, ping , admin, scheduler, operator commands for pd-ctl. Add shuffle region scheduler.  Fixed  Verify PD address when start pd-ctl. Reject read index when new leader hasn’t committed the empty entry. Accelerate campaign faster after split happens. Use default value in column info.  Improved  Save one TS get for single point get by unique or primary key. Prettfiy command label name in metrics. Make Raft apply atomically. Add region ID for scheduler slow log.  "},
		{"url": "https://pingcap.com/meetup/meetup-2017-02-25/",
		"title": "COISF 专场|PingCAP 第 39 期 NewSQL Meetup", 
		"content": " COISF 专场|PingCAP 第 39 期 NewSQL Meetup 2017-02-25 郝立飞 PingCAP PingCAPPingCAP ![]() 微信号pingcap2015功能介绍PingCAP 专注于新型"},
		{"url": "https://pingcap.com/blog-cn/spanner-tangliu/",
		"title": "Spanner - CAP, TrueTime and Transaction", 
		"content": " 最近非常关注的一件事情就是 Google Spanner Cloud 的发布，这应该算是 NewSQL 又一个里程碑的事件。NewSQL 的概念应该就是在 12 年 Google Spanner 以及 F1 的论文发表之后，才开始慢慢流行，然后就开始有企业尝试根据 paper 做自己的 NewSQL，譬如国外的 CockroachDB 以及国内我们 PingCAP。Spanner 的论文在很早就发布了，国内也有很多中文翻译，这里笔者只是想聊聊自己对 Spanner 的理解，以及 Spanner 的一些关键技术的实现，以及跟我们自己的 TiDB 的相关对比。CAP 在分布式领域，CAP 是一个完全绕不开的东西，大家应该早就非常熟悉，这里笔者只是简单的再次说明一下： C：一致性，也就是通常说的线性一致性，假设在 T 时刻写入了一个值，那么在 T 之后的读取一定要能读到这个最新的值。 A：完全 100% 的可用性，也就是无论系统发生任何故障，都仍然能对外提供服务。 P：网络分区容忍性。  在分布式环境下面，P 是铁定存在的，也就是只要我们有多台机器，那么网络隔离分区就一定不可避免，所以在设计系统的时候我们就要选择到底是设计的是 AP 系统还是 CP 系统，但实际上，我们只要深入理解下 CAP，就会发现其实有时候系统设计上面没必要这么纠结，主要表现在： 网络分区出现的概率很低，所以我们没必要去刻意去忽略 C 或者 A。多数时候，应该是一个 CA 系统。 CAP 里面的 A 是 100% 的可用性，但实际上，我们只需要提供 high availability，也就是仅仅需要满足 99.99% 或者 99.999% 等几个 9 就可以了。  Spanner 是一个 CP + HA 系统，官方文档说的可用性是优于 5 个 9 ，稍微小于 6 个 9，也就是说，Spanner 在系统出现了大的故障的情况下面，大概 31s+ 的时间就能够恢复对外提供服务，这个时间是非常短暂的，远远比很多外部的系统更加稳定。然后鉴于 Google 强大的自建网络，P 很少发生，所以 Spanner 可以算是一个 CA 系统。TiDB 在设计的时候也是一个 CP + HA 系统，多数时候也是一个 CA 系统。如果出现了 P，也就是刚好对外服务的 leader 被隔离了，新 leader 大概需要 10s+ 以上的时间才能选举出来对外提供服务。当然，我们现在还不敢说系统的可用性在 6 个 9 了，6 个 9 现在还是我们正在努力的目标。当然，无论是 Spanner 还是 TiDB，当整个集群真的出现了灾难性的事故，导致大多数节点都出现了问题，整个系统当然不可能服务了，当然这个概率是非常小的，我们可以通过增加更多的副本数来降低这个概率发生。据传在一些关键数据上面，Spanner 都有 7 个副本。TrueTime 最开始看到 Spanner 论文的时候，我是直接被 TrueTime 给惊艳到了，这特么的完全就是解决分布式系统时间问题的一个核弹呀（银弹我可不敢说）。在分布式系统里面，时间到底有多么重要呢？之前笔者也写过一篇文章来聊过分布式系统的时间问题，简单来说，我们需要有一套机制来保证相关事务之间的先后顺序，如果事务 T1 在事务 T2 开始之前已经提交，那么 T2 一定能看到 T1 提交的数据。也就是事务需要有一个递增序列号，后开始的事务一定比前面开始的事务序列号要大。那么这跟时间又有啥关系呢，用一个全局序列号生成器不就行呢，为啥还要这么麻烦的搞一个 TrueTime 出来？笔者觉得有几个原因： 全局序列号生成器是一个典型的单点，即使会做一些 failover 的处理，但它仍然是整个系统的一个瓶颈。同时也避免不了网络开销。但全局序列号的实现非常简单，Google 之前的 Percolator 以及现在 TiDB 都是采用这种方式。 为什么要用时间？判断两个事件的先后顺序，时间是一个非常直观的度量方式，另外，如果用时间跟事件关联，那么我们就能知道某一个时间点整个系统的 snapshot。在 TiDB 的用户里面，一个非常典型的用法就是在游戏里面确认用户是否谎报因为回档丢失了数据，假设用户说在某个时间点得到某个装备，但后来又没有了，我们就可以直接在那个特定的时间点查询这个用户的数据，从而知道是否真的有问题。 我们不光可以用时间来确定以前的 snapshot，同样也可以用时间来约定集群会在未来达到某个状态。这个典型的应用就是 shema change。虽然笔者不清楚 Spanner schema change 的实现，但 Google F1 有一篇 Online, Asynchronous Schema Change in F1 论文提到了相关的方法，而 TiDB 也是采用的这种实现方式。简单来说，对于一个 schema change，通常都会分几个阶段来完成，如果集群某个节点在未来一个约定的时间没达到这个状态，这个节点就需要自杀下线，防止因为数据不一致损坏数据。  使用 TrueTime，Spanner 可以非常方便的实现笔者提到的用法，但 TureTime 也并不是万能的： TureTime 需要依赖 atomic clock 和 GPS，这属于硬件方案，而 Google 并没有论文说明如果构造 TrueTime，对于其他用户的实际并没有太多参考意义。 TureTime 也会有误差范围，虽然非常的小，在毫秒级别以下，所以我们需要等待一个最大的误差时间，才能确保事务的相关顺序。  Transaction Spanner 默认将数据使用 range 的方式切分成不同的 splits，就跟 TiKV 里面 region 的概念比较类似。每一个 Split 都会有多个副本，分布在不同的 node 上面，各个副本之间使用 Paxos 协议保证数据的一致性。Spanner 对外提供了 read-only transaction 和 read-write transaction 两种事物，这里简单的介绍一下，主要参考 Spanner 的白皮书。Single Split Write 假设 client 要插入一行数据 Row 1，这种情况我们知道，这一行数据铁定属于一个 split，spanner 在这里使用了一个优化的 1PC 算法，流程如下： API Layer 首先找到 Row 1 属于哪一个 split，譬如 Split 1。 API Layer 将写入请求发送给 Split 1 的 leader。 Leader 开始一个事务。 Leader 首先尝试对于 Row 1 获取一个 write lock，如果这时候有另外的 read-write transaction 已经对于这行数据上了一个 read lock，那么就会等待直到能获取到 write lock。  这里需要注意的是，假设事务 1 先 lock a，然后 lock b，而事务 2 是先 lock b，在 lock a，这样就会出现 dead lock 的情况。这里 Spanner 采用的是 wound-wait 的解决方式，新的事务会等待老的事务的 lock，而老的事务可能会直接 abort 掉新的事务已经占用的 lock。  当 lock 被成功获取到之后，Leader 就使用 TrueTime 给当前事务绑定一个 timestamp。因为用 TrueTime，我们能够保证这个 timestamp 一定大于之前已经提交的事务 timestamp，也就是我们一定能够读取到之前已经更新的数据。 Leader 将这次事务和对应的 timestamp 复制给 Split 1 其他的副本，当大多数副本成功的将这个相关 Log 保存之后，我们就可以认为该事务已经提交（注意，这里还并没有将这次改动 apply）。 Leader 等待一段时间确保事务的 timestamp 有效（TrueTime 的误差限制），然后告诉 client 事务的结果。这个 commit wait 机制能够确保后面的 client 读请求一定能读到这次事务的改动。另外，因为 commit wait 在等待的时候，Leader 同时也在处理上面的步骤 6，等待副本的回应，这两个操作是并行的，所以 commit wait 开销很小。 Leader 告诉 client 事务已经被提交，同时也可以顺便返回这次事务的 timestamp。 在 Leader 返回结果给 client 的时候，这次事务的改动也在并行的被 apply 到状态机里面。  Leader 将事务的改动 apply 到状态机，并且释放 lock。  Leader 同时通知其他的副本也 apply 事务的改动。 后续其他的事务需要等到这次事务的改动被 apply 之后，才能读取到数据。对于 read-write 事务，因为要拿 read lock，所以必须等到之前的 write lock 释放。而对于 read-only 事务，则需要比较 read-only 的 timestamp 是不是大于最后已经被成功 apply 的数据的 timestamp。    TiDB 现在并没有使用 1PC 的方式，但不排除未来也针对单个 region 的 read-write 事务，提供 1PC 的支持。Multi Split Write 上面介绍了单个 Split 的 write 事务的实现流程，如果一个 read-write 事务要操作多个 Split 了，那么我们就只能使用 2PC 了。假设一个事务需要在 Split 1 读取数据 Row 1，同时将改动 Row 2，Row 3 分别写到 Split 2，Split 3，流程如下： client 开始一个 read-write 事务。 client 需要读取 Row 1，告诉 API Layer 相关请求。 API Layer 发现 Row 1 在 Split 1。 API Layer 给 Split 1 的 Leader 发送一个 read request。 Split1 的 Leader 尝试将 Row 1 获取一个 read lock。如果这行数据之前有 write lock，则会持续等待。如果之前已经有另一个事务上了一个 read lock，则不会等待。至于 deadlock，仍然采用上面的 wound-wait 处理方式。 Leader 获取到 Row 1 的数据并且返回。 . Clients 开始发起一个 commit request，包括 Row 2，Row 3 的改动。所有的跟这个事务关联的 Split 都变成参与者 participants。 一个 participant 成为协调者 coordinator，譬如这个 case 里面 Row 2 成为 coordinator。Coordinator 的作用是确保事务在所有 participants 上面要不提交成功，要不失败。这些都是在 participants 和 coordinator 各自的 Split Leader 上面完成的。 Participants 开始获取 lock  Split 2 对 Row 2 获取 write lock。 Split 3 对 Row 3 获取 write lock。 Split 1 确定仍然持有 Row 1 的 read lock。 每个 participant 的 Split Leader 将 lock 复制到其他 Split 副本，这样就能保证即使节点挂了，lock 也仍然能被持有。 如果所有的 participants 告诉 coordinator lock 已经被持有，那么就可以提交事务了。coordinator 会使用这个时候的时间点作为这次事务的提交时间点。 如果某一个 participant 告诉 lock 不能被获取，事务就被取消  如果所有 participants 和 coordinator 成功的获取了 lock，Coordinator 决定提交这次事务，并使用 TrueTime 获取一个 timestamp。这个 commit 决定，以及 Split 2 自己的 Row 2 的数据，都会复制到 Split 2 的大多数节点上面，复制成功之后，就可以认为这个事务已经被提交。 Coordinator 将结果告诉其他的 participants，各个 participant 的 Leader 自己将改动复制到其他副本上面。 如果事务已经提交，coordinator 和所有的 participants 就 apply 实际的改动。 Coordinator Leader 返回给 client 说事务已经提交成功，并且返回事务的 timestamp。当然为了保证数据的一致性，需要有 commit-wait。  TiDB 也使用的是一个 2PC 方案，采用的是优化的类 Google Percolator 事务模型，没有中心的 coordinator，全部是靠 client 自己去协调调度的。另外，TiDB 也没有实现 wound-wait，而是对一个事务需要操作的 key 顺序排序，然后依次上 lock，来避免 deadlock。Strong Read 上面说了在一个或者多个 Split 上面 read-write 事务的处理流程，这里在说说 read-only 的事务处理，相比 read-write，read-only 要简单一点，这里以多个 Split 的 Strong read 为例。假设我们要在 Split 1，Split 2 和 Split 3 上面读取 Row 1，Row 2 和 Row 3。 API Layer 发现 Row 1，Row 2，和 Row 3 在 Split1，Split 2 和 Split 3 上面。 API Layer 通过 TrueTime 获取一个 read timestamp（如果我们能够接受 Stale Read 也可以直接选择一个以前的 timestamp 去读）。 API Layer 将读的请求发给 Split 1，Split 2 和 Split 3 的一些副本上面，这里有几种情况：  多数情况下面，各个副本能通过内部状态和 TureTime 知道自己有最新的数据，直接能提供 read。 如果一个副本不确定是否有最新的数据，就像 Leader 问一下最新提交的事务 timestamp 是啥，然后等到这个事务被 apply 了，就可以提供 read。 如果副本本来就是 Leader，因为 Leader 一定有最新的数据，所以直接提供 read。  各个副本的结果汇总然会返回给 client。  当然，Spanner 对于 Read 还有一些优化，如果我们要进行 stale read，并且这个 stale 的时间在 10s 之前，那么就可以直接在任何副本上面读取，因为 Leader 会每隔 10s 将最新的 timestamp 更新到其他副本上面。现在 TiDB 只能支持从 Leader 读取数据，还没有支持 follower read，这个功能已经实现，但还有一些优化需要进行，现阶段并没有发布。TiDB 在 Leader 上面的读大部分走的是 lease read，也就是只要 Leader 能够确定自己仍然在 lease 有效范围里面，就可以直接读，如果不能确认，我们就会走 Raft 的 ReadIndex 机制，让 Leader 跟其他节点进行 heartbeat 交互，确认自己仍然是 Leader 之后在进行读操作。小结 随着 Spanner Cloud 的发布，我们这边也会持续关注 Spanner Cloud 的进展，TiDB 的原始模型就是基于 Spanner + F1 搭建起来，随着 Spanner Cloud 更多资料的公布，TiDB 也能有更多的参考。另外，我们一直相信，我们走在正确的道路上面，如果你对我们的东西感兴趣，欢迎联系我。 邮箱：tl@pingcap.com 微信：siddontang  "},
		{"url": "https://pingcap.com/blog-cn/lease-read/",
		"title": "TiKV 源码解析系列 - Lease Read", 
		"content": " Raft log read TiKV 是一个要保证线性一致性的分布式 KV 系统，所谓线性一致性，一个简单的例子就是在 t1 的时间我们写入了一个值，那么在 t1 之后，我们的读一定能读到这个值，不可能读到 t1 之前的值。因为 Raft 本来就是一个为了实现分布式环境下面线性一致性的算法，所以我们可以通过 Raft 非常方便的实现线性 read，也就是将任何的读请求走一次 Raft log，等这个 log 提交之后，在 apply 的时候从状态机里面读取值，我们就一定能够保证这个读取到的值是满足线性要求的。当然，大家知道，因为每次 read 都需要走 Raft 流程，所以性能是非常的低效的，所以大家通常都不会使用。我们知道，在 Raft 里面，节点有三个状态，leader，candidate 和 follower，任何 Raft 的写入操作都必须经过 leader，只有 leader 将对应的 raft log 复制到 majority 的节点上面，我们才会认为这一次写入是成功的。所以我们可以认为，如果当前 leader 能确定一定是 leader，那么我们就可以直接在这个 leader 上面读取数据，因为对于 leader 来说，如果确认一个 log 已经提交到了大多数节点，在 t1 的时候 apply 写入到状态机，那么在 t1 之后后面的 read 就一定能读取到这个新写入的数据。那么如何确认 leader 在处理这次 read 的时候一定是 leader 呢？在 Raft 论文里面，提到了两种方法。ReadIndex Read 第一种就是 ReadIndex，当 leader 要处理一个读请求的时候： 将当前自己的 commit index 记录到一个 local 变量 ReadIndex 里面。 向其他节点发起一次 heartbeat，如果大多数节点返回了对应的 heartbeat response，那么 leader 就能够确定现在自己仍然是 leader。 Leader 等待自己的状态机执行，直到 apply index 超过了 ReadIndex，这样就能够安全的提供 linearizable read 了。 Leader 执行 read 请求，将结果返回给 client。  可以看到，不同于最开始的通过 Raft log 的 read，ReadIndex read 使用了 heartbeat 的方式来让 leader 确认自己是 leader，省去了 Raft log 那一套流程。虽然仍然会有网络开销，但 heartbeat 本来就很小，所以性能还是非常好的。但这里，需要注意，实现 ReadIndex 的时候有一个 corner case，在 etcd 和 TiKV 最初实现的时候，我们都没有注意到。也就是 leader 刚通过选举成为 leader 的时候，这时候的 commit index 并不能够保证是当前整个系统最新的 commit index，所以 Raft 要求当 leader 选举成功之后，首先提交一个 no-op 的 entry，保证 leader 的 commit index 成为最新的。所以，如果在 no-op 的 entry 还没提交成功之前，leader 是不能够处理 ReadIndex 的。但之前 etcd 和 TiKV 的实现都没有注意到这个情况，也就是有 bug。解决的方法也很简单，因为 leader 在选举成功之后，term 一定会增加，在处理 ReadIndex 的时候，如果当前最新的 commit log 的 term 还没到新的 term，就会一直等待跟新的 term 一致，也就是 no-op entry 提交之后，才可以对外处理 ReadIndex。使用 ReadIndex，我们也可以非常方便的提供 follower read 的功能，follower 收到 read 请求之后，直接给 leader 发送一个获取 ReadIndex 的命令，leader 仍然走一遍之前的流程，然后将 ReadIndex 返回给 follower，follower 等到当前的状态机的 apply index 超过 ReadIndex 之后，就可以 read 然后将结果返回给 client 了。Lease Read 虽然 ReadIndex 比原来的 Raft log read 快了很多，但毕竟还是有 Heartbeat 的开销，所以我们可以考虑做更进一步的优化。在 Raft 论文里面，提到了一种通过 clock + heartbeat 的 lease read 优化方法。也就是 leader 发送 heartbeat 的时候，会首先记录一个时间点 start，当系统大部分节点都回复了 heartbeat response，那么我们就可以认为 leader 的 lease 有效期可以到 start + election timeout / clock drift bound 这个时间点。为什么能够这么认为呢？主要是在于 Raft 的选举机制，因为 follower 会在至少 election timeout 的时间之后，才会重新发生选举，所以下一个 leader 选出来的时间一定可以保证大于 start + election timeout / clock drift bound。虽然采用 lease 的做法很高效，但仍然会面临风险问题，也就是我们有了一个预设的前提，各个服务器的 CPU clock 的时间是准的，即使有误差，也会在一个非常小的 bound 范围里面，如果各个服务器之间 clock 走的频率不一样，有些太快，有些太慢，这套 lease 机制就可能出问题。TiKV 使用了 lease read 机制，主要是我们觉得在大多数情况下面 CPU 时钟都是正确的，当然这里会有隐患，所以我们也仍然提供了 ReadIndex 的方案。TiKV 的 lease read 实现在原理上面跟 Raft 论文上面的一样，但实现细节上面有些差别，我们并没有通过 heartbeat 来更新 lease，而是通过写操作。对于任何的写入操作，都会走一次 Raft log，所以我们在 propose 这次 write 请求的时候，记录下当前的时间戳 start，然后等到对应的请求 apply 之后，我们就可以续约 leader 的 lease。当然实际实现还有很多细节需要考虑的，譬如： 我们使用的 monotonic raw clock，而 不是 monotonic clock，因为 monotonic clock 虽然不会出现 time jump back 的情况，但它的速率仍然会受到 NTP 等的影响。 我们默认的 election timeout 是 10s，而我们会用 9s 的一个固定 max time 值来续约 lease，这样一个是为了处理 clock drift bound 的问题，而另一个则是为了保证在滚动升级 TiKV 的时候，如果用户调整了 election timeout，lease read 仍然是正确的。因为有了 max lease time，用户的 election timeout 只能设置的比这个值大，也就是 election timeout 只能调大，这样的好处在于滚动升级的时候即使出现了 leader 脑裂，我们也一定能够保证下一个 leader 选举出来的时候，老的 leader lease 已经过期了。  当然，使用 Raft log 来更新 lease 还有一个问题，就是如果用户长时间没有写入操作，这时候来的读取操作因为早就已经没有 lease 了，所以只能强制走一次上面的 ReadIndex 机制来 read，但上面已经说了，这套机制性能也是有保证的。至于为什么我们不在 heartbeat 那边更新 lease，原因就是我们 TiKV 的 Raft 代码想跟 etcd 保持一致，但 etcd 没这个需求，所以我们就做到了外面。小结 在 TiKV 里面，从最开始的 Raft log read，到后面的 Lease Read，我们一步一步的在保证线性一致性的情况下面改进着性能。后面，我们会引入更多的一致性测试 case 来验证整个系统的安全性，当然，也会持续的提升性能。"},
		{"url": "https://pingcap.com/meetup/meetup-2017-02-18/",
		"title": "COISF 专场|PingCAP 第 38 期 NewSQL Meetup", 
		"content": " COISF 专场|PingCAP 第 38 期 NewSQL Meetup 2017-02-18 刘奇 PingCAP PingCAPPingCAP ![]() 微信号pingcap2015功能介绍PingCAP 专注于新型"},
		{"url": "https://pingcap.com/weekly/2017-02-13-tidb-weekly/",
		"title": "Weekly update (February 06 ~ February 12, 2017)", 
		"content": " Weekly update in TiDB Last two weeks, we landed 25 PRs in the TiDB repositories.Added  Support basic privilege framework: #2423, #2557, #2603, #2607, Support ALTER [COLUMN] col_name SET DEFAULT statement. Validate column default value when creating table. Support ALTER TABLE ... DROP DEFAULT statement. Support changing default value and comment in alter table statement.  Fixed  Fix build on i386. Fix output format of prometheus interval log. Clean up log: #2599, #2601 Fix a bug in HashJoin executor: some errors are ignored. Fix a bug in arithmetic expression type inference. Fix a bug in builtin function timediff.  Improved  Speed up unit tests. Make test cases more stable: #2595, #2596 Remove useless code in parser package. Change string default collation to utf8_bin. We do not support case insensitive comparation yet.  Weekly update in TiKV Last week, We landed 14 PRs in the TiKV repositories.Added  Use seek_for_prev directly. Refactor cut_row and cut_idx_key for row value analysis in coprocessor. Extract an interface to support different snapshot format.  Fixed  Fix panic when stop scheduler worker. Fix invalid link in PD readme.  Improved  Evaluate logic operations lazily. Use prefix seek to speed up read for Write CF. Separate advance to advance_ready and advance_apply for async-apply later. Use shutdown to do cleanup when stop worker.  "},
		{"url": "https://pingcap.com/weekly/2017-02-05-tidb-weekly/",
		"title": "Weekly update (January 23 ~ February 05, 2017)", 
		"content": " Weekly update in TiDB Last two weeks, we landed 43 PRs in the TiDB repositories.Added  Support information_schema.table_constraints. Support the UTC_TIMESTAMP builtin function Increase transaction entry count limit. Add logs for expensive query and big transaction: 2536, 2545, 2546  Fixed  Fix GC lifetime metrics. Fix primary key name parsing. Fix a bug of left outer semi join. Fix a bug of exists sub query. Fix a bug abaout prefix index.  Improved  Only plans that have apply will add cache. Use a short-cut way to way And/Or/Xor expresson. Refine the range calculating.  Weekly update in TiKV Last two weeks, we landed 20 PRs in the TiKV repositories.Added  Add start_ts for scan in tikv-ctl. Support HTTP scheme parsing.  Fixed  Fix too long scan slow log.  Improved  Use reverse_seek_le for the reverse seek write. Ignore overwriting data in prewrite. Update raft in raftstore after handling ConfChange. Make coprocessor task run more concurrently. Ignore deleting the value when roll back lock. Update leader lease before applying raft log.  "},
		{"url": "https://pingcap.com/weekly/2017-01-24-tidb-weekly/",
		"title": "Weekly update (January 09 ~ January 22, 2017)", 
		"content": " Weekly update in TiDB Last two weeks, we landed 87 PRs in the TiDB repositories.Added  Support statistic on index data Support file sort operator Add key prefix length limitation on index Support the TIMESTAMPDIFF built-in function. Support the CONV built-in function. Support the SUBSTR built-in function. Support the SIGN built-in function. Support the FROM_DAYS built-in function. Support the FIELD built-in function. Support the FLOOR built-in function. Support the SQRT built-in function. Add some metrics: #2417, #2419, #2435, #2451, #2460, [#2477])(https://github.com/pingcap/tidb/pull/2477) Set a limitation about the key-value count and size in a single transaction. Support Rename Table and Alter Table Rename Table statement.  Fixed  Add a length limitation for logging sql statement when meet sytax error. Fix a bug when binary literal has charset prefix. Fix a few bugs about optimizer: #2439, #2440, #2452 Fix a bug about parsing aggragate function. Fix a bug about two phrase commit. Fix a bug about round function. Fix a bug occurs when subquery contains a aggragetion. Fix a memory leak in Join Executor.  Improved  Use cache to store privilege info. Refactor correlated subquery optimization. Speed up delete statement executor. Handle raft entry too large error. Change the argument name for displaying version from v to V: keep consistent with pd/tikv. Convert distinct to aggragate.  New contributor  zhexuany  Weekly update in TiKV Last two weeks, We landed 50 PRs in the TiKV repositories.Added  Add -V flag for PD and TiKV. Set a limitation for the max size of the raft entry. Make disabling manual compaction configurable. Add fail reason tag in raft request and coprocessor metrics. Add snapshot KV count and size metrics. Add count of written keys metric. Add raft log lag metric. Add thread CPU collector. Support compaction_readahead_size for spinning disk. Support getting region with key in pd-ctl . Statistic region write key-value count and bytes.  Fixed  Fix a pitfall when use different context package. Count KV count in other CF when check splitting. Avoid submitting secondary key if primary key is locked. Use disk size as storage capacity when capacity is set to 0. Check locked for ResolveLock in latch.  Improved  Keep large integer precision and check integer overflow. Check promotable when handle raft MsgTimeoutNow. Unify logger. Resume paused follower when receive MsgHeartbeatResp message. Reset the pending state when add node. Allow running one GC command at the same time. Guarantee replicas are safe if possible. Keep leader alive before initialization. Log slow request for scheduler. Record scan efficiency for coprocessor. Make raft log GC not depend on PeerStorage. Accelerate ticks for new split region leader.  "},
		{"url": "https://pingcap.com/blog-cn/pd-scheduler/",
		"title": "TiKV 源码浅析 - PD Scheduler", 
		"content": " 在前面的文章里面，我们介绍了 PD 一些常用功能，以及它是如何跟 TiKV 进行交互的，这里，我们重点来介绍一下 PD 是如何调度 TiKV 的。介绍 假设我们只有一个 TiKV，那么根本就无需调度了，因为数据只可能在这一台机器上面，client 也只可能跟这一个 TiKV 进行交互。但我们知道，在分布式存储领域，这样的情况不可能一直持续，因为数据量的增量一定会超过当前机器的物理存储极限，必然我们需要将一部分数据迁移到其他机器上面去。在之前的文章里面，我们介绍过，TiKV 是通过 range 的方式将数据进行切分的。我们使用 Region 来表示一个数据 range，每个 Region 有多个副本 peer，通常为了安全，我们会使用至少三个副本。最开始系统初始化的时候，我们只有一个 region，当数据量持续增大，超过了 Region 设置的最大 size（64MB） 阈值的时候，region 就会分裂，生成两个新的 region。region 是 PD 调度 TiKV 的基本单位。当我们新增加一个 TiKV 的时候，PD 就会将原来TiKV 里面的一些 Region 调度到这个新增的 TiKV 上面，这样就能保证整个数据均衡的分布在多个 TiKV 上面。因为一个 Region 通常是 64MB，其实将一个 Region 从一个 TiKV 移动到另一个 TiKV，数据量的变更其实不大，所以我们可以直接使用 Region 的数量来大概的做数据的平衡。譬如，现在假设有六个 TiKV，我们有一百个 region，每个 Region 三个副本 peer，总共三百个 Region peer，我们只要保证每个 TiKV 有五十个左右的 Region peer，就大概知道数据是平衡了。上面我们只是介绍了数据的调度，但实际情况比这个要复杂很多，我们不光要考虑数据的均衡，也需要考虑计算的均衡，这样才能保证整个 TiKV 集群更好更快的对外提供服务。因为 TiKV 使用的是 Raft 分布式一致性算法，Raft 有一个强约束就是为了保证线性一致性，所有的读写都必须通过 leader 发起（后续我们会支持 follower read，能分担读压力）。假设现在有三个 TiKV，如果几乎所有的 leader 都集中在某一个 TiKV 上面，那么会造成这个 TiKV 成为性能瓶颈，最好的做法就是 leader 能够均衡在不同的 TiKV 上面，这样整个系统都能对外提供服务。所以，在 PD，我们主要会对两种资源进行调度，存储 storage 以及计算 leader。关键 Interface 和 Structure 为了满足不同的调度需求，PD 将调度相关的操作都抽象成了 interface，外面可以自由组合形成自己的调度方案。Scheduler Scheduler 是用来调度资源的接口，定义如下：// Scheduler is an interface to schedule resources. type Scheduler interface { GetName() string GetResourceKind() ResourceKind Schedule(cluster *clusterInfo) Operator } GetName 返回 Scheduler 名字，不同的 scheduler 不能重名。GetResourceKind 则是返回这个 Scheduler 要处理的资源类型，现阶段我们就两种，一个 leader，一个 storage。 Scheduler 则是进行实际的调度，它需要的参数就是整个集群的信息 ，在里面会生成实际的调度操作 Operator。Operator 前面我们说了，PD 对于 TiKV 调度的基本单位就是 region，所以 Scheduler 生成的 Operator 就是对一个 Region 进行调度。Operator 定义如下：// Operator is an interface to schedule region. type Operator interface { GetRegionID() uint64 GetResourceKind() ResourceKind Do(region *regionInfo) (*pdpb.RegionHeartbeatResponse, bool) } GetRegionID 得到需要调度的 Region ID，GetResourceKind 的含义跟 Scheduler 的一样。Do 则是对这个 Region 执行实际的操作，返回一个 RegionHeartbeatResponse。在之前的文章里面，我们说过，PD 对于 TiKV 的调度操作，都是在 TiKV Region heartbeat 命令里面返回给 TiKV，然后 TiKV 再去执行的。多个 Operator 也可以组合成一个更上层的 Operator，但需要注意，这些 Operator 一定要有相同的 ResourceKind，也就是说，我们不能在一组 Operator 里面操作不同的 resource。Selector / Filter 假设我们要进行 storage 的调度，选择了一个 region，那么我们就需要做的是将 region 里面的一个副本 peer，迁移到另外的一个新的 TiKV 上面。所以我们在调度的时候，就需要选择一个合适的需要调度的 TiKV，也就是 source，然后就是一个合适的将被调度到的 TiKV，也就是 target。这个就是通过 Selector 来完成的。// Selector is an interface to select source and target store to schedule. type Selector interface { SelectSource(stores []*storeInfo, filters ...Filter) *storeInfo SelectTarget(stores []*storeInfo, filters ...Filter) *storeInfo } Selector 的接口非常的简单，就是根据传入的 storeInfo 列表，以及一批 Filter，选择合适的 source 和 target，供 scheduler 实际去调度。Filter 的定义如下：// Filter is an interface to filter source and target store. type Filter interface { // Return true if the store should not be used as a source store. 	FilterSource(store *storeInfo) bool // Return true if the store should not be used as a target store. 	FilterTarget(store *storeInfo) bool } 如果 Filter 的函数返回 true，就表明我们不能选择这个 store。Controller 通常，我们希望调度越来越快就好，但是实际情况，我们必须要保证调度不能影响现有的系统，不能造成现有系统出现太大的波动。譬如，在做 storage 的调度的时候，PD 需要将 region 的某一个副本从一个 TiKV 迁移到另一个 TiKV，该 region 的 leader peer 会首先在目标 TiKV 上面添加一个新的 peer，这时候的操作是 leader 会生成当前 region 的 snapshot，然后发给 follower。Follower 收到 snapshot 之后，apply 到自己的状态机里面。同时，leader 会给原来要迁移的 peer 发送删除命令，该 follower 会在状态机里面清掉对应的数据。虽然一个 region 大概是 64MB，但过于频繁的一下子删除 64MB 数据，或者新增 64MB 数据，对于整个系统都是一个不小的负担。所以我们一定要控制整个调度的速度。// Controller is an interface to control the speed of different schedulers. type Controller interface { Ctx() context.Context Stop() GetInterval() time.Duration AllowSchedule() bool } Controller 主要用来负责控制整个调度的速度，GetInterval 返回调度的间隔时间，当上一次调度之后，需要等待多久开始下一次的调度。AllowSchedule 则是表明是否允许调度。Coordinator PD 使用 Coodinator 来管理所有的 Scheduler 以及 Controlller。// ScheduleController combines Scheduler with Controller. type ScheduleController struct { Scheduler Controller } 通常，对于调度，Scheduler 和 Controller 是同时存在的，所以在 Coordinator 里面会使用 ScheduleController 来统一进行管理。Coordinator 在 region heartbeat 的时候，会看这个 region 是否需要调度，如果需要，则进行调度。另外，在 Coordinator 里面，我们还有一个 replicaCheckController 定期检查 region 是否需要调度。因为 PD 知道整个集群的情况，所以 PD 就知道什么时候该进行调度。譬如，假设 PD 发现一个 TiKV 已经当掉，那么就会对在这个 TiKV 有副本的 region 生成调度 Operator，移除这个坏掉的副本，添加另一个好的副本，当 region heartbeat 上来的时候，直接接返回这个调度策略让 TiKV 去执行。小结 这里简单的介绍了 PD 调度器的基本原理，需要调度的资源，以及一些关键的调度 Interface 以及 Structure，后面，我们会详细的介绍一些特定的调度策略，以及 PD 是如何通过 label 来进行更精确的调度的。"},
		{"url": "https://pingcap.com/meetup/meetup-2017-01-14/",
		"title": "COISF 专场|PingCAP 第 37 期 NewSQL Meetup", 
		"content": " COISF 专场|PingCAP 第 37 期 NewSQL Meetup 2017-01-14 杨策&amp;amp;黄华超 PingCAP PingCAPPingCAP ![]() 微信号pingcap2015功能介绍PingCAP 专注于新型"},
		{"url": "https://pingcap.com/blog-cn/placement-driver/",
		"title": "TiKV 源码解析系列 - Placement Driver", 
		"content": " 介绍 Placement Driver (后续以 PD 简称) 是 TiDB 里面全局中心总控节点，它负责整个集群的调度，负责全局 ID 的生成，以及全局时间戳 TSO 的生成等。PD 还保存着整个集群 TiKV 的元信息，负责给 client 提供路由功能。作为中心总控节点，PD 通过集成 etcd ，自动的支持 auto failover，无需担心单点故障问题。同时，PD 也通过 etcd 的 raft，保证了数据的强一致性，不用担心数据丢失的问题。在架构上面，PD 所有的数据都是通过 TiKV 主动上报获知的。同时，PD 对整个 TiKV 集群的调度等操作，也只会在 TiKV 发送 heartbeat 命令的结果里面返回相关的命令，让 TiKV 自行去处理，而不是主动去给 TiKV 发命令。这样设计上面就非常简单，我们完全可以认为 PD 是一个无状态的服务（当然，PD 仍然会将一些信息持久化到 etcd），所有的操作都是被动触发，即使 PD 挂掉，新选出的 PD leader 也能立刻对外服务，无需考虑任何之前的中间状态。初始化 PD 集成了 etcd，所以通常，我们需要启动至少三个副本，才能保证数据的安全。现阶段 PD 有集群启动方式，initial-cluster 的静态方式以及 join 的动态方式。在继续之前，我们需要了解下 etcd 的端口，在 etcd 里面，默认要监听 2379 和 2380 两个端口。2379 主要是 etcd 用来处理外部请求用的，而 2380 则是 etcd peer 之间相互通信用的。假设现在我们有三个 pd，分别为 pd1，pd2，pd3，分别在 host1，host2，host3 上面。对于静态初始化，我们直接在三个 PD 启动的时候，给 initial-cluster 设置 pd1=http://host1:2380,pd2=http://host2:2380,pd3=http://host3:2380。对于动态初始化，我们先启动 pd1，然后启动 pd2，加入到 pd1 的集群里面，join 设置为 pd1=http://host1:2379。然后启动 pd3，加入到 pd1，pd2 形成的集群里面， join 设置为 pd1=http://host1:2379。可以看到，静态初始化和动态初始化完全走的是两个端口，而且这两个是互斥的，也就是我们只能使用一种方式来初始化集群。etcd 本身只支持 initial-cluster 的方式，但为了方便，PD 同时也提供了 join 的方式。join 主要是用了 etcd 自身提供的 member 相关 API，包括 add member，list member 等，所以我们使用 2379 端口，因为需要将命令发到 etcd 去执行。而 initial-cluster 则是 etcd 自身的初始化方式，所以使用的 2380 端口。相比于 initial-cluster，join 需要考虑非常多的 case（在 server/join.go prepareJoinCluster 函数里面有详细的解释），但 join 的使用非常自然，后续我们会考虑去掉 initial-cluster 的初始化方案。选举 当 PD 启动之后，我们就需要选出一个 leader 对外提供服务。虽然 etcd 自身也有 raft leader，但我们还是觉得使用自己的 leader，也就是 PD 的 leader 跟 etcd 自己的 leader 是不一样的。当 PD 启动之后，Leader 的选举如下： 检查当前集群是不是有 leader，如果有 leader，就 watch 这个 leader，只要发现 leader 掉了，就重新开始 1。 如果没有 leader，开始 campaign，创建一个 Lessor，并且通过 etcd 的事务机制写入相关信息，如下：// Create a lessor. ctx, cancel := context.WithTimeout(s.client.Ctx(), requestTimeout) leaseResp, err := lessor.Grant(ctx, s.cfg.LeaderLease) cancel() // The leader key must not exist, so the CreateRevision is 0. resp, err := s.txn(). If(clientv3.Compare(clientv3.CreateRevision(leaderKey), &amp;#34;=&amp;#34;, 0)). Then(clientv3.OpPut(leaderKey, s.leaderValue, clientv3.WithLease(clientv3.LeaseID(leaseResp.ID)))). Commit() 如果 leader key 的 CreateRevision 为 0，表明其他 PD 还没有写入，那么我就可以将我自己的 leader 相关信息写入，同时会带上一个 Lease。如果事务执行失败，表明其他的 PD 已经成为了 leader，那么就重新回到 1。 成为 leader 之后，我们对定期进行保活处理:// Make the leader keepalived. ch, err := lessor.KeepAlive(s.client.Ctx(), clientv3.LeaseID(leaseResp.ID)) if err != nil { return errors.Trace(err) } 当 PD 崩溃，原先写入的 leader key 会因为 lease 到期而自动删除，这样其他的 PD 就能 watch 到，重新开始选举。 初始化 raft cluster，主要是从 etcd 里面重新载入集群的元信息。拿到最新的 TSO 信息：// Try to create raft cluster. err = s.createRaftCluster() if err != nil { return errors.Trace(err) } log.Debug(&amp;#34;sync timestamp for tso&amp;#34;) if err = s.syncTimestamp(); err != nil { return errors.Trace(err) } 所有做完之后，开始定期更新 TSO，监听 lessor 是否过期，以及外面是否主动退出：for { select { case _, ok := &amp;lt;-ch: if !ok { log.Info(&amp;#34;keep alive channel is closed&amp;#34;) return nil } case &amp;lt;-tsTicker.C: if err = s.updateTimestamp(); err != nil { return errors.Trace(err) } case &amp;lt;-s.client.Ctx().Done(): return errors.New(&amp;#34;server closed&amp;#34;) } }  TSO 前面我们说到了 TSO，TSO 是一个全局的时间戳，它是 TiDB 实现分布式事务的基石。所以对于 PD 来说，我们首先要保证它能快速大量的为事务分配 TSO，同时也需要保证分配的 TSO 一定是单调递增的，不可能出现回退的情况。TSO 是一个 int64 的整形，它由 physical time + logical time 两个部分组成。Physical time 是当前 unix time 的毫秒时间，而 logical time 则是一个最大 1 &amp;lt;&amp;lt; 18 的计数器。也就是说 1ms，PD 最多可以分配 262144 个 TSO，这个能满足绝大多数情况了。对于 TSO 的保存于分配，PD 会做如下处理： 当 PD 成为 leader 之后，会从 etcd 上面获取上一次保存的时间，如果发现本地的时间比这个大，则会继续等待直到当前的时间大于这个值：last, err := s.loadTimestamp() if err != nil { return errors.Trace(err) } var now time.Time for { now = time.Now() if wait := last.Sub(now) + updateTimestampGuard; wait &amp;gt; 0 { log.Warnf(&amp;#34;wait %v to guarantee valid generated timestamp&amp;#34;, wait) time.Sleep(wait) continue } break } 当 PD 能分配 TSO 之后，首先会向 etcd 申请一个最大的时间，譬如，假设当前时间是 t1，每次最多能申请 3s 的时间窗口，PD 会向 etcd 保存 t1 + 3s 的时间值，然后 PD 就能在内存里面直接使用这一段时间窗口.当当前的时间 t2 大于 t1 + 3s 之后，PD 就会在向 etcd 继续更新为 t2 + 3s：if now.Sub(s.lastSavedTime) &amp;gt;= 0 { last := s.lastSavedTime save := now.Add(s.cfg.TsoSaveInterval.Duration) if err := s.saveTimestamp(save); err != nil { return errors.Trace(err) } } 这么处理的好处在于，即使 PD 当掉，新启动的 PD 也会从上一次保存的最大的时间之后开始分配 TSO，也就是 1 处理的情况。 因为 PD 在内存里面保存了一个可分配的时间窗口，所以外面请求 TSO 的时候，PD 能直接在内存里面计算 TSO 并返回。resp := pdpb.Timestamp{} for i := 0; i &amp;lt; maxRetryCount; i++ { current, ok := s.ts.Load().(*atomicObject) if !ok { log.Errorf(&amp;#34;we haven&amp;#39;t synced timestamp ok, wait and retry, retry count %d&amp;#34;, i) time.Sleep(200 * time.Millisecond) continue } resp.Physical = current.physical.UnixNano() / int64(time.Millisecond) resp.Logical = atomic.AddInt64(¤t.logical, int64(count)) if resp.Logical &amp;gt;= maxLogical { time.Sleep(updateTimestampStep) continue } return resp, nil } 因为是在内存里面计算的，所以性能很高，我们自己内部测试每秒能分配百万级别的 TSO。 如果 client 每次事务都向 PD 来请求一次 TSO，每次 RPC 的开销也是非常大的，所以 client 会批量的向 PD 获取 TSO。client 会首先收集一批事务的 TSO 请求，譬如 n 个，然后直接向 PD 发送命令，参数就是 n，PD 收到命令之后，会生成 n 个 TSO 返回给客户端。  心跳 在最开始我们说过，PD 所有关于集群的数据都是由 TiKV 主动心跳上报的，PD 对 TiKV 的调度也是在心跳的时候完成的。通常 PD 会处理两种心跳，一个是 TiKV 自身 store 的心跳，而另一个则是 store 里面 region 的 leader peer 上报的心跳。对于 store 的心跳，PD 在 handleStoreHeartbeat 函数里面处理，主要就是将心跳里面当前的 store 的一些状态缓存到 cache 里面。store 的状态包括该 store 有多少个 region，有多少个 region 的 leader peer 在该 store 上面等，这些信息都会用于后续的调度。对于 region 的心跳，PD 在 handleRegionHeartbeat 里面处理。这里需要注意，只有 leader peer 才会去上报所属 region 的信息，follower peer 是不会上报的。收到 region 的心跳之后，首先 PD 也会将其放入 cache 里面，如果 PD 发现 region 的 epoch 有变化，就会将这个 region 的信息也保存到 etcd 里面。然后，PD 会对这个 region 进行具体的调度，譬如发现 peer 数目不够，添加新的 peer，或者有一个 peer 已经坏了，删除这个 peer 等，详细的调度实现，我们会在后续讨论。这里再说一下 region 的 epoch，在 region 的 epoch 里面，有 conf_ver 和 version，分别表示这个 region 不同的版本状态。如果一个 region 发生了 membership changes，也就是新增或者删除了 peer，conf_ver 会加 1，如果 region 发生了 split 或者 merge，则 version 加 1。无论是 PD 还是在 TiKV，我们都是通过 epoch 来判断 region 是否发生了变化，从而拒绝掉一些危险的操作。譬如 region 已经发生了分裂，version 变成了 2，那么如果这时候有一个写请求带上的 version 是 1， 我们就会认为这个请求是 stale，会直接拒绝掉。因为 version 变化表明 region 的范围已经发生了变化，很有可能这个 stale 的请求需要操作的 key 是在之前的 region range 里面而没在新的 range 里面。Split / Merge 前面我们说了，PD 会在 region 的 heartbeat 里面对 region 进行调度，然后直接在 heartbeat 的返回值里面带上相关的调度信息，让 TiKV 自己去处理，TiKV 处理完成之后，通过下一个 heartbeat 重新上报，PD 就能知道是否调度成功了。对于 membership changes，比较容易，因为我们有最大副本数的配置，假设三个，那么当 region 的心跳上来，发现只有两个 peer，那么就 add peer，如果有四个 peer，就 remove peer。而对于 region 的 split / merge，则情况稍微要复杂一点，但也比较简单。注意，现阶段，我们只支持 split，merge 处于开发阶段，没对外发布，所以这里仅仅以 split 举例： 在 TiKV 里面，leader peer 会定期检查 region 所占用的空间是否超过某一个阀值，假设我们设置 region 的 size 为 64MB，如果一个 region 超过了 96MB， 就需要分裂。 Leader peer 会首先向 PD 发送一个请求分裂的命令，PD 在 handleAskSplit 里面处理，因为我们是一个 region 分裂成两个，对于这两个新分裂的 region，一个会继承之前 region 的所有的元信息，而另一个相关的信息，譬如 region ID，新的 peer ID，则需要 PD 生成，并将其返回给 leader。 Leader peer 写入一个 split raft log，在 apply 的时候执行，这样 region 就分裂成了两个。 分裂成功之后，TiKV 告诉 PD，PD 就在 handleReportSplit 里面处理，更新 cache 相关的信息，并持久化到 etcd。  路由 因为 PD 保存了所有 TiKV 的集群信息，自然对 client 提供了路由的功能。假设 client 要对 key 写入一个值。 client 先从 PD 获取 key 属于哪一个 region，PD 将这个 region 相关的元信息返回。 client 自己 cache，这样就不需要每次都从 PD 获取。然后直接给 region 的 leader peer 发送命令。 有可能 region 的 leader 已经漂移到其他 peer，TiKV 会返回 NotLeader 错误，并带上新的 leader 的地址，client 在 cache 里面更新，并重新向新的 leader 发送请求。 也有可能 region 的 version 已经变化，譬如 split 了，这时候，key 可能已经落入了新的 region 上面，client 会收到 StaleCommand 的错误，于是重新从 PD 获取，进入状态 1。  小结 PD 作为 TiDB 集群的中心调度模块，在设计上面，我们尽量保证无状态，方便扩展。本篇文章主要介绍了 PD 是如何跟 TiKV，TiDB 协作交互的。后面，我们会详细地介绍核心调度功能，也就是 PD 是如何控制整个集群的。"},
		{"url": "https://pingcap.com/weekly/2017-01-08-tidb-weekly/",
		"title": "Weekly update (January 02 ~ January 08, 2017)", 
		"content": " Weekly update in TiDB Last week, we landed 38 PRs in the TiDB repositories.Added  Add nested loop join. Support the UNIX_TIMESTAMP built-in function. Support the INTERVAL built-in function. Support the FIND_IN_SET built-in function. Support the DATEDIFF built-in function. Enable pushing down IF expr to TiKV coprocessor  Fixed  In prepared statement, limit and offset could be parameter marker. When creating table, index option could be a list. Fix a bug in creating table, some field length is missing. Fix a bug about parsing datetime overflow. Trim leading zeros before parsing int literal. Fix float truncate bug.  Improved  Improve TiDB schema lease checker. Speed up alter table add index statement. Refactor system variable related code. Refactor built-in function, add function class and function signature: #2361, #2384, #2385, #2389, #2391, #2399, #2410 Add unique key information into plan&amp;rsquo;s schema, this will be used for plan optimizing. Fetch tso and compiling statement concurrently: reduce the latency of small transaction. Load data in a batch way: make it easier for loading large data. Extract a built-in function factory for date arithmetic operations.  New contributor  idlesummerbreeze  Weekly update in TiKV Last week, We landed 15 PRs in the TiKV repositories.Added  Support pre vote feature for raft. Schedule replicas according to the location of the stores. Coprocessor support if, IsNull, IfNull and NullIf.  Fixed  Check cluseter ID to avoid PD joining different cluster.  Improved  Return StoreNotMatch error when store ID doesn&amp;rsquo;t match. Use upper bound for scanner. Unify logger to make the log format the same with TiDB/PD.  "},
		{"url": "https://pingcap.com/meetup/meetup-2017-01-07/",
		"title": "COISF 专场|PingCAP 第 36 期 NewSQL Meetup", 
		"content": " COISF 专场|PingCAP 第 36 期 NewSQL Meetup 2017-01-07 蔡杰明&amp;amp;袁进辉 PingCAP PingCAPPingCAP ![]() 微信号pingcap2015功能介绍PingCAP 专注于新型"},
		{"url": "https://pingcap.com/blog/2017-01-06-about-the-tidb-source-code/",
		"title": "About the TiDB Source Code", 
		"content": " The target audience of this document is the contributors in the TiDB community. The document aims to help them understand the TiDB project. It covers the system architecture, the code structure, and the execution process.Table of content  System architecture Overview of the code structure The protocol layer The SQL layer The optimizer The executor The distributed executor  System architecture As is shown in the architecture diagram, the TiDB Server is between the Load Balancer (or Application) and the storage engine layer at the bottom. Within the TiDB server, there are three layers: The MySQL Protocol layer This layer has two functions: At the beginning, it receives the requests from the MySQL client, parses the MySQL Protocol packages to the corresponding commands in TiDB Session. In the end, it transfers the result to the MySQL protocol format and returns it to the Client.   The SQL layer This layer has the following functions: Parse and execute the SQL statement Make and optimize the query plans Generate the optimizer Access data through the Storage Engine API layer Return the result to the MySQL Protocol layer This layer is very important and see The SQL layer for further information.  The Storage Engine API layer This layer provides transactional (distributed or standalone) storage. There is an abstraction layer between the KV layer and the SQL layer and it enables the SQL layer to see the unified interface and ignore the differences among the KV storage engines.  Overview of the code structure See the following list for all the packages and their main functions: tidbThis package can be considered to be the interface between the MySQL Protocol Layer and the SQL layer. There are three main files: session.go: Each session object corresponds to a connection of the MySQL client. The MySQL protocol layer manages the binding between the connection and the session. All the MySQL queries/commands are executed by calling the Session interface. tidb.go: This file includes some functions to be called by session.go. bootstrap.go: If a TiDB Server is started but the system is not yet initialized, the bootstrap.go file will initiate the system. See the following section for the detailed information.  docs  This package contains some brief documents of TiDB. See Documents for all the detailed documents. executorThis is the TiDB executor. A SQL statement will be transferred to the combination a series of executors (operators). The main interface exposed in this package is Executor:  type Executor interface { // Returns the next row of data (If the result is empty, then there is no more data)  Next() (*Row, error) // Close the current Executor and clean up  Close() error //Change the result Schema from the executor, including the details of each Field  Schema() expression.Schema } All kinds of executors implement this interface. The executing engine in TiDB adopts the Volcano model where the executors interact with each other through the above 3 interfaces. Each executor only needs to access the data through the Next interface and the meta data through the Schema interface. planThis is the core of the entire SQL layer. After a SQL statement is parsed to an abstract syntax tree (AST), the query plan is generated and optimized (including logical optimization and physical optimization) in this package.The following functions are also included in this package: validator.go: Validates the AST. preprocess.go: Currently, there is only name resolve. resolver.go: Parses the name. To parse and bind the identifier of database/table/column/alias to the corresponding column or Field. typeinferer.go: Infers the type of the result. For SQL statements, the type of the result does not need inference. logical_plan_builder.go: Makes optimized logical query plans. physical_plan_builder.go: Makes the physical query plans based on the logical plans.  privilegeThe authority control related interface which is implemented in the privilege/privileges directory. sessionctxStores the state information in the session, such as the session variables. The information can be obtained from the session. It is included in a separate directory for clear dependency and to avoid the problems of circular dependencies. tableThe table interface which is a layer of abstraction of the tables in the database. It provides many operations to the table such as getting the information of the column or reading a row of data. The implementation is in the table/tables directory.The directory also includes the abstraction of Column and Index. tidb-serverThe main.go file of the TiDB Server which is mainly the codes to start the server. serverThe implementation of the MySQL Protocol which is to parse the protocol and to pass the command/query. astThe SQL statement will be parsed to be an abstract syntax tree. The data structure is defined in the ast directory. Each node must implement the visitor interface and call the Accept method in the node to traverse the tree.If new syntax support is needed, besides adding the rule to parser, you also need to add the data structure to this directory. ddlThe related codes for asynchronous schema changes which is similar to the implementation in Google F1. domaindomain can be considered as a storage space where databases and tables are created. Somewhat like Name Space, the databases with the same name can exist in different domains. Domains bind with the information schema detail. expressionThe definition of the expressions. See the following for the most important interface:  type Expression interface { ..... } The following lists the expressions that implement the interface: Scalar Function: Scalar Function expressions Aggregate Function: Aggregate Function expressions Column: Column expressions Const: Constant expressions infoschemaThe implementation of InformationSchema which provides the details of the db/table/column. kvKey-Value related interface definition and some of the implementations, including Retriever / Mutator / Transaction / Snapshot / Storage / Iterator, etc. A unified abstraction of the underlying Key-Value storages. modelThe DDL / DML related data structure supported by TiDB, including DBInfo / TableInfo / ColumnInfo / IndexInfo, etc. parserThe syntax parsing module, including lexical analysis (lexer.go) and syntax analysis (parser.y). The main interface to the external is Parse () which is to parse the SQL text into AST. storeThe implementation of the Key-Value store at the bottom. If you want to plug in a new storage engines, you can package the storage engine and put the code in this package. The new storage engine needs to implement the interface defined in the kv package.Currently, there are two storage engines: TiKV, a distributed storage engine, and localstore/{goleveldb/boltdb}, a stand-alone storage engine.For more information about the KV and store, see How to Plug in a New Storage Engine (Currently in Chinese). terrorThe error system for TiDB. For more information, see Detailed specification (Currently in Chinese). contextThe context interface. Session is the implementation of the context interface. The reason that we have an interface is to avoid the circular dependencies. All the state information of session can be accessed using this interface. inspectkvThe auxiliary check package for TiDB SQL data and Key-Value storage. In the future, it will be used to access TiKV from the external and will be re-defined and developed. metaThe definition of the metadata related constants and common functions for TiDB. In meta/autoid, an API is defined for ID auto-increment within a globally unique session. The meta information depends on this tool. mysqlMySQL related constant definitions. structureA layer of encapsulation on top of Key-Value which supports rich Key-Value types, such as string, list, hash, etc. The package is mainly used in asynchronous Schema changes. utilSome utility classes. The 7 package is very important because it contains the …"},
		{"url": "https://pingcap.com/blog-cn/the-design-and-implementation-of-multi-raft/",
		"title": "TiKV 源码解析系列 - multi-raft 设计与实现", 
		"content": " 概述 本文档主要面向 TiKV 社区开发者，主要介绍 TiKV 的系统架构，源码结构，流程解析。目的是使得开发者阅读文档之后，能对 TiKV 项目有一个初步了解，更好的参与进入 TiKV 的开发中。需要注意，TiKV 使用 Rust 语言编写，用户需要对 Rust 语言有一个大概的了解。另外，本文档并不会涉及到 TiKV 中心控制服务 Placement Driver(PD) 的详细介绍，但是会说明一些重要流程 TiKV 是如何与 PD 交互的。TiKV 是一个分布式的 KV 系统，它采用 Raft 协议保证数据的强一致性，同时使用 MVCC + 2PC 的方式实现了分布式事务的支持。架构 TiKV 的整体架构比较简单，如下：Placement Driver : Placement Driver (PD) 负责整个集群的管理调度。Node : Node 可以认为是一个实际的物理机器，每个 Node 负责一个或者多个 Store。Store : Store 使用 RocksDB 进行实际的数据存储，通常一个 Store 对应一块硬盘。Region : Region 是数据移动的最小单元，对应的是 Store 里面一块实际的数据区间。每个 Region 会有多个副本（replica），每个副本位于不同的 Store ，而这些副本组成了一个 Raft group。Raft TiKV 使用 Raft 算法实现了分布式环境下面数据的强一致性，关于 Raft，可以参考论文 “In Search of an Understandable Consensus Algorithm” 以及官网，这里不做详细的解释。简单理解，Raft 是一个 replication log + State Machine 的模型，我们只能通过 leader 进行写入，leader 会将 command 通过 log 的形式复制到 followers，当集群的大多数节点都收到了这个 log，我们就认为这个 log 是 committed，可以 apply 到 State Machine 里面。TiKV 的 Raft 主要移植 etcd Raft，支持 Raft 所有功能，包括： Leader election Log replicationLog compaction Membership changesLeader transfer Linearizable / Lease read  这里需要注意，TiKV 以及 etcd 对于 membership change 的处理，跟 Raft 论文是稍微有一点不一样的，主要在于 TiKV 的 membership change 只有在 log applied 的时候生效，这样主要的目的是为了实现简单，但有一个风险在于如果我们只有两个节点，要从里面移掉一个节点，如果一个 follower 还没收到 ConfChange 的 log entry，leader 就当掉并且不可恢复了，整个集群就没法工作了。所以通常我们都建议用户部署 3 个或者更多个奇数个节点。Raft 库是一个独立的库，用户也可以非常方便的将其直接嵌入到自己的应用程序，而仅仅只需要自行处理存储以及消息的发送。这里简单介绍一下如何使用 Raft，代码在 TiKV 源码目录的 /src/raft 下面。Storage 首先，我们需要定义自己的 Storage，Storage 主要用来存储 Raft 相关数据，trait 定义如下：pub trait Storage { fn initial_state(&amp;amp;self) -&amp;gt; Result&amp;lt;RaftState&amp;gt;; fn entries(&amp;amp;self, low: u64, high: u64, max_size: u64) -&amp;gt; Result&amp;lt;Vec&amp;lt;Entry&amp;gt;&amp;gt;; fn term(&amp;amp;self, idx: u64) -&amp;gt; Result&amp;lt;u64&amp;gt;; fn first_index(&amp;amp;self) -&amp;gt; Result&amp;lt;u64&amp;gt;; fn last_index(&amp;amp;self) -&amp;gt; Result&amp;lt;u64&amp;gt;; fn snapshot(&amp;amp;self) -&amp;gt; Result&amp;lt;Snapshot&amp;gt;; } 我们需要实现自己的 Storage trait，这里详细解释一下各个接口的含义：initial_state：初始化 Raft Storage 的时候调用，它会返回一个 RaftState，RaftState 的定义如下：pub struct RaftState { pub hard_state: HardState, pub conf_state: ConfState, } HardState 和 ConfState 是 protobuf，定义：message HardState { optional uint64 term = 1; optional uint64 vote = 2; optional uint64 commit = 3; } message ConfState { repeated uint64 nodes = 1; } 在 HardState 里面，保存着该 Raft 节点最后一次保存的 term 信息，之前 vote 的哪一个节点，以及已经 commit 的 log index。而 ConfState 则是保存着 Raft 集群所有的节点 ID 信息。在外面调用 Raft 相关逻辑的时候，用户需要自己处理 RaftState 的持久化。entries: 得到 [low, high) 区间的 Raft log entry，通过 max_size 来控制最多返回多少个 entires。term，first_index 和 last_index 分别是得到当前的 term，以及最小和最后的 log index。snapshot：得到当前的 Storage 的一个 snapshot，有时候，当前的 Storage 数据量已经比较大，生成 snapshot 会比较耗时，所以我们可能得在另一个线程异步去生成，而不用阻塞当前 Raft 线程，这时候，可以返回 SnapshotTemporarilyUnavailable 错误，这时候，Raft 就知道正在准备 snapshot，会一段时间之后再次尝试。需要注意，上面的 Storage 接口只是 Raft 库需要的，实际我们还会用这个 Storage 存储 raft log 等数据，所以还需要单独提供其他的接口。在 Raft storage.rs 里面，我们提供了一个 MemStorage，用于测试，大家也可以参考 MemStorage 来实现自己的 Storage。Config 在使用 Raft 之前，我们需要知道 Raft 一些相关的配置，在 Config 里面定义，这里只列出需要注意的：pub struct Config { pub id: u64, pub election_tick: usize, pub heartbeat_tick: usize, pub applied: u64, pub max_size_per_msg: u64, pub max_inflight_msgs: usize, } id: Raft 节点的唯一标识，在一个 Raft 集群里面，id 是不可能重复的。在 TiKV 里面，id 的通过 PD 来保证全局唯一。election_tick：当 follower 在 election_tick 的时间之后还没有收到 leader 发过来的消息，那么就会重新开始选举，TiKV 默认使用 50。heartbeat_tick: leader 每隔 hearbeat_tick 的时间，都会给 follower 发送心跳消息。默认 10。applied: applied 是上一次已经被 applied 的 log index。max_size_per_msg: 限制每次发送的最大 message size。默认 1MB。max_inflight_msgs: 限制复制时候最大的 in-flight 的 message 的数量。默认 256。这里详细解释一下 tick 的含义，TiKV 的 Raft 是定时驱动的，假设我们每隔 100ms 调用一次 Raft tick，那么当调用到 headtbeat_tick 的 tick 次数之后，leader 就会给 follower 发送心跳。RawNode 我们通过 RawNode 来使用 Raft，RawNode 的构造函数如下：pub fn new(config: &amp;amp;Config, store: T, peers: &amp;amp;[Peer]) -&amp;gt; Result&amp;lt;RawNode&amp;lt;T&amp;gt;&amp;gt; 我们需要定义 Raft 的 Config，然后传入一个实现好的 Storage，peers 这个参数只是用于测试，实际要传空。生成好 RawNode 对象之后，我们就可以使用 Raft 了。我们关注如下几个函数：tick: 我们使用 tick 函数定期驱动 Raft，在 TiKV，我们每隔 100ms 调用一次 tick。propose: leader 通过 propose 命令将 client 发过来的 command 写入到 raft log，并复制给其他节点。propose_conf_change: 跟 propose 类似，只是单独用来处理 ConfChange 命令。step: 当节点收到其他节点发过来的 message，主动调用驱动 Raft。has_ready: 用来判断一个节点是不是 ready 了。ready: 得到当前节点的 ready 状态，我们会在之前用 has_ready 来判断一个 RawNode 是否 ready。apply_conf_change: 当一个 ConfChange 的 log 被成功 applied，需要主动调用这个驱动 Raft。advance: 告诉 Raft 已经处理完 ready，开始后续的迭代。对于 RawNode，我们这里重点关注下 ready 的概念，ready 的定义如下：pub struct Ready { pub ss: Option&amp;lt;SoftState&amp;gt;, pub hs: Option&amp;lt;HardState&amp;gt;, pub entries: Vec&amp;lt;Entry&amp;gt;, pub snapshot: Snapshot, pub committed_entries: Vec&amp;lt;Entry&amp;gt;, pub messages: Vec&amp;lt;Message&amp;gt;, } ss: 如果 SoftState 变更，譬如添加，删除节点，ss 就不会为空。hs: 如果 HardState 有变更，譬如重新 vote，term 增加，hs 就不会为空。entries: 需要在 messages 发送之前存储到 Storage。snapshot: 如果 snapshot 不是 empty，则需要存储到 Storage。committed_entries: 已经被 committed 的 raft log，可以 apply 到 State Machine 了。messages: 给其他节点发送的消息，通常需要在 entries 保存成功之后才能发送，但对于 leader 来说，可以先发送 messages，在进行 entries 的保存，这个是 Raft 论文里面提到的一个优化方式，TiKV 也采用了。当外部发现一个 RawNode 已经 ready 之后，得到 Ready，处理如下： 持久化非空的 ss 以及 hs。 如果是 leader，首先发送 messages。 如果 snapshot 不为空，保存 snapshot 到 Storage，同时将 snapshot 里面的数据异步应用到 State Machine（这里虽然也可以同步 apply，但 snapshot 通常比较大，同步会 block 线程）。 将 entries 保存到 Storage 里面。 如果是 follower，发送 messages。 将 committed_entries apply 到 State Machine。 调用 advance 告知 Raft 已经处理完 ready。  Placement Driver 在继续之前，我们先简单介绍一下 Placement Driver(PD)。PD 是 TiKV 的全局中央控制器，存储整个 TiKV 集群的元数据信息，负责整个 TiKV 集群的调度，全局 ID 的生成，以及全局 TSO 授时等。PD 是一个非常重要的中心节点，它通过集成 etcd，自动的支持了分布式扩展以及 failover，解决了单点故障问题。关于 PD 的详细介绍，后续我们会新开一篇文章说明。在 TiKV 里面，跟 PD 的交互是放在源码的 pd 目录下，现在跟 PD 的交互都是通过自己定义的 RPC 实现，协议非常简单，在 pd/mod.rs 里面我们直接提供了用于跟 PD 进行交互的 Client trait，以及实现了 RPC Client。PD 的 Client trait 非常简单，多数都是对集群元信息的 set/get 操作，需要额外注意的几个：bootstrap_cluster：当我们启动一个 TiKV 服务的时候，首先需要通过 is_cluster_bootstrapped 来判断整个 TiKV 集群是否已经初始化，如果还没有初始化，我们就会在该 TiKV 服务上面创建第一个 region。region_heartbeat：定期 Region 向 PD 汇报自己的相关信息，供 PD 做后续的调度。譬如，如果一个 Region 给 PD 上报的 peers 的数量小于预设的副本数，那么 PD 就会给这个 Region 添加一个新的副本 Peer。store_heartbeat：定期 store 向 PD 汇报自己的相关信息，供 PD 做后续调度。譬如，Store 会告诉 PD 当前的磁盘大小，以及剩余空间，如果 PD 发现空间不够了，就不会考虑将其他的 Peer 迁移到这个 Store 上面。ask_split/report_split：当 Region 发现自己需要 split 的时候，就 ask_split 告诉 PD，PD 会生成新分裂 Region 的 ID ，当 Region 分裂成功之后，会 report_split 通知 PD。注意，后面我们会让 PD 支持 gRPC 协议，所以 Client API 到时候可能会有变更。Raftstore 因为 TiKV 目标是支持 100 TB+ 以上的数据，一个 Raft 集群是铁定没法支持这么多数据的，所以我们需要使用多个 Raft 集群，也就是 Multi Raft。在 TiKV 里面，Multi Raft 的实现是在 Raftstore 完成的，代码在 raftstore/store 目录。Region 因为我们要支持 Multi Raft，所以我们需要将数据进行分片处理，让每个 Raft 单独负责一部分数据。通常的数据分片算法就是 Hash 和 Range，TiKV 使用的 Range 来对数据进行数据分片。为什么使用 Range，主要原因是能更好的将相同前缀的 key 聚合在一起，便于 scan 等操作，这个 Hash 是没法支持的，当然，在 split/merge 上面 Range 也比 Hash 好处理很多，很多时候只会涉及到元信息的修改，都不用大范围的挪动数据。当然，Range 有一个问题在于很有可能某一个 Region 会因为频繁的操作成为性能热点，当然也有一些优化的方式，譬如通过 PD 将这些 Region 调度到更好的机器上面，提供 Follower 分担读压力等。总之，在 TiKV 里面，我们使用 Range 来对数据进行切分，将其分成一个一个的 Raft Group，每一个 Raft Group，我们使用 Region 来表示。Region 的 protobuf 协议定义如下：message Region { optional uint64 id = 1 [(gogoproto.nullable) = false]; optional bytes start_key = 2; optional bytes end_key = 3; optional RegionEpoch region_epoch = 4; repeated Peer peers = 5; } message RegionEpoch { optional uint64 conf_ver	= 1 [(gogoproto.nullable) = false]; optional uint64 version = 2 [(gogoproto.nullable) = false]; } message Peer { optional uint64 id = 1 [(gogoproto.nullable) = false]; optional uint64 store_id = 2 [(gogoproto.nullable) = false]; } id：Region 的唯一表示，通过 PD 全局唯一分配。start_key, end_key：用来表示这个 Region 的范围 [start_key, end_key)，对于最开始的 region，start 和 end key 都是空，TiKV 内部会特殊处理。region_epoch：当一个 Region 添加或者删除 Peer，或者 split 等，我们就会认为这个 Region 的 epoch 发生的变化，RegionEpoch 的 conf_ver 会在每次做 ConfChange 的时候递增，而 version 则是会在每次做 split/merge 的时候递增。peers：当前 Region 包含的节点信息。对于一个 Raft Group，我们通常有三个副本，每个副本我们使用 Peer 来表示，Peer 的 id 也是全局由 PD 分配，而 store_id 则表明这个 Peer 在哪一个 Store 上面。RocksDB / Keys Prefix 对于实际数据存储，无论是 Raft Meta，Log，还是 State Machine 的 data，我们都存到一个 RocksDB 实例里面。关于 RocksDB，可以详细参考 facebook/rocksdb。我们使用不同的前缀来对 Raft 以及 State Machine 等数据进行区分，具体可以参考 raftstore/store/keys.rs，对于 State Machine 实际的 data 数据，我们统一添加 ‘z’ 前缀。而对于其他会存在本地的元数 …"},
		{"url": "https://pingcap.com/weekly/2017-01-01-tidb-weekly/",
		"title": "Weekly update (December 26 ~ January 01, 2017)", 
		"content": " Weekly update in TiDB Last week, we landed 28 PRs in the TiDB repositories.Added  Support the CHAR_LENGTH built-in function. Support the CRC32 built-in function. Support the LEAST built-in function.  Fixed  Fix a bug in Add Column with invalid default value. Fix a bug about parsing string to float. Fix a bug when using int and uint as join key. Fix a bug in MySQL Protocol layer about prepared statement.  Improved  Improve string to date parser. Improve TiDB schema lease checker. Refactor optimizer: #2321, #2322 Set a limitation on the quantity of the data in a single transaction. Improve mocked tikv: Split data in a table into multiple mocked regions. This will used in unit tests. Speed up creating table: In some cases, it is not necessary to wait a long time to create table. Use MySQL standard error code when meeting incorrect function argument count error. Find a better way to handle StoreNotMatch error. Refactor built-in function: #2343, #2344, #2362, #2367 Clean up tidb-server codes: #2356, #2358  New contributor  Zyguan AndreMouche  Weekly update in TiKV Last week, We landed 19 PRs in the TiKV repositories.Added  Move raw_get to thread pool. Add ResourceKind for operator and don&amp;rsquo;t resend duplicated AddPeer response when the peer is still pending, see #449. Add member command for pd-ctl.  Fixed  Fix getting valid float.  Improved  Update default configuration to speed up scheduler. Don&amp;rsquo;t panic when receiving stale snapshot. Remove unnecessary region cache. Remove no used constraint feature. Exit with error message if clusert ID mismatches directly. Add replication section in configuration.  "},
		{"url": "https://pingcap.com/blog-cn/tikv-how-to-use-raft/",
		"title": "TiKV 源码解析系列 - 如何使用 Raft", 
		"content": "  本系列文章主要面向 TiKV 社区开发者，重点介绍 TiKV 的系统架构，源码结构，流程解析。目的是使得开发者阅读之后，能对 TiKV 项目有一个初步了解，更好的参与进入 TiKV 的开发中。需要注意，TiKV 使用 Rust 语言编写，用户需要对 Rust 语言有一个大概的了解。另外，本系列文章并不会涉及到 TiKV 中心控制服务 Placement Driver(PD) 的详细介绍，但是会说明一些重要流程 TiKV 是如何与 PD 交互的。TiKV 是一个分布式的 KV 系统，它采用 Raft 协议保证数据的强一致性，同时使用 MVCC + 2PC 的方式实现了分布式事务的支持。 架构 TiKV 的整体架构比较简单，如下：Placement Driver : Placement Driver (PD) 负责整个集群的管理调度。 Node : Node 可以认为是一个实际的物理机器，每个 Node 负责一个或者多个 Store。 Store : Store 使用 RocksDB 进行实际的数据存储，通常一个 Store 对应一块硬盘。 Region : Region 是数据移动的最小单元，对应的是 Store 里面一块实际的数据区间。每个 Region会有多个副本（replica），每个副本位于不同的 Store ，而这些副本组成了一个 Raft group。Raft TiKV 使用 Raft 算法实现了分布式环境下面数据的强一致性，关于 Raft，可以参考论文 “In Search of an Understandable Consensus Algorithm” 以及官网，这里不做详细的解释。简单理解，Raft 是一个 replication log + State Machine 的模型，我们只能通过 leader 进行写入，leader 会将 command 通过 log 的形式复制到 followers，当集群的大多数节点都收到了这个 log，我们就认为这个 log 是 committed，可以 apply 到 State Machine 里面。TiKV 的 Raft 主要移植 etcd Raft，支持 Raft 所有功能，包括： Leader election Log replicationLog compaction Membership changesLeader transfer Linearizable / Lease read  这里需要注意，TiKV 以及 etcd 对于 membership change 的处理，跟 Raft 论文是稍微有一点不一样的，主要在于 TiKV 的 membership change 只有在 log applied 的时候生效，这样主要的目的是为了实现简单，但有一个风险在于如果我们只有两个节点，要从里面移掉一个节点，如果一个 follower 还没收到 ConfChange 的 log entry，leader 就当掉并且不可恢复了，整个集群就没法工作了。所以通常我们都建议用户部署 3 个或者更多个奇数个节点。Raft 库是一个独立的库，用户也可以非常方便的将其直接嵌入到自己的应用程序，而仅仅只需要自行处理存储以及消息的发送。这里简单介绍一下如何使用 Raft，代码在 TiKV 源码目录的 /src/raft 下面。Storage 首先，我们需要定义自己的 Storage，Storage 主要用来存储 Raft 相关数据，trait 定义如下：我们需要实现自己的 Storage trait，这里详细解释一下各个接口的含义：initial_state：初始化 Raft Storage 的时候调用，它会返回一个 RaftState，RaftState 的定义如下：HardState 和 ConfState 是 protobuf，定义：在 HardState 里面，保存着该 Raft 节点最后一次保存的 term 信息，之前 vote 的哪一个节点，以及已经 commit 的 log index。而 ConfState 则是保存着 Raft 集群所有的节点 ID 信息。在外面调用 Raft 相关逻辑的时候，用户需要自己处理 RaftState 的持久化。entries: 得到 [low, high) 区间的 Raft log entry，通过 max_size 来控制最多返回多少个 entires。term，first_index 和 last_index 分别是得到当前的 term，以及最小和最后的 log index。snapshot：得到当前的 Storage 的一个 snapshot，有时候，当前的 Storage 数据量已经比较大，生成 snapshot 会比较耗时，所以我们可能得在另一个线程异步去生成，而不用阻塞当前 Raft 线程，这时候，可以返回 SnapshotTemporarilyUnavailable 错误，这时候，Raft 就知道正在准备 snapshot，会一段时间之后再次尝试。需要注意，上面的 Storage 接口只是 Raft 库需要的，实际我们还会用这个 Storage 存储 raft log 等数据，所以还需要单独提供其他的接口。在 Raft storage.rs 里面，我们提供了一个 MemStorage，用于测试，大家也可以参考 MemStorage 来实现自己的 Storage。Config 在使用 Raft 之前，我们需要知道 Raft 一些相关的配置，在 Config 里面定义，这里只列出需要注意的：id: Raft 节点的唯一标识，在一个 Raft 集群里面，id 是不可能重复的。在 TiKV 里面，id 的通过 PD 来保证全局唯一。election_tick：当 follower 在 election_tick 的时间之后还没有收到 leader 发过来的消息，那么就会重新开始选举，TiKV 默认使用 50。heartbeat_tick: leader 每隔 hearbeat_tick 的时间，都会给 follower 发送心跳消息。默认 10。applied: applied 是上一次已经被 applied 的 log index。max_size_per_msg: 限制每次发送的最大 message size。默认 1MB。max_inflight_msgs: 限制复制时候最大的 in-flight 的 message 的数量。默认 256。这里详细解释一下 tick 的含义，TiKV 的 Raft 是定时驱动的，假设我们每隔 100ms 调用一次 Raft tick，那么当调用到 headtbeat_tick 的 tick 次数之后，leader 就会给 follower 发送心跳。RawNode 我们通过 RawNode 来使用 Raft，RawNode 的构造函数如下：我们需要定义 Raft 的 Config，然后传入一个实现好的 Storage，peers 这个参数只是用于测试，实际要传空。生成好 RawNode 对象之后，我们就可以使用 Raft 了。我们关注如下几个函数：tick: 我们使用 tick 函数定期驱动 Raft，在 TiKV，我们每隔 100ms 调用一次 tick。propose: leader 通过 propose 命令将 client 发过来的 command 写入到 raft log，并复制给其他节点。propose_conf_change: 跟 propose 类似，只是单独用来处理 ConfChange 命令。step: 当节点收到其他节点发过来的 message，主动调用驱动 Raft。has_ready: 用来判断一个节点是不是 ready 了。ready: 得到当前节点的 ready 状态，我们会在之前用 has_ready 来判断一个 RawNode 是否 ready。apply_conf_change: 当一个 ConfChange 的 log 被成功applied，需要主动调用这个驱动 Raft。advance: 告诉 Raft 已经处理完 ready，开始后续的迭代。对于 RawNode，我们这里重点关注下 ready 的概念，ready 的定义如下：ss: 如果 SoftState 变更，譬如添加，删除节点，ss 就不会为空。hs: 如果 HardState 有变更，譬如重新 vote，term 增加，hs 就不会为空。entries: 需要在 messages 发送之前存储到 Storage。snapshot: 如果 snapshot 不是 empty，则需要存储到 Storage。committed_entries: 已经被 committed 的 raft log，可以 apply 到 State Machine 了。messages: 给其他节点发送的消息，通常需要在 entries 保存成功之后才能发送，但对于 leader 来说，可以先发送 messages，在进行 entries 的保存，这个是 Raft 论文里面提到的一个优化方式，TiKV 也采用了。当外部发现一个 RawNode 已经 ready 之后，得到 Ready，处理如下： 持久化非空的 ss 以及 hs。 如果是 leader，首先发送 messages。 如果 snapshot 不为空，保存 snapshot 到 Storage，同时将 snapshot 里面的数据异步应用到 State Machine（这里虽然也可以同步 apply，但 snapshot 通常比较大，同步会 block 线程）。 将 entries 保存到 Storage 里面。 如果是 follower，发送 messages。 将 committed_entries apply 到 State Machine。 调用 advance 告知 Raft 已经处理完 ready。  #####-第一部分完结-"},
		{"url": "https://pingcap.com/weekly/2016-12-26-tidb-weekly/",
		"title": "Weekly update (December 19 ~ December 25, 2016)", 
		"content": " New Release TiDB RC1 is released!Weekly update in TiDB Last week, we landed 34 PRs in the TiDB repositories.Added  Support the RPAD built-in function.. Support the show keys from table from database statement.  Fixed  Retry infinite times if the commit primary key times out. Do not push aggregation down to the memory tables. Fix a bug about the alter table statement.  Improved  Refactor the time type related code: #2259, #2280, #2284, #2289, #2292 Refactor optimizer: extract initialization related code into physical Initialization. Speed up the DDL statement. Avoid generating parser.go every time. Skip the constraint check for prewrite to improve the loading data speed. Speed up the add index statement.  New contributor  silentred  Weekly update in TiKV Last week, We landed 14 PRs in the TiKV repositories.Added  Add configuration to control the replica scheduling speed. Skip constraint check for prewrite to improve the loading data speed. Add region and store commands to pd-ctl. Add configuration to cache index and filter blocks in the block cache.  Fixed  Report snapshot sending status reliably to fix #1377. Store short value in write cf directly to save space and improve performance.  Improved  Remove unnecessary admin operators. Handle Raft ready append login one WriteBatch to reduce the CPU usage and improve performance. Remove the down peer first when scheduling replicas.  "},
		{"url": "https://pingcap.com/meetup/meetup-2016-12-24/",
		"title": "COISF 专场|PingCAP 第 35 期 NewSQL Meetup", 
		"content": " COISF 专场|PingCAP 第 35 期 NewSQL Meetup 2016-12-24 张頔&amp;amp;黄梦龙 PingCAP PingCAPPingCAP ![]() 微信号pingcap2015功能介绍PingCAP 专注于新型"},
		{"url": "https://pingcap.com/blog/2016-12-23-rc1/",
		"title": "TiDB RC1 Release Notes", 
		"content": " The TiDB RC1 is now released. See the following updates in this release:TiKV:  The write speed has been improved. The disk space usage is reduced. Hundreds of TBs of data can be supported. The stability is improved and TiKV can support a cluster with 200 nodes. Supports the Raw KV API and the Golang client.  Placement Driver (PD): + The scheduling strategy framework is optimized and now the strategy is more flexible and reasonable. + The support for label is added to support Cross Data Center scheduling. + PD Controller is provided to operate the PD cluster more easily.TiDB:  The following features are added or improved in the SQL query optimizer:  Eager aggregation More detailed EXPLAIN information Parallelization of the UNION operator Optimization of the subquery performance Optimization of the conditional push-down Optimization of the Cost Based Optimizer (CBO) framework  The implementation of the time related data types are refactored to improve the compatibility with MySQL. More built-in functions in MySQL are supported. The speed of the add index statement is enhanced. The following statements are supported:  Use the CHANGE COLUMN statement to change the name of a column. Use MODIFY COLUMN and CHANGE COLUMN of the ALTER TABLE statement for some of the column type transfer.    New Tools:  Loader is added to be compatible with the mydumper data format in Percona and provides the following functions:  Multi-thread import Retry if error occurs Breakpoint resume Targeted optimization for TiDB  The tool for one-click deployment is added.  "},
		{"url": "https://pingcap.com/blog/2016-12-19-adding-built-in-function/",
		"title": "Adding Built-in Functions", 
		"content": " This document describes how to add built-in functions to TiDB. Background The procedure to add a built-in function Example  Background How is the SQL statement executed in TiDB?The SQL statement is parsed to an abstract syntax tree (AST) by the parser first and then uses the optimizer to generate an execution plan. The plan can then be executed to get the result. This process involves how to access the data in the table, and how to filter, calculate, sort, aggregate, and distinct the data, etc. For a built-in function, the most important part is to parse and to evaluate. See the following two sections for further details:Parse The code for syntax parsing is in the parser directory and mainly involves the two files: misc.go and parser.y. In the TiDB project, run the make parser command to use goyacc to convert the parser.y file to the parser.go file. The code in the parser.go file can be called by other go code for parsing.The process to parse the SQL statement to be structured is as follows: Use Scanner to segment the text to tokens. Each token has a name and value. The name is used to match the pre-defined rules in parser.y in the parser. When the rules are being matched, tokens are obtained continuously from the Scanner. If a rule is completely matched, the token that is matched will be replaced by a new variable. Meanwhile, after each rule is matched, the value in the token can be used to construct the node of subtree in AST. The general format of a built-in function is: name(args). Scanner needs to recognize all the elements of the function, including the name, the parenthesis, and the arguments. The pre-defined rule to be matched in the parser constructs a node in AST. The node contains the arguments and the method for evaluation of the function for the following evaluation.  Evaluation To evaluate is to get the value of the function or the expression based on the input arguments and the runtime environment. The controlling logic is in the evaluator/evaluator.go file. Most of the built-in functions are parsed to FuncCallExpr. The process to evaluate is as follows: Convert ast.FuncCallExpr to expression.ScalarFunction. Call the NewFunction() method in the expression/scalar_function.go. Use FnName to find the corresponding function in the builtin.Funcs table which is in the evaluator/builtin.go file. Call the evaluation function when evaluating ScalarFunction.  The procedure to add a built-in function 1. Edit the misc.go and parser.y files. 1). Add a rule to the tokenMap in the misc.go file and parse the function name to a token.2). Add a rule to parser.y and transfer the token sequence to an AST node.3). Add a unit test case for parser in the parser_test.go file.2. Add the evaluation function to the executor directory. 1). Implement the function in the evaluator/builtin_xx.go file.Note: The functions in the executor directory are categorized to several files. For example, builtin_time.go is a time-related function. The interface of the function is:type BuiltinFunc func([]types.Datum, context.Context) (types.Datum, error) 2). Register the name and the implementation to builtin.Funcs.3. Add the Type Inference information to the plan/typeinferer.go file. Add the type of the returned result of the function to handleFuncCallExpr() in the the plan/typeinferer.go file and make sure the result is consistent with the result in MySQL. See MySQL Const for the complete list of the type definition.4. Add a unit test case for the function to the evaluator directory. 5. Run the make dev command and make sure all the test cases can pass. Example Take the Pull Request to add the timediff() function as an example:1. Add an entry to the tokenMap in the misc.go file: var tokenMap = map[string]int{ &amp;#34;TIMEDIFF&amp;#34;: timediff, } Here, a rule is defined: If the text is found to be timediff, it is converted to a token with the name timediff.Note: SQL is case-insensitive, so the capital letters must be used in the tokenMap.The text in tokenMap must be taken as a special token instead of an identifier. In the following parser rule, the token needs special processing as is shown in parser/parser.y:%token &amp;lt;ident&amp;gt; timediff &amp;#34;TIMEDIFF&amp;#34;  Which means after the &amp;ldquo;timediff&amp;rdquo; token is obtained from the lexer, it is named &amp;ldquo;TIMEDIFF” and this name will be used for the following rule matching.The &amp;ldquo;timediff&amp;rdquo; here must correspond to the &amp;ldquo;timediff&amp;rdquo; of the value in tokenMap. When the parser.y file is generated to the parser.go file, &amp;ldquo;timediff&amp;rdquo; will get a token ID which is an INT.Because &amp;ldquo;timediff&amp;rdquo; is not a keyword in MySQL, the rule is added to FunctionCallNonKeyword in the parser.y file:```	|	&amp;ldquo;TIMEDIFF&amp;rdquo; &amp;lsquo;(&amp;rsquo; Expression &amp;lsquo;,&amp;rsquo; Expression &amp;lsquo;)&amp;rsquo; { $$ = &amp;amp;ast.FuncCallExpr{ FnName: model.NewCIStr($1), Args: []ast.ExprNode{$3.(ast.ExprNode), $5.(ast.ExprNode)}, } }	Here it means: If the token sequence matches the pattern, the tokens are specified as a new variable with the name: `FunctionCallNonKeyword` (The value of `FunctionCallNonKeyword` can be assigned by assigning values to the `$$` variable.), which is a node in AST and the type is `*ast.FuncCallExpr`. The value of the `FnName` member variable is the content of `$1`, which is the value of the first token in the rule. &amp;#34;timediff()” is successfully converted to an AST node. Its member variable, `FnName`, has recorded the function name, ”timediff”, for the following evaluation. **Note:** To use the value of a certain token in the rule, you can use the `$x` format in which `x` is the location of the token in the rule. In the above example, `$1` is `&amp;#34;TIMEDIFF&amp;#34;`，$2 is `’(’`, and $3 is `’)’`. The meaning of `$1.(string)` is to reference the value of the first token and to declare it to be a `string`. #### 2. Register the function in the `Funcs` table in the [`builtin.go`](https://github.com/pingcap/tidb/blob/master/evaluator/builtin.go) file: ast.TimeDiff: {builtinTimeDiff, 2, 2},	The arguments are explained as follows: + `builtinTimediff`: The implementation of the `timediff` function is included in the `builtinTimediff` function. + `2`: The minimum number of the arguments of the function is `2`. + `2`: The maximum number of the arguments of the function is `2`. **Note:** The number of the arguments will be checked to see if it&amp;#39;s legal during the syntax parsing. The implementation of the function is in the the [`builtin.go`](https://github.com/pingcap/tidb/blob/master/evaluator/builtin.go) file. See the following for further details:  func builtinTimeDiff(args []types.Datum, ctx context.Context) (d types.Datum, err error) { sc := ctx.GetSessionVars().StmtCtx t1, err := convertToGoTime(sc, args[0]) if err != nil { return d, errors.Trace(err) } t2, err := convertToGoTime(sc, args[1]) if err != nil { return d, errors.Trace(err) } var t types.Duration t.Duration = t1.Sub(t2) t.Fsp = types.MaxFsp d.SetMysqlDuration(t) return d, nil }#### 3. Add the Type Inference information: ```	case &amp;#34;curtime&amp;#34;, &amp;#34;current_time&amp;#34;, &amp;#34;timediff&amp;#34;: tp = types.NewFieldType(mysql.TypeDuration) tp.Decimal = v.getFsp(x)	 4. Add the unit test case: func (s *testEvaluatorSuite) TestTimeDiff(c *C) { // Test cases from https://dev.mysql.com/doc/refman/5.7/en/date-and-time-functions.html#function_timediff tests := []struct { t1 string t2 string expectStr string }{ {&amp;#34;2000:01:01 00:00:00&amp;#34;, &amp;#34;2000:01:01 00:00:00.000001&amp;#34;, &amp;#34;-00:00:00.000001&amp;#34;}, {&amp;#34;2008-12-31 23:59:59.000001&amp;#34;, &amp;#34;2008-12-30 01:01:01.000002&amp;#34;, &amp;#34;46:58:57.999999&amp;#34;}, } for _, test := range tests { t1 := types.NewStringDatum(test.t1) t2 := types.NewStringDatum(test.t2) result, err := builtinTimeDiff([]types.Datum{t1, t2}, s.ctx) c.Assert(err, IsNil) c.Assert(result.GetMysqlDuration().String(), Equals, test.expectStr) } } "},
		{"url": "https://pingcap.com/weekly/2016-12-19-tidb-weekly/",
		"title": "Weekly update (December 12 ~ December 18, 2016)", 
		"content": " Weekly update in TiDB Last week, we landed 32 PRs in the TiDB repositories.Added  Add the FlagIgnoreTruncate/FlagTruncateAsWarning flag to control the behavior of truncated errors. Add the prompt text flag. Add the rawkv metrics to profile the rawkv API performance. Add a comparable varint encoding/decoding method to make encoded data smaller. Support the timediff built-in function. Support the following built-in functions: ln(), log(), log2(), log10().  Fixed  A bug that ignores primary key’s unsigned attribute. A bug that ignores error in the distsql layer. Allow default value to be Null when the column has the auto_increament attribute to be compatible with MySQL 5.6. A bug in the insert statement that ignores error. Fix bugs in the cost-based optimization framework: #2243  Improved  Refactor the time type related code: #2185, #2190, #2206, #2233, #2261 Remove the util/bytes package to clean up the code. Refactor the code to remove the evaluator.Eval() method: #2222, Improve test coverage for the util/segmentmap package. Recover from panics caused by malformated mysql packet to make tidb-server more robust.  New contributor  Bai Yang  Weekly update in TiKV Last week, we landed 11 PRs in the TiKV repositories.Added  Add a configuration to disable data sync to speed up loading data. Filter the pending peers for Placement Driver (PD) scheduler. Add pd-ctl to operate PD more easily.  Fixed  Update the advertise peer urls from etcd to fix #435.  Improved  Read and verify snapshot file in one step. Use a smaller interval to make Raft tick more accurate. Clean up the tombstone store to fix #401. Use delete_file_in_range when clean up the tombstone regions.  "},
		{"url": "https://pingcap.com/meetup/meetup-2016-12-17/",
		"title": "COISF 专场|PingCAP 第 34 期 NewSQL Meetup", 
		"content": " COISF 专场|PingCAP 第 34 期 NewSQL Meetup 2016-12-17 覃左言&amp;amp;申砾 PingCAP PingCAPPingCAP ![]() 微信号pingcap2015功能介绍PingCAP 专注于新型"},
		{"url": "https://pingcap.com/weekly/2016-12-12-tidb-weekly/",
		"title": "Weekly update (December 05 ~ December 11, 2016)", 
		"content": " Weekly update in TiDB Last week, we landed 41 PRs in the TiDB repositories.Added  Support the built-in function: str_to_date.  Support built-in function schema(). Support pushing the case-when expression to TiKV. Support changing the type and name of a column. Add the `session_variablesandplugins` of memory table to infoschema. Make the union all operator run parallelly. Support explaining the union statement.  Fixed  A bug that causes infinite loop. A bug in the on duplicate… statement when updating the primary key (PK) Make the charset name case-insensitive. Forbid dropping columns with the auto_inc and PK attribute. Fix bugs in the parser.  Improved  Pass the filter to TiKV when scanning indexes. Allocate column/index IDs in the table space to make the IDs shorter.  Weekly update in TiKV Last week, we landed 34 PRs in the TiKV repositories.Added  Support online backup of the RocksDB data for tikv-ctl debugging. Add replication constraints to schedule replicas. Support GrantLeaderScheduler to transfer all leaders to one store. Support ShuffleLeaderScheduler to shuffle leaders in different stores. Support Circle CI for TiKV and Placement Driver (PD). Add the state filter argument to get the stores API.  Fixed  Use channel to fix the possible stale snapshot state, issue #1373. Report ServerIsBusy and let PD ignore the busy store when scheduling to fix #414.  Improved  Speed up the shutdown duration to reduce the close waiting time. Add retry when initializing the cluster ID. Support safe ConfChange to fix #1366. Report pending peers to PD to improve its scheduler. Bind ports lazily to avoid the message channel full error when starting up.  "},
		{"url": "https://pingcap.com/meetup/meetup-2016-12-10/",
		"title": "COISF 专场|PingCAP 第 33 期 NewSQL Meetup", 
		"content": " COISF 专场|PingCAP 第 33 期 NewSQL Meetup 2016-12-10 王康&amp;amp;李康 PingCAP PingCAPPingCAP ![]() 微信号pingcap2015功能介绍PingCAP 专注于新型"},
		{"url": "https://pingcap.com/blog/2016-12-07-Subquery-Optimization-in-TiDB/",
		"title": "Subquery Optimization in TiDB", 
		"content": " MathJax.Hub.Config({ extensions: [&#34;tex2jax.js&#34;], jax: [&#34;input/TeX&#34;, &#34;output/HTML-CSS&#34;], tex2jax: { inlineMath: [ [&#39;$&#39;,&#39;$&#39;], [&#34;(&#34;,&#34;)&#34;] ], displayMath: [ [&#39;$$&#39;,&#39;$$&#39;], [&#34;[&#34;,&#34;]&#34;] ], processEscapes: true }, &#34;HTML-CSS&#34;: { availableFonts: [&#34;TeX&#34;] } });    Introduction to subqueries Subquery is a query within another SQL query. A common subquery is embedded within the FROM clause, for example：SELECT ID FROM (SELECT * FROM SRC) AS T The subexpressions in the FROM clauses can be processed very well by the general SQL optimizers. But when it comes to subqueries in the WHERE clause or the SELECT lists, it becomes very difficult to optimize because subqueries can be anywhere in the expression, e.g. in the CASE...WHEN... clauses.The subqueries that are not in the FROM clause are categorized as &amp;ldquo;correlated subquery&amp;rdquo; and &amp;ldquo;uncorrelated subquery&amp;rdquo;. Correlated subquery refers to a subquery with columns from outer references, for example:SELECT * FROM SRC WHERE EXISTS(SELECT * FROM TMP WHERE TMP.id = SRC.id) Uncorrelated subqueries can be pre-processed in the plan phase and be re-written to a constant. Therefore, this article is mainly focused on the optimization of correlated subqueries.Generally speaking, there are following three types of subqueries: Scalar Subquery like (SELECT...) + (SELECT...) Quantified Comparison like T.a = ANY(SELECT...) Existential Test like NOT EXISTS(SELECT...), T.a IN (SELECT...)  For the simple subqueries like Existential Test, the common practice is to rewrite them to SemiJoin. But it is barely explored in the literature about the generic algorithm and what kind of subqueries need to remove the correlation. For those subqueries whose correlation cannot be removed, the common practice in databases is to execute in Nested Loop, which is called correlated execution.TiDB inherits the subquery strategy in SQL Server [1]. It introduces the Apply operator to use algebraic representation for subqueries which is called normalization, and then removes the correlation based on the cost information.The Apply operator The reason why subqueries are difficult to optimize is that a subquery cannot be represented as a logic operator like Projection or Join, which makes it difficult to find a generic algorithm for subquery transformation. So the first thing is to introduce a logical operation that can represent the subqueries: the Apply operator, which is also called d-Join[2]. The semantics of the Apply operator is:[ R A^{otimes} E = bigcuplimits_{rin R} ({r}otimes E&amp;reg;) ]where E represents a parameterized subquery. In every execution, the Apply operator gets an r record from the R relation and sends r to E as a parameter for the &amp;#x2297; operation of r and E(r). &amp;#x2297; is different based on different query types, usually it’s SemiJoin ∃.For the following SQL statement:SELECT * FROM SRC WHERE EXISTS(SELECT * FROM TMP WHERE TMP.id = SRC.id) the Apply operator representation is as follows:Because the operator above Apply is Selection, formally, it is:[ {SRC} A^exists sigma_{SRC.id=TMP.id}{TMP} ]For the EXISTS subquery in the SELECT list, and the data that cannot pass through the SRC.id=TMP.id equation, the output should be false. So OuterJoin should be used:[ pi_C({SRC} A^{LOJ} sigma_{SRC.id=TMP.id}{TMP}) ]The C Projection is to transform NULL to false. But the more common practice is: If the output of the Apply operator is directly used by the query predicate, it is converted to SemiJoin.Removing the correlation The introduction of the Apply operator enables us to remove the correlation of the subqueries. The two examples in the previous section can be transformed to:[ {SRC} exists_{sigma_{SRC.id = TMP.id}} {TMP} ]and[ {SRC} LOJ_{sigma_{SRC.id = TMP.id}} {TMP} ]Other rules to remove correlation can be formally represented as:(R A^{otimes} E= R {otimes}_{true} E), if no parameters in E resolved from R (1)(R A^{otimes} (sigma_pE) = R {otimes}_p E), if no parameters in E resolved from R (2)(R A^times (sigma_pE)=sigma_p(R A^times E) ) (3)(R A^times (pi_vE) = pi_{vbigcupmathrm{cols}&amp;reg;}(R A^times E) ) (4)(R A^times (E_1 bigcup E_2) = (R A^times E_1) bigcup (R A^times E_2) ) (5)(R A^times (E_1 -  E_2) = (R A^times E_1) -  (R A^times E_2) ) (6)(R A^times (E_1 times  E_2) = (R A^times E_1) Join_{R.key} (R A^times E_2) ) (7)(R A^times (mathcal{G}_{A,F}E) = mathcal{G}_{Abigcup mathrm{attr}&amp;reg;,F} (R A^{times} E) ) (8)(R A^times (mathcal{G}^1_FE) = mathcal{G}_{Abigcup mathrm{attr}&amp;reg;,F&amp;rsquo;} (R A^{LOJ} E) ) (9)Based on the above rules, the correlation among all the SQL subqueries can be removed [3]. But the (5), (6), and (7) rules are seldom used because the the query cost is increased as a result of the rules about common expression. Take the following SQL statement as an example:SELECT C_CUSTKEY FROM CUSTOMER WHERE 1000000 &amp;lt; (SELECT SUM(O_TOTALPRICE) FROM ORDER WHERE O_CUSTKEY = C_CUSTKEY) The two “CUSTKEY”s are the primary keys. When the statement is transformed to Apply, it is represented as:[ sigma_{1000000&amp;lt;X}(CUSTOMER A^times mathcal{G}^1_{X=SUM(O_PRICE)}(sigma_{O_CUSTKEY=C_CUSTKEY}ORDERS)) ]Because of the primary keys, according to rule (9), it can be transformed to the following:[ sigma_{1000000&amp;lt;X} mathcal{G}_{C_CUSTKEY,X = SUM(O_PRICE)}(CUSTOMER A^{LOJ} sigma_{O_CUSTKEY=C_CUSTKEY}ORDERS) ]Note: If there are no primary keys in ORDERS, the (pi) operator should be added to allocate a unique key. Pay attention to the difference between rule (8) and rule (9). For the (mathcal{G}^1_F) aggregation function without the aggregation column, when the input is NULL, the output should be the default value of the F aggregation function. Therefore, the LeftOuterJoin should be used and a NULL record should be the output when the right table is NULL. In this case, based on rule (2), Apply can be completely removed. The statement can be transformed to a SQL statement with join:  [ sigma_{1000000&amp;lt;X}mathcal{G}_{C_CUSTKEY,X=SUM(O_PRICE)}(CUSTOMER LOJ_{O_CUSTKEY=C_CUSTKEY}ORDERS) ]Furthermore, based on the simplification of OuterJoin, the statement can be simplified to:[ sigma_{1000000&amp;lt;X}mathcal{G}_{C_CUSTKEY,X=SUM(O_PRICE)}(CUSTOMER Join_{O_CUSTKEY=C_CUSTKEY}ORDERS) ]Theoretically, the above 9 rules have solved the correlation removal problem. But is correlation removal the best solution for all the scenarios? The answer is no. If the results of the SQL statement are small and the subquery can use the index, then the best solution is to use correlated execution. The Apply operator can be optimized to Segment Apply, which is to sort the data of the outer table according to the correlated key. In this case, the keys that are within one group won&amp;rsquo;t have to be executed multiple times. Of course, this is strongly related to the number of distinct values (NDV) of the correlated keys in the outer table. Therefore, the decision about whether to use correlation removal also depends on statistics. When it comes to this point, the regular optimizer is no longer applicable. Only the optimizer with the Volcano or Cascade Style can take both the logic equivalence rules and the cost-based optimization into consideration. Therefore, a perfect solution for subquery depends on an excellent optimizer framework.Aggregation and subquery In the previous section, the final statement is not completely optimized. The aggregation function above OuterJoin and InnerJoin can be pushed down[4]. If OutJoin cannot be simplified, the formal representation of the push-down rule is:[ mathcal{G_{A,F}}(S LOJ_p R)=pi_C(S LOJ_p(mathcal{G}_{A-attr(S),F}R)) ]The (pi_C) above Join is to convert NULL to the default value when the aggregation function accepts empty values. It is worth mentioning that the above formula can be applied only when the following three conditions are met: All the columns that are related to R within the p predicate are in the Group by column. The key of the S relation is in the Group by column. The aggregations in the (mathcal{G}) …"},
		{"url": "https://pingcap.com/blog-cn/distributed-system-test-3/",
		"title": "分布式系统测试那些事儿 - 信心的毁灭与重建", 
		"content": "  本话题系列文章整理自 PingCAP Infra Meetup 第 26 期刘奇分享的《深度探索分布式系统测试》议题现场实录。文章较长，为方便大家阅读，会分为上中下三篇，本文为下篇。 -接中篇- ScyllaDB 有一个开源的东西，是专门用来给文件系统做 Failure Injection 的, 名字叫做 CharybdeFS。如果你想测试你的系统，就是文件系统在哪不断出问题，比如说写磁盘失败了，驱动程序分配内存失败了，文件已经存在等等，它都可以测模拟出来。CharybdeFS: A new fault-injecting file system for software testingSimulate the following errors: disk IO error (EIO) driver out of memory error (ENOMEM) file already exists (EEXIST) disk quota exceeded (EDQUOT)  再来看看 Cloudera，下图是整个 Cloudera 的一个 Failure Injection 的结构。一边是 Tools，一边是它的整个的 Level 划分。比如说整个 Cluster， Cluster 上面有很多 Host，Host 上面又跑了各种 Service，整个系统主要用于测试 HDFS， HDFS 也是很努力的在做有效的测试。然后每个机器上部署一个 AgenTEST，就用来注射那些可能出现的错误。看一下它们作用有多强大。Cloudera: Simulate the following errors: Packets loss/corrupt/reorder/duplicate/delay Bandwidth limit: Limit the network bandwidth for the specified address and port. DNSFail: Apply an injection to let the DNS fail. FLOOD: Starts a DoS attack on the specified port. BLOCK: Blocks all the packets directed to 10.0.0.0/8 (used internally by EC2). SIGSTOP: Pause a given process in its current state. BurnCPU/BurnIO/FillDISK/RONLY/FIllMEM/CorruptHDFS HANG: Hang a host running a fork bomb. PANIC: Force a kernel panic. Suicide: Shut down the machine.  数据包是可以丢的，可以坏的，可以 reorder 的，比如说你发一个 A，再发一个 B，它可以给你 reorder，变成先发了 B 再发了 A，然后看你应用程序有没有正确的处理这种行为。接着发完一次后面再给你重发，然后可以延迟，这个就比较简单。目前这个里面的大部分，TiKV 都有实现，还有带宽的限制，就比如说把你带宽压缩成 1M。以前我们遇到一个问题很有意思，发现有人把文件存到 Redis 里面，但 Redis 是带多个用户共享的，一个用户就能把整个 Redis 带宽给打满了，这样其他人的带宽就很卡，那这种很卡的时候 Redis 可能出现的行为是什么呢？我们并不需要一个用户真的去把它打满，只要用这种工具，瞬间就能出现我把你的带宽限制到原来的 1%，假设别人在跟你抢带宽，你的程序行为是什么？马上就能出来，也不需要配很复杂的环境。这极大的提高了测试效率，同时能测试到很多 corner case。然后 DNS fail。那 DNS fail 会有什么样的结果？有测过吗？可能都没有想过这个问题，但是在一个真正的分布式系统里面，每一点都是有可能出错的。还有 FLOOD，假设你现在被攻击了，整个系统的行为是什么样的？然后一不小心被这个 IP table 给 block 了，该怎么办。这种情况我们确实出现过。我们一上来并发，两万个连接一打出去，然后发现大部分都连不上，后来一看 IP table 自动启用了一个机制，然后把你们都 block。当然我们后面查了半个小时左右，才把问题查出来。但这种实际上应该是在最开始设计的时候就应该考虑的东西。如果你的进程被暂停了，比如说大家在云上跑在 VM 里面，整个 VM 为了升级，先把你整个暂停了，升级完之后再把你恢复的时候会怎么样？那简单来讲，就是如果假设你程序是有 GC 的，GC 现在把我们的程序卡了五秒，程序行为是正常的吗？五十秒呢？这个很有意思的就是，BurnCPU，就是再写一个程序，把 CPU 全占了，然后让你这个现在的程序只能使用一小部分的 CPU 的时候，你程序的行为是不是正常的。正常来讲，你可能说我 CPU 不是瓶颈啊，我瓶颈在 IO，当别人跟你抢 CPU，把你这个 CPU 压的很低的时候，到 CPU 是瓶颈的时候，正常你的程序的这个行为是不是正常的？还有 IO，跟你抢读的资源，跟你抢写的资源，然后 filedisk 把磁盘写满，写的空间很少。比如说对数据库而言，你创建你的 redo log 的时候，都已经满了会怎么样？然后我突然把磁盘设为只读，就你突然一个写入会出错，但是你接下来正常的读写行为是不是对的？很典型的一个例子，如果一个数据库你现在写入，磁盘满了，那外面读请求是否就能正常响应。 Fill memory，就是瞬间把这个 memory 给压缩下来，让你下次 malloc 的时候可能分布不到内存。这个就和业务比较相关了，就是破坏 HDFS 的文件。其它的就是 Hang、Panic，然后还有自杀，直接关掉机器，整个系统的行为是什么样的？现在比较痛苦的一点是大家各自为政，每一家都做一套，但是没有办法做成一个通用的东西给所有的人去用。包括我们自己也做了一套，但是确实没有办法和其他的语言之间去 share，最早提到的那个 libfu 库实际上是在 C 语言写的，那所有 C 相关的都可以去 call 那个库。Distributed testing Namazu  ZooKeeper:  Found ZOOKEEPER-2212, ZOOKEEPER-2080 (race): (blog article)  Etcd:  Found etcdctl bug #3517 (timing specification), fixed in #3530. The fix also resulted a hint of #3611， Reproduced flaky tests {#4006, #4039}  YARN: Found YARN-4301 (fault tolerance)， Reproduced flaky tests{1978, 4168, 4543, 4548, 4556}   然后 Namazu。大家肯定觉得 ZooKeeper 很稳定呀， Facebook 在用、阿里在用、京东在用。大家都觉得这个东西也是很稳定的，直到这个工具出现了，然后轻轻松松就找到 bug 了，所有的大家认为的这种特别稳定的系统，其实 bug 都还挺多的，这是一个毁三观的事情，就是你觉得东西都很稳定，都很 stable，其实不是的。从上面，我们能看到 Namazu 找到的 Etcd 的几个 bug，然后 YARN 的几个 bug，其实还有一些别的。How TiKV use namazu Use nmz container / non-container mode to disturb cluster.  Run container mode in CI for each commit. (1 hour) Run non-container mode for a stable version. (1 week+)  Use extreme policy for process inspector  Pick up some processes and execute them with SCHED_RR scheduler. others are executed with SCHED_BATCH scheduler  Use [0, 30s] delay for filesystem inspector  接下来说一下 TiKV 用 Namazu 的一些经验。因为我们曾经在系统上、在云上面出现过一次写入磁盘花了五十几秒才完成的情况，所以我们需要专门的工具模拟这个磁盘的抖动。有时候一次写入可能确实耗时比较久，那这种时候是不是 OK 的。大家如果能把这种东西统统用上，我觉得还能为很多开源系统找出一堆 bug。稍微介绍一下我们现在运行的基本策略，比如说我们会用 0 到 30 秒的这个 delay （就是每一次你往文件系统的交互，比如说读或者写，那么我们会给你产生随机的 0 到 30 秒的 delay ），但我们正常应该还是需要去测三十秒到几分钟的延迟的情况，是否会让整个系统崩掉了。How TiKV simulate network transport Drop/Delay messages randomly Isolate Node Partition [1, 2, 3, 4, 5] -&amp;gt; [1, 2, 3] + [4, 5] Out of order messages Filter messages Duplicate and send redundant messages  怎么模拟网络呢？假设你有网络，里面有五台机器，那我现在想做一个脑裂怎么做？不能靠拔网线对吧？比如在 TiKV 的测试框架中，我们就可以直接通过 API 把 5 个节点脑裂成两部分，让 1, 2, 3 号节点互相联通，4, 5 号节点也能联通，这两个分区彼此是隔离的，非常的方便。其实原理很简单，这种情况是用程序自己去模拟，假如是你发的包，自动给你丢掉，或者直接告诉你 unreachable，那这个时候你就知道这个网络就脑裂了，然后你怎么做？就是只允许特定类型的消息进来，把其他的都丢掉，这样一来你可以保证有些 bug 是必然重现的。这个框架给了我们极大的信心用来模拟并重现各种 corner case，确保这些 corner case 在单元测试中每次都能被覆盖到。How to test Rocksdb Treat storage as a black box. Three steps(7*24):  Fill data, Random kill -9 Restart Consistent check.  Results:  Found 2 bugs. Both fixed   然后说说我们怎么测 RocksDB。 RocksDB 在大家印象中是很稳定的，但我们最近发现了两个 bug。测的方法是这样的：我们往 RocksDB 里面填数据，然后随机的一段时间去把它 kill 掉，kill 掉之后我们重启，重新启动之后去检测我们刚才 fail 的 data 是不是一致的，然后我们发现两个可能造成数据丢失的 bug，但是官方的响应速度非常快，几天就都 fix 了。可是大家普遍运行的是这么 stable 的系统，为什么还会这么容易找到 bug？就说这个测试，如果是一直有这个测试的 cover，那么这两个 bug 可能很快就能够被发现。这是我们一个基本的，也就是当成一个纯黑盒的测。大家在测数据库的时候，基本也是当黑盒测。比如说 MySQL 写入数据，kill 掉，比如说我 commit 一个事务，数据库告诉我们 commit 成功，我把数据库 kill 掉，我再去查我刚才提交的数据一样能查到。这是一个正常的行为，如果查不到，说明整个系统有问题。More tools american fuzzy lop  其实还有一些更加先进的工具，大家平时觉得特别稳定的东西，都被摧残的不行。Nginx 、NGPD、tcpdump 、LibreOffice ，如果有用 Linux 的同学可能知道，还有 Flash、sqlite。这个东西一出来，当时大家很兴奋，说怎么一下子找了这么多 bug，为什么以前那么稳定的系统这么不堪一击，会觉得这个东西它还挺智能的。就比如说你程序里面有个 if 分支，它是这样的，假如你程序有一百条指令，它先从前面一直走，走到某条分支指令的时候，它是一直持续探索，一个分支走不下去，它会一直在这儿持续探索，再给你随机的输入，直到我探索进去了，我记下来了下次我知道我用这个输入可以进去特定的分支。那我可以再往下走，比如说你 if 分支进去之后里面还有 if ，那你传统手段可能探测不进去了但它可以，它记录一下，我这个可以进去，然后我重来，反正我继续输入这个，我再往里面走，一旦我探测到一个新的分支，我再记住，我再往里面走。所以它一出来的时候大家都说这个真厉害，一下发现这么多 bug。但最激动的不是这些人，最激动的是黑客，为什么？因为突然有很多栈溢出、堆溢出漏洞被发现了，然后就可以写一堆工具去攻击线上的这么多系统。所以很多的技术的推进在早期的时候是黑客做出来，但是他们的目的当然不一定是为了测试 bug，而是为了怎么黑一个系统进去，这是他们当时做的，所以这个工具也是非常强大、非常有意思的，大家可以拿去研究一下自己的系统。大家印象里面各种文件系统是很稳定的，可是当用 American fuzzy lop 来测试的时候，被惊呆了。 Btrfs 连 5 秒都没有坚持到就跪了，大家用的最多的 Ext4 是最坚挺的，也才抗了两个小时！！！再来说说 Google，Google 怎么做测试对外讲的不多，最近 Chrome team 开源了他们的 Fuzz 测试工具 OSS-Fuzz，这个工具强大的地方在于自动化做的极好： 发现 bug 后自动创建 issue bug 解决后自动 verify  更惊人的是 OSS-Fuzz 集群一周可以跑 ~4 trillion test cases 更多细节大家可以看这篇文章：Announcing OSS-Fuzz: Continuous Fuzzing for Open Source Software另外有些工具能让分布式系统开发人员的生活变得更美好一点。Tracing tools may help you Google Dapper Zipkin OpenTracing  还有 Tracing，比如说我一个 query 过来，然后经过这么多层，经过这么多机器，然后在不同的地方，不同环节耗时多久，实际上这个在分布式系统里面，有个专门的东西做 Tracing ，就是 distribute tracing tools。它可以用一条线来表达你的请求在各个阶段耗时多长，如果有几段，那么分到几个机器，分别并行的时候好了多长时间。大体的结构是这样的：这里是一个具体的例子：很清晰，一看就知道了，不用去看 log，这事其实一点也不新鲜，Google 十几年前就做了一个分布式追踪的工具。然后开源社区要做一个实现叫做 Zipkin，好像是 java 还是什么写的，又出了新的叫 OpenTracing，是 Go 写的。我们现在正准备上这个系统，用来追踪 TiDB 的请求在各个阶段的响应时间。最后想说一下，大家研究系统发现 bug 多了之后，不要对系统就丧失了信心，毕竟bug 一直在那里，只是从前没有发现，现在发现得多了，总体上新的测试方法让系统的质量比以前好了很多。好像有点超时了，先聊到这里吧，还有好多细节没法展开，下次再聊。-本系列完结- "},
		{"url": "https://pingcap.com/meetup/meetup-2016-12-07/",
		"title": "分布式系统测试那些事儿——信心的毁灭与重建", 
		"content": ""},
		{"url": "https://pingcap.com/meetup/memoir/meetup-2016-12-07/",
		"title": "分布式系统测试那些事儿——信心的毁灭与重建", 
		"content": "ScyllaDB 有一个开源的东西，是专门用来给文件系统做 Failure Injection 的, 名字叫做 CharybdeFS。如果你想测试你的系统，就是文件系统在哪不断出问题，比如说写磁盘失败了，驱动程序分配内存失败了，文件已经存在等等，它都可以测模拟出来。 CharybdeFS: A new fault-injecting file system for software testingSimulate the following errors: disk IO error (EIO) driver out of memory error (ENOMEM) file already exists (EEXIST) disk quota exceeded (EDQUOT)   再来看看 Cloudera，下图是整个 Cloudera 的一个 Failure Injection 的结构。一边是 Tools，一边是它的整个的 Level 划分。比如说整个 Cluster， Cluster 上面有很多 Host，Host 上面又跑了各种 Service，整个系统主要用于测试 HDFS， HDFS 也是很努力的在做有效的测试。然后每个机器上部署一个 AgenTEST，就用来注射那些可能出现的错误。看一下它们作用有多强大。 Cloudera: Simulate the following errors: Packets loss/corrupt/reorder/duplicate/delay Bandwidth limit: Limit the network bandwidth for the specified address and port. DNSFail: Apply an injection to let the DNS fail. FLOOD: Starts a DoS attack on the specified port. BLOCK: Blocks all the packets directed to 10.0.0.0/8 (used internally by EC2). SIGSTOP: Pause a given process in its current state. BurnCPU/BurnIO/FillDISK/RONLY/FIllMEM/CorruptHDFS HANG: Hang a host running a fork bomb. PANIC: Force a kernel panic. Suicide: Shut down the machine.   数据包是可以丢的，可以坏的，可以 reorder 的，比如说你发一个 A，再发一个 B，它可以给你 reorder，变成先发了 B 再发了 A，然后看你应用程序有没有正确的处理这种行为。接着发完一次后面再给你重发，然后可以延迟，这个就比较简单。目前这个里面的大部分，TiKV 都有实现，还有带宽的限制，就比如说把你带宽压缩成 1M。以前我们遇到一个问题很有意思，发现有人把文件存到 Redis 里面，但 Redis 是带多个用户共享的，一个用户就能把整个 Redis 带宽给打满了，这样其他人的带宽就很卡，那这种很卡的时候 Redis 可能出现的行为是什么呢？我们并不需要一个用户真的去把它打满，只要用这种工具，瞬间就能出现我把你的带宽限制到原来的 1%，假设别人在跟你抢带宽，你的程序行为是什么？马上就能出来，也不需要配很复杂的环境。这极大的提高了测试效率，同时能测试到很多 corner case。然后 DNS fail。那 DNS fail 会有什么样的结果？有测过吗？可能都没有想过这个问题，但是在一个真正的分布式系统里面，每一点都是有可能出错的。还有 FLOOD，假设你现在被攻击了，整个系统的行为是什么样的？然后一不小心被这个 IP table 给 block 了，该怎么办。这种情况我们确实出现过。我们一上来并发，两万个连接一打出去，然后发现大部分都连不上，后来一看 IP table 自动启用了一个机制，然后把你们都 block。当然我们后面查了半个小时左右，才把问题查出来。但这种实际上应该是在最开始设计的时候就应该考虑的东西。如果你的进程被暂停了，比如说大家在云上跑在 VM 里面，整个 VM 为了升级，先把你整个暂停了，升级完之后再把你恢复的时候会怎么样？那简单来讲，就是如果假设你程序是有 GC 的，GC 现在把我们的程序卡了五秒，程序行为是正常的吗？五十秒呢？这个很有意思的就是，BurnCPU，就是再写一个程序，把 CPU 全占了，然后让你这个现在的程序只能使用一小部分的 CPU 的时候，你程序的行为是不是正常的。正常来讲，你可能说我 CPU 不是瓶颈啊，我瓶颈在 IO，当别人跟你抢 CPU，把你这个 CPU 压的很低的时候，到 CPU 是瓶颈的时候，正常你的程序的这个行为是不是正常的？还有 IO，跟你抢读的资源，跟你抢写的资源，然后 filedisk 把磁盘写满，写的空间很少。比如说对数据库而言，你创建你的 redo log 的时候，都已经满了会怎么样？然后我突然把磁盘设为只读，就你突然一个写入会出错，但是你接下来正常的读写行为是不是对的？很典型的一个例子，如果一个数据库你现在写入，磁盘满了，那外面读请求是否就能正常响应。 Fill memory，就是瞬间把这个 memory 给压缩下来，让你下次 malloc 的时候可能分布不到内存。这个就和业务比较相关了，就是破坏 HDFS 的文件。其它的就是 Hang、Panic，然后还有自杀，直接关掉机器，整个系统的行为是什么样的？现在比较痛苦的一点是大家各自为政，每一家都做一套，但是没有办法做成一个通用的东西给所有的人去用。包括我们自己也做了一套，但是确实没有办法和其他的语言之间去 share，最早提到的那个 libfu 库实际上是在 C 语言写的，那所有 C 相关的都可以去 call 那个库。 Distributed testing Namazu ZooKeeper: Found ZOOKEEPER-2212, ZOOKEEPER-2080 (race): (blog article)  Etcd: Found etcdctl bug #3517 (timing specification), fixed in #3530. The fix also resulted a hint of #3611， Reproduced flaky tests {#4006, #4039}  YARN: Found YARN-4301 (fault tolerance)， Reproduced flaky tests{1978, 4168, 4543, 4548, 4556}    然后 Namazu。大家肯定觉得 ZooKeeper 很稳定呀， Facebook 在用、阿里在用、京东在用。大家都觉得这个东西也是很稳定的，直到这个工具出现了，然后轻轻松松就找到 bug 了，所有的大家认为的这种特别稳定的系统，其实 bug 都还挺多的，这是一个毁三观的事情，就是你觉得东西都很稳定，都很 stable，其实不是的。从上面，我们能看到 Namazu 找到的 Etcd 的几个 bug，然后 YARN 的几个 bug，其实还有一些别的。 How TiKV use namazu Use nmz container / non-container mode to disturb cluster. Run container mode in CI for each commit. (1 hour) Run non-container mode for a stable version. (1 week+)  Use extreme policy for process inspector Pick up some processes and execute them with SCHED_RR scheduler. others are executed with SCHED_BATCH scheduler  Use [0, 30s] delay for filesystem inspector   接下来说一下 TiKV 用 Namazu 的一些经验。因为我们曾经在系统上、在云上面出现过一次写入磁盘花了五十几秒才完成的情况，所以我们需要专门的工具模拟这个磁盘的抖动。有时候一次写入可能确实耗时比较久，那这种时候是不是 OK 的。大家如果能把这种东西统统用上，我觉得还能为很多开源系统找出一堆 bug。稍微介绍一下我们现在运行的基本策略，比如说我们会用 0 到 30 秒的这个 delay （就是每一次你往文件系统的交互，比如说读或者写，那么我们会给你产生随机的 0 到 30 秒的 delay ），但我们正常应该还是需要去测三十秒到几分钟的延迟的情况，是否会让整个系统崩掉了。 How TiKV simulate network transport Drop/Delay messages randomly Isolate Node Partition [1, 2, 3, 4, 5] -&amp;gt; [1, 2, 3] + [4, 5] Out of order messages Filter messages Duplicate and send redundant messages   怎么模拟网络呢？假设你有网络，里面有五台机器，那我现在想做一个脑裂怎么做？不能靠拔网线对吧？比如在 TiKV 的测试框架中，我们就可以直接通过 API 把 5 个节点脑裂成两部分，让 1, 2, 3 号节点互相联通，4, 5 号节点也能联通，这两个分区彼此是隔离的，非常的方便。其实原理很简单，这种情况是用程序自己去模拟，假如是你发的包，自动给你丢掉，或者直接告诉你 unreachable，那这个时候你就知道这个网络就脑裂了，然后你怎么做？就是只允许特定类型的消息进来，把其他的都丢掉，这样一来你可以保证有些 bug 是必然重现的。这个框架给了我们极大的信心用来模拟并重现各种 corner case，确保这些 corner case 在单元测试中每次都能被覆盖到。 How to test Rocksdb Treat storage as a black box. Three steps(7*24): Fill data, Random kill -9 Restart Consistent check.  Results: Found 2 bugs. Both fixed    然后说说我们怎么测 RocksDB。 RocksDB 在大家印象中是很稳定的，但我们最近发现了两个 bug。测的方法是这样的：我们往 RocksDB 里面填数据，然后随机的一段时间去把它 kill 掉，kill 掉之后我们重启，重新启动之后去检测我们刚才 fail 的 data 是不是一致的，然后我们发现两个可能造成数据丢失的 bug，但是官方的响应速度非常快，几天就都 fix 了。可是大家普遍运行的是这么 stable 的系统，为什么还会这么容易找到 bug？就说这个测试，如果是一直有这个测试的 cover，那么这两个 bug 可能很快就能够被发现。这是我们一个基本的，也就是当成一个纯黑盒的测。大家在测数据库的时候，基本也是当黑盒测。比如说 MySQL 写入数据，kill 掉，比如说我 commit 一个事务，数据库告诉我们 commit 成功，我把数据库 kill 掉，我再去查我刚才提交的数据一样能查到。这是一个正常的行为，如果查不到，说明整个系统有问题。 More tools american fuzzy lop   其实还有一些更加先进的工具，大家平时觉得特别稳定的东西，都被摧残的不行。Nginx 、NGPD、tcpdump 、LibreOffice ，如果有用 Linux 的同学可能知道，还有 Flash、sqlite。这个东西一出来，当时大家很兴奋，说怎么一下子找了这么多 bug，为什么以前那么稳定的系统这么不堪一击，会觉得这个东西它还挺智能的。就比如说你程序里面有个 if 分支，它是这样的，假如你程序有一百条指令，它先从前面一直走，走到某条分支指令的时候，它是一直持续探索，一个分支走不下去，它会一直在这儿持续探索，再给你随机的输入，直到我探索进去了，我记下来了下次我知道我用这个输入可以进去特定的分支。那我可以再往下走，比如说你 if 分支进去之后里面还有 if ，那你传统手段可能探测不进去了但它可以，它记录一下，我这个可以进去，然后我重来，反正我继续输入这个，我再往里面走，一旦我探测到一个新的分支，我再记住，我再往里面走。所以它一出来的时候大家都说这个真厉害，一下发现这么多 bug。但最激动的不是这些人，最激动的是黑客，为什么？因为突然有很多栈溢出、堆溢出漏洞被发现了，然后就可以写一堆工具去攻击线上的这么多系统。所以很多的技术的推进在早期的时候是黑客做出来，但是他们的目的当然不一定是为了测试 bug，而是为了怎么黑一个系统进去，这是他们当时做的，所以这个工具也是非常强大、非常有意思的，大家可以拿去研究一下自己的系统。大家印象里面各种文件系统是很稳定的，可是当用 American fuzzy lop 来测试的时候，被惊呆了。 Btrfs 连 5 秒都没有坚持到就跪了，大家用的最多的 Ext4 是最坚挺的，也才抗了两个小时！！！再来说说 Google，Google 怎么做测试对外讲的不多，最近 Chrome team 开源了他们的 Fuzz 测试工具 OSS-Fuzz，这个工具强大的地方在于自动化做的极好： 发现 bug 后自动创建 issue bug 解决后自动 verify  更惊人的是 OSS-Fuzz 集群一周可以跑 ~4 trillion test cases 更多细节大家可以看这篇文章：Announcing OSS-Fuzz: Continuous Fuzzing for Open Source Software另外有些工具能让分布式系统开发人员的生活变得更美好一点。 Tracing tools may help you Google Dapper Zipkin OpenTracing   还有 Tracing，比如说我一个 query 过来，然后经过这么多层，经过这么多机器，然后在不同的地方，不同环节耗时多久，实际上这个在分布式系统里面，有个专门的东西做 Tracing ，就是 distribute tracing tools。它可以用一条线来表达你的请求在各个阶段耗时多长，如果有几段，那么分到几个机器，分别并行的时候好了多长时间。大体的结构是这样的：这里是一个具体的例子：很清晰，一看就知道了，不用去看 log，这事其实一点也不新鲜，Google 十几年前就做了一个分布式追踪的工具。然后开源社区要做一个实现叫做 Zipkin，好像是 java 还是什么写的，又出了新的叫 OpenTracing，是 Go 写的。我们现在正准备上这个系统，用来追踪 TiDB 的请求在各个阶段的响应时间。最后想说一下，大家研究系统发现 bug 多了之后，不要对系统就丧失了信心，毕竟bug 一直在那里，只是从前没有发现，现在发现得多了，总体上新的测试方法让系统的质量比以前好了很多。好像有点超时了，先聊到这里吧，还有好多细节没法展开，下次再聊。"},
		{"url": "https://pingcap.com/weekly/2016-12-05-tidb-weekly/",
		"title": "Weekly update (November 28 ~ December 04, 2016)", 
		"content": " Weekly update in TiDB Last week, we landed 48 PRs in the TiDB repositories and 6 PRs in the TiDB docs repositories.Added  Support the built-in function: str_to_date.  Refactor the time structure: Introduce a TimeInternal interface to replace the go time representation. Add the raw Key-Value API  and make TiKV a raw Key-Value engine. Add a bench tool for the raw Key-Value API. Support the PARTITION keyword: parsed but ignored. Support the Alter User statement to change user’s password. Support the set password = pwd; statement. Use circleci.  Fixed  Check duplicate column names when adding index. Check duplicate index columns when creating table. Fix a bug when join exists in subquery. Make the schema out of date error retry automatically within TiDB. Fix a typo in the INFORMATION_SCHEMA.COLUMNS table. Fix the date_format type infer bug. Do not prune the set variable expression in projection.  Improved  Update the template for new issue. Use the statement context to handle truncated error. Push the aggregate operator down under union all.  Document change The following guides are updated: TiDB Binary Deployment TiDB Docker Deployment Compatibility with MySQL  Weekly update in TiKV Last week, we landed 22 PRs in the TiKV repositories.Added  Support consistency check to find if data is corrupted or not dynamically. Add configuration to control the max running task count. Add the raw Key-Value API.  Fixed  Check the Placement Driver list to fix #1186. Check whether the term is stale for the Raft command to fix #1317. Schedule the log Garbage Collection by size to fix #1337.  Improved  Replace the score type with resource kind to calculate the scores more easily. Replace origin concept balance with schedule and simplify configurations. Use coordinator to control the speed of different schedulers.  "},
		{"url": "https://pingcap.com/meetup/meetup-2016-12-03/",
		"title": "【现场】COISF 专场 Meetup", 
		"content": " 【现场】COISF 专场 Meetup 2016-12-03 COISF PingCAP PingCAPPingCAP ![]() 微信号pingcap2015功能介绍PingCAP 专注于新型分布式数据库的研发，是知名开源数据库 TiDB (GitHub 总计10000+ stars ) 背后的团队，总部设在北京，是国内第一家开源的新型分布式关系型数据库公司、国内领先的大数据技术和解决方案提供商。COISF Meetup今天是一期人数爆满的 Meetup。😊 作为 COISF 专场，感谢众多小伙伴与我们一起见证 COISF 的首次亮相。当然，首场参与是一定会有福利滴。这一期，我们邀请到了一位女神级讲师&amp;ndash;百度网页搜索部工程师雷丽媛，为大家讲解百度文件系统的架构设计；另外，PingCAP 联合创始人崔秋也有出台，为大家深情回顾 TiDB 的发展历程 :)▌开场：COISF Opening Talk在本环节中，PingCAPCo-Founder 崔秋，百度搜索基础架构团队技术负责人颜世光，以及奇虎 360 基础架构组存储负责人陈宗志共同为大家介绍了 COISF 的由来和使命，并对目前基金会内的顶级项目进行了简单介绍。COISF（China Open Infrastructure Software Foundation ）：中国开放基础软件基金会，其核心技术委员会由 PingCAP、百度、奇虎 360、小米（排名不分先后）等公司的基础软件项目团队组成，致力于促进和发展中国的新一代开源基础软件。目前基金会项目包括：Baidu/BFS、Baidu/Tera、PingCAP/TiDB、PingCAP/TiKV、Qihoo360/Zeppelin 等。我们认为，一方面开源是软件开发的未来，能更好地促进创新与合作；另一方面未来几十年中国的基础软件必将蓬勃发展，并在世界范围内扮演重要角色。但当前国内有很多优秀的开源软件, 因为文化和语言的藩篱没能融入西方社区, 无法获得足够的关注与支持，导致发展缓慢。我们通过建设中国统一的基础软件开发社区，甄选优秀的项目加入，集中优势资源促进这些项目的快速发展与成熟。COISF 的使命是：促进中国下一代开源基础软件生态系统的发展。▌Topic 1：百度文件系统－面向实时应用的分布式文件系统Speaker：雷丽媛，COISF BFS PMC，百度网页搜索部工程师，专注于分布式存储领域，目前负责百度结构化数据存储和分布式系统的相关工作。Content：百度的核心业务和数据库系统都依赖分布式文件系统作为底层存储，文件系统的可用性和性能对上层搜索业务的稳定性与效果有着至关重要的影响。现有的分布式文件系统（如HDFS等）是为离线批处理设计的，无法在保证高吞吐的情况下做到低延迟和持续可用，所以百度从搜索的业务特点出发，设计了百度文件系统。本场分享整体介绍了百度文件系统 BFS 的架构设计和子模块。▌Topic 2：TiDB - The Future of DatabaseSpeaker：崔秋，COISF TiDB PMC，PingCAP 联合创始人，重度开源爱好者，曾任职于搜狗、豌豆荚，长期从事广告系统基础组件相关的研究，现主要从事开源 NewSQL 数据库 TiDB/TiKV 相关的设计和研发工作。Content：NewSQL 不仅具有传统 SQL 和 ACID 的事务保证，同时还具有 NoSQL 的 Scale 能力, 这是一种世界前沿的数据库新技术。TiDB 以 Google Spanner/F1 作为理论参考，从零到一地完整实现这种面向未来的数据库。今天我们主要回顾 TiDB 的整个发展历程，从单机到分布式，从 Alpha 到 RC，从开源到社区，分享每一次架构演进背后的思考和感悟，以及每个不同阶段我们所做的取舍。最后，从客户的真实反馈中，我们一起探讨了 TiDB 的适用场景和最佳实践。COISF（China Open Infrastructure Software Foundation ）：中国开放基础软件基金会，其核心技术委员会由 PingCAP、百度、奇虎 360、小米（排名不分先后）等公司的基础软件项目团队组成，致力于促进和发展中国的新一代开源基础软件。为了更好地推动国内开源社区的发展，COISF 专场 Meetup 将定期举办，在这里，我们希望大家不仅能学到技术干货，更能真正感受开源精神的魅力。赞赏长按二维码向我转账受苹果公司新规定影响，微信 iOS 版的赞赏功能被关闭，可通过二维码转账支持公众号。阅读原文 阅读**投诉微信扫一扫 关注该公众号即将打开&amp;rdquo;&amp;ldquo;小程序取消 打开"},
		{"url": "https://pingcap.com/weekly/2016-11-28-tidb-weekly/",
		"title": "Weekly update (November 21 ~ November 27, 2016)", 
		"content": " Weekly update in TiDB Last week, we landed 44 PRs in the TiDB repositories and 3 PRs in the TiDB docs repositories.Added  Support creating anonymous index. Add the mailing list for TiDB users. Support the show events syntax.  Fixed  Enlarge the Time To Live (TTL)for large transactions. Parse float literal using the decimal parser. Prevent panic for malformed packets. Fix the behavior in the aggregate operator: for the select a, c from t groupby t.b statement, a and c should use the first row in the group. Add sequence number in binlog to preserve the original mutation order. Reset the current database after dropping the current database.  Improved  Prevent loading schema by multiple threads. Make unit test run faster. Make explain result clearer. Remove driver.go from the TiDB project to enable users to use the MySQL official driver.  Document change Add the following new guides: TiDB Cluster Troubleshooting Guide TiKV Tuning Guide  Weekly update in TiKV Last week, we landed 20 PRs in the TiKV repositories.Added  Add and report the store labels to Placement Driver (PD). Add pending task metrics for Worker. Dump all the statistics about Column Family compaction and Database.  Fixed  Use fs2 to get disk states to fix #1318 Use the monotonic clock time to improve the safety of leader lease read, to fix #964. Check the format of the listening and advertise address to fix #1332. Get the first value no matter if it is Null in aggregation. Fix a Garbage Collection bug which deletes the latest deleted key before SafePoint. Stop attaching term to the MsgReadIndex message to fix #1240.  Improved  Clean up the command flags and configurations parsing. Abstract a Selector to schedule region peer. Split the Raft Ready handle to two handles: Append and Apply. Use larger Heartbeat and Election timeout to reduce the network pressure. Ignore outdated tasks in Coprocessor to fix #1305.  "},
		{"url": "https://pingcap.com/meetup/meetup-2016-11-26/",
		"title": "PingCAP 第 31 期 NewSQL Meetup", 
		"content": " PingCAP 第 31 期 NewSQL Meetup 2016-11-26 黄华超&amp;amp;邓栓 PingCAP PingCAPPingCAP ![]() 微信号pingcap2015功能介绍PingCAP 专注于新型分布式数据库的研发，是知名开源数据库 TiDB (GitHub 总计10000+ stars ) 背后的团队，总部设在北京，是国内第一家开源的新型分布式关系型数据库公司、国内领先的大数据技术和解决方案提供商。NewSQL Meetup今天是 PingCAP 第 31 期 Meetup，主题是黄华超分享的《PD 的实现和演进》以及邓栓分享的《从容器和微服务的发展看基础架构变迁》。▌Topic 1：PD 的实现和演进Lecturer：黄华超，PingCAP 工程师，曾就职于微信、好赞科技，从事分布式存储相关工作，现负责 PingCAP PD 研发工作。Content：本次分享首先介绍了 PD 在 TiDB 集群的作用，以及集群是如何动态扩容缩容的。然后分别讲解了 PD 的各个功能是如何实现的，其中，着重分享了集群调度的相关设计和思考，以及新的标签调度功能。▌Topic 2：从容器和微服务的发展看基础架构变迁Lecturer：邓栓（Tennix），Rust 中文社区管理员，PingCAP SRE 工程师，负责 TiDB 与 Kubernetes 一体化整合部署方案。Content：近些年来容器和微服务的概念变得特别火热，越来越多的互联网公司开始尝试将以前的单体服务迁移到微服务，并且在实践中使用容器来部署服务，容器和微服务也催生了 DevOps，CaaS，Immutable infrastructure，Service orchestration 等概念。今天主要从容器和微服务角度谈了新技术应用和实践给开发者带来了哪些便利和挑战，基础架构发生了哪些改变，并尝试探讨了未来的应用服务会是什么样的架构。特别鸣谢：场地赞助-泰利驿站PingCAP Meetup"},
		{"url": "https://pingcap.com/blog-cn/percolator-and-txn/",
		"title": "Percolator 和 TiDB 事务算法", 
		"content": " 本文先概括的讲一下 Google Percolator 的大致流程。Percolator 是 Google 的上一代分布式事务解决方案，构建在 BigTable 之上，在 Google 内部 用于网页索引更新的业务，原始的论文在此。原理比较简单，总体来说就是一个经过优化的二阶段提交的实现，进行了一个二级锁的优化。TiDB 的事务模型沿用了 Percolator 的事务模型。 总体的流程如下：读写事务 1) 事务提交前，在客户端 buffer 所有的 update/delete 操作。 2) Prewrite 阶段:首先在所有行的写操作中选出一个作为 primary，其他的为 secondaries。PrewritePrimary: 对 primaryRow 写入 L 列(上锁)，L 列中记录本次事务的开始时间戳。写入 L 列前会检查: 是否已经有别的客户端已经上锁 (Locking)。 是否在本次事务开始时间之后，检查 W 列，是否有更新 [startTs, +Inf) 的写操作已经提交 (Conflict)。  在这两种种情况下会返回事务冲突。否则，就成功上锁。将行的内容写入 row 中，时间戳设置为 startTs。将 primaryRow 的锁上好了以后，进行 secondaries 的 prewrite 流程: 类似 primaryRow 的上锁流程，只不过锁的内容为事务开始时间及 primaryRow 的 Lock 的信息。 检查的事项同 primaryRow 的一致。  当锁成功写入后，写入 row，时间戳设置为 startTs。3) 以上 Prewrite 流程任何一步发生错误，都会进行回滚：删除 Lock，删除版本为 startTs 的数据。4) 当 Prewrite 完成以后，进入 Commit 阶段，当前时间戳为 commitTs，且 commitTs&amp;gt; startTs : commit primary：写入 W 列新数据，时间戳为 commitTs，内容为 startTs，表明数据的最新版本是 startTs 对应的数据。 删除L列。  如果 primary row 提交失败的话，全事务回滚，回滚逻辑同 prewrite。如果 commit primary 成功，则可以异步的 commit secondaries, 流程和 commit primary 一致， 失败了也无所谓。事务中的读操作  检查该行是否有 L 列，时间戳为 [0, startTs]，如果有，表示目前有其他事务正占用此行，如果这个锁已经超时则尝试清除，否则等待超时或者其他事务主动解锁。注意此时不能直接返回老版本的数据，否则会发生幻读的问题。 读取至 startTs 时该行最新的数据，方法是：读取 W 列，时间戳为 [0, startTs], 获取这一列的值，转化成时间戳 t, 然后读取此列于 t 版本的数据内容。  由于锁是分两级的，primary 和 seconary，只要 primary 的行锁去掉，就表示该事务已经成功 提交，这样的好处是 secondary 的 commit 是可以异步进行的，只是在异步提交进行的过程中 ，如果此时有读请求，可能会需要做一下锁的清理工作。"},
		{"url": "https://pingcap.com/blog-cn/mvcc-in-tikv/",
		"title": "TiKV 的 MVCC（Multi-Version Concurrency Control）机制", 
		"content": " 并发控制简介 事务隔离在数据库系统中有着非常重要的作用，因为对于用户来说数据库必须提供这样一个“假象”：当前只有这么一个用户连接到了数据库中，这样可以减轻应用层的开发难度。但是，对于数据库系统来说，因为同一时间可能会存在很多用户连接，那么许多并发问题，比如数据竞争（data race），就必须解决。在这样的背景下，数据库管理系统（简称 DBMS）就必须保证并发操作产生的结果是安全的，通过可串行化（serializability）来保证。虽然 Serilizability 是一个非常棒的概念，但是很难能够有效的实现。一个经典的方法就是使用一种两段锁（2PL）。通过 2PL，DBMS 可以维护读写锁来保证可能产生冲突的事务按照一个良好的次序（well-defined) 执行，这样就可以保证 Serializability。但是，这种通过锁的方式也有一些缺点： 读锁和写锁会相互阻滞（block）。 大部分事务都是只读（read-only）的，所以从事务序列（transaction-ordering）的角度来看是无害的。如果使用基于锁的隔离机制，而且如果有一段很长的读事务的话，在这段时间内这个对象就无法被改写，后面的事务就会被阻塞直到这个事务完成。这种机制对于并发性能来说影响很大。  *多版本并发控制（Multi-Version Concurrency Control, 以下简称 MVCC）*以一种优雅的方式来解决这个问题。在 MVCC 中，每当想要更改或者删除某个数据对象时，DBMS 不会在原地去删除或这修改这个已有的数据对象本身，而是创建一个该数据对象的新的版本，这样的话同时并发的读取操作仍旧可以读取老版本的数据，而写操作就可以同时进行。这个模式的好处在于，可以让读取操作不再阻塞，事实上根本就不需要锁。这是一种非常诱人的特型，以至于在很多主流的数据库中都采用了 MVCC 的实现，比如说 PostgreSQL，Oracle，Microsoft SQL Server 等。TiKV 中的 MVCC 让我们深入到 TiKV 中的 MVCC，了解 MVCC 在 TiKV 中是如何实现的。Timestamp Oracle(TSO) 因为TiKV 是一个分布式的储存系统，它需要一个全球性的授时服务，下文都称作 TSO（Timestamp Oracle），来分配一个单调递增的时间戳。 这样的功能在 TiKV 中是由 PD 提供的，在 Google 的 Spanner 中是由多个原子钟和 GPS 来提供的。Storage 从源码结构上来看，想要深入理解 TiKV 中的 MVCC 部分，src/storage 是一个非常好的入手点。 Storage 是实际上接受外部命令的结构体。pub struct Storage { engine: Box&amp;lt;Engine&amp;gt;, sendch: SendCh&amp;lt;Msg&amp;gt;, handle: Arc&amp;lt;Mutex&amp;lt;StorageHandle&amp;gt;&amp;gt;, } impl Storage { pub fn start(&amp;amp;mut self, config: &amp;amp;Config) -&amp;gt; Result&amp;lt;()&amp;gt; { let mut handle = self.handle.lock().unwrap(); if handle.handle.is_some() { return Err(box_err!(&amp;#34;scheduler is already running&amp;#34;)); } let engine = self.engine.clone(); let builder = thread::Builder::new().name(thd_name!(&amp;#34;storage-scheduler&amp;#34;)); let mut el = handle.event_loop.take().unwrap(); let sched_concurrency = config.sched_concurrency; let sched_worker_pool_size = config.sched_worker_pool_size; let sched_too_busy_threshold = config.sched_too_busy_threshold; let ch = self.sendch.clone(); let h = try!(builder.spawn(move || { let mut sched = Scheduler::new(engine, ch, sched_concurrency, sched_worker_pool_size, sched_too_busy_threshold); if let Err(e) = el.run(&amp;amp;mut sched) { panic!(&amp;#34;scheduler run err:{:?}&amp;#34;, e); } info!(&amp;#34;scheduler stopped&amp;#34;); })); handle.handle = Some(h); Ok(()) } } start 这个函数很好的解释了一个 storage 是怎么跑起来的。Engine 首先是 Engine。 Engine 是一个描述了在储存系统中接入的的实际上的数据库的接口，raftkv 和 Enginerocksdb 分别实现了这个接口。StorageHandle StorageHanle 是处理从sench 接受到指令，通过 mio 来处理 IO。接下来在Storage中实现了async_get 和async_batch_get等异步函数，这些函数中将对应的指令送到通道中，然后被调度器（scheduler）接收到并异步执行。Ok，了解完Storage 结构体是如何实现的之后，我们终于可以接触到在Scheduler 被调用的 MVCC 层了。当 storage 接收到从客户端来的指令后会将其传送到调度器中。然后调度器执行相应的过程或者调用相应的异步函数。在调度器中有两种操作类型，读和写。读操作在 MvccReader 中实现，这一部分很容易理解，暂且不表。写操作的部分是MVCC的核心。MVCC Ok，两段提交（2-Phase Commit，2PC）是在 MVCC 中实现的，整个 TiKV 事务模型的核心。在一段事务中，由两个阶段组成。Prewrite 选择一个 row 作为 primary row， 余下的作为 secondary row。 对primary row 上锁. 在上锁之前，会检查是否有其他同步的锁已经上到了这个 row 上 或者是是否经有在 startTS 之后的提交操作。这两种情况都会导致冲突，一旦都冲突发生，就会回滚（rollback）。 对于 secondary row 重复以上操作。Commit Rollback 在Prewrite 过程中出现冲突的话就会被调用。Garbage Collector 很容易发现，如果没有垃圾收集器（Gabage Collector） 来移除无效的版本的话，数据库中就会存有越来越多的 MVCC 版本。但是我们又不能仅仅移除某个 safe point 之前的所有版本。因为对于某个 key 来说，有可能只存在一个版本，那么这个版本就必须被保存下来。在TiKV中，如果在 safe point 前存在Put 或者Delete，那么说明之后所有的 writes 都是可以被移除的，不然的话只有Delete，Rollback和Lock 会被删除。TiKV-Ctl for MVCC 在开发和 debug 的过程中，我们发现查询 MVCC 的版本信息是一件非常频繁并且重要的操作。因此我们开发了新的工具来查询 MVCC 信息。TiKV 将 Key-Value，Locks 和Writes 分别储存在CF_DEFAULT，CF_LOCK，CF_WRITE中。它们以这样的格式进行编码    default lock write     key z{encoded_key}{start_ts(desc)} z{encoded_key} z{encoded_key}{commit_ts(desc)}   value {value} {flag}{primary_key}{start_ts(varint)} {flag}{start_ts(varint)}    Details can be found here.因为所有的 MVCC 信息在 Rocksdb 中都是储存在 CF Key-Value 中，所以想要查询一个 Key 的版本信息，我们只需要将这些信息以不同的方式编码，随后在对应的 CF 中查询即可。CF Key-Values 的表示形式。"},
		{"url": "https://pingcap.com/weekly/2016-11-21-tidb-weekly/",
		"title": "Weekly update (November 14 ~ November 20, 2016)", 
		"content": " Weekly update in TiDB Last week, we landed 30 PRs in the TiDB repositories, 3 PRs in the TiDB docs repositories.Added  Add a session variable to skip unique constraint check: This could be used when migrating data. More metrics for statement counter.  Fixed  Use reserved keywords as the table/column name. Add missing comments in the show table status result. Fix a bug that gets duplicate auto_inc ID after truncating table..  Improved  Make the Explain result more explicit. Tune the package size to be friendly for rocksdb. Use a better way to reload schema and reduce memory usage. Fetch schemas in a parallel way to make it faster when there are a huge number of schemas and tables.  Document change Add the following new guides: Compatibility with MySQL. Reading data from history versions.  Weekly update in TiKV Last week, we landed 19 PRs in the TiKV repositories.Fixed  Check whether the message is discarded after proposing.  Improved  Still refactor cache to make it more easily to use, with PR 360, 382. Add lock Time To Live (TTL) for large transactions. Try updating the soft limit when requirement check fails. Support configuring fill cache. Limit the WriteBatch size for the ResolveLock and Garbage Collection commands. Clean up unnecessary filter args. Abstract a Selector to handle different strategies to select stores. Enlarge the latches size of the scheduler to reduce lock conflicts.  "},
		{"url": "https://pingcap.com/blog-cn/tidb-syncer/",
		"title": "解析 TiDB 在线数据同步工具 Syncer", 
		"content": "TiDB 是一个完全分布式的关系型数据库，从诞生的第一天起，我们就想让它来兼容 MySQL 语法，希望让原有的 MySQL 用户 (不管是单机的 MySQL，还是多机的 MySQL Sharding) 都可以在基本不修改代码的情况下，除了可以保留原有的 SQL 和 ACID 事务之外，还可以享受到分布式带来的高并发，高吞吐和 MPP 的高性能。对于用户来说，简单易用是他们试用的最基本要求，得益于社区和 PingCAP 小伙伴们的努力，我们提供基于 Binary 和 基于 Kubernetes 的两种不同的一键部署方案来让用户可以在几分钟就可以部署起来一个分布式的 TiDB 集群，从而快速地进行体验。 当然，对于用户来说，最好的体验方式就是从原有的 MySQL 数据库同步一份数据镜像到 TiDB 来进行对于对比测试，不仅简单直观，而且也足够有说服力。实际上，我们已经提供了一整套的工具来辅助用户在线做数据同步，具体的可以参考我们之前的一篇文章:TiDB 作为 MySQL Slave 实现实时数据同步, 这里就不再展开了。后来有很多社区的朋友特别想了解其中关键的 Syncer 组件的技术实现细节，于是就有了这篇文章。首先我们看下 Syncer 的整体架构图, 对于 Syncer 的作用和定位有一个直观的印象。从整体的架构可以看到，Syncer 主要是通过把自己注册为一个 MySQL Slave 的方式，和 MySQL Master 进行通信，然后不断读取 MySQL Binlog，进行 Binlog Event 解析，规则过滤和数据同步。从工程的复杂度上来看，相对来说还是非常简单的，相对麻烦的地方主要是 Binlog Event 解析和各种异常处理，也是容易掉坑的地方。为了完整地解释 Syncer 的在线同步实现，我们需要有一些额外的内容需要了解。###MySQL Replication 我们先看看 MySQL 原生的 Replication 复制方案，其实原理上也很简单：1）MySQL Master 将数据变化记录到 Binlog (Binary Log), 2) MySQL Slave 的 I/O Thread 将 MySQL Master 的 Binlog 同步到本地保存为 Relay Log 3）MySQL Slave 的 SQL Thread 读取本地的 Relay Log，将数据变化同步到自身####MySQL BinlogMySQL 的 Binlog 分为几种不同的类型，我们先来大概了解下，也看看具体的优缺点。1）Row MySQL Master 将详细记录表的每一行数据变化的明细记录到 Binlog。 优点：完整地记录了行数据的变化信息，完全不依赖于存储过程，函数和触发器等等，不会出现因为一些依赖上下文信息而导致的主从数据不一致的问题。 缺点：所有的增删改查操作都会完整地记录在 Binlog 中，会消耗更大的存储空间。2）Statement MySQL Master 将每一条修改数据的 SQL 都会记录到 Binlog。 优点：相比 Row 模式，Statement 模式不需要记录每行数据变化，所以节省存储量和 IO，提高性能。 缺点：一些依赖于上下文信息的功能，比如 auto increment id，user define function, on update current_timestamp/now 等可能导致的数据不一致问题。3）Mixed MySQL Master 相当于 Row 和 Statement 模式的融合。 优点：根据 SQL 语句，自动选择 Row 和 Statement 模式，在数据一致性，性能和存储空间方面可以做到很好的平衡。 缺点：两种不同的模式混合在一起，解析处理起来会相对比较麻烦。####MySQL Binlog Event 了解了 MySQL Replication 和 MySQL Binlog 模式之后，终于进入到了最复杂的 MySQL Binlog Event 协议解析阶段了。在解析 MySQL Binlog Eevent 之前，我们首先看下 MySQL Slave 在协议上是怎么和 MySQL Master 进行交互的。Binlog dump首先，我们需要伪造一个 Slave，向 MySQL Master 注册，这样 Master 才会发送 Binlog Event。注册很简单，就是向 Master 发送 COM_REGISTER_SLAVE 命令，带上 Slave 相关信息。这里需要注意，因为在 MySQL 的 replication topology 中，都需要使用一个唯一的 server id 来区别标示不同的 Server 实例，所以这里我们伪造的 slave 也需要一个唯一的 server id。Binlog Event对于一个 Binlog Event 来说，它分为三个部分，header，post-header 以及 payload。 MySQL 的 Binlog Event 有很多版本，我们只关心 v4 版本的，也就是从 MySQL 5.1.x 之后支持的版本，太老的版本应该基本上没什么人用了。Binlog Event 的 header 格式如下：4 bytes timestamp 1 bytes event type 4 bytes server-id 4 bytes event-size 4 bytes log pos 2 bytes flags header 的长度固定为 19，event type 用来标识这个 event 的类型，event size 则是该 event 包括 header 的整体长度，而 log pos 则是下一个 event 所在的位置。这个 header 对于所有的 event 都是通用的，接下来我们看看具体的 event。FORMAT_DESCRIPTION_EVENT在 v4 版本的 Binlog 文件中，第一个 event 就是 FORMAT_DESCRIPTION_EVENT，格式为:2 bytes binlog-version string[50] mysql-server version 4 bytes create timestamp 1 byte event header length string[p] event type header lengths 我们需要关注的就是 event type header length 这个字段，它保存了不同 event 的 post-header 长度，通常我们都不需要关注这个值，但是在解析后面非常重要的ROWS_EVENT 的时候，就需要它来判断 TableID 的长度了, 这个后续在说明。ROTATE_EVENT而 Binlog 文件的结尾，通常（只要 Master 不当机）就是 ROTATE_EVENT，格式如下:Post-header 8 bytes position Payload string[p] name of the next binlog 它里面其实就是标明下一个 event 所在的 binlog filename 和 position。这里需要注意，当 Slave 发送 Binlog dump 之后，Master 首先会发送一个 ROTATE_EVENT，用来告知 Slave下一个 event 所在位置，然后才跟着 FORMAT_DESCRIPTION_EVENT。其实我们可以看到，Binlog Event 的格式很简单，文档都有着详细的说明。通常来说，我们仅仅需要关注几种特定类型的 event，所以只需要写出这几种 event 的解析代码就可以了，剩下的完全可以跳过。TABLE_MAP_EVENT上面我们提到 Syncer 使用 Row 模式的 Binlog，关于增删改的操作，对应于最核心的ROWS_EVENT ，它记录了每一行数据的变化情况。而如何解析相关的数据，是非常复杂的。在详细说明 ROWS_EVENT 之前，我们先来看看 TABLE_MAP_EVENT，该 event 记录的是某个 table 一些相关信息，格式如下:post-header: if post_header_len == 6 { 4 bytes table id } else { 6 bytes table id } 2 bytes flags payload: 1 byte schema name length string schema name 1 byte [00] 1 byte table name length string table name 1 byte [00] lenenc-int column-count string.var_len[length=$column-count] column-def lenenc-str column-meta-def n bytes NULL-bitmask, length: (column-count + 8) / 7 table id 需要根据 post_header_len 来判断字节长度，而 post_header_len 就是存放到 FORMAT_DESCRIPTION_EVENT 里面的。这里需要注意，虽然我们可以用 table id 来代表一个特定的 table，但是因为 Alter Table 或者 Rotate Binlog Event 等原因，Master 会改变某个 table 的 table id，所以我们在外部不能使用这个 table id 来索引某个 table。TABLE_MAP_EVENT 最需要关注的就是里面的 column meta 信息，后续我们解析 ROWS_EVENT 的时候会根据这个来处理不同数据类型的数据。column def 则定义了每个列的类型。ROWS_EVENTROWS_EVENT 包含了 insert，update 以及 delete 三种 event，并且有 v0，v1 以及 v2 三个版本。 ROWS_EVENT 的格式很复杂，如下：header: if post_header_len == 6 { 4 table id } else { 6 table id } 2 flags if version == 2 { 2 extra-data-length string.var_len extra-data } body: lenenc_int number of columns string.var_len columns-present-bitmap1, length: (num of columns+7)/8 if UPDATE_ROWS_EVENTv1 or v2 { string.var_len columns-present-bitmap2, length: (num of columns+7)/8 } rows: string.var_len nul-bitmap, length (bits set in &amp;#39;columns-present-bitmap1&amp;#39;+7)/8 string.var_len value of each field as defined in table-map if UPDATE_ROWS_EVENTv1 or v2 { string.var_len nul-bitmap, length (bits set in &amp;#39;columns-present-bitmap2&amp;#39;+7)/8 string.var_len value of each field as defined in table-map } ... repeat rows until event-end ROWS_EVENT 的 table id 跟 TABLE_MAP_EVENT 一样，虽然 table id 可能变化，但是 ROWS_EVENT 和 TABLE_MAP_EVENT 的 table id 是能保证一致的，所以我们也是通过这个来找到对应的 TABLE_MAP_EVENT。 为了节省空间，ROWS_EVENT 里面对于各列状态都是采用 bitmap 的方式来处理的。首先我们需要得到 columns present bitmap 的数据，这个值用来表示当前列的一些状态，如果没有设置，也就是某列对应的 bit 为 0，表明该 ROWS_EVENT 里面没有该列的数据，外部直接使用 null 代替就成了。然后就是 null bitmap，这个用来表明一行实际的数据里面有哪些列是 null 的，这里最坑爹的是 null bitmap 的计算方式并不是 (num of columns+7)/8，也就是 MySQL 计算 bitmap 最通用的方式，而是通过 columns present bitmap 的 bits set 个数来计算的，这个坑真的很大。为什么要这么设计呢，可能最主要的原因就在于 MySQL 5.6 之后 Binlog Row Image 的格式增加了 minimal 和 noblob，尤其是 minimal，update 的时候只会记录相应更改字段的数据，比如我一行有 16 列，那么用 2 个 byte 就能搞定 null bitmap 了，但是如果这时候只有第一列更新了数据，其实我们只需要使用 1 个 byte 就能记录了，因为后面的铁定全为 0，就不需要额外空间存放了。bits set 其实也很好理解，就是一个 byte 按照二进制展示的时候 1 的个数，譬如 1 的 bits set 就是1，而 3 的 bits set 就是 2，而 255 的 bits set 就是 8 了。得到了 present bitmap 以及 null bitmap 之后，我们就能实际解析这行对应的列数据了，对于每一列，首先判断是否 present bitmap 标记了，如果为 0，则跳过用 null 表示，然后在看是否在 null bitmap 里面标记了，如果为 1，表明值为 null，最后我们就开始解析真正有数据的列了。但是，因为我们得到的是一行数据的二进制流，我们怎么知道一列数据如何解析？这里，就要靠 TABLE_MAP_EVENT 里面的 column def 以及 meta 了。 column def 定义了该列的数据类型，对于一些特定的类型，譬如 MYSQL_TYPE_LONG, MYSQL_TYPE_TINY 等，长度都是固定的，所以我们可以直接读取对应的长度数据得到实际的值。但是对于一些类型，则没有这么简单了。这时候就需要通过 meta 来辅助计算了。譬如对于 MYSQL_TYPE_BLOB 类型，meta 为 1 表明是 tiny blob，第一个字节就是 blob 的长度，2 表明的是 short blob，前两个字节为 blob 的长度等，而对于 MYSQL_TYPE_VARCHAR 类型，meta 则存储的是 string 长度。当然这里面还有最复杂的 MYSQL_TYPE_NEWDECIMAL， MYSQL_TYPE_TIME2 等类型，关于不同类型的 column 解析还是比较复杂的，可以单独开一章专门来介绍，因为篇幅关系这里就不展开介绍了，具体的可以参考官方文档。搞定了这些，我们终于可以完整的解析一个 ROWS_EVENT 了：）XID_EVENT 在事务提交时，不管是 Statement 还是 Row 模式的 Binlog，都会在末尾添加一个 XID_EVENT 事件代表事务的结束，里面包含事务的 ID 信息。QUERY_EVENTQUERY_EVENT 主要用于记录具体执行的 SQL 语句，MySQL 所有的 DDL 操作都记录在这个 event 里面。###Syncer 介绍完了 MySQL Replication 和 MySQL Binlog Event 之后，理解 Syncer 就变的比较容易了，上面已经介绍过基本的架构和功能了，在 Syncer 中， 解析和同步 MySQL Binlog，我们使用的是我们首席架构师唐刘的 go-mysql 作为核心 lib，这个 lib 已经在 github 和 bilibili 线上使用了，所以是非常安全可靠的。所以这部分我们就跳过介绍了，感兴趣的话，可以看下 github 开源的代码。这里面主要介绍几个核心问题：####MySQL Binlog 模式的选择 在 Syncer 的设计中，首先考虑的是可靠性问题，即使 Syncer 异常退出也可以直接重启起来，也不会对线上数据一致性产生影响。为了实现这个目标，我们必须处理数据同步的可重入问题。 对于 Mixed 模式来说，一个 insert 操作，在 Binlog 中记录的是 insert SQL，如果 Syncer 异常退出的话，因为 Savepoint 还没有来得及更新，会导致重启之后继续之前的 insert SQL，就会导致主键冲突问题，当然可以对 SQL 进行改写，将 insert 改成 replace，但是这里面就涉及到了 SQL 的解析和转换问题，处理起来就有点麻烦了。另外一点就是，最新版本的 MySQL 5.7 已经把 Row 模式作为默认的 Binlog 格式了。所以，在 Syncer 的实现中，我们很自然地选择 Row 模式作为 Binlog 的数据同步模式。####Savepoint 的选取 对于 Syncer 本身来说，我们更多的是考虑让它尽可能的简单和高效，所以每次 Syncer 重启都要尽可能从上次同步的 Binlog Pos 的地方做类似断点续传的同步。如何选取 Savepoint 就是一个需要考虑的问题了。 对于一个 DML 操作来说(以 Insert SQL 操作举例来看)，基本的 Binlog Event 大概是下面的样子：TABLE_MAP_EVENT QUERY_EVENT → begin WRITE_ROWS_EVENT XID_EVENT 我们从 MySQL Binlog Event 中可以看到，每个 Event 都可以获取下一个 Event 开始的 MySQL Binlog Pos 位置，所以只要获取这个 Pos 信息保存下来就可以了。但是我们需要考虑的是，TABLE_MAP_EVENT 这个 event 是不能被 save 的，因为对于 WRITE_ROWS_EVENT 来说，没有 TABLE_MAP_EVENT 基本上没有办法进行数据解析，所以为什么很多人抱怨 MySQL Binlog 协议不灵活，主要原因就在这里，因为不管是 TABLE_MAP_EVENT 还是 WRITE_ROWS_EVENT 里面都没有 Schema 相关的信息的，这个信息只能在某个地方保留起来，比如 MySQL Slave，也就是 MySQL Binlog 是没有办法自解析的。当然，对于 DDL 操作就比较简单了，DDL 本身就是一个 QUERY_EVENT。所以，Syncer 处于性能和安全性的考虑，我们会定期和遇到 DDL 的时候进行 Save。 …"},
		{"url": "https://pingcap.com/blog-cn/stale-read/",
		"title": "通过 raft 的 leader lease 来解决集群脑裂时的 stale read 问题", 
		"content": "问题： 当 raft group 发生脑裂的情况下，老的 raft leader 可能在一段时间内并不知道新的 leader 已经被选举出来，这时候客户端在老的 leader 上可能会读取出陈旧的数据（stale read）。 比如，我们假想一个拥有 5 个节点的 raft group:其中 Node 5 是当前的 raft leader，当出现网络分区时，在 Node 5 的 raft lease 任期还没结束的一段时间内，Node 5 仍然认为自己是当前 term 的 leader，但是此时，另外一边分区已经在新的 term 中选出了新的 leader。如果此时，客户端在新的 leader 上更新了某个值 x，此时是可以更新成功的（因为还是可以复制到多数派）。但是在分区的另一端，此时一个客户端去读取 x 的值，Node 5 还会返回老的值，这样就发生了 stale read。解决方案引入一个新的概念, region leader。region leader 是一个逻辑上的概念, 任意时刻对于某一个 region 来说, 一定只拥有一个 region leader, 每个 region leader 在任期之内尝试每隔 t 时间间隔, 在 raft group 内部更新一下 region leader 的 lease. 所有的读写请求都必须通过 region leader 完成， 但是值得注意的是， region leader 和 raft leader 可能不是一个节点，当 region leader 和 raft leader 不重合的时候，region leader 会将请求转发给当前的 raft leader，当网络出现分区时，会出现以下几种情况： region leader 落在多数派，老 raft leader 在多数派这边 region leader 落在多数派，老 raft leader 在少数派这边 region leader 落在少数派，老 raft leader 在多数派这边 region leader 落在少数派，老 raft leader 在少数派这边  用开篇的例子来分情况讨论：对于第一种情况，region leader 的 lease 不会过期，因为 region leader 的心跳仍然能更新到多数派的节点上，老的 raft leader 仍然能同步到大多数节点上，少数派这边也不会选举出新的 leader， 这种情况下不会出现 stale read。第二种情况，就是开篇提到会出现 stale read 的典型情况，老的 raft leader 被分到了少数派这边，多数派这边选举出了新的 raft leader ，如果此时的 region leader 在多数派这边。因为所有的读写请求都会找到 region leader 进行，即使在原来没有出现网络分区的情况下，客户端的请求也都是要走 node 1 ，经由 node 1 转发给 node 5，客户端不会直接访问 node 5，所以此时即使网络出现分区，新 leader 也正好在多数派这边，读写直接就打到 node 1 上，皆大欢喜，没有 stale read。第三种情况，region leader 落在少数派这边，老 raft leader 在多数派这边，这种情况客户端的请求找到 region leader，他发现的无法联系到 leader（因为在少数派这边没有办法选举出新的 leader），请求会失败，直到本次 region leader 的 lease 过期，同时新的 region leader 会在多数派那边产生（因为新的 region leader 需要尝试走一遍 raft 流程）。因为老的 region leader 没办法成功的写入，所以也不会出现 stale read。但是付出的代价是在 region leader lease 期间的系统的可用性。第四种情况和第三种情况类似，多数派这边会产生新的 raft leader 和 region leader。总体来说，这种方法牺牲了一定的可用性（在脑裂时部分客户端的可用性）换取了一致性的保证。"},
		{"url": "https://pingcap.com/meetup/meetup-2016-11-19/",
		"title": "PingCAP 第 30 期 NewSQL Meetup", 
		"content": " PingCAP 第 30 期 NewSQL Meetup 2016-11-19 刘锦龙&amp;amp;刘寅 PingCAP PingCAPPingCAP ![]() 微信号pingcap2015功能介绍PingCAP 专注于新型分布式数据库的研发，是知名开源数据库 TiDB (GitHub 总计10000+ stars ) 背后的团队，总部设在北京，是国内第一家开源的新型分布式关系型数据库公司、国内领先的大数据技术和解决方案提供商。NewSQL Meetup今天是 PingCAP 第 30 期 Meetup，主题是墨迹天气气象算法负责人刘锦龙分享的《深度学习，众包数据与短时临近预报系统》以及刘寅分享的《谈谈 TiDB-Binlog 的设计》。▌Topic 1：深度学习，众包数据与短时临近预报系统Lecturer：刘锦龙，北大理论物理博士，墨迹天气气象算法负责人，负责墨迹相关天气预测算法的研发工作，主要方向为机器学习和深度学习。Content：深入介绍如何将深度学习的最新技术用于革新传统气象预测的一些研究和应用，以及如何处理从用户获取的众包反馈数据并进而改进天气预报的精准度。▌Topic 2：谈谈 TiDB-Binlog 的设计Lecturer：刘寅，PingCAP engineer，现负责 TiDB 商业产品开发和自动化运维。Content：随着TiDB的不断稳定和完善，我们也逐步开发了很多TiDB周边工具。今天主要介绍了TiDB-Binlog设计上的一些考量和实现细节。TiDB-Binlog 可实时记录TiDB的一切数据变化，可以用来做集群的实时备份和恢复，也可以将数据完整地实时同步到下游的异构数据平台。目前我们已经把TiDB-Binlog部署到真实客户的线上系统中，利用实时同步的特性保障了上线过程的可靠和数据安全。今天的分享着重介绍了Binlog的原理，以及生成、收集和还原的过程细节。特别鸣谢：场地赞助-泰利驿站PingCAP Meetup"},
		{"url": "https://pingcap.com/blog/2016-11-17-mvcc-in-tikv/",
		"title": "MVCC in TiKV", 
		"content": " MVCC Introduction to concurrency control Transaction isolation is important for database management system. Because database should provide an illusion that the user is the only one who connects to the database, which greatly simplifies application development. But, the concurrency controlling problems like data races must be resolved since there will be a lot of connections to the database. Due to this background, the database management system (DBMS) ensures that the resulting concurrent access patterns are safe, ideally by serializablity.Though serializablity is a great concept, it is hard to implement efficiently. A classical solution is a variant of Two-Phase Locking, aka 2PL. Using 2PL, the DBMS maintains read and write locks to ensure that conflicting transactions are executed in a well-defined order, which results in serializable execution schedules. But, locking, however, has several drawbacks: First, readers and writers block each other. Second, most transactions are read-only and therefore harmless from a transaction-ordering perspective. Using a locking-based isolation mechanism, no update transaction is allowed to change a data object that has been read by a potentially long-running read transaction and thus has to wait until the read transaction finishes. This severely limits the degree of concurrency in the system.Multi-Version Concurrency Control(MVCC) is an elegant solution for this problem, in which each update creates a new version of the data object instead of updating data objects in-place, such that concurrenct readers can still see the old version while the update transaction proceeds concurrently. Such stradegy can prevent read-only transactions from waiting, and in fact do not have to use locking at all. This is an extremely desirable property and the reason why many DBMS implements MVCC, e.g., PostgreSQL, Oracle, Microsoft SQL Server.MVCC in TiKV Let&amp;rsquo;s dive into TiKV&amp;rsquo;s MVCC implementation, located at src/storage.Timestamp Oracle(TSO) Since TiKV is a disritbuted storage system, it needs a globally unique time service, called Timestamp Oracle(TSO), to allocate a monotonic increasing timestamp. This function is provided in PD in TiKV, which is provided by TrueTime API by using multiple modern clock references(GPS and atomic alocks) in Spanner. So keep in mind that every TS represents a monotonic increasing timestamp.Storage To divie into the Transaction part in TiKV, src/storage is a good begining, which implements the entries. Storage is a struct that actually receives the get/Scan commands.pub struct Storage { engine: Box&amp;lt;Engine&amp;gt;, sendch: SendCh&amp;lt;Msg&amp;gt;, handle: Arc&amp;lt;Mutex&amp;lt;StorageHandle&amp;gt;&amp;gt;, } impl Storage { pub fn start(&amp;amp;mut self, config: &amp;amp;Config) -&amp;gt; Result&amp;lt;()&amp;gt; { let mut handle = self.handle.lock().unwrap(); if handle.handle.is_some() { return Err(box_err!(&amp;#34;scheduler is already running&amp;#34;)); } let engine = self.engine.clone(); let builder = thread::Builder::new().name(thd_name!(&amp;#34;storage-scheduler&amp;#34;)); let mut el = handle.event_loop.take().unwrap(); let sched_concurrency = config.sched_concurrency; let sched_worker_pool_size = config.sched_worker_pool_size; let sched_too_busy_threshold = config.sched_too_busy_threshold; let ch = self.sendch.clone(); let h = try!(builder.spawn(move || { let mut sched = Scheduler::new(engine, ch, sched_concurrency, sched_worker_pool_size, sched_too_busy_threshold); if let Err(e) = el.run(&amp;amp;mut sched) { panic!(&amp;#34;scheduler run err:{:?}&amp;#34;, e); } info!(&amp;#34;scheduler stopped&amp;#34;); })); handle.handle = Some(h); Ok(()) } } This start function helps to explain how a storage runs.Engine Engine is the trait which describes the actual database used in storage system, which is implemented in raftkv and Enginerocksdb.StorageHandle StorageHandle is the struct that handles commands received from sendch powered by mio.Then the following functions like async_get and async_batch_get will send the corresponding commands to the channel, which can be got by the scheduler to execute asynchronously.All right, the MVCC protocol calling is exactly implemented in Scheduler. The storage receives commands from clients and sends commands as messages to the scheduler. Then the scheduler will process the command or call corresponding asynchronous function. There are two types of operations, reading and writing. Reading is implemented in MvccReader, which is easy to understand. Writing part is the core of MVCC implementation.MVCC Here comes the core of the transaction model of TiKV, which called 2-Phase Commit powered by MVCC. There are two stages in one transaction.Prewrite  Select one row as the primary row, the others as the secondary rows. Lock the primary row. Before locking, it will check whether there is other locks on this row or whether there are some commits located after startTS. These two situations will lead to conflicts.If any of themhappens, rollback will be called. Repeat the operations on secondary row.  Commit  Write to the CF_WRITE with commitTS. Delete the corresponding lock.  Rollback Rollback is called when there are conflicts during the prewrite.Garbage collector It is easy to predict that there will be more and more MVCC versions if there is no Garbage Collector to remove the invalid versions. But we cannot just simply removeall the versions before a safe point. Since there maybe only one version for a key, it will be kept. In TiKV, if there is any Put or Delete before the safe point, then all the latter writes can be deleted, otherwise only Delete, Rollback and Lock will be deleted.TiKV-Ctl for MVCC During developing and debugging, sometimes we need to know the MVCC version information.So we develop a new tool for searching the MVCC information. TiKV stores the Key-Values, Locks and Writes informations in CF_DEFAULT, CF_LOCK, CF_WRITE. All the values of the CF are encoded as following:    default lock write     key z{encoded_key}{start_ts(desc)} z{encoded_key} z{encoded_key}{commit_ts(desc)}   value {value} {flag}{primary_key}{start_ts(varint)} {flag}{start_ts(varint)}    Details can be found here.Since all the MVCC version information is stored as CF Key-Values in Rocksdb, to search for a Key&amp;rsquo;s version information, we just need to encode the key with different formats then search in the corresponding CF. The CF Key-Values are modeled by MvccKv."},
		{"url": "https://pingcap.com/blog-cn/mpp-smp-tidb/",
		"title": "MPP and SMP in TiDB", 
		"content": " 今天主要是想把我们 TiDB 做 SQL 性能优化的一些经验和一些思考，就此跟大家探讨一下。题目写的比较大，但是内容还是比较简单。我们做 TiDB 的 SQL 层时，一开始做的很简单，就是通过最简单的 KV 接口(Get/Set/Seek)去存数据、取数据，做一些非常直白、简单的计算。然而后来我们发现，这个方案在性能上不可接受，可能行不通，我们就重新思考了这个事情。TiDB 的目标是做一个 NewSQL 的 database ，什么是 NewSQL？从 Wikipedia 上我们看到 NewSQL 的定义『NewSQL is a class of modern relational database management systems that seek to provide the same scalable performance of NoSQL systems for online transaction processing (OLTP) read-write workloads while still maintaining the ACID guarantees of a traditional database system.』。首先NewSQL Database 需要能存储海量数据，这点就像一些 NoSQL 数据库一样。然后，能够提供事务的功能。所以 NewSQL 中的计算，主要有两个特点。第一个，就是数据是海量的，这跟 MySQL 传统数据有可能不一样，他们当然可以通过一些 sharding 的方式来进行处理，但是 sharding 之后会损失，比如说你不能跨节点做 Join，没有跨节点事务等。二是，在海量数据情况下，我们还需要对数据进行随时的取用，因为数据存在那，你算不出来就是对用户没有价值、没有意义的，所以我们需要在海量数据的前提下，能够随时把它计算出来。计算主要分两种任务，一种是 OLTP 的 query，就是简单的查询，通过一些索引，就能过滤到大部分数据，然后能够做一些简单的处理和计算。还有一种是 OLAP的 query，这个一般来说会涉及到大量的数据及复杂的 query，比如说 Join，SubQuery 以及 Aggregate 这样一些东西。并且在海量数据的情况下，很多传统数据库上的 OLTP 的 query，看起来可能更像一个 OLAP 的 query，因为涉及到的数据量会非常大。那么在这样一些背景下，我们应该怎么考虑 SQL 计算呢？简单来讲，我们需要想办法在海量数据上，对计算进行优化和提速。传统数据库的提速方法 传统数据库有很多提速的方法，有两种比较有名的，一个是 MPP，它的架构参见下图。计算数据是分在不同的节点上，并且很可能不在一台机器上，它们通过高速的网络连接，让每个节点都自己去处理数据，处理完数据之后再汇总在一起，最后给用户返回结果。这个架构最大的特点就是它是一种 share nothing 的架构，也就是说节点之间的计算是相互不知道的，然后他们只执行自己的事情，不需要去交换数据，这是一种架构。还有一种叫 SMP，这个跟 MPP 对应，它是一种 share everything 的架构。这种架构一般都是在一个 note 上、一个计算节点上进行，然后它们有多个 CPU 同时计算，它们会去通过总线去共享，比如说内存、IO 这样一些东西，这是一种 share everything 的一个架构。可以看到 MPP 和 SMP 这是两种传统数据库中用来提速的一些方案。我看到 PG（PostgresSQL） 最新的代码，他们已经支持了并行的处理，比如他们可以做并行 scan，他们可以去定义并发度，比如说 scan 一个表，他们利用多核这个特性，能够提速很多。当然这个肯定不是线性的，因为你去做并行，做数据交换，是有 overhead 的，这个 overhead 在你并行度太高的时候是挺大的。TiDB 说完传统数据，我们说一下 TiDB。TiDB 的架构如下图所示。虚框所标的是 TiDB SQL layer，它的最上层是 protocol layer ，就是解析 MySQL 协议。然后是 SQL layer，它主要负责 SQL 的解析、查询，查询计划的制定以及生成执行器。它会调用底下的接口来获取数据，然后进行 SQL 的运算。接下来这一层，可以看到，分两个接口，一个就是 KV 的 API ，就是我们会把数据映射为 KV，因为我们最底下一层是一个 KV 的 storage engine 。比如说一行数据我们会用 Row ID 加上 Table ID 加上 Database ID 这些来做一个 key，然后把这行里面的数据作为 value ，再扔到 KV 中，就转成一种 key-value 的模式。对于 index 来说，我们也是转成了 KV 的模式，因为我们的 KV 有一个特点，就是可以进行有序的 scan。比如说你要在某些 Column 上建了 index，我们就会把这个 Column 编码成一个 key，然后再加上 index ID、Table ID 之类的东西，也 send 到这个 KV 里面去。就是说我们的上层，你可以认为只通过这个 API 也是能够正确的获取到数据、访问数据的，大概就是这样一个架构。然后这里还有一个 DistSQL API，这个是我们分布式计算框架的对上层提供了一个抽象，后面我会详细介绍这个 API 。最下面一层，就是我们的 TiKV 。你可以把 TiKV 考虑成一个纯的分布式的带事务的 key-value engine 。为了支持我们分布式 SQL 的 API ，我们给它上面加了更多功能，在这里我们有参考 HBase 的 coprocessor 方案，然后提供一些 EndPoint 的功能，这样对上一层可以提供更丰富的语义。那么，我们怎么让 SQL 在 TiDB/TiKV 中跑的更快？这半年多我们一直在做这个事情。第一就是不管是 NewSQL 数据库还是传统数据库，我们肯定要对 optimizer 进行一些优化，在这方面我们做了特别多特别多的事情，包括常量折叠，后面还会做更多的，比如常量传播这些。然后 Join 怎么去选择，还有就是我们现在有一个 Cost Based Optimize 的一个框架，我们会考虑下层数据的统计信息，然后在统计信息的基础上，再制订查询计划，所以这是一个巨大的坑，我们正在努力的填它。我觉得 Google F1 这部分做的很好，它应该也做了挺多优化，但其它的数据库我觉得倒不一定有我们做的好，比如我看了一下 Spark，它的 Optimizor 比较简单，就是用了大概一部分 Rule 不断地去 Apply。我想主要的原因是 Spark 定位于做一个通用的计算框架，所以对底层的数据信息无法有细致的了解。除了优化器这块儿，确定一个查询计划之后，怎么去执行它，也是很重要的事情。就是说同样的一个计划，可能用不同的执行器执行起来会有不同的效果。因为我们做的是一个 NewSQL 数据库，数据是分布在很多很多节点上的，我们完全可以利用数据广泛分布的特点，提高整体的并行度。而且我们 TiDB SQL layer 是用 Go 来写的，Go 在多核机器上能够发挥并发优势，它的 Goroutine 调度的开销很小，我们可以建很多 Goroutine，利用现在 CPU 越来越多的这个特性，去提高计算的并行度。还有就是说像传统的，比如 MySQL 、PG 上的东西，它主要还是访问内存、访问硬盘这样的一些开销。但对 TiDB 来说，它很大一部分都耗在网络上了，就是说你发一个请求过去，拿到数据，要走一遍网络，这还是有挺大开销的。所以我们一个很重要的目的就是让整个数据的流程尽可能快起来，尽可能的平滑，把网络这种开销尽可能的搞掉。先讲一下，本次分享的标题叫 “MPP and SMP in TiDB” 主要是说我们有一个并行的、分布式的计算框架，怎么用这个计算框架来提高我们 SQL 计算的并行度，还有就是我们提供了coprocessor ，刚才介绍了，是从 HBase 来的。上图中的 Regions 是 TiKV 的一个 region servers ，我们可以在这里面插入一些代码，让它能够执行我们给它定义的一些任务。大家可以先看上图，整个的分层大概就是这样一个流程。它的最上面是执行器，就是我们经过 SQL Optimizer 之后生成执行计划，而后我们会根据执行计划生成这个执行器。接下来是刚才说的 DistSQL API，它会调用我们的 TiClient，就是说是一个 TiKV 的 Client，通过它来访问 TiKV，通过 Rpc 发送请求。它还有一个很重要的功能就是能获取数据分布在哪儿。因为一个表会分成很多 KV，这些 KV 是散列在很多很多 TiKV server 上的，它很重要的功能就是干这个事情，就相当于数据路由，是它一个重要的任务。最下面就是 region server ，这中间可以认为是网络。这样分了几层之后，每一层都有它自己的任务，就是说我们每一层都抽的很薄。Executor 最重要的工作就是制定执行逻辑。就是说它要告诉下面你需要干什么事情。比如你是需要做 count ，还是需要计算 Where ，它理解的是 SQL 逻辑。DistSQL API 是两层之间的封装，就是说我们下面除了 TiKV 之外，还可以接其它的存储引擎，只要你满足我们这个接口的定义就可以。然后 DistSQL API 把上下隔离了，它提供了一个 API ，这个 API 稍后我会详细介绍一下。然后 TiKV Client 就是数据路由，数据的分发，比如说请求失败了怎么办，它干的就是数据请求发送的。Regions 这一层存储了数据，它需要利用上层传下来这个计算逻辑，在这个数据上进行计算。大概就是这些层，每一层只干了自己的事情，不需要关心下一层的实现。这个 API 就是刚才上面那个 DistSQL 提供的对外最重要的一个接口，叫 Select。它有几个参数一个是 client。就是说只要你的 KV 引擎满足带事务、满足 KV 接口，并且满足这个 client 的一些接口，就可以接入 TiDB。有一些其他的厂商跟我们合作，在他们的 KV 上也能 run 我们这个分布式的 SQL ，这是相当于是 KV 的 Client。第二个，就是 SelectRequest。这个东西是由上层执行器构造出来的，它把计算上的逻辑，比如说一些表达式要不要排序、要不要做聚合，所有的信息都放在 req 里边，是一个 Protobuf 结构，然后发给 Select 接口，它会扔到下层，最后扔到那个 region server 上进行计算。还有 concurrency int。这个其实只是个建议，它的作用是提示下层要不要并发的去请求数据。因为我们数据是分在很多 region server 上的，所以要考虑去以多大并发度去发。然后，KeepOrder 这个参数是这样的，就是说下层是有很多 region server 的，我们要把请求发在很多 region server 上，但先发的结果不一定先返回来，因为有网络延迟、计算的延迟，所以这个顺序是不能预先设定好的。但是并不是所有的计算任务都依赖于数据的顺序，大多数情况下从拿到第一个结果开始就可以计算了，你就需要把结果先返回上来。在另一些情况下，我们需要下面按某一种顺序返回结果的，比如说 SQL 语句中有 OrderBy，并且对应的列上有所因，那么制定出来的查询计划很有可能就是首先扫描索引，然后依赖于索引的顺序对数据进行排序，因为索引是有序的，可以节约掉排序的时间。假设我们按照扫索引的顺序给你返回数据的话，你就可以不用后续自己去排了，这个 sort 已经帮你做好了。这个时候，就需要下层数据，下层的这个接口对你返回的数据是按照某个 key 有序的，所以这里就加了一个 KeepOrder 。当你不需要下层数据有序的时候，你就可以把这个设为 false ，假设这是 TiKV ，然后这是 TiDB，假设这个请求发了好几个 region server，虽然这个 TiKV 你可以认为是一个大的 key 的空间，并且按照 key 有序，假设某个后面的 key range 的请求先返回，如果你不要求下层返回有序，你完全可以把这个请求的结果先返回到上面进行计算，让整个计算过程能够更快。接着这个接口返回了一个数据结构，叫 SelectResult ，这个结构可以认为它是一个迭代器，因为我们下层是有很多 TiKV ，然后每个结果是一个 PartialResult。上层分装了一个 SelectResult ，就是一个 PartialResult 的迭代器。通过这个的 next 方法可以拿到下一个 PartialResult ，但是具体的下一个 PartialResult 是哪个的 region server ，是不一定的，就像我刚才说的，取决于你要不要 KeepOrder 。 SelectResult 的内部实现你可以认为是个 pipeline。我们会并发的去往各个 region server 发数据，但是可能有的先返回给你了，有的后返回给你了。虽然某个 region 存储数据的 key 的范围比另外一个 region 的小，但是要不要先把某个 region 的结果返回给你，是由你这个 KeepOrder 决定的。然后如果你是不要 KeepOrder ，那我们就是一个 channel，也就是有数据返回了就往里扔，扔完之后就可以对上返回了。然后这个 API 返回的是 SelectResult 。如果你是要求下面有序，那么这里会对这个 request 建一个slice ，假如前面的没返回来，是不会把后面的 request 返回给你的。主要是定义了这样一个语义，我们就可以很方便的决定下面的这个行为。这个地方，我们是做了一些优化的。当 KeepOrder 为 false 的时候，如果它先到了一部分，你可以先处理那一部分。就是说这个 KeepOrder 是 truth 还是 false，影响了最下面一层的返回逻辑。比如你不要这个 KeepOrder ，那么我给一堆 KV server 发了request 之后，这个 response 是扔到一个统一的 channel 里面的，谁先返回就把谁的扔进来，然后外面调用 next 就已经拿到了。但如果你是 KeepOrder 为 truth ，我下面所有的 request 按照这个 key 的顺序建了一个 slice，然后你调这个的 next 的时候，它是遍历这个 slice，就是如果前面那个 result 没拿到，它是不会往后面走的，即使后面 response 已经到了，它也不会给你返回的。这个接口还是挺重要的。举个例子吧，来看一下我们怎么去做这个分布式的 SQL ，比如 select * from t where age &amp;gt; 20 and age &amp;lt; 30。在这里， t 这个数据可能分布在很多 region 上，我们会把要扫描 t 这个表的信息以及这个 filter，整个推到 region server 上进行计算，然后 Executor 构造的就是比如说我要扫哪个表，它的表的信息是什么。通过这个表的信息，下层就能去构造 KV，Ti-Client 就能通过这些 KV 找到这个表的数据分布在哪些 region 上面，然后它就会把这个请求发在这些 region 上面，同时在 selecter request 信息里面也会带上我要做 filter 的那个 condition，它会把要查询的表的元信息等都发到存储个表数据的 region server 上。然后它再利用表的信息把数据拿出来并过一遍 filter，只有过了这个 filter 的数据，之后才会返回给 TiDB。这样一方面能够增加这个计算并行度，因为我们可能有很多 region server 存储这个数据，当然只有有这个数据的 region server 我们才会发请求过去。第二，你把 filter 推下去很重要的好处就是，只有过了这个 filter 才能返回，这样返回你的数量是减小了很多，你就能减小无意义的网络传输。据我们在 Google 那边了解的情况是，他们好像不太干这个事儿，因为他们内部网络实在太快了，他们可以直接把所有数据 load 过来，在这边去算，可以认为整个 IPC 就是一台巨大的机器。就像所有的机器都通过总线连起来，有这种感觉，它就不考虑效率问题。所以他们的效率我觉得还是值得考虑。但大部分用户可能是用一些廉价的 PC，我们这个方案其实是更好的一个方案。这是比较简单的一种。我们还有分布式 aggregate，这种是更好搞的，就假设我们把这个 ability 也推下去，比如说 count，然后你在这边过完 filter，你只需要每个 region 计算自己上的结果，给我返回一个这边的计数就可以了，然后我在 TiDB Server 再把这个 partial result 制成一个 final result ，比如说你这边是五个，这边十个，这边二十个，那我加起来直接算个 sum 返回给你就可以了，这是它最大的好处。刚才是介绍了一下我们 TiDB 的一个分布式的计算框架，然后下面我们介绍一下之前我们做这两项工作，就是把这个 Join 让它变得更快一些。最开始的时候，TiDB 进行 Join 运算非常慢，其实这也是 MySQL 的缺陷，MySQL 只支持 look up Join ，它是说你先取一个表的数据，然后用每一行数据去另一个表拿数据，这个在数据量大的时候是很慢的。所以我们支持了 Hash Join。Hash Join 的话比如说你有一个大一个小两个表，大表在千万量级，其实也不是特别大，内存还是能放下的。你可以用一个小表去建一个 Hash 表，然后再从大表去读数据，读数据的时候你可以去 Hash 表中拿数据，待拿到 Join on 那个条件能对应上那个小表的数据，再算，再过一些 filter ，看看要不要输出这一行，然后对外返回结果。为了让它算的更快，我们最近干了一些工作。第一，我们读小表的数据和读大表的数据，是可以并行来搞的，你可以想象成两条数据流。因为我们网络延迟是比较大的，所以我们想让数据尽可能的平滑、平顺地去流动，对于 Join 读取 DataSource 有两个事情要做，第一个是读小表数据，第二个是读大表数据，我们读小表数据可以用一个单独的线程来做，然后它就会发请求去读小表数据。拿到小表数据之后，拿到一行，然后它就可以对这行扔到一个 Hash 表中，算一个 key 。与此同时，我们可以新建另一个线程，它在这同时去读大表的数据，读完大表的数据之后，它再扔给一堆 worker，这堆 worker 就是做 Join 这个事的。就比如说它把数据虽然发到这些 worker，这些 worker 拿到大表的一行，算出 Join 的 key ，然后它去 Hash 表中去拿到对应的小表的数据，再看能不能过这个 Join 的 filter，最后再输出结果。这个时候，整个数据可以看到，已经尽可能的去并行，不会因为比如说小表数据没读到，就阻碍大表拿数据。当然，这个中间还有个同步的问 …"},
		{"url": "https://pingcap.com/meetup/memoir/meetup-2016-11-15/",
		"title": "MPP and SMP in TiDB", 
		"content": "  本篇文章整理自第 21 期 PingCAP NewSQL Meetup 上申砾分享的《MPP and SMP in TiDB》内容。干货很多，全文阅读预计需要 20 分钟。 今天主要是想把我们 TiDB 做 SQL 性能优化的一些经验和一些思考，就此跟大家探讨一下。题目写的比较大，但是内容还是比较简单。我们做 TiDB 的 SQL 层时，一开始做的很简单，就是通过最简单的 KV 接口(Get/Set/Seek)去存数据、取数据，做一些非常直白、简单的计算。然而后来我们发现，这个方案在性能上不可接受，可能行不通，我们就重新思考了这个事情。TiDB 的目标是做一个 NewSQL 的 database ，什么是 NewSQL？从 Wikipedia 上我们看到 NewSQL 的定义『NewSQL is a class of modern relational database management systems that seek to provide the same scalable performance of NoSQL systems for online transaction processing (OLTP) read-write workloads while still maintaining the ACID guarantees of a traditional database system.』。首先NewSQL Database 需要能存储海量数据，这点就像一些 NoSQL 数据库一样。然后，能够提供事务的功能。所以 NewSQL 中的计算，主要有两个特点。第一个，就是数据是海量的，这跟 MySQL 传统数据有可能不一样，他们当然可以通过一些 sharding 的方式来进行处理，但是 sharding 之后会损失，比如说你不能跨节点做 Join，没有跨节点事务等。二是，在海量数据情况下，我们还需要对数据进行随时的取用，因为数据存在那，你算不出来就是对用户没有价值、没有意义的，所以我们需要在海量数据的前提下，能够随时把它计算出来。计算主要分两种任务，一种是 OLTP 的 query，就是简单的查询，通过一些索引，就能过滤到大部分数据，然后能够做一些简单的处理和计算。还有一种是 OLAP的 query，这个一般来说会涉及到大量的数据及复杂的 query，比如说 Join，SubQuery 以及 Aggregate 这样一些东西。并且在海量数据的情况下，很多传统数据库上的 OLTP 的 query，看起来可能更像一个 OLAP 的 query，因为涉及到的数据量会非常大。那么在这样一些背景下，我们应该怎么考虑 SQL 计算呢？简单来讲，我们需要想办法在海量数据上，对计算进行优化和提速。传统数据库的提速方法 传统数据库有很多提速的方法，有两种比较有名的，一个是 MPP，它的架构参见下图。计算数据是分在不同的节点上，并且很可能不在一台机器上，它们通过高速的网络连接，让每个节点都自己去处理数据，处理完数据之后再汇总在一起，最后给用户返回结果。这个架构最大的特点就是它是一种 share nothing 的架构，也就是说节点之间的计算是相互不知道的，然后他们只执行自己的事情，不需要去交换数据，这是一种架构。还有一种叫 SMP，这个跟 MPP 对应，它是一种 share everything 的架构。这种架构一般都是在一个 note 上、一个计算节点上进行，然后它们有多个 CPU 同时计算，它们会去通过总线去共享，比如说内存、IO 这样一些东西，这是一种 share everything 的一个架构。可以看到 MPP 和 SMP 这是两种传统数据库中用来提速的一些方案。我看到 PG（PostgresSQL） 最新的代码，他们已经支持了并行的处理，比如他们可以做并行 scan，他们可以去定义并发度，比如说 scan 一个表，他们利用多核这个特性，能够提速很多。当然这个肯定不是线性的，因为你去做并行，做数据交换，是有 overhead 的，这个 overhead 在你并行度太高的时候是挺大的。TiDB 说完传统数据，我们说一下 TiDB。TiDB 的架构如下图所示。虚框所标的是 TiDB SQL layer，它的最上层是 protocol layer ，就是解析 MySQL 协议。然后是 SQL layer，它主要负责 SQL 的解析、查询，查询计划的制定以及生成执行器。它会调用底下的接口来获取数据，然后进行 SQL 的运算。接下来这一层，可以看到，分两个接口，一个就是 KV 的 API ，就是我们会把数据映射为 KV，因为我们最底下一层是一个 KV 的 storage engine 。比如说一行数据我们会用 Row ID 加上 Table ID 加上 Database ID 这些来做一个 key，然后把这行里面的数据作为 value ，再扔到 KV 中，就转成一种 key-value 的模式。对于 index 来说，我们也是转成了 KV 的模式，因为我们的 KV 有一个特点，就是可以进行有序的 scan。比如说你要在某些 Column 上建了 index，我们就会把这个 Column 编码成一个 key，然后再加上 index ID、Table ID 之类的东西，也 send 到这个 KV 里面去。就是说我们的上层，你可以认为只通过这个 API 也是能够正确的获取到数据、访问数据的，大概就是这样一个架构。然后这里还有一个 DistSQL API，这个是我们分布式计算框架的对上层提供了一个抽象，后面我会详细介绍这个 API 。最下面一层，就是我们的 TiKV 。你可以把 TiKV 考虑成一个纯的分布式的带事务的 key-value engine 。为了支持我们分布式 SQL 的 API ，我们给它上面加了更多功能，在这里我们有参考 HBase 的 coprocessor 方案，然后提供一些 EndPoint 的功能，这样对上一层可以提供更丰富的语义。那么，我们怎么让 SQL 在 TiDB/TiKV 中跑的更快？这半年多我们一直在做这个事情。第一就是不管是 NewSQL 数据库还是传统数据库，我们肯定要对 optimizer 进行一些优化，在这方面我们做了特别多特别多的事情，包括常量折叠，后面还会做更多的，比如常量传播这些。然后 Join 怎么去选择，还有就是我们现在有一个 Cost Based Optimize 的一个框架，我们会考虑下层数据的统计信息，然后在统计信息的基础上，再制订查询计划，所以这是一个巨大的坑，我们正在努力的填它。我觉得 Google F1 这部分做的很好，它应该也做了挺多优化，但其它的数据库我觉得倒不一定有我们做的好，比如我看了一下 Spark，它的 Optimizor 比较简单，就是用了大概一部分 Rule 不断地去 Apply。我想主要的原因是 Spark 定位于做一个通用的计算框架，所以对底层的数据信息无法有细致的了解。除了优化器这块儿，确定一个查询计划之后，怎么去执行它，也是很重要的事情。就是说同样的一个计划，可能用不同的执行器执行起来会有不同的效果。因为我们做的是一个 NewSQL 数据库，数据是分布在很多很多节点上的，我们完全可以利用数据广泛分布的特点，提高整体的并行度。而且我们 TiDB SQL layer 是用 Go 来写的，Go 在多核机器上能够发挥并发优势，它的 Goroutine 调度的开销很小，我们可以建很多 Goroutine，利用现在 CPU 越来越多的这个特性，去提高计算的并行度。还有就是说像传统的，比如 MySQL 、PG 上的东西，它主要还是访问内存、访问硬盘这样的一些开销。但对 TiDB 来说，它很大一部分都耗在网络上了，就是说你发一个请求过去，拿到数据，要走一遍网络，这还是有挺大开销的。所以我们一个很重要的目的就是让整个数据的流程尽可能快起来，尽可能的平滑，把网络这种开销尽可能的搞掉。先讲一下，本次分享的标题叫 “MPP and SMP in TiDB” 主要是说我们有一个并行的、分布式的计算框架，怎么用这个计算框架来提高我们 SQL 计算的并行度，还有就是我们提供了coprocessor ，刚才介绍了，是从 HBase 来的。上图中的 Regions 是 TiKV 的一个 region servers ，我们可以在这里面插入一些代码，让它能够执行我们给它定义的一些任务。大家可以先看上图，整个的分层大概就是这样一个流程。它的最上面是执行器，就是我们经过 SQL Optimizer 之后生成执行计划，而后我们会根据执行计划生成这个执行器。接下来是刚才说的 DistSQL API，它会调用我们的 TiClient，就是说是一个 TiKV 的 Client，通过它来访问 TiKV，通过 Rpc 发送请求。它还有一个很重要的功能就是能获取数据分布在哪儿。因为一个表会分成很多 KV，这些 KV 是散列在很多很多 TiKV server 上的，它很重要的功能就是干这个事情，就相当于数据路由，是它一个重要的任务。最下面就是 region server ，这中间可以认为是网络。这样分了几层之后，每一层都有它自己的任务，就是说我们每一层都抽的很薄。Executor 最重要的工作就是制定执行逻辑。就是说它要告诉下面你需要干什么事情。比如你是需要做 count ，还是需要计算 Where ，它理解的是 SQL 逻辑。DistSQL API 是两层之间的封装，就是说我们下面除了 TiKV 之外，还可以接其它的存储引擎，只要你满足我们这个接口的定义就可以。然后 DistSQL API 把上下隔离了，它提供了一个 API ，这个 API 稍后我会详细介绍一下。然后 TiKV Client 就是数据路由，数据的分发，比如说请求失败了怎么办，它干的就是数据请求发送的。Regions 这一层存储了数据，它需要利用上层传下来这个计算逻辑，在这个数据上进行计算。大概就是这些层，每一层只干了自己的事情，不需要关心下一层的实现。这个 API 就是刚才上面那个 DistSQL 提供的对外最重要的一个接口，叫 Select 。它有几个参数一个是 client。就是说只要你的 KV 引擎满足带事务、满足 KV 接口，并且满足这个 client 的一些接口，就可以接入 TiDB。有一些其他的厂商跟我们合作，在他们的 KV 上也能 run 我们这个分布式的 SQL ，这是相当于是 KV 的 Client。第二个，就是 SelectRequest 。这个东西是由上层执行器构造出来的，它把计算上的逻辑，比如说一些表达式要不要排序、要不要做聚合，所有的信息都放在 req 里边，是一个 Protobuf 结构，然后发给 Select 接口，它会扔到下层，最后扔到那个 region server 上进行计算。还有 concurrency int。这个其实只是个建议，它的作用是提示下层要不要并发的去请求数据。因为我们数据是分在很多 region server 上的，所以要考虑去以多大并发度去发。然后，KeepOrder 这个参数是这样的，就是说下层是有很多 region server 的，我们要把请求发在很多 region server 上，但先发的结果不一定先返回来，因为有网络延迟、计算的延迟，所以这个顺序是不能预先设定好的。但是并不是所有的计算任务都依赖于数据的顺序，大多数情况下从拿到第一个结果开始就可以计算了，你就需要把结果先返回上来。在另一些情况下，我们需要下面按某一种顺序返回结果的，比如说 SQL 语句中有 OrderBy，并且对应的列上有所因，那么制定出来的查询计划很有可能就是首先扫描索引，然后依赖于索引的顺序对数据进行排序，因为索引是有序的，可以节约掉排序的时间。假设我们按照扫索引的顺序给你返回数据的话，你就可以不用后续自己去排了，这个 sort 已经帮你做好了。这个时候，就需要下层数据，下层的这个接口对你返回的数据是按照某个 key 有序的，所以这里就加了一个 KeepOrder 。当你不需要下层数据有序的时候，你就可以把这个设为 false ，假设这是 TiKV ，然后这是 TiDB，假设这个请求发了好几个 region server，虽然这个 TiKV 你可以认为是一个大的 key 的空间，并且按照 key 有序，假设某个后面的 key range 的请求先返回，如果你不要求下层返回有序，你完全可以把这个请求的结果先返回到上面进行计算，让整个计算过程能够更快。接着这个接口返回了一个数据结构，叫 SelectResult ，这个结构可以认为它是一个迭代器，因为我们下层是有很多 TiKV ，然后每个结果是一个 PartialResult。上层分装了一个 SelectResult ，就是一个 PartialResult 的迭代器。通过这个的 next 方法可以拿到下一个 PartialResult ，但是具体的下一个 PartialResult 是哪个的 region server ，是不一定的，就像我刚才说的，取决于你要不要 KeepOrder 。SelectResult 的内部实现你可以认为是个 pipeline。我们会并发的去往各个 region server 发数据，但是可能有的先返回给你了，有的后返回给你了。虽然某个 region 存储数据的 key 的范围比另外一个 region 的小，但是要不要先把某个 region 的结果返回给你，是由你这个 KeepOrder 决定的。然后如果你是不要 KeepOrder ，那我们就是一个 channel，也就是有数据返回了就往里扔，扔完之后就可以对上返回了。然后这个 API 返回的是 SelectResult。如果你是要求下面有序，那么这里会对这个 request 建一个slice ，假如前面的没返回来，是不会把后面的 request 返回给你的。主要是定义了这样一个语义，我们就可以很方便的决定下面的这个行为。这个地方，我们是做了一些优化的。当 KeepOrder 为 false 的时候，如果它先到了一部分，你可以先处理那一部分。就是说这个 KeepOrder 是 truth 还是 false，影响了最下面一层的返回逻辑。比如你不要这个 KeepOrder ，那么我给一堆 KV server 发了request 之后，这个 response 是扔到一个统一的 channel 里面的，谁先返回就把谁的扔进来，然后外面调用 next 就已经拿到了。但如果你是 KeepOrder 为 truth ，我下面所有的 request 按照这个 key 的顺序建了一个 slice，然后你调这个的 next 的时候，它是遍历这个 slice，就是如果前面那个 result 没拿到，它是不会往后面走的，即使后面 response 已经到了，它也不会给你返回的。这个接口还是挺重要的。举个例子吧，来看一下我们怎么去做这个分布式的 SQL ，比如 select * from t where age &amp;gt; 20 and age &amp;lt; 30。在这里， t 这个数据可能分布在很多 region 上，我们会把要扫描 t 这个表的信息以及这个 filter，整个推到 region server 上进行计算，然后 Executor 构造的就是比如说我要扫哪个表，它的表的信息是什么。通过这个表的信息，下层就能去构造 KV，Ti-Client 就能通过这些 KV 找到这个表的数据分布在哪些 region 上面，然后它就会把这个请求发在这些 region 上面，同时在 selecter request 信息里面也会带上我要做 filter 的那个 condition，它会把要查询的表的元信息等都发到存储个表数据的 region server 上。然后它再利用表的信息把数据拿出来并过一遍 filter，只有过了这个 filter 的数据，之后才会返回给 TiDB 。这样一方面能够增加这个计算并行度，因为我们可能有很多 region server 存储这个数据，当然只有有这个数据的region server 我们才会发请求过去。第二，你把 filter 推下去很重要的好处就是，只有过了这个 filter 才能返回，这样返回你的数量是减小了很多，你就能减小无意义的网络传输。据我们在 Google 那边了解的情况是，他们好像不太干这个事儿，因为他们内部网络实在太快了，他们可以直接把所有数据 load 过来，在这边去算，可以认为整个 IPC 就是一台巨大的机器。就像所有的机器都通过总线连起来，有这种感觉，它就不考虑效率问题。所以他们的效率我觉得还是值得考虑。但大部分用户可能是用一些廉价的 PC，我们这个方案其实是更好的一个方案。这是比较简单的一种。我们还有分布式 aggregate，这种是更好搞的，就假设我们把这个 ability 也推下去，比如说 count，然后你在这边过完 filter，你只需要每个 region 计算自己上的结果，给我返回一个这边的计数就可以了，然后我在 TiDB Server 再把这个 partial result 制成一个 final result ，比如说你这边是五个，这边十个，这边二十个，那我加起来直接算个 sum 返回给你就可以了，这是它最大的好处。刚才是介绍了一下我们 TiDB 的一个分布式的计算框架，然后下面我们介绍一下之前我们做这两项工作，就是把这个 Join 让它变得更快一些。最开始的时候，TiDB 进行 Join 运算非常慢，其实这也是 MySQL 的缺陷，MySQL 只支持 look up Join ，它是说你先取一个表的数据，然后用每一行数据去另一个表拿数据，这个在数据量大的时候是很慢的。所以我们支持了 Hash Join 。Hash Join 的话比如说你有一个大一个小两个表，大表在千万量级，其实也不是特别大，内存还是能放下的。你可以用一个小表去建一个 Hash 表，然后再从大表去读数据，读数据的时候你可以去 Hash 表中拿数据，待拿到 Join on 那个条件能对应上那个小表的数据，再算，再过一些 filter ，看看要不要输出这一行，然后对外返回结果。为了让它算的更快，我们最近干了一些工作。第一，我们读小表的数据和读大表的数据，是可以并行来搞的，你可以想象成两条数据流。因为我们网络延迟是比较大的，所以我们想让数据尽可能的平滑、平顺地去流动，对于 Join 读取 DataSource 有两个事情要做，第一个是读小表数据，第二个是读大表数据，我们读小表数据可以用一个单独的线程来做，然后它就会发请求去读小表数据。拿到小表数据之后，拿到一行，然后它就可以对这行扔到一个 Hash 表中，算一个 key 。与此同时，我们可以新建另一个线程，它在这同时去读大表的数据，读完大表的数据之后，它再扔给一堆 worker，这堆 worker 就是做 Join 这个事的。就比如说它把数据虽然发到这些 worker，这些 worker 拿到大表的一行，算出 Join 的 key ，然后它去 Hash 表中去拿到对应的小表的数据，再看能不 …"},
		{"url": "https://pingcap.com/blog/2016-11-15-Travelling-Back-in-Time-and-Reclaiming-the-Lost-Treasures/",
		"title": "Travelling Back in Time and Reclaiming the Lost Treasures", 
		"content": " About the History Read feature in TiDB Data is the core and is a matter of life and death for every business. So ensuring the data safety is the top priority of every database. From a macro point of view, the safety of data is not only about whether a database is stable enough that no data is lost, but also about whether a sufficient and convenient solution is in place when data is lost because of the business or human errors, for example, to solve the anti-cheat problem in the game industry or to meet the audit requirements in the financing business. If a proper mechanism is enabled in the database level, it will reduce the workload and the complexity of business development significantly.The traditional solution is to backup data in full volume periodically, in days or daily. These backups are to restore the data in case of accidents. But to restore data using backups is very costly because all the data after the backup time will be lost, which might be the last thing you want. In addition, the storage and computing overhead for full backups is no small cost for every company.But this kind of situation cannot be avoided completely. To err is human. For every fast iterative business, it is impossible for the code of the application to be fully tested. Fault data might be written because of the bug in the application logic or the activity of malicious users. When the issue is spotted, you can roll back the application to the earlier version immediately, but the fault data remains in the database.What can do you when things like this happen? The only thing you know is that the data is faulted. But what is the correct data? You have no idea. It would be great if you could go back in time and find the lost data.The History Read feature of TiDB supports reading the history versions and is specially tailored for this requirement and scenario. All the data before the faulted version can be accessed and therefore the damage can be minimized.How to use the History Read feature? It is very easy to use this feature. You can simply use the following Set statement:set @@tidb_snapshot = &amp;#34;2016-10-10 09:30:11.123&amp;#34; The name of the session variable is tidb_snapshot which is defined in TiDB. The value is a time string with precision of milliseconds. When this statement is executed, the data read by all the read requests issued from this client is at the set time and the write operation is not allowed because the history cannot be changed. If you want to exit the History Read mode and read the latest data, you can just execute the following Set statement:set @@tidb_snapshot = &amp;#34;&amp;#34; which sets the tidb_snapshot variable to be an empty string.It doesn’t matter even if there are Schema changes after the set time in history because TiDB will use the Schema of the set time in history for the SQL request.Comparing the History Read feature in TiDB with the similar features in other databases There is no such feature in MySQL. In other databases such as Oracle and PostgreSQL, this feature is called Temporal Table, which is a SQL standard. To use this feature, you need to use the special table creating grammar for the Temporal Table which has two more fields than the original table. The two extra fields are to store the valid time and are maintained by the system. When the original table is updated, the system inserts the data of the old version into the Temporal Table. When you need to retrieve the history data, you can use a special grammar to set the time in history and get the result.Compared with the similar features of other databases, the History Read feature in TiDB has the following advantages: - It is supported by default in the system. If it is not supported by default, usually we won’t create a Temporal Table on purpose. But when we actually need it, it might not there. - It is very easy to use. No extra table or special grammar is needed. - It provides a global snapshot instead of a view from individual table. - Even if operations like Drop Table and Drop Database are executed, old data can still be retrieved in TiDB.The implementation of the History Read feature in TiDB Multi-version Concurrency Control (MVCC) Note: The implementation in this document is a simplified version and does not involve the distributed transactions. The implementation in TiDB is more complex than this. We will provide the detailed implementation of the transaction model later. Stay tuned!TiDB is on top of TiKV. The storage engine at the bottom level for TiKV is RocksDB where data is stored in Key-Value pairs. A row in a table in the SQL layer needs to be encoded twice to get the final Key in RocksDB: The Key after the first-pass encoding includes table ID and record ID. Using this Key can locate this specific row. Based on the Key from the first encoding, the final Key after the second-pass encoding includes a globally monotone increasing timestamp which is the time it is written.  All the Keys carry a globally unique timestamp, which means that the new writes cannot override the old ones. Even for the delete operations, the write is just a mark to delete, but the actual data is still there. The multiple versions of the data in the same row co-exist in RocksDB according to the time sequence.When a Read transaction starts, a timestamp is allocated from the time allocator of the cluster. For this transaction, all the data written before this timestamp is visible while the data written after the transaction is invisible. In this way, the transaction can guarantee the Repeatable Read isolation level.When a Read request is issued from TiDB to TiKV, the timestamp is carried by the request. When TiKV gets the timestamp, it compares the timestamp and the time of the different versions of the row to find the latest version that is no later than this timestamp and returns it to TiDB.This is the simplified version of how TiDB implements MVCC.Originally, TiDB reads data based on the historical time which is automatically obtained by the system as a transaction starts. Setting the tidb_snapshot session variable is merely enabling TiDB to read data using the time specified by the user to replace the time automatically obtained by the system.You might wonder that if all the versions are kept, will the space occupied by the data inflate indefinitely？This leads to how TiDB collects garbage.The Garbage Collection (GC) mechanism in TiDB TiDB collects garbage periodically and removes the data versions that are too old from RockDB. Therefore, the space occupied by the data won’t inflate indefinitely.Then how old will the data to be removed? The expiration time of the GC is controlled by configuring a parameter. You can set it to be 10 mins, 1 hour, 1 day or never. Therefore the History Read feature of TiDB is limited and only the data after the GC expiration time can be read. You might want to set the time to be as long as possible but this is not without any cost. The longer the expiration time, the more space will be occupied, and the Read performance will degrade. It depends on the business type and requirements as to how to configure the expiration time. If the data is very important and data safety is the top priority or there are very few data updates, it is recommended to set the expiration time to be long; if the data is not very important and the data updates are very frequent, it is recommended to set the expiration time to be short.Summary The History Read feature of TiDB exposes the native TiDB reading mechanism and allows users to use it in the simplest way. We hope this feature can help users create more values."},
		{"url": "https://pingcap.com/weekly/2016-11-14-tidb-weekly/",
		"title": "Weekly update (November 07 ~ November 13, 2016)", 
		"content": " Last week, we landed 25 PRs in the TiDB repositories and 5 PRs in the TiDB docs repositories.Weekly update in TiDB Added  Support the Alter table modify column statement. Support the Drop view statement: parsed but ignored. Add metrics for the transaction size. A tool for testing the SQL performance.  Fixed  A bug in the show create table statement. A few bugs in optimizer: #1962, #1963, #1966, #1975, #1977.  Improved  Improve the cost-based optimizer.  Document change  Adjust the README structure to include the document list Add the following new guides: Data migration from MySQL to TiDB Overview of the monitoring framework Monitoring a TiDB cluster Compatibility with MySQL   Weekly update in TiKV Last week, we landed 23 PRs in the TiKV repositories.Added  Resolve locks in batches to avoid generating a huge Raft log when a transaction rolls back. Add applying snapshot count to enhance the Placement Driver (PD) balance, with PR 1278, 381. Check the system configuration before startup.  Fixed  Add a start flag to check whether a balance starts or not to fix issue 343. Fix the data inconsistency bug when applying snapshot and committed logs at same time.  Improved  Refactor cache to access easily, with PR 359, 379. Update RocksDB config to let user configure it more easily. Slow down Raft heartbeat and election interval to reduce the system pressure. Check the PD list to ensure the address has a valid format. Improve tikv-ctl, with PR 1273, 1281, 1295.  "},
		{"url": "https://pingcap.com/meetup/meetup-2016-11-12/",
		"title": "PingCAP 第 29 期 NewSQL Meetup", 
		"content": " PingCAP 第 29 期 NewSQL Meetup 2016-11-12 王振涛&amp;amp;张金鹏 PingCAP PingCAPPingCAP ![]() 微信号pingcap2015功能介绍PingCAP 专注于新型分布式数据库的研发，是知名开源数据库 TiDB (GitHub 总计10000+ stars ) 背后的团队，总部设在北京，是国内第一家开源的新型分布式关系型数据库公司、国内领先的大数据技术和解决方案提供商。NewSQL Meetup今天是 PingCAP 第 29 期 Meetup，主题是映客服务端架构师王振涛分享的《映客直播服务端架构优化之路》以及张金鹏分享的《MySQL 与 TiDB 的事务机制》。▌ ****Topic 1：映客直播服务端架构优化之路Lecture：王振涛，南开大学计算机硕士毕业，曾先后供职于腾讯、搜狗等互联网公司，拥有多年的服务端研发、面向服务体系结构设计经验，专注于解决海量数据存储和计算带来的分布式、高并发、强一致性等技术难题和挑战。2016 年初加入映客直播，担任服务端架构师，主要负责映客基础平台架构设计、评审和用户体系的研发工作，经历了映客业务快速发展、构建高可用大容量基础服务体系的过程，对分布式计算、微服务、分布式数据库架构、高可用高并发系统设计等方面都有较深刻的理解和实践经验。Content：1、介绍了映客服务端架构演进历程；2、关于服务端技术选型的探索和思考；3、移动直播典型应用场景分析。▌ ****Topic 2：MySQL 与 TiDB 的事务机制Lecture：张金鹏，PingCAP 核心成员，前百度资深研发工程师／京东数据库专家，《MariaDB 原理和实现》作者。Content：在 MySQL 的 InnoDB 存储引擎中，进行写操作时，会将数据修改前的状态纪录在 Undo Log 中，一旦事务，失败利用 Undo Log 来进行回滚，保证事务的原子性。同时 InnoDB 利用 Undo Log 实现了多版本并发控制，InnoDB 的读取操作是不加锁的，事务只能读取到事务开始时已提交的纪录。由于 MySQL 是单机数据库，所有很方便的纪录所有活跃的事务 ID，Purge 线程根据当前活跃的事务情况来定期清理 Undo Log 中过期版本的数据。InnoDB 的事务支持 read uncommitted、read committed、repeatable read、serializable 四种事务隔离级别，InnoDB 通过 next-key lock 来解决 repeatable read 隔离级别下的幻读现象。由于 TiDB 是分布式的数据库，情况变的复杂一些。TiDB 的事务参考的是 Google 的 percolator 模型，通过 PD 获取单调递增的时间戳来作为事务编号。TiDB 的写分为 prewrite 和 commit 两个阶段。如果一个事务写入多行，会选取一行作为 primary row，当 prewrite 阶段成功后会 commit primary row，其他 row 根据 primary row 的提交结果选择提交或者回滚，以保证整个事务的原子性。TiDB 同时实现了 SI 和 SSI 两种事务隔离级别。特别鸣谢：场地赞助-泰利驿站PingCAP Meetup"},
		{"url": "https://pingcap.com/blog-cn/distributed-system-test-2/",
		"title": "分布式系统测试那些事儿 - 错误注入", 
		"content": "  本话题系列文章整理自 PingCAP Infra Meetup 第 26 期刘奇分享的《深度探索分布式系统测试》议题现场实录。文章较长，为方便大家阅读，会分为上中下三篇，本文为中篇。 -接上篇- 当然测试可能会让你代码变得没有那么漂亮，举个例子：这是知名的 Kubernetes 的代码，就是说它有一个 DaemonSetcontroller，这 controller 里面注入了三个测试点，比如这个地方注入了一个 handler ，你可以认为所有的注入都是 interface。比如说你写一个简单的 1+1=2 的程序，假设我们写一个计算器，这个计算器的功能就是求和，那这就很难注入错误。所以你必须要在你正确的代码里面去注入测试逻辑。再比如别人 call 你的这个 add 的 function，然后你是不是有一个 error？这个 error 的问题是它可能永远不会返回一个 error，所以你必须要人肉的注进去，然后看应用程序是不是正确的行为。说完了加法，再说我们做一个除法。除法大家知道可能有处理异常，那上面是不是能正常处理呢？上面没有，上面写着一个比如说 6 ÷ 3，然后写了一个 test，coverage 100%，但是一个除零异常，系统就崩掉了，所以这时候就需要去注入错误。大名鼎鼎的 Kubernetes 为了测试各种异常逻辑也采用类似的方式，这个结构体不算长，大概是十几个成员，然后里面就注入了三个点，可以在里面注入错误。那么在设计 TiDB 的时候，我们当时是怎么考虑 test 这个事情的？首先一个百万级的 test 不可能由人肉来写，也就是说你如果重新定义一个自己的所谓的 SQL 语法，或者一个 query language，那这个时候你需要构建百万级的 test，即使全公司去写，写个两年都不够，所以这个事情显然是不靠谱的。但是除非说我的 query language 特别简单，比如像 MongoDB 早期的那种，那我一个“大于多少”的这种，或者 equal 这种条件查询特别简单的，那你确实是不需要构建这种百万级的 test。但是如果做一个 SQL 的 database 的话，那是需要构建这种非常非常复杂的 test 的。这时候这个 test 又不能全公司的人写个两年，对吧？所以有什么好办法呢？MySQL 兼容的各种系统都是可以用来 test 的，所以我们当时兼容 MySQL 协议，那意味着我们能够取得大量的 MySQL test。不知道有没有人统计过 MySQL 有多少个 test，产品级的 test 很吓人的，千万级。然后还有很多 ORM， 支持 MySQL 的各种应用都有自己的测试。大家知道，每个语言都会 build 自己的 ORM，然后甚至是一个语言的 ORM 都有好几个。比如说对于 MySQL 可能有排第一的、排第二的，那我们可以把这些全拿过来用来测试我们的系统。但对于有些应用程序而言，这时候就比较坑了。就是一个应用程序你得把它 setup 起来，然后操作这个应用程序，比如 WordPress，而后再看那个结果。所以这时候我们为了避免刚才人肉去测试，我们做了一个程序来自动化的 Record&amp;mdash;Replay。就是你在首次运行的时候，我们会记录它所有执行的 SQL 语句，那下一次我再需要重新运行这个程序的时候怎么办？我不需要运行这个程序了，我不需要起来了，我只需要把它前面记录的 SQL record 重新回放一遍，就相当于是我模拟了程序的整个行为。所以我们在这部分是这样做的自动化。那么刚刚说了那么多，实际上做的是什么？实际上做的都是正确路径的测试，那几百万个 test 也都是做的正确的路径测试，但是错误的路径怎么办？很典型的一个例子就是怎么做 Fault injection。硬件比较简单粗暴的模拟网络故障可以拔网线，比如说测网络的时候可以把这个网线拔掉，但是这个做法是极其低效的，而且它是没法 scale 的，因为这个需要人的参与。然后还有比如说 CPU，这个 CPU 的损坏概率其实也挺高的，特别是对于过保了的机器。然后还有磁盘，磁盘大概是三年百分之八点几的损坏率，这是一篇论文里面给出的数据。我记得 Google 好像之前给过一个数据，就是 CPU、网卡还有磁盘在多少年之内的损坏率大概是什么样的。还有一个大家不太关注的就是时钟。先前，我们发现系统时钟是有回跳的，然后我们果断在程序里面加个监测模块，一旦系统时钟回跳，我们马上把这个检测出来。当然我们最初监测出这个东西的时候，用户是觉得不可能吧，时钟还会有回跳？我说没关系，先把我们程序开了监测一下，然后过段时间就检测到，系统时钟最近回跳了。所以怎么配 NTP 很重要。然后还有更多的，比如说文件系统，大家有没有考虑过你写磁盘的时候，磁盘出错会怎么办？好，写磁盘的时候没有出错，成功了，然后磁盘一个扇区坏了，读出来的数据是损坏的，怎么办？大家有没有 checksum ？没有 checksum 然后我们直接用了这个数据，然后直接给用户返回了，这个时候可能是很要命的。如果这个数据刚好存的是个元数据，而元数据又指向别的数据，然后你又根据元数据的信息去写入另外一份数据，那就更要命了，可能数据被进一步破坏了。所以比较好的做法是什么？ Fault injection  Hardware  disk error network card cpu clock  Software  file system network &amp;amp; protocol   Simulate everything  模拟一切东西。就是磁盘是模拟的，网络是模拟的，那我们可以监控它，你可以在任何时间、任何的场景下去注入各种错误，你可以注入任何你想要的错误。比如说你写一个磁盘，我就告诉你磁盘满了，我告诉你磁盘坏了，然后我可以让你 hang 住，比如 sleep 五十几秒。我们确实在云上面出现过这种情况，就是我们一次写入，然后被 hang 了为 53 秒，最后才写进去，那肯定是网络磁盘，对吧？这种事情其实是很吓人的，但是肯定没有人会想说我一次磁盘写入然后要耗掉 53 秒，但是当 53 秒出现的时候，整个程序的行为是什么？TiDB 里面用了大量的 Raft，所以当时出现一个情况就是 53 秒，然后所有的机器就开始选举了，说这肯定是哪儿不对，重新把 leader 都选出来了，这时候卡 53 秒的哥们说“我写完了”，然后整个系统状态就做了一次全新的迁移。这种错误注入的好处是什么？就是知道当出错的时候，你的错误能严重到什么程度，这个事情很重要，就是 predictable，整个系统要可预测的。如果没有做错误路径的测试，那很简单的一个问题，现在假设走到其中一条错误路径了，整个系统行为是什么？这一点不知道是很吓人的。你不知道是否可能破坏数据；还是业务那边会 block 住；还是业务那边会 retry？以前我遇到一个问题很有意思，当时我们在做一个消息系统，有大量连接会连这个，一个单机大概是连八十万左右的连接，就是做消息推送。然后我记得，当时的 swap 分区开了，开了是什么概念？当你有更多连接打进来的时候，然后你内存要爆了对吧？内存爆的话会自动启用 swap 分区，但一旦你启用 swap 分区，那你系统就卡成狗了，外面用户断连之后他就失败了，他得重连，但是重连到你正常程序能响应，可能又需要三十秒，然后那个用户肯定觉得超时了，又切断连接又重连，就造成一个什么状态呢？就是系统永远在重试，永远没有一次成功。那这个行为是不是可以预测？这种错误当时有没有做很好的测试？这都是非常重要的一些教训。硬件测试以前的办法是这样的(Joke)：假设我一个磁盘坏了，假设我一个机器挂了，还有一个假设它不一定坏了也不一定挂了，比如说它着火了会怎么样？前两个月吧，是瑞士还是哪个地方的一个银行做测试，那哥们也挺逗的，人肉对着服务器这样吹气，来看监控数据那个变化，然后那边马上开始报警。这还只是吹气而已，那如果更复杂的测试，比如说你着火从哪个地方开始烧，先烧到硬盘、或者先烧到网卡，这个结果可能也是不一样的。当然这个成本很高，然后也不是能 scale 的一种方案，同时也很难去复制。这不仅仅是硬件的监控，也可以认为是做错误的注入。比如说一个集群我现在烧掉一台会怎么样？着火了，很典型的嘛，虽然重要的机房都会有这种防火、防水等各种的策略，但是真的着火的时候怎么办？当然你不能真去烧，这一烧可能就不止坏一台机器了，但我们需要使用 Fault injection 来模拟。我介绍一下到底什么是 Fault injection。给一个直观的例子，大家知道所有人都用过 Unix 或者 Linux 的系统，大家都知道，很多人习惯打开这个系统第一行命令就是 ls 来列出目录里面的文件，但是大家有没有想过一个有意思的问题，如果你要测试 ls 命令实现的正确性，怎么测？如果没有源代码，这个系统该怎么测？如果把它当成一黑盒这个系统该怎么测？如果你 ls 的时候磁盘出现错误怎么办？如果读取一个扇区读取失败会怎么办？这个是一个很好玩的工具，推荐大家去玩一下。就是当你还没有做更深入的测试之前，可以先去理解一下到底什么是 Fault injection，你就可以体验到它的强大，一会我们用它来找个 MySQL 的 bug。libfiu - Fault injection in userspaceIt can be used to perform fault injection in the POSIX API without having to modify the application&amp;rsquo;s source code, that can help to test failure handling in an easy and reproducible way.那这个东西主要是用来 Hook 这些 API 的，它很重要的一点就是它提供了一个 library ，这个 library 也可以嵌到你的程序里面去 hook 那些 API。就比如说你去读文件的时候，它可以给你返回这个文件不存在，可以给你返回磁盘错误等等。最重要的是，它是可以重来的。举一个例子，正常来讲我们敲 ls 命令的时候，肯定是能够把当前的目录显示出来。这个程序干的是什么呢？就是 run，指定一个参数，现在是要有一个 enable_random，就是后面所有的对于 IO 下面这些 API 的操作，有 5% 的失败率。那第一次是运气比较好，没有遇到失败，所以我们把整个目录列出来了。然后我们重新再跑一次，这时候它告诉我有一次读取失败了，就是它 read 这个 directory 的时候，遇到一个 Bad file descriptor，这时候可以看到，列出来的文件就比上面的要少了，因为有一条路径让它失败了。接下来，我们进一步再跑，发现刚列出来一个目录，然后下次读取就出错了。然后后面再跑一次的时候，这次运气也比较好，把这整个都列出来了，这个还只是模拟的 5% 的失败率。就是有 5% 的概率你去 read、去 open 的时候会失败，那么这时候可以看到 ls 命令的行为还是很 stable 的，就是没有什么常见的 segment fault 这些。大家可能会说这个还不太好玩，也就是找找 ls 命令是否有 bug 嘛，那我们复现 MySQL bug 玩一下。Bug #76020InnoDB does not report filename in I/O error message for readsfiu-run -x -c &amp;ldquo;enable_random name=posix/io/*,probability=0.05&amp;rdquo; bin/mysqld &amp;ndash;basedir=/data/ushastry/server/mysql-5.6.24 &amp;ndash;datadir=/data/ushastry/server/mysql-5.6.24&amp;frasl;76020 &amp;ndash;core-file &amp;ndash;socket=/tmp/mysql_ushastry.sock &amp;ndash;port=150002015-05-20 19:12:07 31030 [ERROR] InnoDB: Error in system call pread(). The operating system error number is 5.2015-05-20 19:12:07 7f7986efc720 InnoDB: Operating system error number 5 in a file operation.InnoDB: Error number 5 means &amp;lsquo;Input/output error&amp;rsquo;.2015-05-20 19:12:07 31030 [ERROR] InnoDB: File (unknown):&amp;lsquo;read&amp;rsquo; returned OS error 105. Cannot continue operation这是用 libfiu 找到的 MySQL 的一个 bug，这个 bug 是这样的，bug 编号是 76020，是说 InnoDB 在出错的时候没有报文件名，那用户给你报了错，你这时候就傻了对吧？这个到底是什么地方出错了呢？然后这个地方它怎么出来的？你可以看到它还是用我们刚才提到的 fiu-run，然后来模拟，模拟的失败概率还是这么多，可以看到，我们的参数一个没变，这时把 MySQL 启动，然后跑一下，出现了，可以看到 InnoDB 在报的时候确实没有报 filename ，File : &amp;lsquo;read&amp;rsquo; returned OS error，然后这边是 auto error，你不知道是哪一个文件名。换一个思路来看，假设没有这个东西，你复现这个 bug 的成本是什么？大家可以想想，如果没有这个东西，这个 bug 应该怎么复现，怎么让 MySQL 读取的东西出错？正常路径下你让它读取出错太困难了，可能好多年没出现过。这时我们进一步再放大一下，这个在 5.7 里面还有，也是在 MySQL 里面很可能有十几年大家都没怎么遇到过的，但这种 bug 在这个工具的辅助下，马上就能出来。所以 Fault injection 它带来了很重要的一个好处就是让一个东西可以变得更加容易重现。这个还是模拟的 5% 的概率。这个例子是我昨天晚上做的，就是我要给大家一个直观的理解，但是分布式系统里面错误注入比这个要复杂。而且如果你遇到一个错误十年都没出现，你是不是太孤独了？ 这个电影大家可能还有印象，威尔史密斯主演的，全世界就一个人活着，唯一的伙伴是一条狗。实际上不是的，比我们痛苦的人大把的存在着。举 Netflix 的一个例子，下图是 Netflix 的系统。他们在 2014 年 10 月份的时候写了一篇博客，叫《 Failure Injection Testing 》，是讲他们整个系统怎么做错误注入，然后他们的这个说法是 Internet Scale，就是整个多数据中心互联网的这个级别。大家可能记得 Spanner 刚出来的时候他们叫做 Global Scale，然后这地方可以看到，蓝色是注射点，黑色的是网络调用，就是所有这些请求在这些情况下面，所有这些蓝色的框框都有可能出错。大家可以想一想，在 Microservice 系统上，一个业务调用可能涉及到几十个系统的调用，如果其中一个失败了会怎么样？如果是第一次第一个失败，第二次第二个失败，第三次第三个失败是怎么样的？有没有系统做过这样的测试？有没有系统在自己的程序里面去很好的验证过是不是每一个可以预期的错误都是可预测的，这个变得非常的重要。这里以 cache 为例，就说每一次访问 Cassandra 的时候可能出错，那么也就给了我们一个错误的注入点。然后我们谈谈 OpenStack.OpenStack fault-injection library:https://pypi.python.org/pypi/os-faults/0.1.2大名鼎鼎的 OpenStack 其实也有一个 Failure Injection Library，然后我把这个例子也贴到这里，大家有兴趣可以看一下这个 OpenStack 的 Failure Injection。这以前大家可能不太关注，其实大家在这一点上都很痛苦， OpenStack 现在还有一堆人在骂，说稳定性太差了，其实他们已经很努力了。但是整个系统确实是做的异乎寻常的复杂，因为组件太多。如果你出错的点特别多，那可能会带来另外一个问题，就是出错的点之间还能组合，就是先 A 出错，再 B 出错，或者 AB 都出错，这也就几种情况，还好。那你要是有十万个错误的点，这个组合怎么弄？当然现在还有新的论文在研究这个，2015 年的时候好像有一篇论文，讲的就是会探测你的程序的路径，然后在对应的路径下面去注入错误。再来说 Jepsen.Jepsen: Distributed Systems Safety Analysis大家所有听过的知名的开源分布式系统基本上都被它找出来过 bug。但是在这之前大家都觉得自己还是很 OK 的，我们的系统还是比较稳定的，所以当新的这个工具或者新的方法出现的时候，就比如说我刚才提到的那篇能够线性 Scale 的去查错的那篇论文，那个到时候查错力就很惊人了，因为它能够自动帮你探测。另外我介绍一个工具 Namazu，后面讲，它也很强大。这里先说Jepsen, 这货算是重型武器了，无论是 ZooKeeper、MongoDB 以及 Redis 等等，所有这些全部都被找出了 bug，现在用的所有数据库都是它找出的 bug，最大的问题是小众语言 closure 编写的，扩展起来有点麻烦。我先说说 Jepsen 的基本原理，一个典型使用 Jepsen 的测试通过会在一个 control node上面运行相关的 clojure 程序，control node 会使用 ssh 登陆到相关的系统 node（jepsen 叫做 db node）进行一些测试操作。当我们的分布式系统启动起来之后，control node 会启动很多进程，每一个进程都能使用特定的 client 访问到我们的分布式系统。一个 generator 为每一个进程生成一系列的操作，比如 get/set/cas，让其执行。每一个操作都会被记录到 history 里面。在执行操作的同时，另一个 nemesis 进程会尝试去破坏这个分布式系统，譬如使用 iptable 断开网络连接等，当所有操作执行完毕之后，jepsen 会使用一个 checker 来分析验证系统的行为是否符合预期。PingCAP 的首席架构师唐刘写过两篇文章介绍我们实际怎么用 Jepsen 来测试 TiDB，大家可以搜索一下，我这里就不详细展开了。 FoundationDB  It is difficult to be deterministic  Random Disk Size File Length Time Multithread    FoundationDB 这就是前辈了，2015 …"},
		{"url": "https://pingcap.com/meetup/meetup-2016-11-10/",
		"title": "分布式系统测试那些事儿——错误注入", 
		"content": ""},
		{"url": "https://pingcap.com/meetup/memoir/meetup-20160-11-10/",
		"title": "分布式系统测试那些事儿——错误注入", 
		"content": "当然测试可能会让你代码变得没有那么漂亮，举个例子：这是知名的 Kubernetes 的代码，就是说它有一个 DaemonSetcontroller，这 controller 里面注入了三个测试点，比如这个地方注入了一个 handler ，你可以认为所有的注入都是 interface。比如说你写一个简单的 1+1=2 的程序，假设我们写一个计算器，这个计算器的功能就是求和，那这就很难注入错误。所以你必须要在你正确的代码里面去注入测试逻辑。再比如别人 call 你的这个 add 的 function，然后你是不是有一个 error？这个 error 的问题是它可能永远不会返回一个 error，所以你必须要人肉的注进去，然后看应用程序是不是正确的行为。说完了加法，再说我们做一个除法。除法大家知道可能有处理异常，那上面是不是能正常处理呢？上面没有，上面写着一个比如说 6 ÷ 3，然后写了一个 test，coverage 100%，但是一个除零异常，系统就崩掉了，所以这时候就需要去注入错误。大名鼎鼎的 Kubernetes 为了测试各种异常逻辑也采用类似的方式，这个结构体不算长，大概是十几个成员，然后里面就注入了三个点，可以在里面注入错误。那么在设计 TiDB 的时候，我们当时是怎么考虑 test 这个事情的？首先一个百万级的 test 不可能由人肉来写，也就是说你如果重新定义一个自己的所谓的 SQL 语法，或者一个 query language，那这个时候你需要构建百万级的 test，即使全公司去写，写个两年都不够，所以这个事情显然是不靠谱的。但是除非说我的 query language 特别简单，比如像 MongoDB 早期的那种，那我一个“大于多少”的这种，或者 equal 这种条件查询特别简单的，那你确实是不需要构建这种百万级的 test。但是如果做一个 SQL 的 database 的话，那是需要构建这种非常非常复杂的 test 的。这时候这个 test 又不能全公司的人写个两年，对吧？所以有什么好办法呢？MySQL 兼容的各种系统都是可以用来 test 的，所以我们当时兼容 MySQL 协议，那意味着我们能够取得大量的 MySQL test。不知道有没有人统计过 MySQL 有多少个 test，产品级的 test 很吓人的，千万级。然后还有很多 ORM， 支持 MySQL 的各种应用都有自己的测试。大家知道，每个语言都会 build 自己的 ORM，然后甚至是一个语言的 ORM 都有好几个。比如说对于 MySQL 可能有排第一的、排第二的，那我们可以把这些全拿过来用来测试我们的系统。但对于有些应用程序而言，这时候就比较坑了。就是一个应用程序你得把它 setup 起来，然后操作这个应用程序，比如 WordPress，而后再看那个结果。所以这时候我们为了避免刚才人肉去测试，我们做了一个程序来自动化的 Record&amp;mdash;Replay。就是你在首次运行的时候，我们会记录它所有执行的 SQL 语句，那下一次我再需要重新运行这个程序的时候怎么办？我不需要运行这个程序了，我不需要起来了，我只需要把它前面记录的 SQL record 重新回放一遍，就相当于是我模拟了程序的整个行为。所以我们在这部分是这样做的自动化。那么刚刚说了那么多，实际上做的是什么？实际上做的都是正确路径的测试，那几百万个 test 也都是做的正确的路径测试，但是错误的路径怎么办？很典型的一个例子就是怎么做 Fault injection。硬件比较简单粗暴的模拟网络故障可以拔网线，比如说测网络的时候可以把这个网线拔掉，但是这个做法是极其低效的，而且它是没法 scale 的，因为这个需要人的参与。然后还有比如说 CPU，这个 CPU 的损坏概率其实也挺高的，特别是对于过保了的机器。然后还有磁盘，磁盘大概是三年百分之八点几的损坏率，这是一篇论文里面给出的数据。我记得 Google 好像之前给过一个数据，就是 CPU、网卡还有磁盘在多少年之内的损坏率大概是什么样的。还有一个大家不太关注的就是时钟。先前，我们发现系统时钟是有回跳的，然后我们果断在程序里面加个监测模块，一旦系统时钟回跳，我们马上把这个检测出来。当然我们最初监测出这个东西的时候，用户是觉得不可能吧，时钟还会有回跳？我说没关系，先把我们程序开了监测一下，然后过段时间就检测到，系统时钟最近回跳了。所以怎么配 NTP 很重要。然后还有更多的，比如说文件系统，大家有没有考虑过你写磁盘的时候，磁盘出错会怎么办？好，写磁盘的时候没有出错，成功了，然后磁盘一个扇区坏了，读出来的数据是损坏的，怎么办？大家有没有 checksum ？没有 checksum 然后我们直接用了这个数据，然后直接给用户返回了，这个时候可能是很要命的。如果这个数据刚好存的是个元数据，而元数据又指向别的数据，然后你又根据元数据的信息去写入另外一份数据，那就更要命了，可能数据被进一步破坏了。所以比较好的做法是什么？  Fault injection  Hardware  disk error network card cpu clock  Software  file system network &amp;amp; protocol   Simulate everything   模拟一切东西。就是磁盘是模拟的，网络是模拟的，那我们可以监控它，你可以在任何时间、任何的场景下去注入各种错误，你可以注入任何你想要的错误。比如说你写一个磁盘，我就告诉你磁盘满了，我告诉你磁盘坏了，然后我可以让你 hang 住，比如 sleep 五十几秒。我们确实在云上面出现过这种情况，就是我们一次写入，然后被 hang 了为 53 秒，最后才写进去，那肯定是网络磁盘，对吧？这种事情其实是很吓人的，但是肯定没有人会想说我一次磁盘写入然后要耗掉 53 秒，但是当 53 秒出现的时候，整个程序的行为是什么？TiDB 里面用了大量的 Raft，所以当时出现一个情况就是 53 秒，然后所有的机器就开始选举了，说这肯定是哪儿不对，重新把 leader 都选出来了，这时候卡 53 秒的哥们说“我写完了”，然后整个系统状态就做了一次全新的迁移。这种错误注入的好处是什么？就是知道当出错的时候，你的错误能严重到什么程度，这个事情很重要，就是 predictable，整个系统要可预测的。如果没有做错误路径的测试，那很简单的一个问题，现在假设走到其中一条错误路径了，整个系统行为是什么？这一点不知道是很吓人的。你不知道是否可能破坏数据；还是业务那边会 block 住；还是业务那边会 retry？以前我遇到一个问题很有意思，当时我们在做一个消息系统，有大量连接会连这个，一个单机大概是连八十万左右的连接，就是做消息推送。然后我记得，当时的 swap 分区开了，开了是什么概念？当你有更多连接打进来的时候，然后你内存要爆了对吧？内存爆的话会自动启用 swap 分区，但一旦你启用 swap 分区，那你系统就卡成狗了，外面用户断连之后他就失败了，他得重连，但是重连到你正常程序能响应，可能又需要三十秒，然后那个用户肯定觉得超时了，又切断连接又重连，就造成一个什么状态呢？就是系统永远在重试，永远没有一次成功。那这个行为是不是可以预测？这种错误当时有没有做很好的测试？这都是非常重要的一些教训。硬件测试以前的办法是这样的(Joke)：假设我一个磁盘坏了，假设我一个机器挂了，还有一个假设它不一定坏了也不一定挂了，比如说它着火了会怎么样？前两个月吧，是瑞士还是哪个地方的一个银行做测试，那哥们也挺逗的，人肉对着服务器这样吹气，来看监控数据那个变化，然后那边马上开始报警。这还只是吹气而已，那如果更复杂的测试，比如说你着火从哪个地方开始烧，先烧到硬盘、或者先烧到网卡，这个结果可能也是不一样的。当然这个成本很高，然后也不是能 scale 的一种方案，同时也很难去复制。这不仅仅是硬件的监控，也可以认为是做错误的注入。比如说一个集群我现在烧掉一台会怎么样？着火了，很典型的嘛，虽然重要的机房都会有这种防火、防水等各种的策略，但是真的着火的时候怎么办？当然你不能真去烧，这一烧可能就不止坏一台机器了，但我们需要使用 Fault injection 来模拟。我介绍一下到底什么是 Fault injection。给一个直观的例子，大家知道所有人都用过 Unix 或者 Linux 的系统，大家都知道，很多人习惯打开这个系统第一行命令就是 ls 来列出目录里面的文件，但是大家有没有想过一个有意思的问题，如果你要测试 ls 命令实现的正确性，怎么测？如果没有源代码，这个系统该怎么测？如果把它当成一黑盒这个系统该怎么测？如果你 ls 的时候磁盘出现错误怎么办？如果读取一个扇区读取失败会怎么办？ 这个是一个很好玩的工具，推荐大家去玩一下。就是当你还没有做更深入的测试之前，可以先去理解一下到底什么是 Fault injection，你就可以体验到它的强大，一会我们用它来找个 MySQL 的 bug。 libfiu - Fault injection in userspaceIt can be used to perform fault injection in the POSIX API without having to modify the application&amp;rsquo;s source code, that can help to test failure handling in an easy and reproducible way. 那这个东西主要是用来 Hook 这些 API 的，它很重要的一点就是它提供了一个 library ，这个 library 也可以嵌到你的程序里面去 hook 那些 API。就比如说你去读文件的时候，它可以给你返回这个文件不存在，可以给你返回磁盘错误等等。最重要的是，它是可以重来的。举一个例子，正常来讲我们敲 ls 命令的时候，肯定是能够把当前的目录显示出来。这个程序干的是什么呢？就是 run，指定一个参数，现在是要有一个 enable_random，就是后面所有的对于 IO 下面这些 API 的操作，有 5% 的失败率。那第一次是运气比较好，没有遇到失败，所以我们把整个目录列出来了。然后我们重新再跑一次，这时候它告诉我有一次读取失败了，就是它 read 这个 directory 的时候，遇到一个 Bad file descriptor，这时候可以看到，列出来的文件就比上面的要少了，因为有一条路径让它失败了。接下来，我们进一步再跑，发现刚列出来一个目录，然后下次读取就出错了。然后后面再跑一次的时候，这次运气也比较好，把这整个都列出来了，这个还只是模拟的 5% 的失败率。就是有 5% 的概率你去 read、去 open 的时候会失败，那么这时候可以看到 ls 命令的行为还是很 stable 的，就是没有什么常见的 segment fault 这些。大家可能会说这个还不太好玩，也就是找找 ls 命令是否有 bug 嘛，那我们复现 MySQL bug 玩一下。 Bug #76020InnoDB does not report filename in I/O error message for readsfiu-run -x -c &amp;ldquo;enable_random name=posix/io/*,probability=0.05&amp;rdquo; bin/mysqld &amp;ndash;basedir=/data/ushastry/server/mysql-5.6.24 &amp;ndash;datadir=/data/ushastry/server/mysql-5.6.24&amp;frasl;76020 &amp;ndash;core-file &amp;ndash;socket=/tmp/mysql_ushastry.sock &amp;ndash;port=150002015-05-20 19:12:07 31030 [ERROR] InnoDB: Error in system call pread(). The operating system error number is 5.2015-05-20 19:12:07 7f7986efc720 InnoDB: Operating system error number 5 in a file operation.InnoDB: Error number 5 means &amp;lsquo;Input/output error&amp;rsquo;.2015-05-20 19:12:07 31030 [ERROR] InnoDB: File (unknown):&amp;lsquo;read&amp;rsquo; returned OS error 105. Cannot continue operation 这是用 libfiu 找到的 MySQL 的一个 bug，这个 bug 是这样的，bug 编号是 76020，是说 InnoDB 在出错的时候没有报文件名，那用户给你报了错，你这时候就傻了对吧？这个到底是什么地方出错了呢？然后这个地方它怎么出来的？你可以看到它还是用我们刚才提到的 fiu-run，然后来模拟，模拟的失败概率还是这么多，可以看到，我们的参数一个没变，这时把 MySQL 启动，然后跑一下，出现了，可以看到 InnoDB 在报的时候确实没有报 filename ，File : &amp;lsquo;read&amp;rsquo; returned OS error，然后这边是 auto error，你不知道是哪一个文件名。换一个思路来看，假设没有这个东西，你复现这个 bug 的成本是什么？大家可以想想，如果没有这个东西，这个 bug 应该怎么复现，怎么让 MySQL 读取的东西出错？正常路径下你让它读取出错太困难了，可能好多年没出现过。这时我们进一步再放大一下，这个在 5.7 里面还有，也是在 MySQL 里面很可能有十几年大家都没怎么遇到过的，但这种 bug 在这个工具的辅助下，马上就能出来。所以 Fault injection 它带来了很重要的一个好处就是让一个东西可以变得更加容易重现。这个还是模拟的 5% 的概率。这个例子是我昨天晚上做的，就是我要给大家一个直观的理解，但是分布式系统里面错误注入比这个要复杂。而且如果你遇到一个错误十年都没出现，你是不是太孤独了？ 这个电影大家可能还有印象，威尔史密斯主演的，全世界就一个人活着，唯一的伙伴是一条狗。实际上不是的，比我们痛苦的人大把的存在着。举 Netflix 的一个例子，下图是 Netflix 的系统。他们在 2014 年 10 月份的时候写了一篇博客，叫《 Failure Injection Testing 》，是讲他们整个系统怎么做错误注入，然后他们的这个说法是 Internet Scale，就是整个多数据中心互联网的这个级别。大家可能记得 Spanner 刚出来的时候他们叫做 Global Scale，然后这地方可以看到，蓝色是注射点，黑色的是网络调用，就是所有这些请求在这些情况下面，所有这些蓝色的框框都有可能出错。大家可以想一想，在 Microservice 系统上，一个业务调用可能涉及到几十个系统的调用，如果其中一个失败了会怎么样？如果是第一次第一个失败，第二次第二个失败，第三次第三个失败是怎么样的？有没有系统做过这样的测试？有没有系统在自己的程序里面去很好的验证过是不是每一个可以预期的错误都是可预测的，这个变得非常的重要。这里以 cache 为例，就说每一次访问 Cassandra 的时候可能出错，那么也就给了我们一个错误的注入点。然后我们谈谈 OpenStack OpenStack fault-injection library:https://pypi.python.org/pypi/os-faults/0.1.2 大名鼎鼎的 OpenStack 其实也有一个 Failure Injection Library，然后我把这个例子也贴到这里，大家有兴趣可以看一下这个 OpenStack 的 Failure Injection。这以前大家可能不太关注，其实大家在这一点上都很痛苦， OpenStack 现在还有一堆人在骂，说稳定性太差了，其实他们已经很努力了。但是整个系统确实是做的异乎寻常的复杂，因为组件太多。如果你出错的点特别多，那可能会带来另外一个问题，就是出错的点之间还能组合，就是先 A 出错，再 B 出错，或者 AB 都出错，这也就几种情况，还好。那你要是有十万个错误的点，这个组合怎么弄？当然现在还有新的论文在研究这个，2015 年的时候好像有一篇论文，讲的就是会探测你的程序的路径，然后在对应的路径下面去注入错误。再来说 JepsenJepsen: Distributed Systems Safety Analysis大家所有听过的知名的开源分布式系统基本上都被它找出来过 bug。但是在这之前大家都觉得自己还是很 OK 的，我们的系统还是比较稳定的，所以当新的这个工具或者新的方法出现的时候，就比如说我刚才提到的那篇能够线性 Scale 的去查错的那篇论文，那个到时候查错力就很惊人了，因为它能够自动帮你探测。另外我介绍一个工具 Namazu，后面讲，它也很强大。这里先说Jepsen, 这货算是重型武器了，无论是 ZooKeeper、MongoDB 以及 Redis 等等，所有这些全部都被找出了 bug，现在用的所有数据库都是它找出的 bug，最大的问题是小众语言 closure 编写的，扩展起来有点麻烦。我先说说 Jepsen 的基本原理，一个典型使用 Jepsen 的测试通过会在一个 control node上面运行相关的 clojure 程序，control node 会使用 ssh 登陆到相关的系统 node（jepsen 叫做 db node）进行一些测试操作。当我们的分布式系统启动起来之后，control node 会启动很多进程，每一个进程都能使用特定的 client 访问到我们的分布式系统。一个 generator 为每一个进程生成一系列的操作，比如 get/set/cas，让其执行。每一个操作都会被记录到 history 里面。在执行操作的同时，另一个 nemesis 进程会尝试去破坏这个分布式系统，譬如使用 iptable 断开网络连接等，当所有操作执行完毕之后，jepsen 会使用一个 checker 来分析验证系统的行为是否符合预期。PingCAP 的首席架构师唐刘写过两篇文章介绍我们实际怎么用 Jepsen 来测试 TiDB，大家可以搜索一下，我这里就不详细展开了。  FoundationDB  It is difficult to be deterministic  Random Disk Size File Length Time Multithread     FoundationDB 这就是前辈了，2015 年被 Apple 收购了。他们为了解决错误注入的问题，或者说怎么去让它重现的这个问题，做了很多事情，很重要的一个事情就是 deterministic 。如果我给你一样的输入，跑几遍，是不是能得到一 …"},
		{"url": "https://pingcap.com/blog/2016-11-09-Deep-Dive-into-TiKV/",
		"title": "A Deep Dive into TiKV", 
		"content": " Table of Content  About TiKV Architecture Protocol Raft Placement Driver (PD) Transaction Coprocessor Key processes analysis  Key-Value operation Membership Change Split   About TiKV TiKV (The pronunciation is: /&amp;lsquo;taɪkeɪvi:/ tai-K-V, etymology: titanium) is a distributed Key-Value database which is based on the design of Google Spanner, F1, and HBase, but it is much simpler without dependency on any distributed file system.Architecture  Placement Driver (PD): PD is the brain of the TiKV system which manages the metadata about Nodes, Stores, Regions mapping, and makes decisions for data placement and load balancing. PD periodically checks replication constraints to balance load and data automatically. Node: A physical node in the cluster. Within each node, there are one or more Stores. Within each Store, there are many Regions. Store: There is a RocksDB within each Store and it stores data in local disks. Region: Region is the basic unit of Key-Value data movement and corresponds to a data range in a Store. Each Region is replicated to multiple Nodes. These multiple replicas form a Raft group. A replica of a Region is called a Peer.  Protocol TiKV uses the Protocol Buffer protocol for interactions among different components. Because Rust doesn’t support gRPC for the time being, we use our own protocol in the following format:Message: Header + Payload Header: | 0xdaf4(2 bytes magic value) | 0x01(version 2 bytes) | msg_len(4 bytes) | msg_id(8 bytes) | The data of Protocol Buffer is stored in the Payload part of the message. At the Network level, we will first read the 16-byte Header. According to the message length (msg_len) information in the Header, we calculate the actual length of the message, and then read the corresponding data and decode it.The interaction protocol of TiKV is in the kvproto project and the protocol to support push-down is in the tipb project. Here, let’s focused on the kvproto project only.About the protocol files in the kvproto project: msgpb.proto: All the protocol interactions are in the same message structure. When a message is received, we will handle the message according to its MessageType. metapb.proto: To define the public metadata for Store, Region, Peer, etc. raftpb.proto: For the internal use of Raft. It is ported from etcd and needs to be consistent with etcd. raft_serverpb.proto: For the interactions among the Raft nodes. raft_cmdpb.proto: The actual command executed when Raft applies. pdpb.proto: The protocol for the interaction between TiKV and PD. kvrpcpb.proto: The Key-Value protocol that supports transactions. mvccpb.proto: For internal Multi-Version Concurrency Control (MVCC). coprocessor.proto: To support the Push-Down operations.  There are following ways for external applications to connect to TiKV: For the simple Key-Value features only, implement raft_cmdpb.proto. For the Transactional Key-Value features, implement kvrpcpb.proto. For the Push-Down features, implement coprocessor.proto. See tipb for detailed push-down protocol.  Back to the TopRaft TiKV uses the Raft algorithm to ensure the data consistency in the distributed systems. For more information, see https://raft.github.io/.The Raft in TiKV is completely migrated from etcd. We chose etcd Raft because it is very simple to implement, very easy to migrate and it is production proven.The Raft implementation in TiKV can be used independently. You can apply it in your project directly.See the following details about how to use Raft: Define its own storage and implement the Raft Storage trait. See the following Storage trait interface:  // initial_state returns the information about HardState and ConfState in Storage  fn initial_state(&amp;amp;self) -&amp;gt; Result&amp;lt;RaftState&amp;gt;; // return the log entries in the [low, high] range  fn entries(&amp;amp;self, low: u64, high: u64, max_size: u64) -&amp;gt; Result&amp;lt;Vec&amp;lt;Entry&amp;gt;&amp;gt;; // get the term of the log entry according to the corresponding log index  fn term(&amp;amp;self, idx: u64) -&amp;gt; Result&amp;lt;u64&amp;gt;; // get the index from the first log entry at the current position  fn first_index(&amp;amp;self) -&amp;gt; Result&amp;lt;u64&amp;gt;; // get the index from the last log entry at the current position  fn last_index(&amp;amp;self) -&amp;gt; Result&amp;lt;u64&amp;gt;; // generate a current snapshot  fn snapshot(&amp;amp;self) -&amp;gt; Result&amp;lt;Snapshot&amp;gt;;  Create a raw node object and pass the corresponding configuration and customized storage instance to the object. About the configuration, we need to pay attention to election_tick and heartbeat_tick. Some of the Raft logics step by periodical ticks. For every Tick, the Leader will decide if the frequency of the heartbeat elapsing exceeds the frequency of the heartbeat_tick. If it does, the Leader will send heartbeats to the Followers and reset the elapse. For a Follower, if the frequency of the election elapsing exceeds the frequency of the election_tick, the Follower will initiate an election. After a raw node is created, the tick interface of the raw node will be called periodically (like every 100ms) and drives the internal Raft Step function. If data is to be written by Raft, the Propose interface is called directly. The parameters of the Propose interface is an arbitrary binary data which means that Raft doesn’t care the exact data content that is replicated by it. It is completely up to the external logics as how to handle the data. If it is to process the membership changes, the propose_conf_change interface of the raw node can be called to send a ConfChange object to add/remove a certain node. After the functions in the raw node like Tick and Propose of the raw node are called, Raft will initiate a Ready state. Here are some details of the Ready state:There are three parts in the Ready state: The part that needs to be stored in Raft storage, which are entries, hard state and snapshot. The part that needs to be sent to other Raft nodes, which are messages. The part that needs to be applied to other state machines, which are committed_entries.   After handling the Ready status, the Advance function needs be called to inform Raft of the next Ready process.In TiKV, Raft is used through mio as in the following process: Register a base Raft tick timer (usually 100ms). Every time the timer timeouts, the Tick of the raw node is called and the timer is re-registered. Receive the external commands through the notify function in mio and call the Propose or the propose_conf_change interface. Decide if a Raft is ready in the mio tick callback (Note: The mio tick is called at the end of each event loop, which is different from the Raft tick.). If it is ready, proceed with the Ready process.  In the descriptions above, we covered how to use one Raft only. But in TiKV, we have multiple Raft groups. These Raft groups are independent to each other and therefore can be processed following the same approach.In TiKV, each Raft group corresponds to a Region. At the very beginning, there is only one Region in TiKV which is in charge of the range (-inf, +inf). As more data comes in and the Region reaches its threshold (64 MB currently), the Region is split into two Regions. Because all the data in TiKV are sorted according to the key, it is very convenient to choose a Split Key to split the Region. See Split for the detailed splitting process.Of course, where there is Split, there is Merge. If there are very few data in two adjacent Regions, these two regions can merge to one big Region. Region Merge is in the TiKV roadmap but it is not implemented yet.Back to the TopPlacement Driver Placement Driver (PD) is in charge of the managing and scheduling of the whole TiKV cluster. It is a central service and we have to ensure that it is highly available and stable.The first issue to be resolved is the single point of failure of PD. Our solution is to start multiple PD servers. These servers elect a Leader through the election mechanism in etcd and the leader provides services to the outside. If the leader is …"},
		{"url": "https://pingcap.com/weekly/2016-11-07-tidb-weekly/",
		"title": "Weekly update (October 31 ~ November 06, 2016)", 
		"content": " Weekly update (October 31 ~ November 06, 2016) Last week, we landed 42 PRs in the TiDB repositories and 29 PRs in the TiKV repositories.New release TiDB Beta 4 Weekly update in TiDB Added  The aggregation info to the explain statement. Support the show processlist syntax. TiDB supports mydumper now. Support the show create database statement. Support the buildin function from_unixtime. Push down the aggregation operator to the position before join. Get cluster ID from Placement Driver.  Fixed  A bug in parser that misses the recognize identifier with leading digits. A bug in the load data command.   Improved  The performance of the DropTable statement. The performance of the AddIndex statement. The query metrics.  Weekly update in TiKV Added  Multiversion concurrency control (MVCC) support for tikv-ctl. Support -v/--version to print the version information. Metrics for MVCC key versions and deleted key versions. Use random cluster ID to bootstrap clusters to avoid wrong joining cluster by the users, with PR 370, 1257  Fixed  Panic directly if the RocksDB writes fail to fix 1262. Add the start time for store monitor to fix 1207. Return the &amp;lsquo;no leader&amp;rsquo; error to avoid panic when calling the GetLeader API with no leader.  Improved  Skip values for scan. Make cache code cleaner, with PR 365, 366, 367. Allow liner reverse seek in the lock column family. Separate compaction to a different thread worker. Avoid filling block cache when scanning. Include the snapshot size in the used_size store. Dump the RocksDB statistics with the SIGUSR1 signal.  "},
		{"url": "https://pingcap.com/meetup/meetup-2016-11-05/",
		"title": "PingCAP 第 28 期 NewSQL Meetup", 
		"content": " PingCAP 第 28 期 NewSQL Meetup 2016-11-05 时延军&amp;amp;韩飞 PingCAP PingCAPPingCAP ![]() 微信号pingcap2015功能介绍PingCAP 专注于新型分布式数据库的研发，是知名开源数据库 TiDB (GitHub 总计10000+ stars ) 背后的团队，总部设在北京，是国内第一家开源的新型分布式关系型数据库公司、国内领先的大数据技术和解决方案提供商。NewSQL Meetup今天是 PingCAP 第28 期 Meetup，主题是 TalkingData 数据经理时延军分享的《Spark 架构设计要点剖析》以及韩飞分享的《Performing group-by before join》。▌ ****Topic 1：Spark 架构设计要点剖析Lecture：时延军，TalkingData 数据经理，负责领域工程数据平台架构和研发，曾在 COMODO 中国负责基础数据平台建设，在车语传媒考拉 FM 负责后端数据平台架构（支持离线+实时分析处理）。推崇工程师文化，热爱开源，乐于分享，兴趣广泛，熟悉大数据技术生态，擅长软件系统架构、分布式计算系统设计。Content：1、RDD 特性，RDD 是如何抽象数据集的；2、详解 Spark 基本架构；3、Spark 内部核心组件及其交互；4、逻辑执行计划与物理执行计划；5、Spark 资源管理与任务调度。▌ ****Topic2：Performing group-by before joinLecture：韩飞，PingCAP 研发工程师（PingCAP SQL 小王子），TiDB SQL Optimizer 主要作者，专注于 SQL 优化技术。前阿里云研发工程师，参与开发 ODPS SQL 查询优化器 Lot。Content：Efficient processing of aggregation queries is essential for decision support applications. This talk introduces a class of query trans-formations, called eager aggregation that allows a query optimizer to push group-by operations down the query tree. Eager aggregation partially pushes a group-by past a join.特别鸣谢：场地赞助-泰利驿站PingCAP Meetup"},
		{"url": "https://pingcap.com/blog-cn/tidb-as-mysql-slave/",
		"title": "TiDB 作为 MySQL Slave 实现实时数据同步", 
		"content": " 由于 TiDB 本身兼容绝大多数的 MySQL 语法，所以对于绝大多数业务来说，最安全的切换数据库方式就是将 TiDB 作为现有数据库的从库接在主 MySQL 库的后方，这样对业务方实现完全没有侵入性下使用 TiDB 对现有的业务进行备份，应对未来数据量或者并发量增长带来的单点故障风险，如需上线 TiDB，也只需要简单的将业务的主 MySQL 地址指向 TiDB 即可。下面我们详细介绍了如何将 MySQL 的数据迁移到 TiDB，并将 TiDB 作为 MySQL 的 Slave 进行数据同步。这里我们假定 MySQL 以及 TiDB 服务信息如下:+------------------+-------------+----------------------------------------+ | Name | Address | Port | User | Password | +------------------+-------------+----------------------------------------+ | MySQL | 127.0.0.1 | 3306 | root | | | TiDB | 127.0.0.1 | 4000 | root | | +------------------+-------------+--------+-----------+-------------------+ 使用 checker 进行 Schema 检查 在迁移之前，我们可以使用 TiDB 的 checker 工具，checker 是我们开发的一个小工具，用于检测目标 MySQL 库中的表的表结构是否支持无缝的迁移到 TiDB，TiDB 支持绝大多数的 MySQL 常用的原生数据类型，所以大多数情况 checker 的返回应该是 ok。如果 check 某个 table schema 失败，表明 TiDB 当前并不支持，我们不能对该 table 里面的数据进行迁移。checker 包含在 TiDB 工具集里面，我们可以直接下载。下载 TiDB 工具集 Linux # 下载 tool 压缩包 wget http://download.pingcap.org/tidb-tools-latest-linux-amd64.tar.gz wget http://download.pingcap.org/tidb-tools-latest-linux-amd64.sha256 # 检查文件完整性，返回 ok 则正确 sha256sum -c tidb-tools-latest-linux-amd64.sha256 # 解开压缩包 tar -xzf tidb-tools-latest-linux-amd64.tar.gz cd tidb-tools-latest-linux-amd64 使用 checker 检查的一个示范  在 MySQL 的 test database 里面创建几张表，并插入数据:  USE test; CREATE TABLE t1 (id INT, age INT, PRIMARY KEY(id)) ENGINE=InnoDB; CREATE TABLE t2 (id INT, name VARCHAR(256), PRIMARY KEY(id)) ENGINE=InnoDB; INSERT INTO t1 VALUES (1, 1), (2, 2), (3, 3); INSERT INTO t2 VALUES (1, &amp;#34;a&amp;#34;), (2, &amp;#34;b&amp;#34;), (3, &amp;#34;c&amp;#34;);  使用 checker 检查 test database 里面所有的 table  ./bin/checker -host 127.0.0.1 -port 3306 -user root test 2016/10/27 13:11:49 checker.go:48: [info] Checking database test 2016/10/27 13:11:49 main.go:37: [info] Database DSN: root:@tcp(127.0.0.1:3306)/test?charset=utf8 2016/10/27 13:11:49 checker.go:63: [info] Checking table t1 2016/10/27 13:11:49 checker.go:69: [info] Check table t1 succ 2016/10/27 13:11:49 checker.go:63: [info] Checking table t2 2016/10/27 13:11:49 checker.go:69: [info] Check table t2 succ  使用 checker 检查 test database 里面某一个 table  这里，假设我们只需要迁移 table t1。./bin/checker -host 127.0.0.1 -port 3306 -user root test t1 2016/10/27 13:13:56 checker.go:48: [info] Checking database test 2016/10/27 13:13:56 main.go:37: [info] Database DSN: root:@tcp(127.0.0.1:3306)/test?charset=utf8 2016/10/27 13:13:56 checker.go:63: [info] Checking table t1 2016/10/27 13:13:56 checker.go:69: [info] Check table t1 succ Check database succ! 一个无法迁移的 table 例子 我们在 MySQL 里面创建如下表：CREATE TABLE t_error ( c timestamp(3) NOT NULL DEFAULT CURRENT_TIMESTAMP(3) ON UPDATE CURRENT_TIMESTAMP(3) ) ENGINE=InnoDB DEFAULT CHARSET=latin1; 使用 checker 进行检查，会报错，表明我们没法迁移 t_error 这张表。./bin/checker -host 127.0.0.1 -port 3306 -user root test t_error 2016/10/27 13:19:28 checker.go:48: [info] Checking database test 2016/10/27 13:19:28 main.go:37: [info] Database DSN: root:@tcp(127.0.0.1:3306)/test?charset=utf8 2016/10/27 13:19:28 checker.go:63: [info] Checking table t_error 2016/10/27 13:19:28 checker.go:67: [error] Check table t_error failed with err: line 1 column 56 near &amp;#34;) ON UPDATE CURRENT_TIMESTAMP(3) ) ENGINE=InnoDB DEFAULT CHARSET=latin1&amp;#34; github.com/pingcap/tidb/parser/yy_parser.go:111: github.com/pingcap/tidb/parser/yy_parser.go:124: /home/jenkins/workspace/WORKFLOW_TOOLS_BUILDING/go/src/github.com/pingcap/tidb-tools/checker/checker.go:122: parse CREATE TABLE `t_error` ( `c` timestamp(3) NOT NULL DEFAULT CURRENT_TIMESTAMP(3) ON UPDATE CURRENT_TIMESTAMP(3) ) ENGINE=InnoDB DEFAULT CHARSET=latin1 error /home/jenkins/workspace/WORKFLOW_TOOLS_BUILDING/go/src/github.com/pingcap/tidb-tools/checker/checker.go:114: 2016/10/27 13:19:28 main.go:68: [error] Check database test with 1 errors and 0 warnings. 使用 mydumper/myloader 全量导入数据 我们使用 mydumper 从 MySQL 导出数据，然后用 myloader 将其导入到 TiDB 里面。注意，虽然我们也支持使用 MySQL 官方的 mysqldump 工具来进行数据的迁移工作，但相比于 mydumper/myloader，性能会慢很多，对于大量数据的迁移会花费很多时间，这里我们并不推荐。mydumper/myloader 是一个更强大的数据迁移工具，具体可以参考 https://github.com/maxbube/mydumper。下载 Binary Linux # 下载 mydumper 压缩包 wget http://download.pingcap.org/mydumper-linux-amd64.tar.gz wget http://download.pingcap.org/mydumper-linux-amd64.sha256 # 检查文件完整性，返回 ok 则正确 sha256sum -c mydumper-linux-amd64.sha256 # 解开压缩包 tar -xzf mydumper-linux-amd64.tar.gz cd mydumper-linux-amd64 从 MySQL 导出数据 我们使用 mydumper 从 MySQL 导出数据，如下:./bin/mydumper -h 127.0.0.1 -P 3306 -u root -t 16 -F 128 -B test -T t1,t2 -o ./var/test 上面，我们使用 -B test 表明是对 test 这个 database 操作，然后用 -T t1,t2 表明只导出 t1，t2 两张表。-t 16 表明使用 16 个线程去导出数据。-F 128 是将实际的 table 切分成多大的 chunk，这里就是 128MB 一个 chunk。注意：在阿里云一些需要 super privilege 的云上面，mydumper 需要加上 --no-locks 参数，否则会提示没有权限操作。给 TiDB 导入数据 我们使用 myloader 将之前导出的数据导入到 TiDB。./bin/myloader -h 127.0.0.1 -P 4000 -u root -t 16 -q 1 -d ./var/test 这里 -q 1 表明每个事务包含多少个 query，默认是 1000，我们这里使用 1 就可以了。导入成功之后，我们可以用 MySQL 官方客户端进入 TiDB，查看:mysql -h127.0.0.1 -P4000 -uroot mysql&amp;gt; show tables; +----------------+ | Tables_in_test | +----------------+ | t1 | | t2 | +----------------+ mysql&amp;gt; select * from t1; +----+------+ | id | age | +----+------+ | 1 | 1 | | 2 | 2 | | 3 | 3 | +----+------+ mysql&amp;gt; select * from t2; +----+------+ | id | name | +----+------+ | 1 | a | | 2 | b | | 3 | c | +----+------+ 使用 syncer 增量导入数据实现数据和 MySQL 实时同步 上面我们介绍了如何使用 mydumper/myloader 将 MySQL 的数据全量导入到 TiDB，但如果后续 MySQL 的数据有更新，我们仍然希望快速导入，使用全量的方式就不合适了。TiDB 提供 syncer 工具能方便的将 MySQL 的数据增量的导入到 TiDB 里面。syncer 也属于 TiDB 工具集，如何获取可以参考 下载 TiDB 工具集。假设我们之前已经使用 mydumper/myloader 导入了 t1 和 t2 两张表的一些数据，现在我们希望这两张表的任何更新，都是实时的同步到 TiDB 上面。MySQL 开启 binlog 在使用 syncer 之前，我们必须保证： MySQL 开启 binlog 功能，参考 Setting the Replication Master Configuration Binlog 格式必须使用 row format，这也是 MySQL 5.7 之后推荐的 binlog 格式，可以使用如下语句打开:  SET GLOBAL binlog_format = ROW; 获取同步 position 我们通过 show master status 得到当前 binlog 的 position，syncer 的初始同步位置就是从这个地方开始。show master status; +------------------+----------+--------------+------------------+-------------------+ | File | Position | Binlog_Do_DB | Binlog_Ignore_DB | Executed_Gtid_Set | +------------------+----------+--------------+------------------+-------------------+ | mysql-bin.000003 | 1280 | | | | +------------------+----------+--------------+------------------+-------------------+ 我们将 position 相关的信息保存到一个 syncer.meta 文件里面，用于 syncer 的同步:# cat syncer.meta binlog-name = &amp;#34;mysql-bin.000003&amp;#34; binlog-pos = 1280 注意：syncer.meta 只需要第一次使用的时候配置，后续 syncer 同步新的 binlog 之后会自动将其更新到最新的 position。启动 syncer syncer 的配置文件 config.toml:log-level = &amp;#34;info&amp;#34; server-id = 101 # meta 文件地址 meta = &amp;#34;./syncer.meta&amp;#34; worker-count = 1 batch = 1 pprof-addr = &amp;#34;:10081&amp;#34; [from] host = &amp;#34;127.0.0.1&amp;#34; user = &amp;#34;root&amp;#34; password = &amp;#34;&amp;#34; port = 3306 [to] host = &amp;#34;127.0.0.1&amp;#34; user = &amp;#34;root&amp;#34; password = &amp;#34;&amp;#34; port = 4000 启动 syncer:./bin/syncer -config config.toml 2016/10/27 15:22:01 binlogsyncer.go:226: [info] begin to sync binlog from position (mysql-bin.000003, 1280) 2016/10/27 15:22:01 binlogsyncer.go:130: [info] register slave for master server 127.0.0.1:3306 2016/10/27 15:22:01 binlogsyncer.go:552: [info] rotate to (mysql-bin.000003, 1280) 2016/10/27 15:22:01 syncer.go:549: [info] rotate binlog to (mysql-bin.000003, 1280) 在 MySQL 插入新的数据 INSERT INTO t1 VALUES (4, 4), (5, 5); 登录到 TiDB 查看：mysql -h127.0.0.1 -P4000 -uroot -p mysql&amp;gt; select * from t1; +----+------+ | id | age | +----+------+ | 1 | 1 | | 2 | 2 | | 3 | 3 | | 4 | 4 | | 5 | 5 | +----+------+ syncer 每隔 30s 会输出当前的同步统计，如下2016/10/27 15:22:31 syncer.go:668: [info] [syncer]total events = 1, insert = 1, update = 0, delete = 0, total tps = 0, recent tps = 0, binlog name = mysql-bin.000003, binlog pos = 1280. 2016/10/27 15:23:01 syncer.go:668: [info] [syncer]total events = 2, insert = 2, update = 0, delete = 0, total tps = 0, recent tps = 0, binlog name = mysql-bin.000003, binlog pos = 1538. 可以看到，使用 syncer，我们就能自动的将 MySQL 的更新同步到 TiDB。"},
		{"url": "https://pingcap.com/meetup/memoir/meetup-2016-11-01-1/",
		"title": "How to write a good commit message", 
		"content": " Why a good commit message matters  Speed up the review process. Help the future maintaners establish the content of the change. The future maintaner could be you yourself several months later. Help to build a good release notes.  Why a good formatted commit message matters  Allow filtering commitsFor example, you can get new features in this release:$ git log HEAD &amp;ndash;grep feature Provide better information when browsing the history$ git log HEAD &amp;ndash;pretty=format:%s Allow generating change log or Release Notes using scriptYou can use conventional-changelog to generate the change log, the output could be like Change log example.  What is a good commit message A common commit message:Fix login bug A good example:Redirect user to the requested page after login https://trello.com/path/to/relevant/card Users were being redirected to the home page after login, which is less useful than redirecting to the page they had originally requested before being redirected to the login form. * Store requested path in a session variable * Redirect to the stored location after successfully logging in the user Source: https://robots.thoughtbot.com/5-useful-tips-for-a-better-commit-message A good commit message answers the following questions:  Why is it necessary?It may fix a bug, it may add a feature, it may improve performance, reliability, stability, or just be a change for the sake of correctness. How does it address the issue?For short obvious patches this part can be omitted, but it should be a high level description of what the approach was. What effects does the patch have?(In addition to the obvious ones, this may include benchmarks, side effects, etc.)  How to write a good commit message  Good habits Good format Good language  Good habits Make sure that the commit message must contain all the information required to fully understand &amp;amp; review the patch for correctness. Pay attention to the first commit line which works as the subject of the commit message. It is mandatory and is the most important. Describe why a change is being made. Describe the limitations of the current code. Limit using the -m commit flag. AVOID the following examples: git commit -m &amp;ldquo;Fix login bug&amp;rdquo; git commit -m &amp;ldquo;Up&amp;rdquo; git commit -m &amp;ldquo;just on line&amp;rdquo;  A commit contains exactly one logical change. AVOID the following commits: End-of-day commits Per-file commit Lazy commit messages: “misc fixes and cleanups” Two changes in one patch. &amp;ldquo;Fixed bug 2345 and renamed all pd to PD`&amp;ldquo;.  Do not assume the reviewer understands the original problem. Do not assume the code is self-evident/self-documenting.  Good format Apply the PingCAP format: https://github.com/pingcap/tikv/blob/master/CONTRIBUTING.md&amp;lt;subsystem&amp;gt;: &amp;lt;what changed&amp;gt; &amp;lt;BLANK LINE&amp;gt; &amp;lt;why this change was made&amp;gt; &amp;lt;BLANK LINE&amp;gt; &amp;lt;footer&amp;gt;(optional) &amp;lt;subsystem&amp;gt;: &amp;lt;what changed&amp;gt;  This line works as the subject and states the summary of changes. Capitalize the first letter and leave no period at the end. Use the imperative mood. The subject fits the following sentence:If applied, this commit will your subject line here (for example, update getting started documentation) 69 characters the most, 50 or less is preferred About the , use the following format: If it’s one subsystem: pd: If there are more than one subsystems: util/codec,util/types: If there are many subsystems: *:    More detailed explanatory text, if necessary. Don&amp;rsquo;t describe the code, describe the intent (why) and the approach(what) in a present tense. Wrap it to about 72 characters or so. Further paragraphs come after blank lines. Bullet points are okay, too. If it seems difficult to summarize what your commit does, it may be because it includes several logical changes or bug fixes, and are better split up into several commits using git add -p.  (optional) DocImpact/APIImpact/SecurityImpact/UpgradeImpact Close-Bug/Related-Bug/Partial-Bug: #1555 Signed-off-by = CLA  Good language  Use the active voice Use articles wherever needed Use simple verb tenses Use language consistently Avoid lengthy compound words Use relatively short sentences Use full sentences Do not abbreviate unless it’s absolutely necessary  Tools  Commitizen helps you write a good commit message validate-commit-msg helps you check if your message complies with the format. conventional-changelog helps you build the Change log  Special thanks to  http://who-t.blogspot.jp/2009/12/on-commit-messages.html http://chris.beams.io/posts/git-commit/ https://wiki.openstack.org/wiki/GitCommitMessages https://ruby-china.org/topics/15737 http://www.oschina.net/news/69705/git-commit-message-and-changelog-g  "},
		{"url": "https://pingcap.com/blog-cn/distributed-system-test-1/",
		"title": "分布式系统测试那些事儿 - 理念", 
		"content": "  本话题系列文章整理自 PingCAP NewSQL Meetup 第 26 期刘奇分享的《深度探索分布式系统测试》议题现场实录。文章较长，为方便大家阅读，会分为上中下三篇，本文为上篇。 今天主要是介绍分布式系统测试。对于 PingCAP 目前的现状来说，我们是觉得做好分布式系统测试比做一个分布式系统更难。就是你把它写出来不是最难的，把它测好才是最难的。大家肯定会觉得有这么夸张吗？那我们先从一个最简单的、每个人都会写的 Hello world 开始。A simple “Hello world” is a miracle We should walk through all of the bugs in: Compiler Linker VM (maybe) OS  其实这个 Hello world 能够每次都正确运行已经是一个奇迹了，为什么呢？首先，编译器得没 bug，链接器得没 bug ；然后我们可能跑在 VM 上，那 VM 还得没 bug；并且 Hello world 那还有一个 syscall，那我们还得保证操作系统没有 bug；到这还不算吧，我们还得要硬件没有 bug。所以一个最简单程序它能正常运行起来，我们要穿越巨长的一条路径，然后这个路径里面所有的东西都不能出问题，我们才能看到一个最简单的 Hello world。但是分布式系统里面呢，就更加复杂了。比如大家现在用的很典型的微服务。假设你提供了一个微服务，然后在微服务提供的功能就是输出一个 Hello world ，然后让别人来 Call。A RPC “Hello world” is a miracle We should walk through all of the bugs in: Coordinator (zookeeper, etcd) RPC implementation Network stack Encoding/Decoding library Compiler for programming languages or [protocol buffers, avro, msgpack, capn]  那么我们可以看一下它的路径。我们起码需要依赖 Coordinator 去做这种服务发现，比如用 zookeeper，etcd ，大家会感觉是这东西应该很稳定了吧？但大家可以去查一下他们每一次 release notes，里边说我们 fix 了哪些 bug，就是所有大家印象中非常稳定的这些东西，一直都在升级，每一次升级都会有 bug fix。但换个思路来看，其实我们也很幸运，因为大部分时候我们没有碰到那个 bug，然后 RPC 的这个实现不能有问题。当然如果大家深度使用 RPC，比如说 gRPC，你会发现其实 bug 还是挺多的，用的深一点，基本上就会发现它有 bug。还有系统网络协议栈，去年 TCP 被爆出有一个 checksum 问题，就是 Linux 的 TCP 协议栈，这都是印象中永远不会出问题的。再有，编解码，大家如果有 Go 的经验的话，可以看一下 Go 的 JSON 历史上从发布以来更新的记录，也会发现一些 bug。还有更多的大家喜欢的编解码，比如说你用 Protocol buffers、Avro、Msgpack、Cap&amp;rsquo;n 等等，那它们本身还需要 compiler 去生成一个代码，然后我们还需要那个 compiler 生成的代码是没有 bug 的。然后这一整套下来，我们这个程序差不多是能运行的，当然我们没有考虑硬件本身的 bug。其实一个正确的运行程序从概率上来讲（不考虑宇宙射线什么的这种），已经是非常幸运的了。当然每一个系统都不是完善的，那通常情况下，为什么我们这个就运行的顺利呢？因为我们的测试永远都测到了正确的路径，我们跑一个简单的测试一定是把正确的路径测到了，但是这中间有很多错误路径其实我们都没有碰到。然后我不知道大家有没有印象，如果写 Go 程序的时候，错误处理通常写成 if err != nil，然后 return error ，不知道大家写了多少。那其它程序、其它的语言里就是 try.catch，然后里面各种 error 处理。就是一个真正完善的系统，最终的错误处理代码实际上通常会比你写正常逻辑代码还要多的，但是我们的测试通常 cover 的是正确的逻辑，就是实际上我们测试的 cover 是一小部分。那先纠正几个观念，关于测试的。就是到底怎么样才能得到一个好的、高质量的程序，或者说得到一个高质量的系统？Who is the tester ?  Quality comes from solid engineering. Stop talking and go build things. Don’t hire too many testers.  Testing is owned by the entire team. It is a culture, not a process.  Are testers software engineers? Yes. Hiring good people is the first step. And then keep them challenged.  我们的观念是说先有 solid engineering 。我觉得这个几乎是勿庸置疑的吧，不知道大家的经验是什么？然后还有一个就是不扯淡，尽快去把东西 build 起来，然后让东西去运转起来。我前一段时间也写了一个段子，就是：“你是写 Rust 的，他是写 Java 的，你们这聊了这么久，人家 Rust （编译速度慢） 的程序已经编译过了，你 Java 还没开始写。”原版是这样的:“你是砍柴的，他是放羊的，你们聊了一天，他的羊吃饱了，你的柴呢？”然后最近还有一个特别有争议的话题：CTO 应该干嘛。就是 CTO 到底该不该写代码，这个也是众说纷纭。因为每一个人都受到自己环境的局限，所以每个人的看法都是不一样的。那我觉得有点像，就是同样是聊天，然后不同人有不同的看法。Test automation  Allow developers to get a unit test results immediately. Allow developers to run all unit tests in one go. Allow code coverage calculations. Show the testing evolution on the dashboards. Automate everything.  我们现在很有意思的一个事情是，迄今为止 PingCAP 没有一个测试人员，这是在所有的公司看来可能都是觉得不可思议的事情，那为什么我们要这么干？因为我们现在的测试已经不可能由人去测了。究竟复杂到什么程度呢？我说几个基本数字大家感受一下：我们现在有六百多万个 Test，这是完全自动化去跑的。然后我们还有大量从社区收集到的各种 ORM Test，一会我会提到这一点。就是这么多 Test 已经不可能是由人写出来的了，以前的概念里面是 Test 是由人写的，但实际上 Test 不一定是人写的，Test 也是可以由机器生成的。举个例子，如果给你一个合法的语法树，你按照这个语法树去做一个输出，比如说你可以更换变量名，可以更换它的表达式等等，你可以生成很多的这种 SQL 出来。Google Spanner 就用到这个特性，它会有专门的程序自动生成符合 SQL 语法的语句，然后再交给系统去执行。如果执行过程中 crash 了，那说明这个系统肯定有 bug。但是这地方又蹦出另外一个问题，就是你生成了合法的 SQL 语句，但是你不知道它语句执行的结构，那你怎么去判断它是不是对的？当然业界有很聪明的人。我把它扔给几个数据库同时跑一下，然后取几个大家一致的结果，那我就认为这个结果基本上是对的。如果一个语句过来，然后在我这边执行的结果和另外几个都不一样，那说明我这边肯定错了。就算你是对的，可能也是错的，因为别人执行下来都是这个结果，你不一样，那大家都会认为你是错的。所以说在测试的时候，怎么去自动生成测试很重要。去年，在美国那边开始流行一个新的说法，叫做 “怎么在你睡觉的时候发现 bug”。那么实际上测试干的很重要的事情就是这个，就是自动化测试是可以在你睡觉的时候发现 bug。好像刚才我们还提到 fault injection ，好像还有 fuzz testing。然后所有测试的人都是工程师，因为只有这样你才不会甩锅。这是我们现在坚信的一个事情，就是所有的测试必须要高度的自动化，完全不由人去干预。然后很重要的一个就是雇最优秀的人才，同时给他们挑战，就是如果没有挑战，这些人才会很闲，精力分散，然后很难合力出成绩。因为以现在这个社会而言，很重要一个特性是什么？就是对于复杂性工程需要大量的优秀人才，如果优秀的人才力不往一处使力的话，这个复杂性工程是做不出来的。我今天看了一下龙芯做了十年了，差不多是做到英特尔凌动处理器的水平。他们肯定是有很优秀的人才，但是目前还得承认，我们在硬件上面和国外的差距还比较大，其实软件上面的差距也比较大，比如说我们和 Spanner 起码差了七年，2012 年 Spanner 就已经大规模在 Google 使用了，对这些优秀的作品，我们一直心存敬仰。我刚才已经反复强调过自动化这个事情。不知道大家平时写代码 cover 已经到多少了？如果 cover 一直低于 50%，那就是说你有一半的代码没有被测到，那它在线上什么时候都有可能出现问题。当然我们还需要更好的方法去在上线之前能够把线上的 case 回放。理论上你对线上这个回放的越久你就越安全，但是前提是线上代码永远不更新，如果业务方更新了，那就又相当于埋下了一个定时炸弹。比如说你在上面跑两个月，然后业务现在有一点修改，然而那两个又没有 cover 住修改，那这时候可能有新的问题。所以要把所有的一切都自动化，包括刚才的监控。比如说你一个系统一过去，然后自动发现有哪些项需要监控，然后自动设置报警。大家觉得这事是不是很神奇？其实这在 Google 里面是司空见惯的事情，PingCAP 现在也正在做。Well… still not enough ?  Each layer can be tested independently. Make sure you are building the right tests. Don’t bother great people unless the testing fails. Write unit tests for every bug.  这么多还是不够的，就是对于整个系统测试来讲，你可以分成很多层、分成很多模块，然后一个一个的去测。还有很重要的一点，就是早期的时候我们发现一个很有意思的事情。就是我们 build 了大量 Test，然后我们的程序都轻松的 pass 了大量的 Test，后来发现我们一个 Test 是错的，那意味着什么？意味着我们的程序一直是错的，因为 Test 会把你这个 cover 住。所以直到后来我们有一次觉得自己写了一个正确的代码，但是跑出来的结果不对，我们这时候再去查，发现以前有一个 Test 写错了。所以一个正确的 Test 是非常重要的，否则你永远被埋在错误里面，然后埋在错误里面感觉还特别好，因为它告诉你是正确的。还有，为什么要自动化呢？就是你不要去打扰这些聪明人。他们本身很聪明，你没事别去打扰他们，说“来，你过来给我做个测试”，那这时候不断去打扰他们，是影响他们的发挥，影响他们做自己的挑战。这一条非常重要，所有出现过的 bug，历史上只要出现过一次，你一定要写一个 Test 去 cover 它，那这个法则大家应该已经都清楚了。我看今天所在的人的年龄，应该《圣斗士星矢》是看过的，对吧？这个圣斗士是有一个特点的，所有对他们有效的招数只能用一次，那这个也是一样的，就保证你不会被再次咬到，就不会再次被坑到。我印象中应该有很多人 fix bug 是这样的：有一个 bug 我 fix 了，但没有 Test，后来又出现了，然后这时候就觉得很奇怪，然后积累的越多，最后就被坑的越惨。这个是目前主流开源社区都在坚持的做法，基本没有例外。就是如果有一个开源社区说我发现一个 bug，我没有 Test 去 cover 它，这个东西以后别人是不敢用的。Code review  At least two LGTMs (Looks good to me) from the maintainers. Address comments. Squash commit logs. Travis CI/Circle CI for PRs.  简单说一下 code review 的事情，它和 Test 还是有一点关系，为什么？因为在 code review 的时候你会提一个新的 pr，然后这个 pr 一定要通过这个 Test。比如说典型的 Travis CI，或者 CircleCI 的这种 Test。为什么要这样做呢？因为要保证它被 merge 到 master 之前你一定要发现这个问题，如果已经 merge 到 master 了，首先这不好看，因为你要 revert 掉，这个在 commit 记录上是特别不好看的一个事情。另外一个就是它出现问题之前，你就先把它发现其实是最好的，因为有很多工具会根据 master 自动去 build。比如说我们会根据 master 去自动 build docker 镜像，一旦你代码被 commit 到 master，然后 docker 镜像就出来了。那你的用户就发现，你有新的更新，我要马上使用新的，但是如果你之前的 CI 没有过，这时候就麻烦了，所以 CI 没过，一定不能进入到 CD 阶段。Who to blame in case of bugs? The entire team.另外一个观念纠正一下，就是出现 bug 的时候，责任是谁的？通常我见过的很多人都是这样，就说“这个 bug 跟我没关系，他的模块的 bug”。那 PingCAP 这边的看法不一样，就是一旦出现 bug，这应该是整个 team 的责任，因为你有自己的 code review 机制，至少有两个以上的人会去看它这个代码，然后如果这个还出现问题，那一定不是一个人的问题。除了刚才说的发现一些 bug，还有一些你很难定义，说这是不是 bug，怎么系统跑的慢，这算不算 bug，怎么对 bug 做界定呢？我们现在的界定方式是用户说了算。虽然我们觉得这不是 bug，这不就慢一点吗，但是用户说了这个东西太慢了，我们不能忍，这就是 bug，你就是该优化的就优化。然后我们团队里面出现过这样的事情，说“我们这个已经跑的很快了，已经够快了”，对不起，用户说慢，用户说慢就得改，你就得去提升。总而言之，标准不能自己定，当然如果你自己去定这个标准，那这个事就变成“我这个很 OK 了，我不需要改了，可以了。”这样是不行的。Profiling  Profile everything, even on production  once-in-a-lifetime chance  Bench testing  另外，在 Profile 这个事情上面，我们强调一个，即使是在线上，也需要能做 Profile，其实 Profile 的开销是很小的。然后很有可能是这样的，有一次线上系统特别卡，如果你把那个重启了，你可能再也没有机会复现它了，那么对于这些情况它很可能是一辈子发生一次的，那一次你没有抓住它，你可能再也没有机会抓住它了。当然我们后面会介绍一些方法，可以让这个能复现，但是有一些确实是和业务相关性极强的，那么可能刚好又碰到一个特别的环境才能让它出现，那真的可能是一辈子就那么一次的，你一定要这次抓住它，这次抓不住，你可能永远就抓不住了。因为有些犯罪它一辈子只犯一次，它犯完之后你再也没有机会抓住它了。Embed testing to your design  Design for testing or Die without good tests Tests may make your code less beautiful  再说测试和设计的关系。测试是一定要融入到你的设计里面，就是在你设计的时候就一定要想这个东西到底应该怎么去测。如果在设计的时候想不到这个东西应该怎么测，那这个东西就是正确性实际上是没法验证的，这是非常恐怖的一件事情。我们把测试的重要程度看成这样的：你要么就设计好的测试，要么就挂了，就没什么其它的容你选择。就是说在这一块我们把它的重要性放到一个最高的程度。未完待续&amp;hellip; "},
		{"url": "https://pingcap.com/meetup/memoir/meetup-2016-11-01/",
		"title": "分布式系统测试那些事儿——理念", 
		"content": " 本话题系列文章整理自 PingCAP NewSQL Meetup 第 26 期刘奇分享的《深度探索分布式系统测试》议题现场实录。文章较长，为方便大家阅读，会分为上中下三篇，本文为上篇。 今天主要是介绍分布式系统测试。对于 PingCAP 目前的现状来说，我们是觉得做好分布式系统测试比做一个分布式系统更难。就是你把它写出来不是最难的，把它测好才是最难的。大家肯定会觉得有这么夸张吗？那我们先从一个最简单的、每个人都会写的 Hello world 开始。 A simple “Hello world” is a miracleWe should walk through all of the bugs in: Compiler Linker VM (maybe) OS   其实这个 Hello world 能够每次都正确运行已经是一个奇迹了，为什么呢？首先，编译器得没 bug，链接器得没 bug ；然后我们可能跑在 VM 上，那 VM 还得没 bug；并且 Hello world 那还有一个 syscall，那我们还得保证操作系统没有 bug；到这还不算吧，我们还得要硬件没有 bug。所以一个最简单程序它能正常运行起来，我们要穿越巨长的一条路径，然后这个路径里面所有的东西都不能出问题，我们才能看到一个最简单的 Hello world。但是分布式系统里面呢，就更加复杂了。比如大家现在用的很典型的微服务。假设你提供了一个微服务，然后在微服务提供的功能就是输出一个 Hello world ，然后让别人来 Call。 A RPC “Hello world” is a miracleWe should walk through all of the bugs in: Coordinator (zookeeper, etcd) RPC implementation Network stack Encoding/Decoding library Compiler for programming languages or [protocol buffers, avro, msgpack, capn]   那么我们可以看一下它的路径。我们起码需要依赖 Coordinator 去做这种服务发现，比如用 zookeeper，etcd ，大家会感觉是这东西应该很稳定了吧？但大家可以去查一下他们每一次 release notes，里边说我们 fix 了哪些 bug，就是所有大家印象中非常稳定的这些东西，一直都在升级，每一次升级都会有 bug fix。但换个思路来看，其实我们也很幸运，因为大部分时候我们没有碰到那个 bug，然后 RPC 的这个实现不能有问题。当然如果大家深度使用 RPC，比如说 gRPC，你会发现其实 bug 还是挺多的，用的深一点，基本上就会发现它有 bug。还有系统网络协议栈，去年 TCP 被爆出有一个 checksum 问题，就是 Linux 的 TCP 协议栈，这都是印象中永远不会出问题的。再有，编解码，大家如果有 Go 的经验的话，可以看一下 Go 的 JSON 历史上从发布以来更新的记录，也会发现一些 bug。还有更多的大家喜欢的编解码，比如说你用 Protocol buffers、Avro、Msgpack、Cap&amp;rsquo;n 等等，那它们本身还需要 compiler 去生成一个代码，然后我们还需要那个 compiler 生成的代码是没有 bug 的。然后这一整套下来，我们这个程序差不多是能运行的，当然我们没有考虑硬件本身的 bug。其实一个正确的运行程序从概率上来讲（不考虑宇宙射线什么的这种），已经是非常幸运的了。当然每一个系统都不是完善的，那通常情况下，为什么我们这个就运行的顺利呢？因为我们的测试永远都测到了正确的路径，我们跑一个简单的测试一定是把正确的路径测到了，但是这中间有很多错误路径其实我们都没有碰到。然后我不知道大家有没有印象，如果写 Go 程序的时候，错误处理通常写成 if err != nil，然后 return error ，不知道大家写了多少。那其它程序、其它的语言里就是 try.catch，然后里面各种 error 处理。就是一个真正完善的系统，最终的错误处理代码实际上通常会比你写正常逻辑代码还要多的，但是我们的测试通常 cover 的是正确的逻辑，就是实际上我们测试的 cover 是一小部分。 那先纠正几个观念，关于测试的。就是到底怎么样才能得到一个好的、高质量的程序，或者说得到一个高质量的系统？ Who is the tester ? Quality comes from solid engineering. Stop talking and go build things. Don’t hire too many testers.  Testing is owned by the entire team. It is a culture, not a process.  Are testers software engineers? Yes. Hiring good people is the first step. And then keep them challenged.   我们的观念是说先有 solid engineering 。我觉得这个几乎是勿庸置疑的吧，不知道大家的经验是什么？然后还有一个就是不扯淡，尽快去把东西 build 起来，然后让东西去运转起来。我前一段时间也写了一个段子，就是：“你是写 Rust 的，他是写 Java 的，你们这聊了这么久，人家 Rust （编译速度慢） 的程序已经编译过了，你 Java 还没开始写。”原版是这样的:“你是砍柴的，他是放羊的，你们聊了一天，他的羊吃饱了，你的柴呢？”然后最近还有一个特别有争议的话题：CTO 应该干嘛。就是 CTO 到底该不该写代码，这个也是众说纷纭。因为每一个人都受到自己环境的局限，所以每个人的看法都是不一样的。那我觉得有点像，就是同样是聊天，然后不同人有不同的看法。 Test automation Allow developers to get a unit test results immediately. Allow developers to run all unit tests in one go. Allow code coverage calculations. Show the testing evolution on the dashboards. Automate everything.   我们现在很有意思的一个事情是，迄今为止 PingCAP 没有一个测试人员，这是在所有的公司看来可能都是觉得不可思议的事情，那为什么我们要这么干？因为我们现在的测试已经不可能由人去测了。究竟复杂到什么程度呢？我说几个基本数字大家感受一下：我们现在有六百多万个 Test，这是完全自动化去跑的。然后我们还有大量从社区收集到的各种 ORM Test，一会我会提到这一点。就是这么多 Test 已经不可能是由人写出来的了，以前的概念里面是 Test 是由人写的，但实际上 Test 不一定是人写的，Test 也是可以由机器生成的。举个例子，如果给你一个合法的语法树，你按照这个语法树去做一个输出，比如说你可以更换变量名，可以更换它的表达式等等，你可以生成很多的这种 SQL 出来。Google Spanner 就用到这个特性，它会有专门的程序自动生成符合 SQL 语法的语句，然后再交给系统去执行。如果执行过程中 crash 了，那说明这个系统肯定有 bug。但是这地方又蹦出另外一个问题，就是你生成了合法的 SQL 语句，但是你不知道它语句执行的结构，那你怎么去判断它是不是对的？当然业界有很聪明的人。我把它扔给几个数据库同时跑一下，然后取几个大家一致的结果，那我就认为这个结果基本上是对的。如果一个语句过来，然后在我这边执行的结果和另外几个都不一样，那说明我这边肯定错了。就算你是对的，可能也是错的，因为别人执行下来都是这个结果，你不一样，那大家都会认为你是错的。所以说在测试的时候，怎么去自动生成测试很重要。去年，在美国那边开始流行一个新的说法，叫做 “怎么在你睡觉的时候发现 bug”。那么实际上测试干的很重要的事情就是这个，就是自动化测试是可以在你睡觉的时候发现 bug。好像刚才我们还提到 fault injection ，好像还有 fuzz testing。然后所有测试的人都是工程师，因为只有这样你才不会甩锅。这是我们现在坚信的一个事情，就是所有的测试必须要高度的自动化，完全不由人去干预。然后很重要的一个就是雇最优秀的人才，同时给他们挑战，就是如果没有挑战，这些人才会很闲，精力分散，然后很难合力出成绩。因为以现在这个社会而言，很重要一个特性是什么？就是对于复杂性工程需要大量的优秀人才，如果优秀的人才力不往一处使力的话，这个复杂性工程是做不出来的。我今天看了一下龙芯做了十年了，差不多是做到英特尔凌动处理器的水平。他们肯定是有很优秀的人才，但是目前还得承认，我们在硬件上面和国外的差距还比较大，其实软件上面的差距也比较大，比如说我们和 Spanner 起码差了七年，2012 年 Spanner 就已经大规模在 Google 使用了，对这些优秀的作品，我们一直心存敬仰。我刚才已经反复强调过自动化这个事情。不知道大家平时写代码 cover 已经到多少了？如果 cover 一直低于 50%，那就是说你有一半的代码没有被测到，那它在线上什么时候都有可能出现问题。当然我们还需要更好的方法去在上线之前能够把线上的 case 回放。理论上你对线上这个回放的越久你就越安全，但是前提是线上代码永远不更新，如果业务方更新了，那就又相当于埋下了一个定时炸弹。比如说你在上面跑两个月，然后业务现在有一点修改，然而那两个又没有 cover 住修改，那这时候可能有新的问题。所以要把所有的一切都自动化，包括刚才的监控。比如说你一个系统一过去，然后自动发现有哪些项需要监控，然后自动设置报警。大家觉得这事是不是很神奇？其实这在 Google 里面是司空见惯的事情，PingCAP 现在也正在做。 Well… still not enough ? Each layer can be tested independently. Make sure you are building the right tests. Don’t bother great people unless the testing fails. Write unit tests for every bug.   这么多还是不够的，就是对于整个系统测试来讲，你可以分成很多层、分成很多模块，然后一个一个的去测。还有很重要的一点，就是早期的时候我们发现一个很有意思的事情。就是我们 build 了大量 Test，然后我们的程序都轻松的 pass 了大量的 Test，后来发现我们一个 Test 是错的，那意味着什么？意味着我们的程序一直是错的，因为 Test 会把你这个 cover 住。所以直到后来我们有一次觉得自己写了一个正确的代码，但是跑出来的结果不对，我们这时候再去查，发现以前有一个 Test 写错了。所以一个正确的 Test 是非常重要的，否则你永远被埋在错误里面，然后埋在错误里面感觉还特别好，因为它告诉你是正确的。还有，为什么要自动化呢？就是你不要去打扰这些聪明人。他们本身很聪明，你没事别去打扰他们，说“来，你过来给我做个测试”，那这时候不断去打扰他们，是影响他们的发挥，影响他们做自己的挑战。这一条非常重要，所有出现过的 bug，历史上只要出现过一次，你一定要写一个 Test 去 cover 它，那这个法则大家应该已经都清楚了。我看今天所在的人的年龄，应该《圣斗士星矢》是看过的，对吧？这个圣斗士是有一个特点的，所有对他们有效的招数只能用一次，那这个也是一样的，就保证你不会被再次咬到，就不会再次被坑到。我印象中应该有很多人 fix bug 是这样的：有一个 bug 我 fix 了，但没有 Test，后来又出现了，然后这时候就觉得很奇怪，然后积累的越多，最后就被坑的越惨。这个是目前主流开源社区都在坚持的做法，基本没有例外。就是如果有一个开源社区说我发现一个 bug，我没有 Test 去 cover 它，这个东西以后别人是不敢用的。 Code review At least two LGTMs (Looks good to me) from the maintainers. Address comments. Squash commit logs. Travis CI/Circle CI for PRs.   简单说一下 code review 的事情，它和 Test 还是有一点关系，为什么？因为在 code review 的时候你会提一个新的 pr，然后这个 pr 一定要通过这个 Test。比如说典型的 Travis CI，或者 CircleCI 的这种 Test。为什么要这样做呢？因为要保证它被 merge 到 master 之前你一定要发现这个问题，如果已经 merge 到 master 了，首先这不好看，因为你要 revert 掉，这个在 commit 记录上是特别不好看的一个事情。另外一个就是它出现问题之前，你就先把它发现其实是最好的，因为有很多工具会根据 master 自动去 build。比如说我们会根据 master 去自动 build docker 镜像，一旦你代码被 commit 到 master，然后 docker 镜像就出来了。那你的用户就发现，你有新的更新，我要马上使用新的，但是如果你之前的 CI 没有过，这时候就麻烦了，所以 CI 没过，一定不能进入到 CD 阶段。 &amp;gt; **Who to blame in case of bugs? ** The entire team. 另外一个观念纠正一下，就是出现 bug 的时候，责任是谁的？通常我见过的很多人都是这样，就说“这个 bug 跟我没关系，他的模块的 bug”。那 PingCAP 这边的看法不一样，就是一旦出现 bug，这应该是整个 team 的责任，因为你有自己的 code review 机制，至少有两个以上的人会去看它这个代码，然后如果这个还出现问题，那一定不是一个人的问题。除了刚才说的发现一些 bug，还有一些你很难定义，说这是不是 bug，怎么系统跑的慢，这算不算 bug，怎么对 bug 做界定呢？我们现在的界定方式是用户说了算。虽然我们觉得这不是 bug，这不就慢一点吗，但是用户说了这个东西太慢了，我们不能忍，这就是 bug，你就是该优化的就优化。然后我们团队里面出现过这样的事情，说“我们这个已经跑的很快了，已经够快了”，对不起，用户说慢，用户说慢就得改，你就得去提升。总而言之，标准不能自己定，当然如果你自己去定这个标准，那这个事就变成“我这个很 OK 了，我不需要改了，可以了。”这样是不行的。 Profiling Profile everything, even on production  once-in-a-lifetime chance  Bench testing   另外，在 Profile 这个事情上面，我们强调一个，即使是在线上，也需要能做 Profile，其实 Profile 的开销是很小的。然后很有可能是这样的，有一次线上系统特别卡，如果你把那个重启了，你可能再也没有机会复现它了，那么对于这些情况它很可能是一辈子发生一次的，那一次你没有抓住它，你可能再也没有机会抓住它了。当然我们后面会介绍一些方法，可以让这个能复现，但是有一些确实是和业务相关性极强的，那么可能刚好又碰到一个特别的环境才能让它出现，那真的可能是一辈子就那么一次的，你一定要这次抓住它，这次抓不住，你可能永远就抓不住了。因为有些犯罪它一辈子只犯一次，它犯完之后你再也没有机会抓住它了。 Embed testing to your design Design for testing or Die without good tests Tests may make your code less beautiful   再说测试和设计的关系。测试是一定要融入到你的设计里面，就是在你设计的时候就一定要想这个东西到底应该怎么去测。如果在设计的时候想不到这个东西应该怎么测，那这个东西就是正确性实际上是没法验证的，这是非常恐怖的一件事情。我们把测试的重要程度看成这样的：你要么就设计好的测试，要么就挂了，就没什么其它的容你选择。就是说在这一块我们把它的重要性放到一个最高的程度。"},
		{"url": "https://pingcap.com/weekly/2016-10-31-tidb-weekly/",
		"title": "Weekly update (October 24 ~ October 30, 2016)", 
		"content": " Weekly update (October 24 ~ October 30, 2016) Last week, we landed 24 PRs in the TiDB repositories and 28 PRs in the TiKV repositories.Notable changes to TiDB  Support coalesce/case when pushing down on local storage。 Support showing indexes in the table syntax. Split eval.go into some smaller files to make code cleaner. Fix a bug about truncating data. Fix the mysql version number format. Fix a bug about dropping the nonexist foreign key. Fix a bug in the parser where hexadecimal was parsed as string. Add comments for executor package to improve code readability. Add a command line flag to print binary version. Add a command line flag to prevent cartesian product.  Notable changes to TiKV  Compact region automatically after deleting lots of keys. Remove the rocksdb DSN for the tikv-server command flag. Use an atomic value to control the snapshot progress. Add timer for latch waiting. Check the region range before replicating a peer. Make sure the callback command is called. Coprocessor supports the multiply operator. Coprocessor supports the case whenoperator. Record the last compacted index to speed up compacting the Raft log.  Notable changes to Placement Driver  Refactor store/region cache to make code clearer, including PR 353, 365, 366. Support the GetPDMembers API.  New contributors  Librazy  "},
		{"url": "https://pingcap.com/meetup/meetup-2016-10-29/",
		"title": "PingCAP 第 27 期 NewSQL Meetup", 
		"content": " PingCAP 第 27 期 NewSQL Meetup 2016-10-29 付力力&amp;amp;刘寅 PingCAP PingCAPPingCAP ![]() 微信号pingcap2015功能介绍PingCAP 专注于新型分布式数据库的研发，是知名开源数据库 TiDB (GitHub 总计10000+ stars ) 背后的团队，总部设在北京，是国内第一家开源的新型分布式关系型数据库公司、国内领先的大数据技术和解决方案提供商。NewSQL Meetup今天是 PingCAP 第 27 期 Meetup，主题是神策数据联合创始人&amp;amp;首席架构师付力力分享的《Impala 在用户行为分析中的应用与优化》以及刘寅分享的《How we build CI/CD for TiDB at scale》。▌ ****Topic 1：Impala 在用户行为分析中的应用与优化多冷的天都不能阻止技术童鞋们浓厚的求知欲 :-DLecture：付力力，神策数据联合创始人&amp;amp;首席架构师，曾任百度、豌豆荚资深研发工程师，熟悉大规模数据处理、数据仓库、OLAP 数据库等领域。Content： 介绍用户行为分析的典型应用场景； 简单介绍 Impala 的架构和实现； 使用 Impala 进行用户行为分析的基本做法； 针对特定场景对 Impala 进行的一些优化和改造。  ▌ ****Topic 2：How we build CI/CD for TiDB at scaleLecture：刘寅，PingCAP engineer，现负责 TiDB 商业产品开发和自动化运维。Content：主要分享了我们如何为分布式数据库 TiDB 构建持续集成和持续交付平台，以支撑 TiDB 背后上千万的自动化测试 case，和多平台构建及发布。其中，重点介绍了以 Jenkins 为核心的开源工具，配合 Docker ／ Kubernetes 来搭建分布式可扩展的 CI/CD 系统。Jenkins 2.0 之后的 pipeline script 的支持极大地提升分布式构建的灵活性，我们可以明确定义整个构建过程的不同阶段，并且决定这些阶段运行在集群的某个节点上，让耗时的任务并行处理，极大缩短从代码提交到上线发布的周期。同时结合实际场景的例子，讲解了 jenkins 的一些实用技巧和我们遇到的坑。特别鸣谢：场地赞助-泰利驿站PingCAP Meetup"},
		{"url": "https://pingcap.com/weekly/2016-10-24-tidb-weekly/",
		"title": "Weekly update (October 17 ~ October 23, 2016)", 
		"content": " Weekly update (October 17 ~ October 23, 2016) Last week, we landed 30 PRs in the TiDB repositories and 26 PRs in the TiKV repositories.Notable changes to TiDB  Set the concurrency for the SQL executor using the Set statement Convert the Limit+Sort operator to the TopN operator on local storage Fix the gotouinue leak problem Support the logic/bitwise operator on local storage Support creating user without password Eliminate common aggregation function Support the Drop User statement Fix bugs  Notable changes to TiKV  Provide a tool tikv-ctl to browse the internal data in RocksDB. Make the Raft column family configurable. Avoid deleting other regions&amp;rsquo; data accidentally. Coprocessor supports minus, intdiv and mod operations. Support canceling a applying snapshot job which is in queue directly. Use get_region_by_id to check the stale region. Skip fetching unnecessary values to speed up the scan performance. Add flow control to fix issue 1190 Clean up temporary snapshot file when startup. Shrink the send buffer after sending a big query to avoid occupying too much memory.  Notable changes to Placement Driver  Add uptime to show the online time for a store. Embed metapb.Store into storeInfo to make the code cleaner.  "},
		{"url": "https://pingcap.com/meetup/meetup-2016-10-22/",
		"title": "PingCAP 第 26 期 NewSQL Meetup", 
		"content": " PingCAP 第 26 期 NewSQL Meetup 2016-10-22 张成远&amp;amp;刘奇 PingCAP PingCAPPingCAP ![]() 微信号pingcap2015功能介绍PingCAP 专注于新型分布式数据库的研发，是知名开源数据库 TiDB (GitHub 总计10000+ stars ) 背后的团队，总部设在北京，是国内第一家开源的新型分布式关系型数据库公司、国内领先的大数据技术和解决方案提供商。NewSQL Meetup今天是 PingCAP 第 26 期 Meetup，主题是开源项目 speedy 作者张成远分享的《京东分布式数据库实践》以及刘奇分享的《深度探索分布式系统测试》。 我司 CEO 亲自出台，现场不时传来三观碎一地的声音┑(￣Д ￣)┍另外，本周初次试水直播 (✿◡‿◡)▌ ****Topic 1：京东分布式数据库实践Lecture：张成远，《Mariadb 原理与实现》作者，开源项目 speedy 作者。目前就职于京东数据库系统研发团队，负责京东分布式数据库系统架构与研发工作，主导了京东分布式数据库系统在公司的落地及大规模推广。擅长高性能服务器开发，擅长分布式数据库/存储/缓存等大规模分布式系统架构。Content： 介绍京东分布式数据库的设计与实现； 介绍去 oracle 的发展历程以及遇到的一些坑； 如何做到高效的运维监控等。  ▌ ****Topic 2：深度探索分布式系统测试现场已爆满，本张照片拍摄于门缝&amp;hellip;Lecture：刘奇，PingCAP 联合创始人兼 CEO，先后创建了 Codis、TiDB/TiKV 等知名开源项目。现从事开源的分布式 NewSQL 数据库 TiDB/TiKV 开发。擅长高并发、大规模、分布式数据库系统架构设计。Content：主讲人自我点评称：“这是一次毁三观的分享”，因为这里定义了什么是及格的测试。如果您曾经认为自己的分布式系统测试做得非常好，听完之后，您会发现自己可能还远不到好的级别。分布式系统测试是很少被提及的话题，但分布式系统测试的困难甚至大于写一个分布式系统。一般大家普遍的看法是平时用得很多的分布式系统都是比较稳定的，然而当新的测试方法和工具出现时，可以发现很多新的 bug 或者极大的提高了测试的复现率。复现率是解决 bug 的基础，分布式系统 bug 的复现难度也远大于单机系统。本周刘奇和大家分享了分布式系统测试的一些困难，以及 PingCAP 和其它大型分布式系统的测试经验。为了方便未到现场的童鞋，后续小编会将本次分享内容整理成文档共享出来，让我们一起，毁~三~观~ :)PingCAP Meetup"},
		{"url": "https://pingcap.com/blog-cn/talk-principles-practice/",
		"title": "Building a Reliable Large-Scale Distributed Database - Principles and Practice", 
		"content": " 大家好，我叫申砾，是 PingCAP Tech Leader，负责 TiDB 技术相关的工作。我曾就职网易有道、360 搜索，主要在做垂直搜索相关的事情，现在主要关注分布式计算/存储领域。过去的一年半时间我在 PingCAP 做分布式关系数据库 TiDB。目前我们的整个项目已经开源了大概一年时间，获得了不少关注。在 Github 上 Star 数量超过 5k，并且 Contributor 数量为 50+，拥有一个活跃的社区，在国内和国际上都有一定的知名度。 今天主要想和大家分享一下我们在做一款开源的分布式数据库产品过程中得到的一些经验和体会，包括技术上、产品上以及开源社区方面的内容，不会涉及太多技术上的细节。数据库现状 近年来，随着移动互联网、物联网、人工智能等技术的兴起，我们已经进入了一个信息爆炸的大数据时代，需要处理和分析的数据越来越多，这些数据如何保存、如何应用是一个重要的问题。传统的 SQL 数据库一般通过中间件、分库分表等方案获得 Scale 的能力。但是这些方案仍然很难做到对应用透明且保证数据均匀分布，同时也无法支持一致性的跨节点事务、JOIN 等操作。在进行扩容的时候往往需要人工介入，随着集群规模的增大，维护和扩展的复杂度呈指数级上升。以 Google 的 BigTable 论文为开端，涌现出了一大批 NoSQL 方案。这些方案致力于解决扩展性，而牺牲一致性。如果采用 NoSQL 方案替换原有关系型数据库，往往要涉及大规模的业务重构，这相当于将数据库层的计算逻辑复杂度转嫁给业务层，同时还要损失掉事务等特性。以上两种方案都没有完美地解决高可用的问题，跨机房多活、故障恢复、扩容经常都需要繁重的人工介入。最近几年，人们希望有一种既有 SQL/NoSQL 的优点，又能避免他们的不足的新型数据库，于是提出了 NewSQL 的概念。Google 发布的 Spanner/F1，算是第一个真正在大规模业务上验证过的分布式数据库，向业界证明了 NewSQL 这条道路的正确性。TiDB 作为 Google Spanner/F1 的开源实现，正是业界盼望已久的 NewSQL 开源数据库。什么是 NewSQL 并不是所有号称 NewSQL 的数据库都是 NewSQL。我们认为作为 NewSQL 数据库需要有下面几点特性：首先是 Scale。这点上我想大家都深有体会，不管什么数据解决方案，最基本的要求就是能有足够的能力，保存用户所有的数据。第二是事务。ACID Transaction，这个东西如果业务不需要，就感觉不到；一旦你的业务有这种需求，就能体会到它的重要性了。事实证明这个需求是广泛存在的，Google 的 BigTable 没有提供事务，结果内部很多业务都有需求，于是各个组造了一堆轮子，Jeff Dean 看不下去，出来说他最大的错误就是没有给 BigTable 提供事务。第三是 SQL。SQL 作为一门古老的语言，在现在的技术领域内依然有强大的生命力，基本是流行的各种 NoSQL 上面都会有人来做一套 SQL-on-X。最近刚看到一个 2016 最佳编程语言榜单，SQL 依然能上榜。我想 SQL 在业界的应用还会延续很多很多年。第四是 Auto-failover / Self recovery / Survivability。Spanner 能够做到任何一个数据中心宕机，底层可以完全的 Auto-Failover，上层的业务甚至是完全无感知的。这个 Failover 的过程是完全不需要人工介入的。国内很多互联网公司也都在做这个，但是还没有那一家能做的特别好，比如光纤被挖断之后，大家发现支付工具无法支付了。除了业务不被中断这一好处之外，Auto-failover 还会极大地降低运维的成本，如 Google 这么牛的公司，在维护一百多个节点的 MySQL Sharding 的数据库的时候，都已经非常痛苦，宁可重新去写一个数据库，也不想去维护这个 datebase cluster。为了给业界提供 NewSQL 数据库，我们开发了 TiDB，提供如下特性： 无限水平扩展 分布式 ACID 事务 强一致 高可靠 提供 SQL 并且支持 MySQL 协议  TiDB 目前是世界上最受欢迎的开源 NewSQL 数据库之一，可能是国人发起和主要维护的最大也是 stars 数最多的开源的数据库项目。我们在开发 TiDB 过程中学到的东西 下面就和大家分享一下我们在开发这个项目过程中学到的一些东西。Always believe shit is about to happen 大家写程序的时候，总会做一些错误检查，处理异常情况，但是很多情况可能大家普遍不会想到，比如光纤被挖断、IDC 整个 down 掉，还有前几天的阿里云 IO 问题。这说明即使以这些顶级大厂的技术能力，依然不能保证基础设施不出问题。所以我们会特别强调 Auto-failover 的作用。我们的整个设计始终会考虑容灾。其中最重要的就是如何保存多副本，一份数据写足够多的副本并且使用健壮的副本分布策略，才能保证安全。Spanner 默认使用 5 副本，重要的数据使用 7 副本。传统的保存副本的方案是 Master-slave 模式。但是这并不是一个完美的方案。如果要在 master 和 slave 之间保持强一致，那么不但要担心效率问题 (速度取决于最慢的那个 slave)，还需要考虑各种容错。比如如果写 Slave 失败了，如何处理，是否要回滚 Master？所以依靠 master-slave 实现可靠复制对性能和复杂度都有比较大的挑战。而如果不要做到强一致，那么就面临 master 挂掉之后，slave 和 master 之间数据还没有同步完成，造成丢数据的问题。Multi-Paxos / Raft 是一个更好的选择，采用这种方案，多数节点写成功即可成功，在保证可靠性的同时，尽可能的提升了复制效率。Don&amp;rsquo;t rely on humans 面对容灾，我们需要尽可能的将工作自动化，因为人会累，会犯错，但是机器不会。我们希望能提供一套自动的方案，来支撑业务的需求，抵御各种异常情况。比如做 Scale 的时候，只需要点击几下即可增加新节点，然后系统自动将部分数据迁移到新节点。发生灾难时，系统能够自动下线 down 掉的节点，并将数据迁移到其他的节点。Talk is cheap, show me the tests 其实做数据库这么长时间，我认为最难的事情是测试。首先我们做的是一个支持 SQL 的数据库，想一下 SQL 的各种语法、操作符、函数，如何进行测试？同时我们是一个分布式的数据库，做测试就更难了。一直困扰我们的问题就是如何对我们的产品进行测试，比如一个 PR 上来后，如何检查逻辑是否正确，是否影响性能，是否支持容错？我们不断的探索和实践，有了一些自己的方案：首先是单元测试。我们的 Code Review 规范明确的写了，如果改了逻辑、bug，没有单测不予 Review。第二是集成测试。我们为了验证逻辑正确性，引入了大量的集成测试，其中一大块是 MySQL 源代码中的 test，因为我们支持 MySQL 语法和协议，所以可以直接拿过来用，省掉了我们大量的时间，很难想象没有这些测试，代码会变成什么样子。除此之外还有大量的 ORM 框架自带的 Test，我们也会运行。我们精选出一批集成测试 case，每次提交 PR 之前必需要跑过。当 PR merge 后，我们内部的 CI 会自动运行，跑更多更耗时的测试。为了测试分布式场景下的系统容错能力，我们也引入了一些工具，比如 Jepson/Namazu，可以进行错误注入。很多知名的分布式系统都被这些工具找出来过 bug，比如 etcd/zookeeper。&amp;ldquo;All problems in computer science can be solved by another level of indirection&amp;rdquo; 在一年半的时间里，TiDB 从零开始，成长为一个庞大的项目。首先我们是实现了一个内存的数据库，在一个简单的 memkv 基础上，做了一个小的 SQL 层，然后慢慢的替换 memkv 为持久化存储。我们将 SQL 和存储引擎之间的接口进行了抽象。后续的分布式存储引擎，也依赖于这个接口。这样就屏蔽了下层存储引擎的差别。无论是 TiDB 还是 TiKV 我们在开发过程中始终遵循良好的分层+抽象的原则，有效的降低了开发的复杂度和耦合度，另外对测试也有很大的帮助，我们可以更容易的 mock 某一层，做更精细的控制。抽象是对抗大系统复杂性的有效武器。Don&amp;rsquo;t try to teach your user, just follow them 想要做一款成功的数据库产品，就需要让用户觉得方便好用。在这一点上，中间件或者 Sharding 方案往往需要侵入用户代码、或者是修改用户逻辑，在做扩容的时候，也要消耗用户大量的精力。我们要求 TiDB 能够尽可能的减少用户工作，可以一行代码都不用修改就能从单机 MySQL 迁移到 TiDB 上。并且要尽可能的和行业标准贴近，减少用户的工作量。Make it right, and then make it fast 我们大概用了一个月的时间，做了一个可以用的数据库，在接下来的工作中，我们准备了整套测试框架，然后不断的完善功能，保证每次都是对的，与此同时，我们还会调整架构，最终做出一个还算不错的数据库，但是初期性能并不好，比如我们和第一家客户去聊，然后测试一下，发现一个简单的 SQL 跑了 600s，但是我们并不气馁，经过半个月的优化，我们将这个 SQL 优化到了 60ms。之所以能这么快优化到这么好，是因为我们有良好的架构，清晰的分层，完善的测试。所以这里并不是说我们不需要做优化，而是不要过早优化。这种大型系统，应该在早期关注架构的正确性以及弹性，为后面的优化留下足够的操作空间，保证每个模块可以单独的去优化。Embrace the community TiDB 是一个开源的数据库，我们从第一天起就坚定的走拥抱社区的路线，希望能够成为整个大数据生态的一部分。我们通过开源社区获得了大量高质量的第三方库，提高开发进度。与此同时，我们也向开源社区贡献了很多代码，比如 Etcd、Rocksdb、go-hbase、go-mysql 等。客户在进行数据库技术选型时，如果使用一些闭源的产品，那么就很容易被绑死在某一个厂商，或者是某一个云上。而使用 TiDB 不会有这方面的困扰，无论是迁移过来还是从 TiDB 迁移到其他的产品，都很容易，也不会被某个特定的云厂商绑死。如果一个公司符合如下任何一种情况，TiDB 或许是一个很好的解决方案： 项目选型阶段。为了快速开发，简化生产力和运维，使用 TiDB，再也不用对数据库进行分库分表或者选用数据库中间件，TiDB 帮你搞定所有底层的跨节点的分布式事务、聚合查询难点，开发人员专注于业务设计，维护人员运维非常容易。 目前使用 MySQL 且数据量很大，但是查询速度特别慢。TiDB 是 MySQL 兼容的，且在大数据量性能大大优于 MySQL。 有跨数据中心数据强一致性需求或者自动运维需求。  Q&amp;amp;A 提问：TiDB 是否和其他的数据库(比如 MySQL、Oracle) 进行过性能对比？ 申砾：我们内部用 Sysbench 做过对比，单套 TiDB 在写入能力上超过 MySQL，读取方面比 MySQL 略低。从整体上看，延迟会大于 MySQL，但是吞吐可以远高于 MySQL。我们正在不断提高性能，不久之后会对外发布性能测试结果。提问：TiDB 是如何支持跨机房容灾？ 申砾：TiDB 使用 Raft 做复制，PD 是整个集群的管理节点。PD 会自动根据存储节点的 IDC 位置指定副本存放策略，保证同一个 Raft Group 中的副本分布在多个机房。提问：TiDB 是否会出商业版本？ 申砾：是的，PingCAP 会提供 TiDB 的商业版。（含监控、部署、数据处理、调度及支持服务等）"},
		{"url": "https://pingcap.com/blog-cn/time-travel/",
		"title": "回到过去，找回遗失的珍宝 - TiDB 的历史读功能", 
		"content": " 数据作为业务的核心，关系着整个业务的生死，所以对于数据库来说，数据的安全性是放在首位的，从宏观角度来看，安全性不仅仅在于的数据库本身足够稳定不会主动的丢失数据，有的时候更是对业务本身甚至人为失误造成损失是否有足够且便捷的应对方案，例如在游戏行业中经常遇到的反作弊(作弊玩家回档)问题，对于金融业务的审计需求等等，如果在数据库层面上提供相关机制，会让业务开发的工作量和复杂度减少很多。传统的方案会定期备份数据，几天一次，甚至一天一次，把数据全量备份。当意外发生的时候，可以用来还原。但是用备份数据还原，代价还是非常大的，所有备份时间点后的数据都会丢失，你绝对不希望走到这一步。另外全量备份带来的存储和计算资源的额外开销，对于企业来说也是一笔不小的成本。可是这种事情是无法完全避免的，我们所有的人都会犯错。对于一个快速迭代的业务，应用的代码不可能做到全面充分的测试，很可能因为应用逻辑的 Bug 导致数据写错，或者被恶意用户找到 bug，当你发现问题时，可以立即把应用回滚到旧版本，但是写错的数据却会一直留在数据库里。出现这种问题的时候，你该怎么办？你只知道有些数据不对了，但是对的数据是什么，你不知道。如果能回到过去，找回之前的数据该多好。TiDB 针对这样的需求和场景支持历史版本的读取，所以可以将错误的版本之前的数据取出来，将损失降到最低。如何使用 TiDB 的历史读功能 使用这个功能非常简单，只需要执行一个 SET 语句：set @@tidb_snapshot = &amp;quot;2016-10-10 09:30:11.123&amp;quot;这个 session variable 的名字是 TiDB 里定义的 tidb_snapshot, 值是一个时间的字符串，精确到毫秒，执行了这个语句之后，之后这个客户端发出的所有读请求，读到的都是这个时间点看到的数据，这时是不能进行写操作的，因为历史是无法改变的。如果想退出历史读模式，读取最新数据，只需要再次执行一个 SET 语句：set @@tidb_snapshot = &amp;quot;&amp;quot;把 tidb_snapshot 设置成空字符串就可以了。即使在那个历史时间点后，发生了 Schema 更改也没有关系，TiDB 会使用当时的 Schema 执行 SQL 请求。TiDB 历史读功能和其他数据库的比较 这个功能 MySQL 并不支持，但是在其他的数据库里，比如 Oracle, PostgreSQL 里有类似的功能，叫做历史表(Temporial Table)，是一个SQL 标准。使用的方法是需要你用特殊的建表语法，额外创建一张历史表，历史表比原表多了两个系统定义的字段，代表有效时间，这多出的两个字段是系统维护的。当原表更新数据的时候，系统会把旧版本数据插入到历史表里，当你查询历史数据时，需要用一个特殊的语法指定历史时间，得到需要的结果。TiDB 和其他数据库的历史表功能相比，主要有以下两个优势：1，系统默认支持如果不是默认的行为，我们通常不会特意去建一张历史表，到真正需要用到的时候，你会发现历史表没有创建。2，使用方便不需要额外建一张表，不需要用特殊的语法查询。3，全局视角，而不是以表为单位TiDB 即使执行了 Drop Table, Drop Database 这样的操作，也可以读到旧的数据。TiDB 的历史读功能的实现 MVCC 为了方便理解，我们这里把实现原理做了一下简化，去掉了分布式事务相关的部分，TiDB 真正的实现会复杂一些。如果想了解完整的实现细节，请持续关注 TiDB，我们会在后期逐步完善事务模型部分的文档帮助大家了解。TiDB 的底层是 TiKV， TiKV 底层的存储引擎是 RocksDB, 存储的都是基本的 Key/Value Pair。在 SQL 层的一张表的一行，经过两次编码后，才得到最终的，存在 RocksDB 里的 key。第一次编码得到的 Key，包含了 table ID 和 record ID，得到这个 key 可以定位到这一行。第二次编码在第一次编码的 Key 的基础上，添加一个全局递增的时间戳，这个时间戳就是数据写入的时间。所有的 Key 都带着一个全局唯一的时间戳，也就意味着，新的写入不会覆盖旧的写入，即使是删除操作，写入也只是一个删除标记，并没有真正的删数据。同一行数据的多个版本是同时存在 RocksDB 里，按照时间顺序，连续的排列在一起。当一个读事务开始时，会从一个集群的时间分配器获取一个时间戳，这个时间点之前写入的数据，对这个事务是可见的，这个时间点之后写入的数据，对这个事务是不可见的，所以这个事务可以保证 Repeatable Read 这个隔离级别。TiDB 在向 TiKV 发起读请求时会带上这个时间戳，在 TiKV 拿到这个时间戳后，会比较这个时间戳和这一行的多个版本，找到不大于这时间戳的最大的版本，返回给 TiDB。以上就是 TiDB MVCC 的简化版的原理。所以 TiDB 其实原本就是用一个历史时间来读取数据的，只不过这个历史时间是系统在事务开始时自动获取的当前时间。使用 tidb_snapshot 这个 session variable，实际上是用一个用户指定的时间取代系统自动获取的时间去读取数据。你可能会有一个疑问，如果所有的版本都保留，数据占用的空间会不会无限膨胀？这里就需要介绍 TiDB 的垃圾回收机制了。TiDB 的垃圾回收 TiDB 会定期执行垃圾回收的任务，把过老的旧版本删掉，真正的从 RocksDB 里删掉，这样空间就不会无限膨胀了。那么多久以前的老的数据会被删掉呢？这个 GC 的过期时间，是通过配置一个参数来控制的，你可以配置成十分钟，一个小时，一天或永远不回收。所以，TiDB 的历史读功能是有限制的，只能读取到 GC 过期时间之后的数据，你可能会希望把时间设置的尽量久，但是这也是有代价的，GC 过期时间设置的越久，空间占用的会越大，读性能也会有所下降，如何配置这个时间，就要看业务的类型和需求了。如果数据非常重要，安全是首要考虑的因素，或数据更新变动很少，建议把 GC 过期时间设置的长一点。如果数据不那么重要，或数据更新很频繁，建议把 GC 过期时间设置的短一点。总结 TiDB 的历史读功能，把 TiDB 原生的读取机制开放出来，让用户用最简单的方式使用，我们希望这个功能，可以给 TiDB 的用户创造更多的价值。"},
		{"url": "https://pingcap.com/blog/2016-10-17-how-we-build-tidb/",
		"title": "How we build TiDB", 
		"content": " This is the speech Max Liu gave at Percona Live Open Source Database Conference 2016. The slides are here. Speaker introduction Why another database? What to build? How to design?  The principles or the philosophy  Disaster recovery Easy to use The community and ecosystem  Loose coupling – the logical architecture The alternatives  How to develop  The architecture TiKV core technologies  TiKV software stack Placement Driver Raft MVCC Transaction  TiDB core technologies  Mapping table data to Key-Value store Predicate push-down Schema changes   How to test? The future plan  Speaker introduction First, about me. I am an infrastructure engineer and I am also the CEO of PingCAP. Currently, my team and I are working on two open source projects: TiDB and TiKV. Ti is short for Titanium, which is a chemical element known for its corrosion resistance and it is widely used in high-end technologies.So today we will cover the following topics: Why another database? What kind of database we want to build? How to design such a database, including the principles, the architecture, and design decisions? How to develop such a database, including the architecture and the core technologies for TiKV and TiDB? How to test the database to ensure the quality and stability?  Back to the topWhy another database Before we start, let&amp;#39;s go back to the very beginning and ask yourself a question: Why another database. We all know that there are many databases, such as the traditional Relational database and NoSQL. So why another one? Relational databases like MySQL, Oracle, PostgreSQL, etcetera: they are very difficult to scale. Even though we have sharding solutions, YouTube/vitess, MySQL proxy, but none of them supports distributed transactions and cross-node join. NoSQL like HBase, MongoDB, and Cassandra: They scale well, but they don&amp;#39;t support SQL and consistent transactions. NewSQL, represented by Google Spanner and F1, which is as scalable as NoSQL systems and it maintains the ACID transactions. That&amp;#39;s exactly what we need. Inspired by Spanner and F1, we are making a NewSQL database. Of course, it&amp;#39;s open source.  Back to the topWhat to build? So we are building a NewSQL database with the following features: First of all, it supports SQL. We have been using SQL for decades and many of our applications are using SQL. We cannot just give it up. Second, it must be very easy to scale. You can easily increase the capacity or balance the load by adding more machines. Third, it supports ACID transaction, which is one of the key features of relational database. With a strong consistency guarantee, developers can write correct logic with less code. Last, it is highly available in case of machine failures or even downtime of an entire data center. And it can recover automatically.  In short, we want to build a distributed, consistent, scalable, SQL Database. We name it TiDB.Back to the topHow to design? Now we have a clear picture of what kind of database we want to build, the next step is how, how to design it, how to develop it and how to test it. In the next few slides, I am going to talk about how to design TiDB.In this section, I will introduce how we design TiDB, including the principles, the architecture and design decisions.Back to the topThe principles or the philosophy Before we design, we have several principles or philosophy in mind: TiDB must be user-oriented.  It must ensure that no data is ever lost and the system can automatically recover from machine failures or even downtime of the entire datacenters. It should be easy to use. It should be cross-platform and can run on any environment, no matter it&amp;#39;s on premise, cloud or container. As an open source project, we are dedicated to being an important part of the big community through our active engagement, contribution and collaboration.  We need TiDB to be easy to maintain so we chose the loose coupling approach. We design the database to be highly layered with a SQL layer and a Key-Value layer. If there is a bug in SQL layer, we can just update the SQL layer. The alternatives: Although our project is inspired by Google Spanner and F1, we are different from those projects. When we design TiDB and TiKV, we have our own practices and decisions in choosing different technologies.  Back to the topDisaster recovery The first and foremost design principle is to build a database where no data is lost. To ensure the safety of the data, we found that multiple replicas are just not enough and we still need to keep Binlog in both the SQL layer and the Key-Value layer. And of course, we must make sure that we always have a backup in case the entire cluster crashes.Back to the topEasy to use The second design principle is about the usability. After years of struggling among different workarounds and trade-offs, we are fully aware of the pain points of the users. So when it comes to us to design a database, we are going to make it easy to use and there should be no scary sharding keys, no partition, no explicit handmade local index or global index, and making scale transparent to the users.Back to the topCross-platform The database we are building also needs to be cross-platform. The database can run on the on premise devices. Here is a picture of TiDB running on a Raspberry Pi cluster with 20 nodes.It can also support the popular containers such as Docker. And we are making it work with Kubernetes. Of course, it can be run on any cloud platform, whether it&amp;#39;s public, private or hybrid.Back to the topThe community and ecosystem The next design principle is about the community and ecosystem. We want to stand on the shoulders of the giants instead of creating something new and scary. TiDB supports MySQL protocol and is compatible with most of the MySQL drivers (ODBC, JDBC) and SQL syntax, MySQL clients and ORM, and the following MySQL management tools and bench tools.Back to the topetcd etcd is a great project. In our Key-Value store, TiKV, which I will dive deep into later, we have been working with the etcd team very closely. We share the Raft implementation, and we do code reviews on Raft module for each other.Back to the topRocksDB RocksDB is also a great project. It&amp;#39;s mature, fast, tunable, and widely used in very large scale production environments, especially in facebook . TiKV uses RocksDB as it&amp;#39;s local storage. While we were testing it in our system, we found some bugs. The RocksDB team fixed those bugs very quickly.Back to the topNamazu A few months ago, we need a tool to simulate slow, unstable disk, and the team member found Namazu. But at that time, Namazu didn&amp;#39;t support hooking fsync. When the team member raised this request to their team, they responded immediately and implement the feature in just a few hours and they are very open to implement other features as well. We are deeply impressed by their responsiveness and their efficiency.Back to the topRust community The Rust community is amazing. Besides the good developing experience of using Rust, we also build the Prometheus driver in Rust to collect the metrics.We are so glad to be a part of this great family. So many thanks to the Rust team, gRPC, Prometheus and Grafana.Back to the topSpark connector We are using the Spark connector in TiDB. TiDB is great for small or medium queries and Spark is better for complex queries with lots of data. We believe we can learn a lot from the Spark community too, and of course we would like to contribute as much as possible.So overall, we&amp;#39;d like to be a part of the big open source community and would like to engage, contribute and collaborate to build great things together.Back to the topLoose coupling – the logical architecture This diagram shows the logical architecture of the database.As I mentioned earlier about our design principle, we are adopting the loose coupling approach. From the diagram, we can see that it is highly-layered. We have TiDB to work as the MySQL server, and TiKV …"},
		{"url": "https://pingcap.com/weekly/2016-10-17-tidb-weekly/",
		"title": "Weekly update (October 01 ~ October 16, 2016)", 
		"content": " Weekly update (October 01 ~ October 16, 2016) Last week, we landed 27 PRs in the TiDB repositories and 32 PRs in the TiKV repositories.Notable changes to TiDB  Support projection elimination so that the executor can run faster when it is not necessary to have a projection layer. Write DDL binlog to file. Convert sort and limit to top-n in the query planning phrase. Support reading history data even if the schema changes.  Add comments for the server package and the plan package. Add metrics for GC configuration. Verify the data for utf-8 columns. Fix bugs.  Notable changes to TiKV  Compact the lock column family periodically. Add random latency filter to simulate the transport test. Improve the datum test coverage. Cache the Raft log term to avoid calling rocksdb::get every time. Check the store ID before sending to raftstore and refresh expired store address for the resolve process to fix issue 1153. Add new regions&amp;rsquo; meta in StaleEpoch error to fix issue 974. Coprocessor supports timezone for timestamp comparison.  Notable changes to Placement Driver  Ignore balancing the peer with on-going operation to fix TiKV issue 1084.  "},
		{"url": "https://pingcap.com/meetup/meetup-2016-10-15/",
		"title": "PingCAP 第 25 期 NewSQL Meetup", 
		"content": " PingCAP 第 25 期 NewSQL Meetup 2016-10-15 武毅&amp;amp;张金鹏 PingCAP PingCAPPingCAP ![]() 微信号pingcap2015功能介绍PingCAP 专注于新型分布式数据库的研发，是知名开源数据库 TiDB (GitHub 总计10000+ stars ) 背后的团队，总部设在北京，是国内第一家开源的新型分布式关系型数据库公司、国内领先的大数据技术和解决方案提供商。NewSQL Meetup今天是 PingCAP 第 25 期 Meetup，顶着帝都的大雾霾，依然来了很多小伙伴。这一次我们有换新场地噢，但不变的是分享内容依然满满干货。本周的主题分别是百分点集团高级架构师武毅分享的《分布式数据处理在个性化系统的应用》以及张金鹏分享的《TiKV 性能优化》。▌ ****Topic 1：分布式数据处理在个性化系统的应用Lecture：武毅，现任百分点集团高级架构师，负责大数据平台基础架构的设计与研发，曾参与个性化推荐系统等多个大型系统的设计和开发。Linux 爱好者，活跃于 GitHub，Ubuntu 等社区，重点关注分布式技术，平台技术。Content：相信大家也都在各自的领域用到过不同的分布式存储／计算开源工具，本周我们分享了一些在运营个性化系统时使用分布式存储／计算工具遇到的坑和经验。▌ ****Topic 2：TiKV 性能优化Content：RocksDB 的 Column Families 之间会共享 WAL，但是又有各自的 memtables 和 sst files，共享 WAL 使得实现跨 CF 的 atomic 操作变成可能，不同 CF 的 memtables 和 sst files 是分离开的，这样我们可以将不同类型的数据分别存放在不同的 CF 内，根据数据的性质给 CF 定制不同配置，使数据的写入和访问达到最佳状态。在目前 TiKV 中，读命令只能发给 leader，以防读取到旧的状态，在之前的版本中通过走一次 Raft 来确定当前节点是否是 leader，引入 leader lease 之后，命令发送到在 lease 内的leader 上时，不需要再走一次 Raft 了，可以直接读取本地数据。当 RocksDB tombstone keys 太多的时候 seek 操作会非常慢，可以根据情况使用 iterator 的 upper bound 功能或者使用 RocksDB 的 singledelete 来解决这个问题.最后我们给出了一些与 MySQL 的性能对比数据。可以看出，我们在写入性能、聚合操作和一些复杂查询上已经完全超过 MySQL 了。✏️分享两张新场地的图片给你们！看～是不是宽敞又明亮 😊PingCAP Meetup"},
		{"url": "https://pingcap.com/blog-cn/how-do-we-build-tidb/",
		"title": "How do we build TiDB", 
		"content": " 首先我们聊聊 Database 的历史，在已经有这么多种数据库的背景下我们为什么要创建另外一个数据库；以及说一下现在方案遇到的困境，说一下 Google Spanner 和 F1，TiKV 和 TiDB，说一下架构的事情，在这里我们会重点聊一下 TiKV。因为我们产品的很多特性是 TiKV 提供的，比如说跨数据中心的复制，Transaction，auto-scale。再聊一下为什么 TiKV 用 Raft 能实现所有这些重要的特性，以及 scale，MVCC 和事务模型。东西非常多，我今天不太可能把里面的技术细节都描述得特别细，因为几乎每一个话题都可以找到一篇或者是多篇论文。但讲完之后我还在这边，所以详细的技术问题大家可以单独来找我聊。后面再说一下我们现在遇到的窘境，就是大家常规遇到的分布式方案有哪些问题，比如 MySQL Sharding。我们创建了无数 MySQL Proxy，比如官方的 MySQL proxy，Youtube 的 Vitess，淘宝的 Cobar、TDDL,以及基于 Cobar 的 MyCAT，金山的 Kingshard，360 的 Atlas，京东的 JProxy，我在豌豆荚也写了一个。可以说，随便一个大公司都会造一个MySQL Sharding的方案。为什么我们要创建另外一个数据库？ 昨天晚上我还跟一个同学聊到，基于 MySQL 的方案它的天花板在哪里，它的天花板特别明显。有一个思路是能不能通过 MySQL 的 server 把 InnoDB 变成一个分布式数据库，听起来这个方案很完美，但是很快就会遇到天花板。因为 MySQL 生成的执行计划是个单机的，它认为整个计划的 cost 也是单机的，我读取一行和读取下一行之间的开销是很小的，比如迭代 next row 可以立刻拿到下一行。实际上在一个分布式系统里面，这是不一定的。另外，你把数据都拿回来计算这个太慢了，很多时候我们需要把我们的 expression 或者计算过程等等运算推下去，向上返回一个最终的计算结果，这个一定要用分布式的 plan，前面控制执行计划的节点，它必须要理解下面是分布式的东西，才能生成最好的 plan，这样才能实现最高的执行效率。比如说你做一个 sum，你是一条条拿回来加，还是让一堆机器一起算，最后给我一个结果。 例如我有 100 亿条数据分布在 10 台机器上，并行在这 10 台 机器我可能只拿到 10 个结果，如果把所有的数据每一条都拿回来，这就太慢了，完全丧失了分布式的价值。聊到 MySQL 想实现分布式，另外一个实现分布式的方案是什么，就是 Proxy。但是 Proxy 本身的天花板在那里，就是它不支持分布式的 transaction，它不支持跨节点的 join，它无法理解复杂的 plan，一个复杂的 plan 打到 Proxy 上面，Proxy 就傻了，我到底应该往哪一个节点上转发呢，如果我涉及到 subquery sql 怎么办？所以这个天花板是瞬间会到，在传统模型下面的修改，很快会达不到我们的要求。另外一个很重要的是，MySQL 支持的复制方式是半同步或者是异步，但是半同步可以降级成异步，也就是说任何时候数据出了问题你不敢切换，因为有可能是异步复制，有一部分数据还没有同步过来，这时候切换数据就不一致了。前一阵子出现过某公司突然不能支付了这种事件，今年有很多这种类似的 case，所以微博上大家都在说“说好的异地多活呢？”……为什么传统的方案在这上面解决起来特别的困难，天花板马上到了，基本上不可能解决这个问题。另外是多数据中心的复制和数据中心的容灾，MySQL 在这上面是做不好的。 在前面三十年基本上是关系数据库的时代，那个时代创建了很多伟大的公司，比如说 IBM、Oracle、微软也有自己的数据库，早期还有一个公司叫 Sybase，有一部分特别老的程序员同学在当年的教程里面还可以找到这些东西，但是现在基本上看不到了。 另外是 NoSQL。NoSQL 也是一度非常火，像 Cassandra，MongoDB 等等，这些都属于在互联网快速发展的时候创建这些能够 scale 的方案，但 Redis scale 出来比较晚，所以很多时候大家把 Redis 当成一个 Cache，现在慢慢大家把它当成存储不那么重要的数据的数据库。因为它有了 scale 支持以后，大家会把更多的数据放在里面。 然后到了 2015，严格来讲是到 2014 年到 2015 年之间，Raft 论文发表以后，真正的 NewSQL 的理论基础终于完成了。我觉得 NewSQL 这个理论基础，最重要的划时代的几篇论文，一个是谷歌的 Spanner，是在 2013 年初发布的，再就是 Raft 是在 2014 年上半年发布的。这几篇相当于打下了分布式数据库 NewSQL 的理论基础，这个模型是非常重要的，如果没有模型在上面是堆不起来东西的。说到现在，大家可能对于模型还是可以理解的，但是对于它的实现难度很难想象。前面我大概提到了我们为什么需要另外一个数据库，说到 Scalability 数据的伸缩，然后我们讲到需要 SQL，比如你给我一个纯粹的 key-velue 系统的 API，比如我要查找年龄在 10 岁到 20 岁之间的 email 要满足一个什么要求的。如果只有 KV 的 API 这是会写死人的，要写很多代码，但是实际上用 SQL 写一句话就可以了，而且 SQL 的优化器对整个数据的分布是知道的，它可以很快理解你这个 SQL，然后会得到一个最优的 plan，他得到这个最优的 plan 基本上等价于一个真正理解 KV 每一步操作的人写出来的程序。通常情况下，SQL 的优化器是为了更加了解或者做出更好的选择。另外一个就是 ACID 的事务，这是传统数据库必须要提供的基础。以前你不提供 ACID 就不能叫数据库，但是近些年大家写一个内存的 map 也可以叫自己是数据库。大家写一个 append-only 文件，我们也可以叫只读数据库，数据库的概念比以前极大的泛化了。另外就是高可用和自动恢复，他们的概念是什么呢？有些人会有一些误解，因为今天还有朋友在现场问到，出了故障，比如说一个机房挂掉以后我应该怎么做切换，怎么操作。这个实际上相当于还是上一代的概念，还需要人去干预，这种不算是高可用。未来的高可用一定是系统出了问题马上可以自动恢复，马上可以变成可用。比如说一个机房挂掉了，十秒钟不能支付，十秒钟之后系统自动恢复了变得可以支付，即使这个数据中心再也不起来我整个系统仍然是可以支付的。Auto-Failover 的重要性就在这里。大家不希望在睡觉的时候被一个报警给拉起来，我相信大家以后具备这样一个能力，5 分钟以内的报警不用理会，挂掉一个机房，又挂掉一个机房，这种连续报警才会理。我们内部开玩笑说，希望大家都能睡个好觉，很重要的事情就是这个。说完应用层的事情，现在很有很多业务，在应用层自己去分片，比如说我按照 user ID 在代码里面分片，还有一部分是更高级一点我会用到一致性哈希。问题在于它的复杂度，到一定程度之后我自动的分库，自动的分表，我觉得下一代数据库是不需要理解这些东西的，不需要了解什么叫做分库，不需要了解什么叫做分表，因为系统是全部自动搞定的。同时复杂度，如果一个应用不支持事务，那么在应用层去做，通常的做法是引入一个外部队列，引入大量的程序机制和状态转换，A 状态的时候允许转换到 B 状态，B 状态允许转换到 C 状态。举一个简单的例子，比如说在京东上买东西，先下订单，支付状态之后这个商品才能出库，如果不是支付状态一定不能出库，每一步都有严格的流程。Google Spanner / F1 说一下 Google 的 Spanner 和 F1，这是我非常喜欢的论文，也是我最近几年看过很多遍的论文。Google Spanner 已经强大到什么程度呢？Google Spanner 是全球分布的数据库，在国内目前普遍做法叫做同城两地三中心，它们的差别是什么呢？以 Google 的数据来讲，谷歌比较高的级别是他们有 7 个副本，通常是美国保存 3 个副本，再在另外 2 个国家可以保存 2 个副本，这样的好处是万一美国两个数据中心出了问题，那整个系统还能继续可用，这个概念就是比如美国 3 个副本全挂了，整个数据都还在，这个数据安全级别比很多国家的安全级别还要高，这是 Google 目前做到的，这是全球分布的好处。现在国内主流的做法是两地三中心，但现在基本上都不能自动切换。大家可以看到很多号称实现了两地三中心或者异地多活，但是一出现问题都说不好意思这段时间我不能提供服务了。大家无数次的见到这种 case，我就不列举了。Spanner 现在也提供一部分 SQL 特性。在以前，大部分 SQL 特性是在 F1 里面提供的，现在 Spanner 也在逐步丰富它的功能，Google 是全球第一个做到这个规模或者是做到这个级别的数据库。事务支持里面 Google 有点黑科技（其实也没有那么黑），就是它有 GPS 时钟和原子钟。大家知道在分布式系统里面，比如说数千台机器，两个事务启动先后顺序，这个顺序怎么界定(事务外部一致性)。这个时候 Google 内部使用了 GPS 时钟和原子钟，正常情况下它会使用一个 GPS 时钟的一个集群，就是说我拿的一个时间戳，并不是从一个 GPS 上来拿的时间戳，因为大家知道所有的硬件都会有误差。如果这时候我从一个上拿到的 GPS 本身有点问题，那么你拿到的这个时钟是不精确的。而 Google 它实际上是在一批 GPS 时钟上去拿了能够满足 majority 的精度，再用时间的算法，得到一个比较精确的时间。同时大家知道 GPS 也不太安全，因为它是美国军方的，对于 Google 来讲要实现比国家安全级别更高的数据库，而 GPS 是可能受到干扰的，因为 GPS 信号是可以调整的，这在军事用途上面很典型的，大家知道导弹的制导需要依赖 GPS，如果调整了 GPS 精度，那么导弹精度就废了。所以他们还用原子钟去校正 GPS，如果 GPS 突然跳跃了，原子钟上是可以检测到 GPS 跳跃的，这部分相对有一点黑科技，但是从原理上来讲还是比较简单，比较好理解的。最开始它 Spanner 最大的用户就是 Google 的 Adwords，这是 Google 最赚钱的业务，Google 就是靠广告生存的，我们一直觉得 Google 是科技公司，但是他的钱是从广告那来的，所以一定程度来讲 Google 是一个广告公司。Google 内部的方向先有了 Big table ，然后有了 MegaStore ，MegaStore 的下一代是 Spanner ，F1 是在 Spanner 上面构建的。TiDB and TiKV TiKV 和 TiDB 基本上对应 Google Spanner 和 Google F1，用 Open Source 方式重建。目前这两个项目都开放在 GitHub 上面，两个项目都比较火爆，TiDB 是更早一点开源的， 目前 TiDB 在 GitHub 上 有 4300 多个 Star，每天都在增长。 另外，对于现在的社会来讲，我们觉得 Infrastructure 领域闭源的东西是没有任何生存机会的。没有任何一家公司，愿意把自己的身家性命压在一个闭源的项目上。举一个很典型的例子，在美国有一个数据库叫 FoundationDB，去年被苹果收购了。 FoundationDB 之前和用户签的合约都是一年的合约。比如说，我给你服务周期是一年，现在我被另外一个公司收购了，我今年服务到期之后，我是满足合约的。但是其他公司再也不能找它服务了，因为它现在不叫 FoundationDB 了，它叫 Apple了，你不能找 Apple 给你提供一个 enterprise service。 TiDB 和 TiKV 为什么是两个项目，因为它和 Google 的内部架构对比差不多是这样的：TiKV 对应的是 Spanner，TiDB 对应的是 F1 。F1 里面更强调上层的分布式的 SQL 层到底怎么做，分布式的 Plan 应该怎么做，分布式的 Plan 应该怎么去做优化。同时 TiDB 有一点做的比较好的是，它兼容了 MySQL 协议，当你出现了一个新型的数据库的时候，用户使用它是有成本的。大家都知道作为开发很讨厌的一个事情就是，我要每个语言都写一个 Driver，比如说你要支持 C++，你要支持 Java，你要支持 Go 等等，这个太累了，而且用户还得改他的程序，所以我们选择了一个更加好的东西兼容 MySQL 协议，让用户可以不用改。一会我会用一个视频来演示一下，为什么一行代码不改就可以用，用户就能体会到 TiDB 带来的所有的好处。 这个图实际上是整个协议栈或者是整个软件栈的实现。大家可以看到整个系统是高度分层的，从最底下开始是 RocksDB ，然后再上面用 Raft 构建一层可以被复制的 RocksDB，在这一层的时候它还没有 Transaction，但是整个系统现在的状态是所有写入的数据一定要保证它复制到了足够多的副本。也就是说只要我写进来的数据一定有足够多的副本去 cover 它，这样才比较安全，在一个比较安全的 Key-value store 上面， 再去构建它的多版本，再去构建它的分布式事务，然后在分布式事务构建完成之后，就可以轻松的加上 SQL 层，再轻松的加上 MySQL 协议的支持。然后，这两天我比较好奇，自己写了 MongoDB 协议的支持，然后我们可以用 MongoDB 的客户端来玩，就是说协议这一层是高度可插拔的。TiDB 上可以在上面构建一个 MongoDB 的协议，相当于这个是构建一个 SQL 的协议，可以构建一个 NoSQL 的协议。这一点主要是用来验证 TiKV 在模型上面的支持能力。 这是整个 TiKV 的架构图，从这个看来，整个集群里面有很多 Node，比如这里画了四个 Node，分别对应了四个机器。每一个 Node 上可以有多个 Store，每个 Store 里面又会有很多小的 Region，就是说一小片数据，就是一个 Region 。从全局来看所有的数据被划分成很多小片，每个小片默认配置是 64M，它已经足够小，可以很轻松的从一个节点移到另外一个节点，Region 1 有三个副本，它分别在 Node1、Node 2 和 Node4 上面， 类似的Region 2，Region 3 也是有三个副本。每个 Region 的所有副本组成一个 Raft Group, 整个系统可以看到很多这样的 Raft groups。Raft 细节我不展开了，大家有兴趣可以找我私聊或者看一下相应的资料。因为整个系统里面我们可以看到上一张图里面有很多 Raft group 给我们，不同 Raft group 之间的通讯都是有开销的。所以我们有一个类似于 MySQL 的 group commit 机制 ，你发消息的时候实际上可以 share 同一个 connection ， 然后 pipeline + batch 发送, 很大程度上可以省掉大量 syscall 的开销。另外，其实在一定程度上后面我们在支持压缩的时候，也有非常大的帮助，就是可以减少数据的传输。对于整个系统而言，可能有数百万的 Region，它的大小可以调整，比如说 64M、128M、256M，这个实际上依赖于整个系统里面当前的状况。比如说我们曾经在有一个用户的机房里面做过测试，这个测试有一个香港机房和新加坡的机房。结果我们在做复制的时候，新加坡的机房大于 256M 就复制不过去，因为机房很不稳定，必须要保证数据切的足够小，这样才能复制过去。如果一个 Region 太大以后我们会自动做 SPLIT，这是非常好玩的过程，有点像细胞的分裂。然后 TiKV 的 Raft 实现，是从 etcd 里面 port 过来的，为什么要从 etcd 里面 port 过来呢？首先 TiKV 的 Raft 实现是用 Rust 写的。作为第一个做到生产级别的 Raft 实现，所以我们从 etcd 里面把它用 Go 语言写的 port 到这边。 这个是 Raft 官网上面列出来的 TiKV 在里面的状态，大家可以看到 TiKV 把所有 Raft 的 feature 都实现了。 比如说 Leader Election、Membership Changes，这个是非常重要的，整个系统的 scale 过程高度依赖 Membership Changes，后面我用一个图来讲这个过程。后面这个是 Log Compaction，这个用户不太关心。 这是很典型的细胞分裂的图，实际上 Region 的分裂过程和这个是类似的。我们看一下扩容是怎么做的。 比如说以现在的系统假设，我们刚开始说只有三个节点，有 Region1 分别是在 1 、2、4，我用虚线连接起来代表它是 一个 Raft group ，大家可以看到整个系统里面有三个 Raft group，在每一个 Node 上面数据的分布是比较均匀的，在这个假设每一个 Region 是 64M ，相当于只有一个 Node 上面负载比其他的稍微大一点点。这是一个在线的视频。默认的时候，我们都是推荐 3 个副本或者 5 个副本的配置。Raft 本身有一个特点，如果一个 leader down 掉之后，其它的节点会选一个新的 leader，那么这个新的 leader 会把它还没有 commit 但已经 reply 过去的 log 做一个 commit ，然后会再做 apply，这个有点偏 Raft 协议，细节我不讲了。复制数据的小的 Region，它实际上是跨多个数据中心做的复制。这里面最重要的一点是永远不丢失数据，无论如何我保证我的复制一定是复制到 majority，任何时候我只要对外提供服务，允许外面写入数据一定要复制到 majority。很重要的一点就是恢复的过程一定要是自动化的，我前面已经强调过，如果不能自动化恢复，那么中间的宕机时间或者对外不可服务的时间，便不是由整个系统决定的，这是相对回到了几十年前的状态。MVCC MVCC 我稍微仔细讲一下这一块。MVCC 的好处，它很好支持 Lock-free 的 snapshot read ，一会儿我有一个图会展示 MVCC 是怎么做的。isolation level 就不讲了，MySQL 里面的级别是可以调的，我们的 TiKV 有 SI，还有 SI+lock，默认是支持 SI 的这种隔离级别，然后你写一个 select for update 语句，这个会自动的调整到 SI 加上 lock 这个隔离级别。这个隔离级别基本上和 SSI 是一致的。还有一个就是 GC 的问题，如果你的系统里面的数据产生了很多版本，你需要把这个比较老的数据给 GC 掉，比如说正常情况下我们是不删除数据的， 你写入一行，然后再写入一行，不断去 update 同一行的时候，每一次 update 会产生新的版本，新的版本就会在系统里存在，所以我们需要一个 GC 的模块把比较老的数据给 GC 掉，实际上这个 GC 不是 Go 里面的GC，不是 Java 的 GC，而是数据的 GC。 这是一个数据版本，大家可以看到我们的数据分成两块，一个是 meta， …"},
		{"url": "https://pingcap.com/weekly/2016-09-30-tidb-weekly/",
		"title": "Weekly update (September 26 ~ September 30, 2016)", 
		"content": " Weekly update (September 26 ~ September 30, 2016) Last week, we landed 17 PRs in the TiDB repositories and 13 PRs in the TiKV repositories.Notable changes to TiDB  Make the GC alive time configurable so that users can keep the deleted data as long as they want. Add the metrics for all kinds of statements to provide more information for insights. Improve the kv package test coverage. Record the processed row count during DDL so that users can track the progress of DDL. Consider the limit clause in the cost based optimizer (CBO) framework. Add metrics for kv errors. Make truncate statement a DDL to convert the truncate operation into a drop table process followed by a create table process. Then TiDB can delete the truncated data with a background worker.  Notable changes to TiKV  Add a metric for Raft vote to monitor server&amp;rsquo;s stability. Improve the test coverage for scheduler, Raft. Check the stale snapshot to fix 1084. Reuse iterator to speed up scan. Remove delete_file_in_range to fix 1121.  Notable changes to Placement Driver (PD)  Add more metrics to monitor the server.  "},
		{"url": "https://pingcap.com/weekly/2016-09-26-tidb-weekly/",
		"title": "Weekly update (September 19 ~ September 25, 2016)", 
		"content": " Weekly update (September 19 ~ September 25, 2016) Last week, we landed 20 PRs in the TiDB repositories and 24 PRs in the TiKV repositories.Notable changes to TiDB  Support DML binlog. Support reading the history data. Add more metrics to distsql, DDL, and store. Replace the vendor tool with glide. Improve test coverage. Code cleanup. Improve the test code by splitting a big test file into smaller files.  Notable changes to TiKV  Port the read index feature from etcd. Add the upper bound mod support to the iterator to improve the seek performance. Support the recovery mode for RocksDB. Support adding/removing column families dynamically.  Notable changes to Placement Driver (PD)  Remove the watch leader mechanism for the clients becausse the PD server can proxy requests to the leader. Add the GetRegionByID command.  "},
		{"url": "https://pingcap.com/meetup/meetup-2016-09-24/",
		"title": "PingCAP 第 24 期 NewSQL Meetup", 
		"content": " PingCAP 第 24 期 NewSQL Meetup 2016-09-24 杜川&amp;amp;杨哲 PingCAP PingCAPPingCAP ![]() 微信号pingcap2015功能介绍PingCAP 专注于新型分布式数据库的研发，是知名开源数据库 TiDB (GitHub 总计10000+ stars ) 背后的团队，总部设在北京，是国内第一家开源的新型分布式关系型数据库公司、国内领先的大数据技术和解决方案提供商。NewSQL Meetup今天是 PingCAP 第 24 期 Meetup，主题是阿里云 ODPS 研发工程师杜川分享的《LLVM 简介及其在大规模 OLAP 中的应用》以及来自小米云平台的杨哲分享的《阻塞访问数据库的相关问题》。▌ ****Topic 1：LLVM 简介及其在大规模 OLAP 中的应用Lecture：杜川，阿里云 ODPS 研发工程师，分布式数据库爱好者，重点关注 SQL 运行时优化以及 Code Generation 技术。Content：LLVM 是一个开源的编译器框架及生态链，已在工业界得到广泛的应用（著名的 Clang 编译器就是基于LLVM实现的）。因其前后端分离，模块化等优势，近年来被引入数据库领域，作为 JIT Code Generation 的工具，并吸引了越来越多的关注。本次分享介绍了 LLVM，及其在大规模 OLAP 中的应用。▌ ****Topic 2：阻塞访问数据库的相关问题Lecture：杨哲，id 杨肉或 yangzhe1991，现就职于小米云平台存储组。曾就职于网易有道、豌豆荚任资深工程师等职位。主要研究分布式数据库，在小米、有道、豌豆荚分别负责 HBase、Cassandra 和 Codis 的开发与维护。Content：分享了关于数据库若干问题的一些想法。PingCAP Meetup"},
		{"url": "https://pingcap.com/weekly/2016-09-19-tidb-weekly/",
		"title": "Weekly update (September 12 ~ September 18, 2016)", 
		"content": " Weekly update (September 12 ~ September 18, 2016) Last week, we landed 18 PRs in the TiDB repositories and 26 PRs in the TiKV repositories.Notable changes to TiDB  Add the Prometheus metrics and support push. Support the streaming aggregation operator. Rename xapi to distsql to improve readability. Add git hash to the TiDB server status API. Improve test coverage in the abstract syntax tree (AST). Enable the division operator for the distributed SQL statements. Fix several bugs.  Notable changes to TiKV  Remove the stale peers that are out of region to fix 804. Ignore the tombstone stores when resolving the store address. Discard the droppable messages when channel is full to fix 1028. Capture the signal TERM/INT to close server gracefully. Capture the signal USR1 to log metrics. Support destroying regions asynchronously. Support the pushing metrics to Prometheus Push Gateway.  Notable changes to Placement Driver  Add the API document. Support the pushing metrics to Prometheus Push Gateway.  "},
		{"url": "https://pingcap.com/weekly/2016-09-12-tidb-weekly/",
		"title": "Weekly update (September 05 ~ September 11, 2016)", 
		"content": " Weekly update (September 05 ~ September 11, 2016) Last week, we landed 20 PRs in the TiDB repositories and 32 PRs in the TiKV repositories..Notable changes to TiDB  Support mydumper Use MySQL standard error code in the DDL execution results Use heap sort operator to handle the statements with Limit and Orderby Add support for the CLIENT_CONNECT_ATTRS Add the constant propagation support to the SQL optimizer Optimize the index scan executor to improve the performance Optimize the distributed SQL API protocol to improve the performance Fix several bugs.  Notable changes to TiKV  Add the divide operation support to Coprocessor. Switch to the prometheus metrics from the statsd metrics. Abort applying the snapshot if it conflicts with a new one to fix 1014. Ensure that the data from the last write before the GC Safe-Point time can&amp;rsquo;t be removed to fix 1021. Add the document for Scheduler. Calculate the used space correctly. Stop GC scan if there are no keys left to fix the endless-loop problem.  Notable changes to Placement Driver  Support store removing gracefully. Add the Admin API. Reduce the default min-capacity-used-ratio value for an earlier auto-balance.  "},
		{"url": "https://pingcap.com/blog-cn/talk-tidb-pattern/",
		"title": "演讲实录|黄东旭：分布式数据库模式与反模式", 
		"content": " 我叫黄东旭，是 PingCAP 的联合创始人兼 CTO，也是本场论坛的主持人。我原来在 MSRA，后来到了网易、豌豆荚。跟在座的大部分数据分析师不太一样的是，我是一个数据库开发，虽然是 CTO，但是还在写代码。同时，我也是一些用的比较广泛的分布式的开源软件的作者。比如说我们做的 TiDB、TiKV 这些大型的分布式关系型数据库的项目。我们现在正在做一个 OLTP 的数据库，主要 focus 在大数据的关系型数据库的存储和可扩展性，还有关系的模型，以及在线交易型数据库上的应用。所以，今天整个数据库的模式和反模式，我都会围绕着如何在一个海量的并发，海量的数据存储的容量上，去做在线实时的数据库业务的一些模式来讲。并从数据库的开发者角度，来为大家分享怎样写出更加适合数据库的一些程序。基础软件的发展趋势 一开始我先简单介绍一下，现在我认为的一些基础软件上的发展趋势。开源 第一点，开源是一个非常大的趋势。大家可以看到一些比较著名的基础软件，基本都是开源的，比如 Docker，比如 k8s。甚至在互联网公司里面用的非常多的软件，像 MySQL、Hadoop 等这种新一代的大数据处理的数据库等基础软件，也大多是开源的。其实这背后的逻辑非常简单：在未来其实你很难去将你所有的技术软件都用闭源, 因为开源会慢慢组成一个生态，而并不是被某一个公司绑定住。比如国家经常说去 IOE，为什么？很大的原因就是基本上你的业务是被基础软件绑死的，这个其实是不太好的一个事情。而且现在跟过去二十年前不一样，无论是开源软件的质量，还是社区的迭代速度，都已经是今非昔比，所以基本上开源再也不是低质低量的代名词，在互联网公司已经被验证很多次了。分布式 第二，分布式会渐渐成为主流的趋势。这是为什么？这个其实也很好理解，因为随着数据量越来越大，大家可以看到，随着现在的硬件发展，我感觉摩尔定律有渐渐失效的趋势。所以单个节点的计算资源或者计算能力，它的增长速度是远比数据的增长速度要慢的。在这种情况下，你要完成业务，存储数据，要应对这么大的并发，只有一种办法就是横向的扩展。横向的扩展，分布式基本是唯一的出路。scale-up 和 scale-out 这两个选择其实我是坚定的站在 scale-out 这边。当然传统的关系数据库都会说我现在用的 Oracle，IBM DB2，他们现在还是在走 scale-up 的路线，但是未来我觉得 scale-out 的方向会渐渐成为主流。 碎片化碎片化 第三，就是整个基础软件碎片化。现在看上去会越来越严重。但是回想在十年前、二十年前，大家在写程序的时候，我上面一层业务，下面一层数据库。但是现在你会发现，随着可以给你选择的东西越来越多，可以给你在开源社区里面能用到的组件越来越多，业务越来越复杂，你会发现，像缓存有一个单独的软件，比如 redis，队列又有很多可以选择的，比如说 zeromq, rabbitmq, celery 各种各样的队列；数据库有 NoSQL、HBase，关系型数据库有 MySQL 、PG 等各种各样的基础软件都可以选。但是就没有一个非常好东西能够完全解决自己的问题。所以这是一个碎片化的现状。微服务 第四，是微服务的模式兴起。其实这个也是最近两年在软件架构领域非常火的一个概念。这个概念的背后思想，其实也是跟当年的 SOA 是一脉相承的。就是说一个大的软件项目，其实是非常难去 handle 复杂度的，当你业务变得越来越大以后，维护成本和开发成本会随着项目的代码量呈指数级别上升的。所以现在比较流行的就是，把各个业务之间拆的非常细，然后互相之间尽量做到无状态，整个系统的复杂度可以控制，是由很多比较简单的小的组件组合在一起，来对外提供服务的。这个服务看上去非常美妙，一会儿会说有什么问题。最典型的问题就是，当你的上层业务都拆成无状态的小服务以后，你会发现原有的逻辑需要有状态的存储服务的时候你是没法拆的。我所有的业务都分成一小块，每一小块都是自己的数据库或者数据存储。比如说一个简单的 case，我每一个小部分都需要依赖同一个用户信息服务，这个信息服务会变成整个系统的一个状态集中的点，如果这个点没有办法做弹性扩展或者容量扩展的话，就会变成整个系统很致命的单点。所以现在整个基础软件的现状，特别在互联网行业是非常典型的几个大的趋势。我觉得大概传统行业跟互联网行业整合，应该在三到五年，这么一个时间。所以互联网行业遇到的今天，可能就是传统行业，或者其他的行业会遇到的明天。所以，通过现在整个互联网里面，在数据存储、数据架构方面的一些比较新的思想，我们就能知道如何去做这个程序的设计，应对明天数据的量级。现有存储系统的痛点 其实今天主要的内容是讲存储系统，存储系统现在有哪些痛点？其实我觉得在座的各位应该也都能切身的体会到。弹性扩展 首先，大数据量级下你如何实现弹性扩展？因为我们今天主要讨论的是 OLTP ，是在线的存储服务，并不是离线分析的服务。所以在线的存储服务，它其实要做到的可用性、一致性，是要比离线的分析业务强得多的。但是在这种情况下，你们怎样做到业务无感知的弹性扩展，你的数据怎么很好的满足现有的高并发、大吞吐，还有数据容量的方案。可用性 第二，在分布式的存储系统下，你的应用的可用性到底是如何去定义，如何去保证？其实这个也很好理解，因为在大规模的分布式系统里面，任何一个节点，任何一个数据中心或者支架都有可能出现硬件的故障，软件的故障，各种各样的故障，但这个时候你很多业务是并没有办法停止，或者并没有办法去容忍 Down time 的。所以在一个新的环境之下，你如何对你系统的可用性做定义和保证，这是一个新的课题。一会儿我会讲到最新的研究方向和成果。可维护性 第三，对于大规模的分布式数据库来说它的可维护性，这个怎么办？可维护性跟单机的系统是明显不同的，因为单机的数据库，或者传统的单点的数据库，它其实做到主从，甚至做到一主多从，我去维护 master ，别让它挂掉，这个维护性主要就是维护单点。在一个大规模的分布式系统上，你去做这个事情是非常麻烦的。可以简单说一个案例，就是 Google 的 Spanner。Spanner 是 Google 内部的一个大规模分布式系统，整个谷歌内部只部署了一套，在生产环节中只部署了一套。这一套系统上有上万甚至上数十万的物理节点。但是整个数据库的维护团队，其实只有很小的一组人。想像一下，上十万台的物理节点，如果你要真正换一块盘、做一次数据恢复或者人工运维的话，这是根本不可能做到的事情。但是对于一个分布式系统来说，它的可维护性或者说它的维护应该是转嫁给数据库自己。开发复杂度 还有，就是对于一个分布式数据库来说，它在开发业务的时候复杂度是怎么样的。大家其实可能接触的比较多的，像 Hbase、 Cassandra、Bigtable 等这种开源的实现，像 NoSQL 数据库它其实并没有一个很好的 cross-row transaction 的 support。另外，对于很多的 NoSQL 数据库并没有一个很好的 SQL 的 interface，这会让你写程序变得非常麻烦。比如说对于一些很普通的业务，一个表，我需要去 select from table，然后有一个fliter 比如一个条件大于 10，小于 100，这么简单的逻辑，如果在 HBase 上去做的话，你要写十行、二十行、三十行；如果你在一个关系的数据库，或者支持 SQL 的数据库，其实一行就搞定了。其实这个对于很多互联网公司来说，在过去的几年之内基本上已经完成了这种从 RDBMS 到 NoSQL 的改造，但是这个改造的成本和代价是非常非常高的。比如我原来的业务可能在很早以前是用 MySQL 已经写的稳定运行好久了，但是随着并发、容量、可扩展性的要求，我需要迁移 Bigtable、Hbase、Cassandra、MongoDB 这种 NoSQL 数据库上，这时基本上就要面临代码的完整重写。这个要放在互联网公司还可以，因为它们有这样的技术能力去保证迁移的过程。反正我花这么多钱，招这么牛的工程师，你要帮我搞定这个事情。但是对于传统的行业，或者传统的机构来说，这个基本上是不可能的事情。你不可能让他把原来用 Oracle 用SQL 的代码改成 NoSQL 的 code。因为 NoSQL 很少有跨行事务，首先你要做一个转账，你如果不是一个很强的工程师，你这个程序基本写不对，这是一个很大的问题。这也是为什么一直以来像这种 NoSQL 的东西并没有很好的在传统行业中去使用的一个最核心的原因，就是代价实在太大。存储系统的扩展模型 所以其实在去讲这些具体到底该怎么解决，或者未来数据库会是什么样的之前，我想简单讲一下扩展的模型。对于一个关系型数据库也好，对于存储的系统本身也好，它的扩展模型有哪些。Sharding 模式 第一种模式是 Sharding 模式。如果在座的各位有运维过线上的 MySQL 的话，对这个模型会非常熟悉。最简单的就是分库、分表加中间件，就是说我不同的业务可能用不同的库，不同的表。当一个单表太大的时候，我通过一些 Cobar、Mycat 等这样的数据库中间件来去把它分发到具体的数据库的实例上。这种模型是目前用的最普遍的模型，它其实也解决了很大部分的问题。为什么这十年在关系型数据库上并没有很好的扩展方案，但是大家看上去这种业务还没有出现死掉的情况，就是因为后面有各种各样 Sharding 的中间件或者分库分表这种策略在硬扛着。像这种中间件 Sharding 第一个优势就是实现非常简单。你并不需要对数据库内做任何的改造，你也并不需要去比如说从你原来的 SQL 代码转到 NoSQL 的代码。但是它也有自己的缺点。首先，对你的业务层有很强的侵入性。这是没有办法的，比如你想用一个中间件，你就需要给它指定一个 Sharding key。另外，原来比如你的业务有一些 join ,有一些跨表跨行的事务，像这种事务你必须得改掉，因为很多中间件并没有办法支持这个跨 shard 的分布式 join。 第二个比较大的缺陷是它的分片基本是固定的，自动化程度、扩展性都非常差，你必须得有一个专职的 DBA 团队给你的 MySQL 或者 PG 的 Sharding 的集群去做运维。我之前在豌豆荚做过一段时间 MySQL cluster 的分片的维护工作。当时我记得是一个 16 个节点的 MySQL 的集群，我们需要扩展到 32 个节点的规模，整整提前演练了一个月，最后上线了一个礼拜。上线那个礼拜，晚上基本上没有办法睡觉，所以非常痛苦。再说一个 Google 的事情，Google 在刚才我说的 Spanner 和 F1 这两个数据库没有上线之前，Google 的广告系统的业务是由 100 多个节点的 MySQL 的集群对外提供服务的。如 Google 这么牛的公司，在维护一百多个节点的 MySQL Sharding 的数据库的时候，都已经非常痛苦，宁可重新去写一个数据库，也不想去维护这个 datebase cluster。其实大家可以看到，像这种 Sharding 的方案，它比较大的问题就是它的维护代价或者维护集群的复杂度，并不是随着节点数呈线性增长，而是随着节点的增加非线性的增长上去。比如你维护 2 个节点的还好，维护 4 个节点的也还可以，但是你维护 16 个、64 个、128 个基本就是不可能的事情。第三就是一些复杂的查询优化，并没有办法在中间件这一层，去帮你产生一个足够优化的执行计划，因此，对于一些复杂查询来说，Sharding 的方案是没法做的。所以对你的业务层有很高的要求。这是一种思路，是目前来说互联网公司里边用的最多的一种 MySQL 或者 PG 这种关系型数据库的扩展方案。Region Base 模型 第二种扩展模型是 Region Base。这张图是我项目里面扒出来的图。它整个思路有点像 Bigtable，它相当于把底下的存储层分开，数据在最底层存储上已经没有表、行这样结构的划分，每一块数据都由一个固定的 size 比如 64 M、128 M 连续的 Key-value pairs 组成。其实这个模型背后最早的系统应该是谷歌在 06 年发表的 Bigtable 这篇论文里面去描述的。这个模型有什么好处呢？一是它能真正实现这种弹性的扩展。第二个，它是一个真正高度去中心化。去中心化这个事情，对于一个大的 Cluster 来说是一个非常重要的特性。**还有一个优势，在 KV 层实现真正具有一定的自动 Failover 的能力。 **Failover指的是什么呢？比如说在一个集群比较大的情况下，或者你是一个 cluster ，你任何一个节点，任何一个数据损坏，如果能做到业务端的透明，你就真正实现了 Auto-Failover 的能力。其实在一些对一致性要求不那么高的业务里面，Auto-Failover 就是指， 比如在最简单的一个 MySQL 组从的模型里，当你的组挂掉了以后，我监控的程序自动把 slave 提上来，这也是一种 Failover 的方式。但是这个一致性或者说数据的正确性并不能做到很好的保证。你怎么做到一致性的 Auto-Failover，其实背后需要做非常非常多的工作。这是 Region 模型的一些优势。但是它的劣势也同样明显，这种模型的实现非常复杂。我一会儿会说到背后的关键技术和理论，但是它比起写中间件真的复杂太多了。你要写一个能用的 MySQL 或者 PG 的中间件，可能只需要一两个工程师，花一两周的时间就能写出一个能用的数据库中间件；但是你如果按照这个模型做一个弹性扩展的数据库的话，你的工作量就会是数量级的增加。第二个劣势就是它业务层的兼容性。像 Region Base 的模型，最典型的分布式存储系统就是 HBase。HBase 它对外的编程接口和 SQL 是千差万别，因为它是一个 Key Value 的数据库。你的业务层的代码兼容性都得改，这个对于一些没有这么强开发能力的用户来说，是很难去使用的，或者它说没有 SQL 对于用户端这么友好。可用性级别 我一会儿会讲一下，刚才我们由 Region Base 这个模型往上去思考的一些东西，在此之前先说一些可用性。高可用。其实说到高可用这个词，大多数的架构师都对它非常熟悉。我的系统是高可用的，任何一个节点故障业务层都不受影响，但是真的不受影响吗？我经过很多的思考得到的一个经验就是主从的模型是不可能保证同时满足强一致性和高可用性的。可能这一点很多人觉得，我主从，我主挂了，从再提起来就好，为什么不能保护这个一致性呢？就是因为在一个集群的环境下，有一种故障叫脑裂。脑裂是什么情况？整个集群是全网络联通的，但是出现一种情况，就是我只是在集群内部分成了两个互不联通的一个子集。这两个子集又可以对外提供服务，其实这个并不是非常少见的状况，经常会发生。像这种情况，你贸然把 slave 提起来，相当于原来的 master 并没有完全的被 shutdown，这个时候两边可能都会有读写的情况，造成数据非常严重的不一致，当然这个比较极端了。所以你会发现阿里或者说淘宝，年年都在说我们有异地多活。但是去年甚至前几个月，杭州阿里的数据中心光纤被挖断，支付宝并没有直接切到重复层，而是宁可停止服务，完全不动，也不敢把 slave 数据中心提起来。所以其实任何基于主从模型的异地多活方案都是不行的。这个问题有没有办法解决呢？其实也是有的。还是说到 Google，我认为它才是全世界最大的数据库公司，因为它有全世界最大的数据量。你从来没有听说过 Google 哪一个业务因为哪一个数据中心光纤挖断，哪一个磁盘坏了而对外终止服务的，几乎完全没有。因为 Google 的存储系统大多完全抛弃了基于主从的一致性模型。它的所有数据都不是通过主从做复制的，而是通过类似 Raft 或者 Paxos 这种分布式选举的算法做数据的同步。这个算法的细节不展开了，总体来说是一个解决在数据的一致性跟自动的数据恢复方面的一个算法。同时，它的 latency 会比多节点强同步的主从平均表现要好的一个分布式选举的算法。在 Google 内部其实一直用的 Paxos，它最新的 Spanner 数据库是用 Paxos 做的 replication 。在社区里面，跟 Paxos 等价的一个算法就是 Raft。Raft 这个算法的性能以及可靠性都是跟 Paxos 等价的实现。这个算法就不展开了。我认为这才是新一代的强一致的数据库应该使用的数据库复制模型。分布式事务 说到事务。对于一个数据库来说，我要做传统的关系型数据库业务，事务在一个分布式环境下，并不像单机的数据库有这么多的方法，这么多的优化。其实在分布式事务这个领域只有一种方法，并且这么多年了从分布式事务开始到现在，在这个方法上并没有什么突破，基本只有一条出路就是两阶段提交。其实可以看一下 Google 的系统。对于我们做分布式系统的公司来说，Google 就是给大家带路的角色。Google 最新的数据库系统上它使用的分布式事务的方法仍然是两阶段提交。其实还有没有什么优化的路呢？其实也是有的。两阶段提交最大的问题是什么呢？一个是延迟。因为第一阶段先要把数据发过去，第二阶段要收到所有参与的节点的 response 之后你才能去 commit 。这个过程，相当于你走了很多次网络的 roundtrip，latency 也会变得非常高。所以其实优化的方向也是有的，但是你的 latency 没法优化，只能通过吞吐做优化，就是 throughput 。比如说我在一万个并发的情况下，每个用户的 latency 是 100 毫秒，但是一百万并发，一千万并发的时候，我每个用户的 latency 还可以是 100 毫秒，这在传统的单点关系型数据库上，是没有办法实现的。第二就是去中心化的事务管理器。另外没有什么东西是银弹，是包治百病的，你要根据你的业务的特性去选择合适的一致性算法。NewSQL 其实刚刚这些 pattern 会发展出一个新的类别，我们能不能把关系数据库上的一些 SQL、Transaction 跟 NoSQL 跟刚才我说到的 Region Base 的可扩展的模型融合起来。这个思想应该是在 2013 年左右的时候，学术界提出来比较多的东西，NewSQL。NewSQL 首先要解决 Scalability 的问题， 刚给我们说过 scalability 是一个未来的数据库必须要有的功能，第二个就是 SQL，SQL 对于业务开发者来说是很好的编程的接口。第三，ACID Transaction，我希望我的数据库实现转帐和存钱这种强一致性级别的业务。第四，就是整个 cluster 可以支持无穷大的数据规模，同时任何数据节点的宕机、损坏都需要集群自己去做监控，不需要 DBA 的介入。案例：Google Spanner / F1 有没有这样的系统？其实有的。刚才一直提到 Google 的 Spanner 系统。Spanner 系统是在 2012 年底于 OSDI 的会议上发布了论文； F1 这篇论文在 2013 年的 VLDB 发布的，去描述了整个 Google 内部的分布式关系型数据库的实现。首先，根据 Spanner 的论文 Spanner 和 F1 在生产环境只有一个部署，上万物理节点遍布在全球各种数据中心内，通过 Paxos 进行日志复制。第二，整 …"},
		{"url": "https://pingcap.com/meetup/meetup-2016-09-10/",
		"title": "PingCAP 第 23 期 NewSQL Meetup", 
		"content": " PingCAP 第 23 期 NewSQL Meetup 2016-09-10 金坤&amp;amp;黄华超 PingCAP PingCAPPingCAP功能介绍PingCAP 专注于新型分布式数据库的研发，是知名开源数据库 TiDB (GitHub 总计10000+ stars ) 背后的团队，总部设在北京，是国内第一家开源的新型分布式关系型数据库公司、国内领先的大数据技术和解决方案提供商。NewSQL Meetup今天是 PingCAP 第 23 期 Meetup，主题是金坤分享的《How to write a good commit message》以及黄华超分享的《QuorumKV：微信分布式 KV 存储系统》。【Topic 1】How to write a good commit messageContent：This talk about writing good commit messages aims to act as the beginning of a series of talks about writing quality technical content. To emphasise the importance of the commit messages, the talk asked the audience to set up a profile of the potential reviewer who is as cool and picky as the writer of the technical content, or the writer himself in 5 years. Then the talk introduced what is a good commit message and how to write a good commit message by encouraging the audiences to establish good habits, good format and use simple and consistent language, especially to resist the temptation of using lengthy sentences. Best practices and tools from other projects and also covered to trigger further discussions and action items to improve our project.【Topic 2】QuorumKV：微信分布式 KV 存储系统Content：本次分享首先介绍了 QuorumKV 诞生的背景以及微信的一些业务情况。并分别从单机存储引擎、分布式协议、数据迁移和冷热数据分离等方面介绍了系统的设计和实现。最后，与大家共同探讨了 QuorumKV 目前基于 Paxos 的一些改造和发展。PingCAP Meetup"},
		{"url": "https://pingcap.com/weekly/2016-09-05-tidb-weekly/",
		"title": "Weekly update (August 29 ~ September 04, 2016)", 
		"content": " Weekly update (August 29 ~ September 04, 2016) Last week, we landed 29 PRs in the TiDB repositories and 24 PRs in the TiKV repositories.Notable changes to TiDB  Support the unhex and the ceiling/ceil functions Improve the Parser to handle rn. Solve the potential concurrency issues Support Load Data Use the Pipeline model to filter data through indexes to improve the performance Improve the code to reduce memory allocation and improve the performance Fix several bugs.  Notable changes to TiKV  Coprocessor supports the new decimal type. Use the Raft column family to save Raft meta and logs. See Benchmark. Tune the write column family to reduce memory usage.  Notable changes to Placement Driver  Check duplicated store addresses to prevent user from bootstrapping cluster in the wrong way, see issues 287, 288. Support the remove store API to remove a dead TiKV store. Use glide instead of the original godep to manage vendor. Remove join itself to prevent user from starting a removed PD server again.  Benchmark Use sysbench to benchmark using the (CF_RAFT) column family to save the Raft log and previously the default (CF_DEFAULT) column family in 3-node TiKV.# Prepare data sysbench --test=./lua-tests/db/oltp.lua --mysql-host=${host} --mysql-port=${port}   --mysql-user=${user} --mysql-password=${password} --oltp-tables-count=1   --oltp-table-size=${table_size} --rand-init=on prepare # Run benchmark sysbench --test=./lua-tests/db/insert.lua --mysql-host=${host} --mysql-port=${port}   --mysql-user=${user} --mysql-password=${password} --oltp-tables-count=1   --oltp-table-size=${table_size} --num-threads=${threads} --report-interval=60   --max-requests=1280000 --percentile=99 run    Threads Table Size CF_DEFAULT qps CF_DEFAULT avg/.99 latency CF_RAFT qps CF_RAFT avg/.99 latency     32 6400000 3885 8.24&amp;frasl;13.48 3979 8.04/13.70   64 7680000 3653 17.52&amp;frasl;34.10 4477 14.29&amp;frasl;24.49   128 8960000 3422 37.39&amp;frasl;70.10 4642 27.57&amp;frasl;57.45    As we can see, the qps is increased by about 22%, and the latency is decreased by about 18%.New contributors  Dagang Wei  "},
		{"url": "https://pingcap.com/meetup/meetup-2016-09-03/",
		"title": "PingCAP 第 22 期 NewSQL Meetup", 
		"content": " PingCAP 第 22 期 NewSQL Meetup 2016-09-03 宋昭&amp;amp;张帅 PingCAP PingCAPPingCAP ![]() 微信号pingcap2015功能介绍PingCAP 专注于新型分布式数据库的研发，是知名开源数据库 TiDB (GitHub 总计10000+ stars ) 背后的团队，总部设在北京，是国内第一家开源的新型分布式关系型数据库公司、国内领先的大数据技术和解决方案提供商。NewSQL Meetup今天是 PingCAP 第 22 期 Meetup，主题是360 基础架构组研发工程师宋昭分享的《360 开发的大容量 redis -pika》以及 美团云工程师张帅分享的《分布式对象存储系统设计介绍》。▌ ****Topic 1：360 开发的大容量 redis -pikaLecture：宋昭，360 基础架构组研发工程师。专注于分布式存储领域，目前负责 360 开源项目 pika 相关的设计和开发工作。Content：目前 pika 在 360 内部大量使用，有 300 多实例，主要解决大容量的 redis（400G,800G）场景；在外部，被微博、美团、万达电商、garena、apus 等使用于线上核心系统中。本次分享主要介绍 pika 的系统设计和实现。▌ ****Topic 2：分布式对象存储系统设计介绍Lecture：张帅，美团云工程师。对分布式数据库及分布式存储系统有浓厚的兴趣。Content：分享关于大规模分布式对象存储的一些想法和思考。PingCAP Meetup"},
		{"url": "https://pingcap.com/blog-cn/tidb-transaction-model/",
		"title": "TiKV 事务模型概览，Google Spanner 开源实现", 
		"content": "随着时代的发展，应用和数据的规模越来越大。然而在这个一切都可以水平扩展的时代，你会发现，大多数应用的最下层的关系型数据库，竟然难以找到一个优雅易用的水平扩展解决方案，一直以来不得不依赖静态 Sharding ，牺牲掉事务，然后在业务层各种 Workarounds。作为后端开发者应该深有体会。层出不穷的 NoSQL 看似解决了数据水平扩展的问题，但是由于跨行事务的缺失和接口的局限，在很多业务中落地还是需要付出很多代价的。最近 Google 基础设施的神人 Jeff Dean 在一次采访中回顾自己作为工程师最大的后悔是什么的问题时提到，他最后悔的事情是没有在 BigTable 中加入跨行事务模型，以至于后来各种各样的团队尝试在 BigTable 上不停的造事务的轮子，但其实这个特性应该是由 BigTable 提供。同样的观点也在他后来的论文中反复提到过。Google 2012 年在 OSDI 上发表了 Spanner，作为 BigTable 的下一代产品，最主要的特性就是支持跨行事务和在分布式场景上实现 Serializable 的事务隔离级别。我们在2015年底从零开始按照论文做 Spanner 的开源实现 TiKV，于近期开源，和 Spanner 一样，也是一个支持分布式事务和水平扩展的 KV 数据库。一个分布式数据库涉及的技术面非常广泛，今天我们主要探讨的是 TiKV 的 MVCC（多版本并发控制） 和 Transaction 实现。MVCC 其实并不是一个老的概念了，在传统的单机关系型数据库使用 MVCC 技术来规避大量的悲观锁的使用，提高并发事务的读写性能。值得注意的是 MVCC 只是一个思想，并不是某个特定的实现，它表示每条记录都有多个版本的，互相不影响，以一个 kv 数据库为例从逻辑上的一行的表示就并不是Record := {key, value} 而是Record := {key, value, version} 支持分布式 MVCC 在 KV 系统中比较著名的应该是在 BigTable。在 TiKV 中我们的整个事务模型是构建在一个分布式 MVCC 的基础之上：可以看到，整个 TiKV 的底层本地存储是依赖了 RocksDB，RocksDB 是一个单机的嵌入式 KV 数据库，是一个 LSM Tree的实现，是 Facebook 基于 LevelDB 的修改版本，其特点是写入性能特别好，数据存储是有序的 KV Pairs，对于有序 key 的迭代的场景访问效率较高。对于 MVCC 层，每一个 Key，在底层的 RocksDB 上都会存储同一个 Key 的多个版本，在底层存储上看来，形式如：MateKey --&amp;gt; key 的所有版本信息 DataKey(key+version_1)--&amp;gt;Value_v1 DataKey(key+version_2)--&amp;gt;Value_v2 暴露给上层的接口行为定义： &amp;gt; * MVCCGet(key, version), 返回某 key 小于等于 version 的最大版本的值 &amp;gt; * MVCCScan(startKey, endKey, limit, version), 返回 [startKey, endKey) 区间内的 key 小于等于 version 的最大版本的键和值，上限 limit 个 &amp;gt; * MVCCPut(key, value, version) 插入某个键值对，如果 version 已经存在，则覆盖它。上层事物系统有责任维护自增version来避免read-modify-write &amp;gt; * MVCCDelete(key, version) 删除某个特定版本的键值对, 这个需要与上层的事务删除接口区分，只有 GC 模块可以调用这个接口给出一个 MVCCGet 的伪代码实现：MVCCGet(key, version) { versions = kv.Get(key) // read meta targetVer = nil for ver in versions { if ver &amp;lt;= version { targetVer = ver break } } return kv.Get(mvccEncode(key, targetVer)), targetVer } 核心思想是，先读取 meta key 然后通过 meta key 中找到相应的可见版本，然后再读取 data key，由于这些 key 都拥有相同的前缀，所以在实际的访问中，读放大的程度是可以接受的。类似的 MVCCScan 和 MVCCPut 由于篇幅的限制就不展示了，但是思想是类似的。细心的朋友可能会发现，这个方案会遇到一个 key 的 meta key 膨胀的问题，当如果一个 key 短时间内修改过于频繁，会导致 meta key 的 value 过大，这个问题可以通过 meta 拆分的方式解决，核心的思想也比较简单，本质上就是对 meta key 建立索引，将一个 meta key 变成多个 meta key：Meta0 (v127 - v0) next: 0 （0表示没有后续 Meta） 第一次分裂：Meta0 (v128 - v96) next:1 Meta1 (v95 - v0) next:0 第二次分裂：Meta0 (v224 - v192) next:2 Meta1 (v95 - v0) next: 0 Meta2 (v191 - v96) next:1 这样一来，即可规避过大的读放大问题。对 TiKV 的 MVCC 模型有了基础概念之后，就可以介绍我们的分布式事务模型，总体来讲，我们的分布式事务模型本质上是一个两阶段提交的算法，其实本质上来说，在一个分布式系统中实现跨节点事务，只有两阶段提交一种办法（3PC 本质上也是 2PC 的一个优化）。在 Spanner 中同样也是一个 2PC，但是 Google 比较创新的引入了 TrueTime API 来作为事务 ID 生成器从而实现了 Serializable 的隔离级别，具体的实现在这里就不赘述了，有兴趣的朋友可以去看 Spanner 的论文。值得一提的是，由于 TrueTime 引入了专有的硬件（GPS 时钟和原子钟）来实现跨洲际机房的时钟同步方案，大多数业务场景其实并没有这种跨洲际机房数据同步的需求，所以我们在 TiKV 中最终选择的事务模型和 Spanner 有所区别，采用了 Google 的另一套分布式事务方案 Percolator 的模型。Percolator 是 Google 的上一代分布式事务解决方案，构建在 BigTable 之上，在 Google 内部用于网页索引更新的业务。原理比较简单，总体来说就是一个经过优化的 2PC 的实现，依赖一个单点的授时服务 TSO 来实现单调递增的事务编号生成，提供 SI 的隔离级别。传统的分布式事务模型中，一般都会有一个中央节点作为事务管理器，Percolator 的模型通过对于锁的优化，去掉了单点的事务管理器的概念，将整个事务模型中的单点局限于授时服务器上，在生产环境中，单点授时是可以接受的，因为 TSO 的逻辑极其简单，只需要保证对于每一个请求返回单调递增的 id 即可，通过一些简单的优化手段（比如 pipeline）性能可以达到每秒生成百万 id 以上，同时 TSO 本身的高可用方案也非常好做，所以整个 Percolator 模型的分布式程度很高。下面我们详细介绍一下 TiKV 中事务的实现方式。总体来说，TiKV 的读写事务分为两个阶段：1、Prewrite 阶段；2、Commit 阶段。客户端会缓存本地的写操作，在客户端调用 client.Commit() 时，开始进入分布式事务 prewrite 和 commit 流程。Prewrite 对应传统 2PC 的第一阶段： 首先在所有行的写操作中选出一个作为 primary row，其他的为 secondary rows PrewritePrimary: 对 primaryRow 写入锁（修改 meta key 加入一个标记），锁中记录本次事务的开始时间戳。上锁前会检查： i.该行是否已经有别的客户端已经上锁 (Locking) ii.是否在本次事务开始时间之后，检查versions ，是否有更新 [startTs, +Inf) 的写操作已经提交 (Conflict) 在这两种种情况下会返回事务冲突。否则，就成功上锁。将行的内容写入 row 中，版本设置为 startTs 将 primaryRow 的锁上好了以后，进行 secondaries 的 prewrite 流程： i.类似 primaryRow 的上锁流程，只不过锁的内容为事务开始时间 startTs 及 primaryRow 的信息 ii.检查的事项同 primaryRow 的一致 iii.当锁成功写入后，写入 row，时间戳设置为 startTs  以上 Prewrite 流程任何一步发生错误，都会进行回滚：删除 meta 中的 Lock 标记 , 删除版本为 startTs 的数据。当 Prewrite 阶段完成以后，进入 Commit 阶段，当前时间戳为 commitTs，TSO 会保证 commitTs &amp;gt; startTsCommit 的流程是，对应 2PC 的第二阶段： commit primary: 写入 meta 添加一个新版本，时间戳为 commitTs，内容为 startTs, 表明数据的最新版本是 startTs 对应的数据 删除 Lock 标记  值得注意的是，如果 primary row 提交失败的话，全事务回滚，回滚逻辑同 prewrite 失败的回滚逻辑。如果 commit primary 成功，则可以异步的 commit secondaries，流程和 commit primary 一致， 失败了也无所谓。Primary row 提交的成功与否标志着整个事务是否提交成功。事务中的读操作： 检查该行是否有 Lock 标记，如果有，表示目前有其他事务正占用此行，如果这个锁已经超时则尝试清除，否则等待超时或者其他事务主动解锁。注意此时不能直接返回老版本的数据，否则会发生幻读的问题。 读取至 startTs 时该行最新的数据，方法是：读取 meta ，找出时间戳为 [0, startTs], 获取最大的时间戳 t，然后读取为于 t 版本的数据内容。  由于锁是分两级的，Primary 和 Seconary row，只要 Primary row 的锁去掉，就表示该事务已经成功提交，这样的好处是 Secondary 的 commit 是可以异步进行的，只是在异步提交进行的过程中，如果此时有读请求，可能会需要做一下锁的清理工作。因为即使 Secondary row 提交失败，也可以通过 Secondary row 中的锁，找到 Primary row，根据检查 Primary row 的 meta，确定这个事务到底是被客户端回滚还是已经成功提交。大致的事务提交流程介绍到这，通过 MVCC， TiKV 的事务默认隔离级别是 Repeatable Read（SI）, 也对外暴露显式的加锁的 API，用于为客户端实现 SELECT … FOR UPDATE 等隔离级别为 SSI 的语句。大家可以看到，本质上 TiKV 的事务模型是基于 Percolator 的思想，但是对比原论文，做了很多工程上的优化，我们将原来论文中的 L 列和 W 列去掉，通过和MVCC 的 Meta 来存储相关的事务信息。对于事务冲突的情况，原始的 Percolator 的论文中并没有做过多的描述，采取的策略也比较简单，读时遇到锁就等直到锁超时或者被锁的持有者清除，写时遇到锁，直接回滚然后给客户端返回失败由客户端进行重试。TiKV 采用的是乐观事务模型，只有最后 2pc 的阶段会对数据加锁，但是对于频繁冲突的场景，回滚和客户端重试的代价可能很高， TiKV 在存储节点本地添加了一个简单的 Scheduler 层，在 2PC 读写遇到锁的时候并不是粗暴的直接回滚返回，而是尝试在本地排队等一下 ，如果超时或者其他异常，再返回客户端重试，减小了网络的开销。另外的一个问题是无效版本的清理（GC），对于在线上运行的 MVCC 系统来说，如果没有 GC 策略，那么版本将会膨胀得越来越多，而且对于 MVCC 来说，数据的删除并不是真正的删除，而是标记删除，当无用版本积累太多，会对于读性能有很大的影响。同时GC 策略并不能简单的指定一个版本（safe point），然后删除之前的所有版本，很显然比如有一个 key 只有一个版本，这个版本就不能动，比如有一个 key 在 safe point 前的最后一个版本是 tombstone (已经删除)，而且这个 key 之后再没有被操作过，那么这个 key 的所有版本都是可以被整体删除的。另外，在实际实现的过程中还会遇到一个问题，当 gc 一个 key 时发现 meta 中有锁（可能是由于清除 secondary lock 时客户端崩溃或者其他原因），你并不能简单删除之，因为如果这个 key 中的锁是 secondary lock，在 gc 进程去查看这个锁对应的 primary key 的对应版本是提交还是回滚时，如果 primary key 的那个版本已经被 gc 删除掉了，对于 gc 进程来说就没有办法确定该事务到底是提交还是回滚，可能出现数据误删的情况。TiKV 通过对事务的 primary key 的 meta version 进行一个特殊的标记，由于没有集中事务管理器的存在，判断一个事务的执行状态只有 primary key 的 meta 中有记录，所以在 gc 时会绕过这些 primary key 的 version 解决了这个问题，保证了数据的安全。"},
		{"url": "https://pingcap.com/weekly/2016-08-29-tidb-weekly/",
		"title": "Weekly update (August 22 ~ August 28, 2016)", 
		"content": " Weekly update (August 22 ~ August 28, 2016) Last week, we landed 26 PRs in the TiDB repositories and 26 PRs in the TiKV repositories.Notable changes to TiDB  Support the MySQL SetOption Command and Multiple Statements. Support filter push-down for the Time/Decimal type. Support converting OuterJoin to InnerJoin by using Null Reject. Support multiple-thread Hash Join. Support Garbage Collector. Optimize the code to improve the Performance. Fix several bugs.  Notable changes to TiKV  Coprocessor supports the time type. Output the version information when TiKV starts. Append the write column family when committing lock-only keys to fix bug #921. Use randomized Placement Driver (PD) server and remove getting PD leader to solve issues #942 and #956. Support the Debug traits for messages in the sending channel. Coprocessor uses configuration to make the endpoint threadpool size configurable.  Notable changes to Placement Driver  Output the version information when PD starts. Save the next timestamp oracle(TSO) to solve issue #191.  "},
		{"url": "https://pingcap.com/meetup/meetup-2016-08-27/",
		"title": "PingCAP 第 21 期 NewSQL Meetup", 
		"content": " PingCAP 第 21 期 NewSQL Meetup 2016-08-27 韩飞&amp;amp;申砾 PingCAP PingCAPPingCAP ![]() 微信号pingcap2015功能介绍PingCAP 专注于新型分布式数据库的研发，是知名开源数据库 TiDB (GitHub 总计10000+ stars ) 背后的团队，总部设在北京，是国内第一家开源的新型分布式关系型数据库公司、国内领先的大数据技术和解决方案提供商。NewSQL Meetup今天是 PingCAP 第 21 期 Meetup，主题是韩飞分享的《An Introduction to Join-Reorder in TiDB》以及申砾分享的《MPP and SMP in TiDB》。▌ ****Topic 1：An Introduction to Join-Reorder in TiDBContent：本次分享详细介绍了 TiDB 中 Join-Reorder 的流程。包括 Join-Reorder 的动机，outer-join 的 reorder 局限性和解决办法。为了解决某些 outer join re-association 的问题，我们可以引入的新算子 Generalized outerJoin。最后介绍了通过为 Join Query 建立 Query Graph 进行启发式搜索和动态规划的Join-Reorder算法。▌ ****Topic 2：MPP and SMP in TiDBContent：TiDB 是一个支持水平扩展的分布式数据库，除了提供海量数据存储能力之外，还需要提供海量数据的计算能力，这样才能帮助用户更好、更容易地使用数据。为此我们开发了一套分布式计算框架，一方面利用海量的存储节点的计算能力，加快数据处理速度；另一方面在单个计算节点内，我们利用 Go 的并发优势，通过 SMP 方式提高计算并行度。本次 Talk 首先介绍了 TiDB 分布式计算架构，并举例说明计算的具体流程；然后分享了最近 TiDB 针对索引查询和 Join 做的一系列优化，性能有大幅度提高；最后列出了一些 NewSQL database 中如何做计算值得思考的问题。PingCAP Meetup"},
		{"url": "https://pingcap.com/weekly/2016-08-22-tidb-weekly/",
		"title": "Weekly update (August 13 ~ August 21, 2016)", 
		"content": " Weekly update (August 13 ~ August 21, 2016) Last week, we landed 26 PRs in the TiDB repositories and 15 PRs in the TiKV repositories.Notable changes to TiDB  Upgrade the query optimizer. Upgrade the lexer. Replace golang protobuf with gogo protobuf. Optimize the distributed executor. Repair the Time and Decimal types to improve the compatibility with MySQL. Support the Set names binary statement. Support Covering Index. Optimize the table scanning when the condition is false constant. Fix several bugs.  Notable changes to TiKV  Add the leader lease read support for better performance, see benchmark. Use delete_file_in_range from RocksDB to destroy Regions quickly to avoid blocking Raft storage threads. Support GC for obsolete data versions. Coprocessor supports covering index for Select Where and the aggregation operations. Check whether the key is already rolled back when prewrite to fix a transaction bug. Support the --log-file flag to redirect log to the log file.  Notable changes to Placement Driver  Refine the join flag to support multiple join scenarios. Use unix socket in test to avoid the &amp;ldquo;Address Already in Use&amp;rdquo; error. Redirect requests to the Leader if the current member is a Follower.  Benchmark Use sysbench to benchmark leader lease read and previous Raft quorum read in 3-node TiKV.Insert # Prepare data sysbench --test=./lua-tests/db/oltp.lua --mysql-host=${host} --mysql-port=${port}   --mysql-user=${user} --mysql-password=${password} --oltp-tables-count=$1   --oltp-table-size=5120000 --rand-init=on prepare # Run benchmark sysbench --test=./lua-tests/db/insert.lua --mysql-host=${host} --mysql-port=${port}   --mysql-user=${user} --mysql-password=${password} --oltp-tables-count=1   --oltp-table-size= 5120000 --num-threads=${threads} --report-interval=60   --max-requests=1280000 --percentile=99 run |Threads|Leader lease read qps|Leader lease read avg/.99 latency|Raft quorum read qps|Raft quorum read/.99 latency| |&amp;mdash;|&amp;mdash;|&amp;mdash;|&amp;mdash;|&amp;mdash;|&amp;mdash;| |32|2296|13.93&amp;frasl;15.28|1315|24.33&amp;frasl;94| |64|2199|29.1&amp;frasl;145|1325|48.29&amp;frasl;473| |128|1854|69&amp;frasl;931|1290|99&amp;frasl;697|As we can see, the qps is increased by about 70%, and the latency is decreased by about 40%.Select # Prepare data sysbench --test=./lua-tests/db/oltp.lua --mysql-host=${host} --mysql-port=${port}   --mysql-user=${user} --mysql-password=${password} --oltp-tables-count=1   --oltp-table-size=5120000 --rand-init=on prepare # Run benchmark sysbench --test=./lua-tests/db/select.lua --mysql-host=${host} --mysql-port=${port}   --mysql-user=${user} --mysql-password=${password} --oltp-tables-count=1   --oltp-table-size=5120000 --num-threads=${threads} --report-interval=60   --max-requests=1280000 --percentile=99 run |Threads|Leader lease read qps|Leader lease read avg/.99 latency|Raft quorum read qps|Raft quorum read/.99 latency| |&amp;mdash;|&amp;mdash;|&amp;mdash;|&amp;mdash;|&amp;mdash;|&amp;mdash;| |32|21010|1.52&amp;frasl;7.53|12221|2.62&amp;frasl;6.69| |64|25948|2.47&amp;frasl;10.20|12637|5.06/11.62| |128|27283|4.69&amp;frasl;13.68|11069|11.56&amp;frasl;35.88|As we can see, the qps is increased by about 130%, and the latency is decreased by about 50%.New contributors  hhkbp2  "},
		{"url": "https://pingcap.com/meetup/meetup-2016-08-20/",
		"title": "PingCAP 第 20 期 NewSQL Meetup", 
		"content": " PingCAP 第 20 期 NewSQL Meetup 2016-08-20 雷丽媛&amp;amp;温文鎏 PingCAP PingCAPPingCAP ![]() 微信号pingcap2015功能介绍PingCAP 专注于新型分布式数据库的研发，是知名开源数据库 TiDB (GitHub 总计10000+ stars ) 背后的团队，总部设在北京，是国内第一家开源的新型分布式关系型数据库公司、国内领先的大数据技术和解决方案提供商。NewSQL Meetup今天是 PingCAP 第 20 期 Meetup，主题是百度网页搜索部工程师雷丽媛分享的《搜索引擎背后的万亿量级存储系统 Tera 》以及温文鎏分享的《Cloudtable：分布式强一致的 KV 存储系统》。【Topic 1】搜索引擎背后的万亿量级存储系统 Tera近景福利：今日的美女讲师 :)Lecture：雷丽媛，百度网页搜索部工程师。专注于分布式存储领域，目前负责百度结构化数据存储和分布式文件系统的相关工作。Content：介绍支撑搜索引擎核心的海量存储——Tera 的设计与实现【 ****Topic 2】Cloudtable：分布式强一致的 KV 存储系统Content：如何搭建一个适用于互联网公司业务的大容量分布式强一致性 KV 存储系统?通过结合分布式一致性协议 Raft，嵌入式存储引擎 RocksDB，HBASE 的架构和接口，YY 云存储团队在过去的两年开发了 Cloudtable 存储系统，它是一个分布式强一致性的 KV 存储系统。今天，前 YY 云存储工程师温文鎏分享了他们在构建 Cloudtbable 系统的实践和经验。PingCAP Meetup"},
		{"url": "https://pingcap.com/blog-cn/building-distributed-db-with-raft/",
		"title": "基于 Raft 构建弹性伸缩的存储系统的一些实践", 
		"content": "最近几年来，越来越多的文章介绍了 Raft 或者 Paxos 这样的分布式一致性算法，且主要集中在算法细节和日志同步方面的应用。但是呢，这些算法的潜力并不仅限于此，基于这样的分布式一致性算法构建一个完整的可弹性伸缩的高可用的大规模存储系统，是一个很新的课题，我结合我们这一年多以来在 TiKV 这样一个大规模分布式数据库上的实践，谈谈其中的一些设计和挑战。本次分享的主要内容是如何使用 Raft 来构建一个可以「弹性伸缩」存储。其实最近这两年也有很多的文章开始关注类似 Paxos 或者 Raft 这类的分布式一致性算法，但是主要内容还是在介绍算法本身和日志复制，但是对于如何基于这样的分布式一致性算法构建一个大规模的存储系统介绍得并不多，我们目前在以 Raft 为基础去构建一个大规模的分布式数据库 TiKV ，在这方面积累了一些第一手的经验，今天和大家聊聊类似系统的设计，本次分享的内容不会涉及很多 Raft 算法的细节，大家有个 Paxos 或者 Raft 的概念，知道它们是干什么的就好。##先聊聊 Scale 其实一个分布式存储的核心无非两点，一个是 Sharding 策略，一个是元信息存储，如何在 Sharding 的过程中保持业务的透明及一致性是一个拥有「弹性伸缩」能力的存储系统的关键。如果一个存储系统，只有静态的数据 Sharding 策略是很难进行业务透明的弹性扩展的，比如各种 MySQL 的静态路由中间件（如 Cobar）或者 Twemproxy 这样的 Redis 中间件等，这些系统都很难无缝地进行 Scale。 ##Sharding 的几种策略 在集群中的每一个物理节点都存储若干个 Sharding 单元，数据移动和均衡的单位都是 Sharding 单元。策略主要分两种，一种是 Range 另外一种是 Hash。针对不同类型的系统可以选择不同的策略，比如 HDFS 的Datanode 的数据分布就是一个很典型的例子：###首先是 Range Range 的想法比较简单粗暴，首先假设整个数据库系统的 key 都是可排序的，这点其实还是蛮普遍的，比如 HBase 中 key 是按照字节序排序，MySQL 可以按照自增 ID 排序，其实对于一些存储引擎来说，排序其实是天然的，比如 LSM-Tree 或者 BTree 都是天然有序的。Range 的策略就是一段连续的 key 作为一个 Sharding 单元：例如上图中，整个 key 的空间被划分成 (minKey, maxKey)，每一个 Sharding 单元（Chunk）是一段连续的 key。按照 Range 的 Sharding 策略的好处是临近的数据大概率在一起（例如共同前缀），可以很好的支持 range scan 这样的操作，比如 HBase 的 Region 就是典型的 Range 策略。但是这种策略对于压力比较大的顺序写是不太友好的，比如日志类型的写入 load，写入热点永远在于最后一个 Region，因为一般来说日志的 key 基本都和时间戳有关，而时间显然是单调递增的。但是对于关系型数据库来说，经常性的需要表扫描（或者索引扫描），基本上都会选用 Range 的 Sharding 策略。###另外一种策略是 Hash 与 Range 相对的，Sharding 的策略是将 key 经过一个 Hash 函数，用得到的值来决定 Sharding ID，这样的好处是，每一个 key 的分布几乎是随机的，所以分布是均匀的分布，所以对于写压力比较大、同时读基本上是随机读的系统来说更加友好，因为写的压力可以均匀的分散到集群中，但是显然的，对于 range scan 这样的操作几乎没法做。比较典型的 Hash Sharding 策略的系统如：Cassandra 的一致性 Hash，Redis Cluster 和 Codis 的 Pre-sharding 策略，Twemproxy 有采用一致性 Hash 的配置。当然这两种策略并不是孤立的，可以灵活组合，比如可以建立多级的 Sharding 策略，最上层用 Hash ，每一个 Hash Sharding 中，数据有序的存储。在做动态扩展的时候，对于 Range 模型的系统会稍微好做一些，简单来说是采用分裂，比如原本我有一个 [1, 100) 的 Range Region，现在我要分裂，逻辑上我只需要简单的将这个 region 选取某个分裂点，如分裂成 [1,50), [50, 100) 即可，然后将这两个 Region 移动到不同的机器上，负载就可以均摊开。但是对于 Hash 的方案来说，做一次 re-hash 的代价是挺高的，原因也是显而易见，比如现在的系统有三个节点，现在我添加一个新的物理节点，此时我的 hash 模的 n 就会从 3 变成 4，对于已有系统的抖动是很大，尽管可以通过 ketama hash 这样的一致性 hash 算法尽量的降低对已有系统的抖动，但是很难彻底的避免。###Sharding 与高可用方案结合 选择好了 sharding 的策略，那剩下的就是和高可用方案结合，不同的复制方案达到的可用性及一致性级别是不同的。很多中间件只是简单的做了 sharding 的策略，但是并没有规定每个分片上的数据的复制方案，比如 redis 中间件 twemproxy 和 codis，MySQL 中间件 cobar 等，只是在中间层进行路由，并未假设底层各个存储节点上的复制方案。但是，在一个大规模存储系统上，这是一个很重要的事情，由于支持弹性伸缩的系统一般来说整个系统的分片数量，数据分片的具体分布都是不固定的，系统会根据负载和容量进行自动均衡和扩展，人工手动维护主从关系，数据故障恢复等操作在数据量及分片数量巨大的情况下几乎是不可能完成的任务。选择一个高度自动化的高可用方案是非常重要的。在 TiKV 中，我们选择了按 range 的 sharding 策略，每一个 range 分片我们称之为 region，因为我们需要对 scan 的支持，而且存储的数据基本是有关系表结构的，我们希望同一个表的数据尽量的在一起。另外在 TiKV 中每一个 region 采用 Raft 算法在多个物理节点上保证数据的一致性和高可用。从社区的多个 Raft 实现来看，比如 Etcd / LogCabin / Consul 基本都是单一 raft group 的实现，并不能用于存储海量的数据，所以他们主要的应用场景是配置管理，很难直接用来存储大量的数据，毕竟单个 raft group 的参与节点越多，性能越差，但是如果不能横向的添加物理节点的话，整个系统没有办法 scale。scale 的办法说来也很简单，采用多 raft group，这就很自然的和上面所说的 sharding 策略结合起来了，也就是每一个分片作为一个 raft group，这是 TiKV 能够存储海量数据的基础。但是管理动态分裂的多 raft group 的复杂程度比单 group 要复杂得多，目前 TiKV 是我已知的开源项目中实现 multiple raft group 的仅有的两个项目之一。正如之前提到过的我们采用的是按照 key range 划分的 region，当某一个 region 变得过大的时候（目前是 64M），这个 region 就会分裂成两个新的 region，这里的分裂会发生在这个 region 所处的所有物理节点上，新产生的 region 会组成新的 raft group。###总结 构建一个健壮的分布式系统是一个很复杂的工程，上面提到了在 TiKV 在实践中的一些关键的设计和思想，希望能抛砖引玉。因为 TiKV 也是一个开源的实现，作为 TiDB 的核心存储组件，最近也刚发布了 Beta 版本，代码面前没有秘密，有兴趣深入了解的同学也可以直接阅读源码和我们的文档，谢谢大家。##Q&amp;amp;A Q1：如何在这个 region 的各个副本上保证分裂这个操作安全的被执行？ 其实这个问题比较简单，就是将 split region 这个操作作为一个 raft log，走一遍 raft 状态机，当这个 log 成功 apply 的时候，即可以认为这个操作被安全的复制了（因为 raft 算法干得就是这个事情）。确保 split log 操作被 accept 后，对新的 region 在走一次 raft 的选举流程（也可以沿用原来的 leader，新 region 的其他节点直接发心跳）。split 的过程是加上网络隔离，可能会产生很复杂的 case，比如一个复杂的例子：a, b 两个节点，a 是 leader, 发起一个分裂 region 1 [a, d) -&amp;gt; region 1 [a, b) + region 2 [b, d), region 2的 heartbeart 先发到 b，但这时候 region 2 分裂成了 region 2 [b, c) + region 3 [c, d)，给 b 发送的 snapshot 是最新的 region 2 的 snapshot [b, c)，region 1的 split log 到了 b，b 的老 region 1 也分裂成了 region 1 [a, b) + region 2 [b，d), 这之后 a 给 b 发送的最新的 region 2 的 snapshot [b, c) 到了，region 2 被 apply 之后，b 节点的 region 2 必须没有 [c, d) 区间的数据。Q2：如何做到透明？ 在这方面，raft 做得比 paxos 好，raft 很清晰的提供了 configuration change 的流程，configuration change 流程用于应对 raft gourp 安全的动态添加节点和移除节点，有了这个算法，在数据库中 rebalance 的流程其实能很好的总结为：对一个 region: add replica / transfer leadership / remove local replica这三个流程都是标准的 raft 的 configuration change 的流程，TiKV 的实现和 raft 的 paper 的实现有点不一样的是：config change 的 log 被 apply 后，才会发起 config change 操作，一次一个 group 只能处理一个 config change 操作，避免 disjoint majority，不过这点在 diego 的论文里提到过。主要是出于正确性没问题的情况下，工程实现比较简单的考虑。 另外这几个过程要做到业务层透明，也需要客户端及元信息管理模块的配合。毕竟当一个 region 的 leader 被转移走后，客户端对这个 region 的读写请求要发到新的 leader 节点上。客户端这里指的是 TiKV 的 client sdk，下面简称 client , client 对数据的读写流程是这样的：首先 client 会本地缓存一份数据的路由表，这个路由表形如：{startKey1, endKey1} -&amp;gt; {Region1, NodeA} {startKey2, endKey2} -&amp;gt; {Region2, NodeB} {startKey3, endKey3} -&amp;gt; {Region3, NodeC} … client 根据用户访问的 key，查到这个 key 属于哪个区间，这个区间是哪个 region，leader 现在在哪个物理节点上，然后客户端查到后直接将这个请求发到这个具体的 node 上，刚才说过了，此时 leader 可能已经被 transfer 到了其他节点，此时客户端会收到一个 region stale 的错误，客户端会向元信息管理服务请求然后更新自己的路由表缓存。这里可以看到，路由表是一个很重要的模块，它需要存储所有的 region 分布的信息，同时还必须准确，另外这个模块需要高可用。另一方面，刚才提到的数据 rebalance 工作，需要有一个拥有全局视角的调度器，这个调度器需要知道哪个 node 容量不够了，哪个 node 的压力比较大，哪个 node region leader 比较多？以动态的调整 regions 在各个 node 中的分布，因为每个 node 是几乎无状态的，它们无法自主的完成数据迁移工作，需要依靠这个调度器发起数据迁移的操作（raft config change）。大家应该也注意到了，这个调度器的角色很自然的能和路由表融合成一个模块，在 Google Spanner 的论文中，这个模块的名字叫 Placement Driver， 我们在 TiKV 中沿用了这个名称，简称 pd，pd 主要的工作就是上面提到的两项：1. 路由表 2. 调度器。 Spanner 的论文中并没有过多的介绍 pd 的设计，但是设计一个大规模的分布式存储系统的一个核心思想是一定要假设任何模块都是会 crash 的，模块之间互相持有状态是一件很危险的事情，因为一旦 crash，standby 要立刻启动起来，但是这个新实例状态不一定和之前 crash 的实例一致，这时候就要小心会不会引发问题. 比如一个简单的 case ：因为 pd 的路由表是存储在 etcd 上的，但是 region 的分裂是由 node 自行决定的 ( node 才能第一时间知道自己的某个 region 大小是不是超过阈值)，这个 split 事件如果主动的从 node push 到 pd ，如果 pd 接收到这个事件，但是在持久化到 etcd 前宕机，新启动的 pd 并不知道这个 event 的存在，路由表的信息就可能错误。我们的做法是将 pd 设计成彻底无状态的，只有彻底无状态才能避免各种因为无法持久化状态引发的问题。每个 node 会定期的将自己机器上的 region 信息通过心跳发送给 pd, pd 通过各个 node 通过心跳传上来的 region 信息建立一个全局的路由表。这样即使 pd 挂掉，新的 pd 启动起来后，只需要等待几个心跳时间，就又可以拥有全局的路由信息，另外 etcd 可以作为缓存加速这一过程，也就是新的 pd 启动后，先从 etcd 上拉取一遍路由信息，然后等待几个心跳，就可以对外提供服务。但是这里有一个问题，细心的朋友也可能注意到了，如果集群出现局部分区，可能某些 node 的信息是错误的，比如一些 region 在分区之后重新发起了选举和分裂，但是被隔离的另外一批 node 还将老的信息通过心跳传递给 pd，可能对于某个 region 两个 node 都说自己是 leader 到底该信谁的？在这里，TiKV 使用了一个 epoch 的机制，用两个逻辑时钟来标记，一个是 raft 的 config change version，另一个是 region version，每次 config change 都会自增 config version，每次 region change（比如split、merge）都会更新 region version. pd 比较的 epoch 的策略是取这两个的最大值，先比较 region version, 如果 region version 相等则比较 config version 拥有更大 version 的节点，一定拥有更新的信息。"},
		{"url": "https://pingcap.com/weekly/2016-08-12-tidb-weekly/",
		"title": "Weekly update (August 05 ~ August 12, 2016)", 
		"content": " Weekly update (August 05 ~ August 12, 2016) Last week, we landed 20 PRs in the TiDB repositories and 11 PRs in the TiKV repositories.Notable changes to TiDB  Rewrite Lexer and improve the speed of parsing SQL texts by 40%. Add a command line flag for log output file and rotate log files regularly. Optimize the 2 phase commit process and adopt faster methods to clear locks. Optimize the execution speed of the Insert On Duplicate Update statement. Update the Documents repository. Fix several bugs.  Notable changes to TiKV  Support the Scan and Resolve transaction lock. Support garbage collection(GC) stale peer. Use a Write column family to store transaction commit logs. Remove the unnecessary Seek operation. Fix random quorum test.  Notable changes to Placement Driver  Support the get PD leader API.  "},
		{"url": "https://pingcap.com/meetup/memoir/meetup-2016-08-11/",
		"title": "TiDB 优化器实现的基础 -- 统计信息的收集", 
		"content": " 收集统计信息的意义 一个 SQL 数据库里，优化器实现的好坏对性能的影响是决定性的。一个未经优化的执行计划和经过充分优化后的执行计划，执行时间的差别往往是成千上万倍。而对一个 SQL 优化器来说，统计信息是必不可少的条件，只有依赖统计信息提供的数据，优化器才可以正确估算不同的执行计划的执行代价，以选择最优的执行计划。就像一个大厨无论多么优秀，没有上等食材也是无法做出美味的饭菜。统计信息包含的内容 统计信息有两类，包括 Table 统计信息和 Column 统计信息。Table 统计信息包含 Row Count 和 Row Size。Column 统计信息包含 Null Count，Max Value，Min Value，Distinct Count 以及 Histogram。其中 Histogram 用来记录这个 Column 的数据详细分布，可以用来估算大于、小于或等于某个 Value 的 Row Count。统计信息采集的步骤  在 TiDB 执行 ANALYZE TABLE 语句，手动触发收集动作。  我们知道，一个 Table 的数据往往是在不断变化的，我们无法保证统计信息时刻保持在最新状态，总会有一定的误差，如果我们不及时更新，随着时间的推进，这个误差可能会越来越大，影响到优化器的优化效果。有时我们会需要让统计信息更新的频率低一些来降低系统的压力，因为每次的统计信息收集都是开销很大的操作。有时我们会需要立即更新统计数据，因为我们刚刚向一个表导入了大量的数据，马上就需要查询。所以定期更新统计信息的功能，我们希望可以用独立的模块，用更灵活的策略来实现，TiDB 本身只需要支持基本的手动触发就可以了。 全表扫描。  全表扫描的执行过程比较长，整个扫表的任务会被分解为一系列 Region 的小请求，每个 Region 的请求会返回该 Region 包含的 Table 的部分数据。 用扫描得到的数据，记录 Row Count 和 Row Size，并对数据采样。  扫描得到的数据量有可能会非常大，以至于无法把全部数据保留在内存里进行处理，所以需要进行采样，当采样的概率均匀的时候，计算生成的统计信息的误差是可以接受的。这里我们设定的采样数是 1 万，无论 Table 有多大，最后保留的样本数不会超过 1 万。后面会详细介绍采样时使用到的算法。 采样数据生成 Column 统计信息，并保存到 KV。  采样得到的数据会进行计算生成 Histogram。采样算法 要得到一个均匀分布的采样池，一个最简单的算法是，当我扫描整个表的时候，读到的每一行，都以一个固定的概率决定是否加入采样池，这个概率 P =（采样池大小/表的行数）。但是由于在扫描之前，我们并不知道一个表总共有多少行，所以如果使用这个算法进行采样，就需要扫描两次，第一次获取整个表的行数，第二次进行真正的采样。一个更优化的算法，蓄水池算法，可以在表的大小未知的情况下，一次扫描得到均匀分布的采样池。 算法的实现  假如我们的样本池大小为 M = 100 ，从头开始扫描全表，当我们读到的记录个数 K 小于 100 时，我们把每一条记录都加入采样池，这样保证了在记录总数小于采样池大小时，所有记录都会被选中。我们继续扫描，当我们扫描到的第 K = 101 条时，我们用概率 P = (M/K) = (100&amp;frasl;101) 决定是否把这个新的记录加入采样池，如果加入了采样池，采样池的总数会超过 M 的限制，这时我们需要随机选择一个旧的采样丢掉，保证采样池大小不会超过限制。执行这样采样算法一直到全表扫描结束，我们可以得到一个均匀分布的采样池。 算法的证明  这个算法可以用归纳法来证明，如果表的大小 N &amp;lt;= K，所有记录都会放入采样池，满足均匀分布的要求。现在假设当读取完 K 个元素时，采样池满足均匀分布的要求，采样概率 P = (M / K)，当读取第 K + 1 个记录时，我们应用蓄水池算法，以 P&amp;rsquo; = M / (K + 1)) 的概率决定是否加入采样池，这时出现了两种情况，被加入和没有被加入，我们继续分析这两种情况下，这条记录被选中的概率。如果记录没有被选中，旧样本被保留概率是 Po1 =（1 - P&amp;rsquo;） P = (1 - M / (K + 1)) (M / K)， 新样本被保留的概率是 Pn1 = 0。如果记录被选中，采样池中已有的采样有 (M - 1) / M 的概率被保留，旧样本概率是 Po2 = P&amp;rsquo; P Po = (M / (K + 1)) (M / K) （ (M -1) / M)，新样本的整体概率是 Pn2 = P&amp;rsquo; = M / (K + 1)。把两种情况下的概率相加， 得到旧采样被保留的概率 Po = Po1 + Po2 = （1 - M / (K + 1)) (M / K) + (M / (K + 1)) (M / K) *（ (M -1) / M) = M / (K + 1)，新采样被保留的概率为 Pn = Pn1 + Pn2 = 0 + M / (K + 1) = M / (K + 1)，所有采样被保留的概率都是 M / (K + 1)，满足均匀分布的要求。Histogram Histogram 的类型主要有两种，Frequency Histogram 和 Height-Balanced Histogram。当 NDV &amp;lt; Bucket Count 时，Frequency 可以包含全部 value 分布信息，每一个 distinct value 占用一个 bucke。但是当 NDV 大于 bucket count 时，Frequency Histogram 没有足够的 bucket 存放 value，我们就需要用另外的 Histogram 类型，比如 Height-Balanced Histogram。Height-Balanced Histogram 把所有 value 从小到大排序，平均放到所有 bucket 里，但是缺点是无法记录有哪些 popular value。Oracle 还实现了一种 Hibrid Histogram，综合了 Frequency Histogram 和 Height-Balanced Histogram 的优点，TiDB 实现的 Histogram 主要参考的就是 Oracle 的 Hibraid Histogram。Hibrid Histogram 包含 N 个 bucket，我们设定的 N 的默认值是 256，每个 bucket 包含三个字段 &amp;lsquo;number&amp;rsquo;，&amp;rsquo;value&amp;rsquo; 和 &amp;lsquo;repeat&amp;rsquo;，‘number’ 代表放在这个 bucket 里的最后一个 value 在 table 里排序后的 offset，&amp;rsquo;value&amp;rsquo; 就是放在这个 bucket 里的最大的那一个 value，&amp;rsquo;repeat&amp;rsquo; 代表最大的 value 的个数。 Hibrid Histogram 在生成的过程中，如果一个 bucket 装满了，遇到下一个 value 的时候，比较一下这个新的 value 和前一个 value 是否相等，如果相等，增加 repeat 值，直到遇到一个更大的 value，换下一个 bucket 存放这个 value，这样保证任何一个 value 只会在 一个 bucket 内存在，相比 Height-Balanced Histogram，可以包含更准确的 value 分布信息。我们用一个实例来说明 Hibrid Histogram 是如何存储 value 分布信息的。给定 value 集合 [&amp;lsquo;a&amp;rsquo;, &amp;lsquo;a&amp;rsquo;, &amp;lsquo;b&amp;rsquo;, &amp;lsquo;c&amp;rsquo;, c&amp;rsquo;, &amp;lsquo;c&amp;rsquo;, &amp;lsquo;c&amp;rsquo;, &amp;rsquo;d&amp;rsquo;, &amp;rsquo;d&amp;rsquo;, ‘e’] 和 bucket count 3，生成的 histogram 如下：[number, value, repeat] [2, &amp;#39;b&amp;#39;, 0] [6, &amp;#39;c&amp;#39;, 3] [9, &amp;#39;e&amp;#39;, 0] 我们可以看到这个集合内， ’c&amp;rsquo; 的个数有 4 个，在第二个 bucket 里准确记录了 ‘c&amp;rsquo; 的 repeat 数量 3，这样我们在查询条件为 where column = &amp;lsquo;c&amp;rsquo; 的时候，就可以准确的估算执行开销。统计信息的收集，使 TiDB 的优化器掌握数据分布详情，准确估算执行开销，从而实现高效的 CBO (cost base optimization)。"},
		{"url": "https://pingcap.com/meetup/meetup-2016-08-06/",
		"title": "PingCAP 第 19 期 NewSQL Meetup", 
		"content": " PingCAP 第 19 期 NewSQL Meetup 2016-08-06 方君&amp;amp;韩飞 PingCAP PingCAPPingCAP ![]() 微信号pingcap2015功能介绍PingCAP 专注于新型分布式数据库的研发，是知名开源数据库 TiDB (GitHub 总计10000+ stars ) 背后的团队，总部设在北京，是国内第一家开源的新型分布式关系型数据库公司、国内领先的大数据技术和解决方案提供商。NewSQL Meetup今天是 PingCAP 第 19 期 Meetup，主题是百度基础架构部工程师方君分享的《What&amp;rsquo;s New in Spark 2.0》以及韩飞分享 的《An Overview of Cost Based Optimization and Join Reorder》。▌ ****Topic 1：What&amp;rsquo;s New in Spark 2.0Lecture：方君，百度基础架构部工程师，专注于分布式计算与流式计算领域，目前在百度负责 Spark 计算平台和计算表示层的相关工作。Content: DataSet API Performance Optimization Structure Streaming  ▌ ****Topic 2：An Overview ofCost Based Optimization and Join ReorderContent:自从 System R 优化框架面世，基于 interesting order 的动态规划算法一直是大部分优化器采用的基础算法。本次分享介绍了优化器在没有 histogram 信息下的代价估计算法，以及举例说明 TiDB 中的动态规划算法实现。最近有好多小伙伴在微信后台留言，想加入到我们的 Meetup 中来。在这里统一答复大家：我们的 Meetup 是每周六上午十点，在 PingCAP 公司内开讲哦。有兴趣的小伙伴届时带着你们对技术满满的热情来参加就好啦 :)PingCAP Meetup"},
		{"url": "https://pingcap.com/weekly/2016-08-05-tidb-weekly/",
		"title": "Weekly update (July 30 ~ August 05, 2016)", 
		"content": " Weekly update (July 30 ~ August 05, 2016) Last week, we landed 28 PRs in the TiDB repositories and 32 PRs in the TiKV repositories.Notable changes to TiDB  Add support for constant folding in the SQL optimizer. Optimize the query speed of the secondary index. In certain scenarios, only the data in the index needs to be read. Use dynamic programing to decide the join path for multiple tables. Adjust the command line to support setting the listening address. Prometheus metrics and golang pprof share the same port. Update the Documents repository. Fix several bugs.Notable changes to TiKV Split the operations on RocksDB from scheduler thread to a worker pool. Support leader lease mechanism when the quorum check is enabled. Use pending snapshot regions check to avoid receiving multiple overlapping snapshots at the same time. Check down peers and report to Placement Driver(PD). Support time monitor to check whether time jumps back or not.  Notable changes to Placement Driver  Reduce the &amp;ldquo;1234&amp;rdquo; and &amp;ldquo;9090&amp;rdquo; ports in PD. Now PD has only &amp;ldquo;2379&amp;rdquo; and &amp;ldquo;2380&amp;rdquo; ports for external use. Support the join flag to let PD join an existing cluster dynamically. Support the remove PD member API to remove a PD from a cluster dynamically. Support the list PD members API.  "},
		{"url": "https://pingcap.com/blog-cn/cloud-native-db/",
		"title": "云时代数据库的核心特点", 
		"content": " 引言 最近几年，随着云计算相关技术的发展，各种不同类型的云层出不穷，服务越来越多不同类型的企业业务，传统企业也渐渐开始探索上云的道路。在云上，作为业务最核心的数据库，相比之前的传统方案会有哪些变化呢？在正式聊云时代的数据库特点之前，我们需要了解一下目前云时代架构发生的变化。畅想一下，未来的服务都跑在云端，任何的服务资源都可以像水电煤一样按需选购。从 IaaS 层的容器/虚拟机，到 PaaS 层的数据库，缓存和计算单元，再到 SaaS 层的不同类型的应用，我们只需要根据自身业务特点进行资源选配，再也不用担心应用服务支撑不住高速的业务增长，因为在云上一切都是弹性伸缩的。有了可靠的基础软件架构，我们就可以把更多精力放到新业务的探索，新模式的创新，就有可能产生更多不一样的新场景，从而催生更强大能力的云端服务，这是一件多么 cool 的事情。当然，理想要一步一步实现，未来的基础软件栈到底会怎样呢？社区在这方面正在进行积极地探索，其中最有代表性的就是基于容器（以 Docker 为代表）的虚拟化技术和微服务（Microservice）。在云时代，一切都应该是可伸缩的，使用 k8s（Kubernetes）在保证资源平衡的前提下，通过 Docker 部署我们依托于容器的微服务模块，我们不用关心服务到底跑在哪里，只需要关心我们需要多少服务资源。Docker 提供了极大的便利性，一次构建，到处运行，我们可以很好地解决开发、测试和上线的环境一致性问题。（如果不能很好地保证测试和实际上线环境的一致性，则很有可能需要花费远超过开发的时间去发现和修复问题。）k8s 更是在 Docker 构建的基础上增加了更多的云特性，包括 Docker 的升级，高可用和弹性伸缩等等。 关于 Docker/k8s 相关的讨论已经很多了，因为时间关系，关于具体的细节就不再展开。我们只需要了解，有了它，可以很轻松地解决服务的安装和部署。下面再聊聊微服务，微服务将一个服务拆分成相对独立的更小的子服务单元，不同的子服务单元之间通过统一的接口（HTTP/RPC 等）进行数据交互。相比于传统的解决方案，这种架构有很多的优点。 更好的开发效率和可维护性。微服务将一个单独的服务进行更细力度的拆分，每一个子服务单元专注于更小的功能模块，可以更好地根据业务建立对应的数据模型，降低复杂度，使得开发变得更轻松，维护和部署变得更加友好. 更好的可扩展性。每个不同的子服务单元相互独立，彼此之间没有任何依赖，所以可以根据业务的具体需要，灵活地部署多个子服务单元进行水平扩展。 更强的容错性。当其中一个子服务出现故障的时候，可以通过辅助的负载均衡工具，自动路由到其他的子服务，不会影响整体服务的可用性.  当然，微服务也不是一个银弹，相对来说，这种方案会使整体系统的设计更加复杂，同时也加大了网络的延迟，对整个系统测试的复杂度也会更高。Docker 提供的隔离型和可移植性，与微服务是一种天然的契合，微服务将整个软件进行拆分和解耦，而通过 Docker/k8s 可以很自然地做到独立的部署，高可用和容错性，似乎一切都可以完美地运转起来。但是真的是这样么？我们是不是忽略了什么？是的，我们在讨论前面的问题的时候忽略了一个很重要的东西：状态。从整个技术发展的角度来看，微服务是一个非常有意义的探索。每个人都期望着每个微服务的子服务都是无状态的，这样我可以自由地启停和伸缩，没有任何的心智负担，但是现实的业务情况是什么样的呢？比如一个电商网站，用户正在下单购买一件商品，此时平台是通过订单子服务的 A 应用来提供服务的，突然，因为机器故障，订单子服务的 A 应用不可用了，改由订单子服务的 B 应用提供服务，那么它是必须要知道刚才用户的订单信息的，否则正在访问自己订单页面的用户会发现自己的订单信息突然不见了。虽然我们尽量想把子服务设计成无状态的，但是很多时候状态都是不可避免的，我们不得不通过存储层保存状态，业界最主要的还是各种数据库，包括 RDBMS 和 NoSQL，比如使用 MySQL、MongoDB、HBase、Cassandra 等，特别是有些场景还要考虑数据一致性问题的时候，更加重了对存储层的依赖。由此可见，云计算时代系统的架构发生了巨大的变化，这一方面为用户提供了更优秀的特性，另一方面也对云计算的组件提出了更高的要求。数据库作为云计算最基础的组件之一，也需要适应这种架构的变化。（这里我们主要关注 SQL 数据库，云时代的数据库以下简称云数据库。）那么云数据库主要有一些什么样的特点呢？我认为主要有以下几点。 弹性伸缩 传统的数据库方案，常见的会选用 Oracle，MySQL，PostgreSQL。在云时代，数据量的规模有爆发性的增长，传统的数据库很容易遇到单机的存储瓶颈，不得不选用一些集群方案，常见的比如 Oracle RAC、 MySQL Sharding 等，而这些集群方案或多或少都有一些不令人满意的地方。比如说，Oracle RAC 通过共享存储的硬件方案解决集群问题，这种方式基本上只能通过停机换用更大的共享内存硬件来解决扩容问题，RAC 节点过多会带来更多的并发问题，同样也会带来更高的成本。以 MySQL Sharding 为代表的数据分片方案，很多时候不得不提前对数据量进行规划，把扩容作为很重要的一个计划来做，从 DBA 到运维到测试到开发人员，很早之前就要做相关的准备工作，真正扩容的时候，为了保证数据安全，经常会选择停服务来保证没有新的数据写入，新的分片数据同步后还要做数据的一致性校验。当然业界大公司有足够雄厚的技术实力，可以采用更复杂的方案，将扩容停机时间尽量缩短（但是很难缩减到 0），但是对于大部分中小互联网公司和传统企业，依然无法避免较长时间的停服务。在云时代，理想中所有的资源都是根据用户业务需求按需分配的，服务器资源，应用容器资源，当然也包括数据库资源。添加或者减少新的数据库资源，完全就像日常吃饭那样稀疏平常，甚至用户基本感知不到。比如作为一个电商用户，在双 11 促销活动之前，可以通过增加数据库节点的方式，扩大更多的资源池，用来部署相应的容器服务，当活动结束之后，再将多余的资源移除去支持其他的服务，这样可以极大地提高资源的利用率，同样可以弹性地支撑各种峰值业务。高可用 传统的 MySQL 方案，数据复制的时候默认采用异步的方式，对于一个写入的请求，主库写入成功后就会返回成功信息给客户端，但是这个时候数据可能还没有同步给从库，一旦主库这个时候挂掉了，启动从库的时候就会有丢失数据的风险。当然，也有人会选择半同步的复制方式，这种方式在正常情况下是同步的，但是在遇到数据压力比较大的时候，依然会退化为异步的方式，所以本质上来说，同样有丢失数据的风险。其他也有一些多主的同步方案，比如在应用层做数据同步，但是这种方式一是需要应用层的配合，二是在对网络超时的处理非常复杂，增加心智负担。在云时代，因为所有的数据库资源都是分布式存储的，每个数据库节点出现问题都是很正常的事情，所以就必须有一种可以实现数据一致性的数据复制方式来保证服务的高可用，业界给出的答案就是：Paxos/Raft（关于 Paxos 和 Raft 的实现细节我们不在这里展开）。PingCAP 在做的 TiDB 就是选择了 Raft 协议，Raft 协议看起来更像是一个多副本的自适应的主从复制协议，对于每次写请求，Raft 都会保证大多数写成功才会返回客户端，即使 Raft Group的Leader 挂掉了，在一个有限的时间范围内，会很快地选出一个新的 Leader 出来，继续提供服务。同样，对于一个 3 副本的 Raft Group，只要 2 个写入成功，就可以保证成功，而大多数情况下，最先写入成功的往往是与 Leader 网络情况最好的那个副本，所以这种 Majority 写的方式，可以很自然地选择速度最快的副本进行数据同步复制。另外，Raft 协议本身支持 Config Change，增加一个新的节点，可以很容易地做副本数据分布的变更，而不需要停止任何服务。同样，在云时代，数据库的 DDL 操作也会是一个非常有趣的事情。以一个常见的 Add Column 操作为例，在表规模已经很大的情况下，在传统的实现方案中，比较有参考意义的是，通过一些工具，创建类似表级别的触发器，将原表的数据同步到一个新的临时表中，当数据追平的时候，再进行一个锁表操作，将临时表命名为原表，这样一个 Add Column 操作就完成了。但是在云时代，分布式的数据存储方式决定了这种方案很难实现，因为每个数据库节点很难保证 Schema 状态变更的一致性，而且当数据规模增长到几十亿，几百亿甚至更多的时候，很短的阻塞时间都有可能会导致很大的负载压力变化，所以 DDL 操作必须是保证无阻塞的在线操作。值得欣慰的是，Google 的 F1 给我们提供了很好的实现参考，TiDB 即是根据 F1 的启发进行的研发，感兴趣的同学可以看下相关的内容。易用透明 我们可以将云数据库想象成一个提供无限大容量的数据库，传统数据库遇到单机数据存储瓶颈的问题将不复存在。已有的程序基本上不怎么需要修改已有的代码，就可以很自然地接入到云数据库中来获得无限 Scale 的能力。增减数据库节点，或者节点的故障恢复，对于应用层来说完全透明。另外，云数据库的监控、运维、部署、备份等等操作都可以在云端通过高效的自动化工具来自动完成，极大地降低了运维成本。多租户 云数据库本身应该是可以弹性伸缩的，所以很自然的，从资源利用率的角度来考虑，多个不同用户的数据库服务底层会跑在一个共享的云数据库中。因此多租户技术会成为云数据库的标配。但是这里面就有一个不得不面对的问题，如何做到不同用户的隔离性？用户数据隔离是相对比较容易的，比如还是以电商用户（这里说的是电商企业，不是顾客客户）为例，每个用户都有一个唯一的 ID，这样在云数据库的底层存储中，可以保证每个用户数据都带有自己 ID 前缀，用户登陆进来的时候可以根据这个前缀规则，获取他对应的数据，同时他看不到其他用户的数据。在一个真实的多租户环境下面，纯粹的数据隔离往往是不够的，你还需要做到资源公平性的隔离。比如有的用户写一个 SQL，这个 SQL 没有做优化，主要做的事情是一个全表描扫，这个表的数据量特别特别大，这样他会吃掉很多的 CPU、Memory、IO 等资源，导致其他用户很轻量级的 SQL 操作都可能会变得很慢，影响到其他用户实际的体验。那么针对这种情况怎么做隔离？与此类似的还有，网络带宽怎么做隔离？大家都是跑在一个云数据库上面的，如果一个用户存放的数据特别大，他把带宽都吃掉了，别人就显得非常慢了。还有一种情况，如果我本身作为一个租户，内部又怎么做隔离，大家知道 MySQL 可以建很多 Database，不同的 Database 给不同的团队来用，那么他们之间内部隔离又怎么做，这个问题就进一步更加复杂了。目前来讲没有特别好的方法，在一个分布式的环境下面去做很好的隔离，有两个方向可以考虑：第一种是最简单也是有效的方法，制定一些规则，把某些用户特别大的数据库表迁移到独享的服务器节点上面，这样就不会影响其他用户的服务，但是这里面就涉及到定制化的事情了，本身理念其实与云数据库并不相符。第二种就是依靠统计信息，做资源隔离和调度，但是这里面对技术的要求就比较高了。因为云数据库是分布式的，所以一般的统计都要横跨很多的机器，因为网络原因，不可能做到完全准确的统计，所有统计都是有延迟的。比如说对于某个用户，现在统计到的流量是 1 个 G，他可能突然就有一次峰值的网络访问，可能下一次统计消耗的流量是 5 个 G（这里面只是举例说明问题），如果你给他流量限制是 1 个 G，中间统计的间隔是多少比较合适，如果间隔比较小，那么这个对整个系统的压力就比较大，可能影响正常的用户 SQL 访问，另外本身这个流量限制的系统也是很复杂的系统。调度算法一直是整个分布式系统领域很困难的一个问题，如何做到隔离性和公平调度也是未来云数据库非常有挑战的一个事情。低成本 低成本应该是云时代基础设施最明显的特点。首先，云数据库的高可用和容错能力，使得我们不再需要昂贵的硬件设备，只需要普通的 X86 服务器就可以提供服务。然后，受益于 Docker 的虚拟化技术，使得不同类型的应用容器可以跑在同一个物理机上，这样可以极大地提高资源的利用率。其次，多租户的支持，使得不同的用户可以共用一套底层的数据库存储系统，在数据库层面再一次提高了资源的利用效率。再次，云数据库的自动化运维工具，降低了整个核心数据库的运维成本。最后，云数据库资源是按需分配的，用户完全可以根据自身的业务特点，选购合适的服务资源。高吞吐 云数据库虽然可以做到弹性扩容，但是本身是分布式存储的，虽然可以通过 Batch Write、Pipeline 和 Router Cache 等方式加快访问 SQL 请求的数据，但是相对传统单机的数据库来说，在数据访问链路上至少也要多走一次网络，所以大部分并发量不大的小数据量请求，都会比单机延迟要高一些。也就是说，当没有足够高的并发 SQL 访问的话，其实不能完全体现云数据库的性能优势，所以这也是我们在选用云数据库的时候需要认识到的问题，云数据库更多的是追求高吞吐，而不是低延迟。当并发大到一定规模，云数据库高吞吐特性就显现出来了，即使在很高的并发下，依然可以维持相当稳定的延迟，而不会像单机数据库那样，延迟线性增长。当然，延迟的问题，在合理的架构设计方案下，可以通过缓存的方式得到极大的缓解。数据安全 云数据库的物理服务器分布在多个机房，这就为跨数据库中心的数据安全提供了最基础的硬件支持。谈到金融业务，大家耳熟能详的可能就是两地三中心，比如北京有两个机房，上海有一个。未来一切服务都跑在云上，金融类的业务当然也不例外。相比其他业务，金融类业务对数据安全要求就要高得多。当然，每个公司内部都有核心的业务，所以如果上云的话，也会有同样的强烈需要。这样，对云数据库来说，数据的一致性、分布式事务、跨数据中心的数据安全等更高端的需求有可能会日益强烈。常见的数据备份也有可能会被其他新的模式所取代或者弱化，比如基于 Paxos/Raft 的多副本方案，本身就保证了会有多份备份。自动负载平衡 对于云数据库来说，负载平衡是一个很重要的问题，它直接决定了整个云数据库系统性能的好坏，如果一个数据库节点的数据访问过热的话，就需要考虑把数据迁移到其他的数据库节点来分担负载，不然就很容易出现性能瓶颈。整个负载平衡是一个动态的过程，调度算法需要保证资源配比的最大平衡，还有保证数据迁移的过程对系统整体的负载影响最小。这在未来也是云数据库需要解决的一个核心问题。小结 从目前已有的 SQL 数据库实现方案来看，NewSQL 应该是最贴近于云数据库理念的实现。NewSQL 本身具有 SQL、ACID 和 Scale 的能力，天然就具备了云数据库的一些特点。但是，从 NewSQL 到云数据库，依然有很多需要挑战的难题，比如多租户、性能等。上面提到的一些云数据库的特点，也是 PingCAP 目前在着力实现的部分，TiDB 作为国内第一个 NewSQL 的开源项目，在与社区的共同努力下，我们在上月底刚刚发布了 Beta 版本，欢迎各位上 GitHub 了解我们。随着整个社区技术水平的发展和云时代新的业务需求的驱动，除了 PingCAP 的 TiDB，相信会有更多的团队在这方面进行探索， 期待早日看到云数据库成熟的那一天。Q&amp;amp;A 问：由于客户数据环境复杂多样，在迁移到云端的时候怎么怎么做规划，以便后期统一运维管理？或者说，怎么把用户 SQL Server 或者 MongoDB 逐渐迁移到 TiDB 之类的分布式数据库？ 崔秋：因为每个业务场景都不太相同，所以在选用云端服务的时候，首先要了解自身业务和云服务具体的优缺点。 如果你的业务本身比较简单，比如你之前用的 MongoDB，现在很多云服务厂商都会提供云端的 MongoDB 服务。这个时候你就要根据业务特点来做判断，如果 MongoDB 本身容量不大，远期的业务数据不会增长过快的话，这个时候其实你可以直接使用 MongoDB 的服务的。但是如果你本身的数据量比较大，或者数据增长比较快的话，就可能要考虑数据的扩容问题，MongoDB 在这方面做的不是太好。 你可以考虑 SQL 数据库的集群方案。比如 TiDB，它本身是支持弹性扩容，高并发高吞吐和跨数据库中心数据安全的，另外有一点明显的好处是 TiDB 兼容 MySQL 协议，所以如果你的应用程序是使用 MySQL，就基本上可以无缝地迁移到 TiDB，这方面是非常方便的。后续我们会提供常用的数据库迁移工具，帮用户把数据从 MongoDB/SQL Server 等平滑迁移到 TiDB 上面。 还是那个原则，不要为了上云而上云，一定要了解清楚自己的业务特点，云数据库会帮助你提供很多特性，如果真的很适用你的业务的话，就可以考虑。问：但从产品的角度来看，云厂商提供的 RDS 产品是 Copy 客户数据库的思路，或者说是为了支持不同的数据库而支持。请问这种局面以后会有什么改变吗？ 崔秋：现在确实蛮多云数据库服务其实就是在传统的 RDS 上面包了一层自动化的监控，运维和部署工具，就卖给用户提供服务了，但是实际上本身解决的仅仅是自动化管控的问题，云服务提供的数据库特性还是单机的 RDS，或者 RDS Sharing 的特性。如果本身底层的数据库满足不了你的需求的话，云服务也是满足不了的。 如果你需要不停服务的弹性扩容，单机的 RDS 服务肯定是搞不定的，RDS Sharing 也很难帮助你做到，这就对底层的数据库有了更高的要求，当然这方面是 TiDB 的强项了。 现在很多云上的 RDS 产品还远远没有达到理想中的云数据库的要求，不过随着社区的发展和业务需求的推动，我个人觉得，这方面最近几年会有更多的变化。如果对于这方面感兴趣的话，可以关注下 TiDB。问：从 Oracle 分流数据到 TiDB、Oracle 增量修改、Update 的记录，如何同步到 TiDB？有没有工具推荐，比如类似 Ogg？ 崔秋：目前 TiDB 还没有相应的工具。如果真的需要在线从 Oracle 这边分流的话，可以考虑使用 Oracle 的触发器，将数据的变化记录下来，然后转化为 SQL，同步到 TiDB，当然这需要一定开发的工作量。"},
		{"url": "https://pingcap.com/blog-cn/tidb-optimization-for-subquery/",
		"title": "TiDB 中的子查询优化技术", 
		"content": " 子查询简介 子查询是嵌套在另一个查询中的 SQL 表达式，比较常见的是嵌套在 FROM 子句中，如 SELECT ID FROM (SELECT * FROM SRC) AS T。对于出现在 FROM 中的子表达式，一般的 SQL 优化器都会处理的很好。但是当子查询出现在 WHERE 子句或 SELECT 列表中时，优化的难度就会大大增加，因为这时子查询可以出现在表达式中的任何位置，如 CASE...WHEN... 子句等。对于不在 FROM 子句出现的子查询，分为“关联子查询”(Correlated Subquery) 和“非关联子查询”。关联子查询是指子查询中存在外部引用的列，例如：ELECT * FROM SRC WHERE EXISTS(SELECT * FROM TMP WHERE TMP.id = SRC.id) 对于非关联子查询，我们可以在 plan 阶段进行预处理，将其改写成一个常量。因此，本文只考虑关联子查询的优化。一般来说，子查询语句分为三种： 标量子查询（Scalar Subquery），如(SELECT&amp;hellip;) + (SELECT&amp;hellip;) 集合比较（Quantified Comparision），如T.a = ANY(SELECT&amp;hellip;) 存在性测试（Existential Test），如NOT EXISTS(SELECT&amp;hellip;)，T.a IN (SELECT&amp;hellip;)  对于简单的存在性测试类的子查询，一般的做法是将其改写成 SEMI-JOIN。但是很少有文献给出通用性的算法，指出什么样的查询可以“去关联化”。对于不能去关联化的子查询，数据库的做法通常是使用类似 Nested Loop 的方式去执行，称为 correlated execution。TiDB 沿袭了 SQL Server 对子查询的处理思想，引入 Apply 算子将子查询用代数形式表示，称为归一化，再根据 Cost 信息，进行去关联化。Apply 算子 子查询难以优化的原因是，人们通常不能把一个子查询执行表示成一个类似 Projection、Join 这样的逻辑算子。这使得找到一个通用子查询转换的算法是很难的。所以我们第一件要做的事就是，引入一个可以表示子查询的逻辑算子：Apply。Apply 算子的语义是：公式中的 E 代表一个“参数化”的子查询。在每一次执行中，Apply 算子会向关系 R 取一条记录 r，作为参数传入 E 中，然后让 r 和 E&amp;reg; 做 ⊗ 操作。⊗ 会根据子查询类型的不同而不同，通常是半连接 ⋉。对于 SQL 语句：SELECT * FROM SRC WHERE EXISTS(SELECT * FROM TMP WHERE TMP.id = SRC.id) 它的 Apply 算子表示是：对于出现在 SELECT 列表中、GROUP BY 列表中的子查询，道理也是类似的。所以 Apply 是可以表示出现在任意位置的子查询的。去关联化 引入了 Apply，我们就可以将子查询去关联化了。去关联化的规则如下：根据上述规则，你可以将所有的确定性 SQL 子查询去关联化。例如 SQL 语句：SELECT C_CUSTKEY FROM CUSTOMER WHERE 1000000 &amp;lt; (SELECT SUM(O_TOTALPRICE) FROM ORDER WHERE O_CUSTKEY = C_CUSTKEY) 其中两个 CUSTKEY 均为主键。转换成 Apply 之后的表达式为：因为主键的存在，利用规则（9），可以转化为：此时根据规则（2），我们可以彻底消除 Apply，转化为只有连接的 SQL 表达式：再根据外连接化简的原则，可以进一步化简为：利用上述九条规则，理论上已经解决去关联化的问题了。是不是对于所有的情况，去关联化都是最好的呢？答案是否定的。如果 SQL 的结果很小，同时子查询可以利用索引，有时候使用 correlated execution 是最好的。是否去关联化还需要统计信息的帮助。而到了这一步，普通的优化器已经无能为力了。只有 Volcano 或 Cascade Style 的优化器，可以同时考虑逻辑等价规则和代价选择。因此，想要完美解决子查询的问题，要需要优秀的优化器框架的支撑。半连接 TiDB 在去关联化方面，目前只支持将关联子查询改写成半连接和左外半连接。例如，对于查询：SELECT * FROM SRC WHERE EXISTS(SELECT * FROM TMP WHERE TMP.id = SRC.id) TiDB 做出的 Plan 为：当子查询出现在 SELECT 子句当中时：SELECT CASE WHEN EXISTS(SELECT * FROM TMP WHERE TMP.id = SRC.id) THEN 1 ELSE 2 END FROM SRC Projection 算子需要知道 Exists 结果是 True 或者 False。这时需要左外半连接，当然外表匹配时，有一个辅助列 aux 输出 True，当不匹配时，输出 False。对于半连接的算法实现，其实和 Join 差别不大，可以选择 MergeSortJoin，HashJoin，IndexLoopUpJoin，NestedLoop 等等。确定使用 SemiJoin 之后，优化器会根据统计信息选择最合适的算法，这里不再赘述。"},
		{"url": "https://pingcap.com/meetup/memoir/meetup-2016-08-01/",
		"title": "TiDB 中的子查询优化技术", 
		"content": "  子查询优化一直是 SQL 查询优化中非常难的一部分，尤其是关联子查询的改写。TiDB 为了兼容 MySQL，允许用户在任何位置编写子查询。对于非关联子查询，TiDB 会对其进行提前求值，对于关联子查询，TiDB 会尽可能的对其进行去关联化，例如改写成 SemiJoin。本文会重点介绍 TiDB 对关联子查询的优化手段。 子查询简介 子查询是嵌套在另一个查询中的 SQL 表达式，比较常见的是嵌套在 FROM 子句中，如 SELECT ID FROM (SELECT * FROM SRC) AS T。对于出现在 FROM 中的子表达式，一般的 SQL 优化器都会处理的很好。但是当子查询出现在 WHERE 子句或 SELECT 列表中时，优化的难度就会大大增加，因为这时子查询可以出现在表达式中的任何位置，如 CASE&amp;hellip;WHEN&amp;hellip; 子句等。对于不在 FROM 子句出现的子查询，分为“关联子查询”(Correlated Subquery) 和“非关联子查询”。关联子查询是指子查询中存在外部引用的列，例如：SELECT * FROM SRC WHERE EXISTS(SELECT * FROM TMP WHERE TMP.id = SRC.id) 对于非关联子查询，我们可以在 plan 阶段进行预处理，将其改写成一个常量。因此，本文只考虑关联子查询的优化。一般来说，子查询语句分为三种： 标量子查询（Scalar Subquery），如(SELECT&amp;hellip;) + (SELECT&amp;hellip;) 集合比较（Quantified Comparision），如T.a = ANY(SELECT&amp;hellip;) 存在性测试（Existential Test），如NOT EXISTS(SELECT&amp;hellip;)，T.a IN (SELECT&amp;hellip;)  对于简单的存在性测试类的子查询，一般的做法是将其改写成 SEMI-JOIN。但是很少有文献给出通用性的算法，指出什么样的查询可以“去关联化”。对于不能去关联化的子查询，数据库的做法通常是使用类似 Nested Loop 的方式去执行，称为 correlated execution。TiDB 沿袭了 SQL Server 对子查询的处理思想，引入Apply 算子将子查询用代数形式表示，称为归一化，再根据 Cost 信息，进行去关联化。Apply 算子 子查询难以优化的原因是，人们通常不能把一个子查询执行表示成一个类似 Projection、Join 这样的逻辑算子。这使得找到一个通用子查询转换的算法是很难的。所以我们第一件要做的事就是，引入一个可以表示子查询的逻辑算子：Apply。Apply 算子的语义是：公式中的 E 代表一个“参数化”的子查询。在每一次执行中，Apply 算子会向关系 R 取一条记录 r，作为参数传入 E 中，然后让 r 和 E&amp;reg; 做 ⊗ 操作。⊗ 会根据子查询类型的不同而不同，通常是半连接 ⋉。对于SQL 语句：SELECT * FROM SRC WHERE EXISTS(SELECT * FROM TMP WHERE TMP.id = SRC.id) 它的 Apply 算子表示是：对于出现在 SELECT 列表中、GROUP BY 列表中的子查询，道理也是类似的。所以 Apply 是可以表示出现在任意位置的子查询的。去关联化 引入了 Apply，我们就可以将子查询去关联化了。去关联化的规则如下：根据上述规则，你可以将所有的确定性 SQL 子查询去关联化。例如 SQL 语句：SELECT C_CUSTKEY FROM CUSTOMER WHERE 1000000 &amp;lt; (SELECT SUM(O_TOTALPRICE) FROM ORDER WHERE O_CUSTKEY = C_CUSTKEY) 其中两个 CUSTKEY 均为主键。转换成 Apply 之后的表达式为：因为主键的存在，利用规则（9），可以转化为：此时根据规则（2），我们可以彻底消除 Apply，转化为只有连接的 SQL 表达式：再根据外连接化简的原则，可以进一步化简为：利用上述九条规则，理论上已经解决去关联化的问题了。是不是对于所有的情况，去关联化都是最好的呢？答案是否定的。如果 SQL 的结果很小，同时子查询可以利用索引，有时候使用 correlated execution 是最好的。是否去关联化还需要统计信息的帮助。而到了这一步，普通的优化器已经无能为力了。只有 Volcano 或 Cascade Style 的优化器，可以同时考虑逻辑等价规则和代价选择。因此，想要完美解决子查询的问题，要需要优秀的优化器框架的支撑。半连接 TiDB 在去关联化方面，目前只支持将关联子查询改写成半连接和左外半连接。例如，对于查询：SELECT * FROM SRC WHERE EXISTS(SELECT * FROM TMP WHERE TMP.id = SRC.id) TiDB 做出的 Plan 为：当子查询出现在 SELECT 子句当中时：SELECT CASE WHEN EXISTS(SELECT * FROM TMP WHERE TMP.id = SRC.id) THEN 1 ELSE 2 END FROM SRC Projection 算子需要知道 Exists 结果是 True 或者 False。这时需要左外半连接，当然外表匹配时，有一个辅助列 aux 输出 True，当不匹配时，输出 False。对于半连接的算法实现，其实和 Join 差别不大，可以选择 MergeSortJoin，HashJoin，IndexLoopUpJoin，NestedLoop 等等。确定使用 SemiJoin 之后，优化器会根据统计信息选择最合适的算法，这里不再赘述。"},
		{"url": "https://pingcap.com/meetup/meetup-2016-07-30/",
		"title": "PingCAP 第 18 期 NewSQL Meetup", 
		"content": " PingCAP 第 18 期 NewSQL Meetup 2016-07-30 常冰琳&amp;amp;张阳 PingCAP PingCAPPingCAP ![]() 微信号pingcap2015功能介绍PingCAP 专注于新型分布式数据库的研发，是知名开源数据库 TiDB (GitHub 总计10000+ stars ) 背后的团队，总部设在北京，是国内第一家开源的新型分布式关系型数据库公司、国内领先的大数据技术和解决方案提供商。NewSQL Meetup今天是 PingCAP 第 18 期 Meetup，主题是小米云平台工程师常冰琳分享的《Kudu 的设计思想和具体实现》以及张阳分享的《Kubernetes in PingCAP》。▌ ****Topic 1：Kudu 的设计思想和具体实现lecture：常冰琳 小米云平台工程师，长期专注于 Hadoop 生态的分布式计算框架，Kudu PMC&amp;amp;Commiter, Hadoop Nativetask 项目发起者(已合入 Hadoop)。目前在小米负责 SQL 类数据分析平台，利用 Impala 和 Kudu 搭建实时数据分析云服务。Content：本次分享将简单介绍 Kudu 的设计思想和具体实现，以及小米作为 Kudu 最早用户的一些实践经验。 设计目标 数据模型，分区和副本设计 Tablet 存储设计 其他底层细节 小米实践  ▌ ****Topic 2：Kubernetes in PingCAPContent：本次分享，主要与大家沟通了 Kubernetes 在 TiKV 及 TiDB 中的一些应用场景，包括部署、运维以及与 Jenkins CI 的集成等。同时，对大家集中提问的 stateful 的 TiKV 在 rolling update、recovery 等情况下的“状态”维护上的一些问题，进行了探讨，基于此问题，大家在分享结束后也积极交流了各自对于 Kubernetes 本身的一些见解。两个小时的分享时间很快就过去了，还没尽兴的小伙伴们便又开始了跟讲师和 PingCAP 团队单聊的节奏。于是，这些“自动配对，小组交流”的画面便出现在了 PingCAP 公司内的各个角落。接下来，我们也会从以往 Meetup 议题中筛选出现场关注度较高的技术点，邀请讲师以深度文章的形式分享出来。如果大家有任何相关技术问题，也欢迎通过微信留言与我们交流探讨。PingCAP Meetup"},
		{"url": "https://pingcap.com/weekly/2016-07-29-tidb-weekly/",
		"title": "Weekly update (July 23 ~ July 29, 2016)", 
		"content": " Weekly update (July 23 ~ July 29, 2016) Last week, we landed 27 PRs in the TiDB repositories and 34 PRs in the TiKV repositories.Notable changes to TiDB  Support cost based query optimization. Set the new query optimizer as default to improve the speed of complex queries. Meanwhile, a start-up parameter is provided to switch to the old query optimizer. Use Varint to encode the Column Value with integer type and Column ID, which saves storage space significantly. Add a Documents repository. Supprt the Hex Function. Simplify the compling and deployment for better usability. Fix several bugs.  New Contributor Huaiyu XuNotable changes to TiKV  Support building TiKV by linking static RocksDB automatically. You can build TiKV in one machine and copy the binary to other machines to use directly as long as the machines have the same architecture and operation system. Supprt getting snapshot asynchronously for higher throughput and better performance, see benchmark. Use PipeBuf to receive data to reduce system call and memory allocation. Use Varint to encode un-comparable integers to save disk space. Use one SendCh to clean up duplicated code. Add user documents on how to build and use TiKV.  Notable changes to Placement Driver  Embed etcd to for easier deployment. Add a health check for Store. Add user documents on how to build and use PD. Clean up the command flags.  Benchmark Use sysbench to benchmark getting snapshot asynchronously and synchronously in 3-node TiKV.# Prepare data sysbench --test=./lua-tests/db/oltp.lua --mysql-host=${host} --mysql-port=${port}   --mysql-user=${user} --mysql-password=${password} --oltp-tables-count=1   --oltp-table-size=5120000 --rand-init=on prepare # Run benchmark sysbench --test=./lua-tests/db/select.lua --mysql-host=${host} --mysql-port=${port}   --mysql-user=${user} --mysql-password=${password} --oltp-tables-count=1   --oltp-table-size=5120000 --num-threads=${threads} --report-interval=60   --max-requests=5120000 --percentile=99 run |Threads|Async qps|Async avg/.99 latency|Sync qps|Sync avg/.99 latency| |&amp;mdash;|&amp;mdash;|&amp;mdash;|&amp;mdash;|&amp;mdash;|&amp;mdash;| |32|13347|2.4&amp;frasl;4.61|12345|2.59&amp;frasl;4.78| |64|14210|4.50&amp;frasl;7.78|11868|5.39&amp;frasl;8.50| 128|14075|9.09/15.22|12324|10.38&amp;frasl;16.68|As we can see, the qps is increased by about 15%, and the latency is decreased by about 10%."},
		{"url": "https://pingcap.com/meetup/meetup-2016-07-23/",
		"title": "PingCAP 第 17 期 NewSQL Meetup", 
		"content": " PingCAP 第 17 期 NewSQL Meetup 2016-07-23 崔秋 PingCAP PingCAPPingCAP ![]() 微信号pingcap2015功能介绍PingCAP 专注于新型分布式数据库的研发，是知名开源数据库 TiDB (GitHub 总计10000+ stars ) 背后的团队，总部设在北京，是国内第一家开源的新型分布式关系型数据库公司、国内领先的大数据技术和解决方案提供商。NewSQL Meetup今天是 PingCAP 第 17 期 Meetup，主题是崔秋分享的《How does TiKV auto-balance work?》。▌ ****Topic：How does TiKV auto-balance work?TiDB最近发布了Beta版本，相比传统的关系型数据库，TiDB具有在线弹性伸缩，高可用和强一致性，一致性的分布式事务和MySQL协议兼容性等特性，特别适用于大规模高并发的海量数据场景。本次交流主要介绍了 TiKV 的 Balance Scheduler 框架和算法实现演进，对于大家主要关注的 TiKV 集群的在线弹性扩容实现细节和 TiKV Balance 中在线服务高可用的问题，进行了深度的探讨。在 TiKV 里面，数据是按照 Range 进行存放的，称为一个 Region。PD(Placement Driver) 负责整个 TiKV 集群的管理和调度。在 TiKV 里面，数据移动的基本单元是 Region，所以 PD 的 auto balance 也是针对 Region 进行处理。对于一个 Region 来说，它会不会被 Balance，有两种方式：1）HeartbeartRegion 会定期地上报当前的状态信息给 PD，如果 PD 发现该 Region 副本数不足或者超过阀值，则会通知该 Region 进行 Membership Change 处理。2）Balance LoopPD 会每隔一段时间检测整个系统是否需要调度。如果 PD 发现某个 Store 能用的空间不多，或者某个 Store Leader Region 数量太多，load 比较高，就会在该 Store 里面选择一个 Region，将其在该 Store 的副本迁移到另一个 Store 上面。PingCAP Meetup"},
		{"url": "https://pingcap.com/weekly/2016-07-23-tidb-weekly/",
		"title": "Weekly update (July 17 ~ July 22, 2016)", 
		"content": " Weekly update (July 17 ~ July 22, 2016) Last week, we landed 22 PRs in the TiDB repositories and 15 PRs in the TiKV repositories.Notable changes to TiDB  Refactor the query optimizer to imporve the query efficiency for Join and SubQuery Add distributed SQL support for aggregate functions Improve the stability of the TiDB service Refactor the Decimal codes to improve the compatibility with MySQL Optimize the TiDB compatibility and performance for Zabbix Enhance the performance and the Sysbench result is improved significantly  Notable changes to TiKV  Add asynchronous scheduler support for higher throughput and better performance, see Benchmark. Use PipeBuf to speed up socket read/write. Add pushing down max/min support for the coprocessor. Re-use RocksDB write ahead log (WAL) to guarantee consistency when writing data in different column families. Support using make install on the CentOS platform to install TiKV.  Notable changes to Placement Driver  Refactor the balance framework to make a cluster more balanced and stable. Support web UI in Docker.  Benchmark Use sysbench to benchmark asynchronous scheduler and previous 8 threadpools in 3-node TiKV.# Prepare data sysbench --test=./lua-tests/db/oltp.lua --mysql-host=${host} --mysql-port=${port}   --mysql-user=${user} --mysql-password=${password} --oltp-tables-count=$1   --oltp-table-size=1000 --rand-init=on prepare # Run benchmark sysbench --test=./lua-tests/db/insert.lua --mysql-host=${host} --mysql-port=${port}   --mysql-user=${user} --mysql-password=${password} --oltp-tables-count=1   --oltp-table-size=1000 --num-threads=${threads} --report-interval=60   --max-requests=1280000 --percentile=99 run |Threads|Async scheduler qps|Async scheduler avg/.99 latency|8 threadpools qps|Thread pool avg/.99 latency| |&amp;mdash;|&amp;mdash;|&amp;mdash;|&amp;mdash;|&amp;mdash;|&amp;mdash;| |32|2049|15.6&amp;frasl;29.1|1652|19.4&amp;frasl;36.3| |64|2042|31.3&amp;frasl;85.5|1693|37.8&amp;frasl;83| |128|2125|60.2&amp;frasl;147|1649|77&amp;frasl;175|As we can see, the qps is increased by about 25%, and the latency is decreased by about 15%."},
		{"url": "https://pingcap.com/meetup/meetup-2016-07-16/",
		"title": "PingCAP 第 16 期 NewSQL Meetup", 
		"content": " PingCAP 第 16 期 NewSQL Meetup 2016-07-16 田琪&amp;amp;孟圣智 PingCAP PingCAPPingCAP ![]() 微信号pingcap2015功能介绍PingCAP 专注于新型分布式数据库的研发，是知名开源数据库 TiDB (GitHub 总计10000+ stars ) 背后的团队，总部设在北京，是国内第一家开源的新型分布式关系型数据库公司、国内领先的大数据技术和解决方案提供商。NewSQL Meetup今天是 PingCAP 第 16 期 Meetup，主题是来自京东的田琪分享的《Cool Extensions of Raft for NewSQL》，以及来自百度的孟圣智分享的《基于 Ceph 构建文件共享服务的实践》 。▌ ****Topic1：Cool Extensions of Raft for NewSQLlecturer：田琪，京东数据库系统部负责人，开源 docker 镜像存储系统 speedy 作者，TiDB committer, etcd contributorTopic summary:主要分享了 Raft 协议在 etcd 中的实现，与 etcd 在 Raft 协议方面近期更新地比较重要的特性，以及引进这些特性的缘由。 the functionality of leader transfer the future improvement of leader transfer the functionality of quorum checking implement leader lease based on quorum checking some issues about leader lease how to implement efficient read-only query some other Raft internal details  ▌Topic 2：基于 Ceph 构建文件共享服务的实践lecturer：孟圣智，百度资深研发工程师，在存储领域有多年经验，Ceph contributor，Ceph-Dokan 项目作者，曾在国内最早落地 OpenStack Manila 项目，现在在百度负责 Memcache、Redis 类存储的研发和维护。Topic summary: Ceph 的基本架构 CephFS 的实现方式 CephFS 的多客户端方案 OpenStack 中使用 Ceph 的经验 基于 CephFS 实现文件共享服务的实践  PingCAP Meetup"},
		{"url": "https://pingcap.com/meetup/meetup-2016-07-09/",
		"title": "PingCAP 第15期 NewSQL Meetup", 
		"content": " PingCAP 第15期 NewSQL Meetup 2016-07-09 申砾&amp;amp;周昱行 PingCAP PingCAPPingCAP ![]() 微信号pingcap2015功能介绍PingCAP 专注于新型分布式数据库的研发，是知名开源数据库 TiDB (GitHub 总计10000+ stars ) 背后的团队，总部设在北京，是国内第一家开源的新型分布式关系型数据库公司、国内领先的大数据技术和解决方案提供商。NewSQL Meetup今天是 PingCAP 第15期 Meetup ，主题是申砾分享的《TiDB 存储模型变更》以及周昱行分享的《TiDB 优化器统计信息的采集》。▌Part 1：《TiDB 存储模型变更》TiDB 在 Key-Value 存储模型之上，将一行数据拆分成多个 Key-Value pair。这样做有利于列较多并且 update 较为频繁的业务场景，同时对 Online Schema 变更较为友好。但是这种存储模型对于需要读取/写入大量 row 的业务场景并不适用。为此我们修改了 TiDB 的存储模型，将一行内需要频繁修改和很少修改的数据存储在不同的 column family 中，以更好地适应不同热度的数据,以及生存期差别比较大的数据。同时，非常有效地适配了读写放大以及空间放大的问题。▌Part 2：《TiDB 优化器统计信息的采集》统计信息是实现基于代价的优化（CBO）的必要条件，本期为大家介绍 TiDB 收集统计信息使用的采样算法和直方图生成算法。PingCAP Meetup"},
		{"url": "https://pingcap.com/meetup/meetup-2016-07-02/",
		"title": "PingCAP 第14期 NewSQL Meetup", 
		"content": " PingCAP 第14期 NewSQL Meetup 2016-07-02 马涛&amp;amp;刘奇 PingCAP PingCAPPingCAP ![]() 微信号pingcap2015功能介绍PingCAP 专注于新型分布式数据库的研发，是知名开源数据库 TiDB (GitHub 总计10000+ stars ) 背后的团队，总部设在北京，是国内第一家开源的新型分布式关系型数据库公司、国内领先的大数据技术和解决方案提供商。NewSQL Meetup今天是 PingCAP 第14期 Meetup ，主题是酷克数据联合创始人马涛分享的《HashData 数据仓库的动态缩容扩容实现》以及 PingCAP 联合创始人兼 CEO 刘奇针对近日发布的 TiDB Beta 版进行的现场 Demo 演示。▌Part 1：《 HashData 数据仓库的动态缩容扩容实现》讲师：马涛，酷克数据联合创始人，数据库领域从业近10年，最初 Pivotal HAWQ 项目成员，06年至11年就职人大金仓做内核开发。目前主要负责 OLAP 系统内核和外围云化工作。通过对比 Greenplum，Dynamo 和 HashData 的当前实现，为大家简单介绍数据处理系统动态缩容扩容的实现。阐述数据系统缩容和扩容的需求集合和设计方案，深入介绍 HashData 选择的设计、目前实现和后续改进。▌Part 2：《 TiDB Beta 版现场 Demo 演示》讲师：刘奇，PingCAP 联合创始人兼 CEO。针对6月30日发布的 TiDB Beta 版，刘奇在现场进行演示，与大家共同见证了 TiDB 界面的首次亮相。直接通过标准的 MySQL 客户端连接，后端三台普通 x86 服务器集群，演示了常用的 SQL 插入和查询，并演示了在大压力数据写入的场景下，TiDB 自动扩容的全过程，期间无需人为干预，TiDB 自动完成数据迁移和扩容及流量的负载均衡，业务层完全透明。小伙伴们都惊呆了。TiDB Beta 版已如约亮相，说好的 “三五好友，吃吃喝喝”，说来就来 :)PingCAP Meetup"},
		{"url": "https://pingcap.com/meetup/meetup-2016-06-25/",
		"title": "PingCAP 第13期 NewSQL Meetup", 
		"content": " PingCAP 第13期 NewSQL Meetup 2016-06-25 闫宇&amp;amp;崔秋 PingCAP PingCAPPingCAP ![]() 微信号pingcap2015功能介绍PingCAP 专注于新型分布式数据库的研发，是知名开源数据库 TiDB (GitHub 总计10000+ stars ) 背后的团队，总部设在北京，是国内第一家开源的新型分布式关系型数据库公司、国内领先的大数据技术和解决方案提供商。NewSQL Meetup今天是 PingCAP 第13期 Meetup ，主题是百度资深研发工程师、百度 BAC 存储负责人闫宇分享的《 百度 redis3 生产环境实践》以及 PingCAP 联合创始人崔秋分享的《TiKV Auto Balance 》。▌Topic 1：《百度 redis3 生产环境实践》讲师：闫宇，百度资深研发工程师，百度 BAC 存储负责人**（百度 BAC 的 redis3 服务目前机器规模达到1400台左右，总数据量接近100T，日 pv 超过1500亿，用户涵盖了百度贴吧、百度糯米、手机百度等百度内部几百个业务线。）内容方向：1）介绍百度BAC的 redis3 服务的整体架构；2）交流在 redis3 实践中的一些经验。以下为本次分享的干货PPT：▌Topic 2：《TiKV Auto Balance》讲师：崔秋，PingCAP 联合创始人内容方向：1）PD - God View of TiKV； 2）TiKV 如何成为真正意义上的分布式存储引擎。以下为本次分享的干货PPT：最后，附赠一张今日爆满全场听讲图PingCAP Meetup"},
		{"url": "https://pingcap.com/meetup/meetup-2016-06-18/",
		"title": "PingCAP 第12期 NewSQL Meetup", 
		"content": " PingCAP 第12期 NewSQL Meetup 2016-06-18 张金鹏 PingCAP PingCAPPingCAP ![]() 微信号pingcap2015功能介绍PingCAP 专注于新型分布式数据库的研发，是知名开源数据库 TiDB (GitHub 总计10000+ stars ) 背后的团队，总部设在北京，是国内第一家开源的新型分布式关系型数据库公司、国内领先的大数据技术和解决方案提供商。NewSQL Meetup今天是 PingCAP 第12期 Meetup ，主题是张金鹏分享的《 rocksdb 日志分析和性能调优经验 》。▌张金鹏《 rocksdb 日志分析和性能调优经验 》首先和大家一起分享如何分析 rocksdb 的 LOG，包括观察 compaction 相关的统计信息。例如每个 level 导致的 compaction 个数，每个 compaction job 的平均持续时长，compaction 导致的 read 总量和 write 量，以及写放大等；也可以观察整个系统是否有 stall 情况，持续多长时间，时间占比是多少等；另外，还有跟踪某个具体的 compaction job 的 input files 组成，output files，以及 compacting 过程中 drop 掉的 key 个数等信息。然后根据 rocksdb 的 LOG 以及观察到的系统负载情况，来对不同参数组进行测试。最后对比不同参数组的一些效果，包括同样的数据量导致的 compaction 放大比例；整个系统的 stall 情况；以及是否存在长时间的 compaction 导致的长时间高 CPU 及高 IO，从而对 TiKV 服务本身造成负面影响等情况。PingCAP Meetup"},
		{"url": "https://pingcap.com/blog-cn/tidb-api-union-scan/",
		"title": "TiDB 下推 API 实现细节 - Union Scan", 
		"content": " TiDB 集群的架构分为上层的 SQL 层和底层的 KV 层，SQL 层通过调用 KV 层的 API 读写数据，由于 SQL 层的节点和 KV 层节点通常不在一台机器上，所以，每次调用 KV 的 API 都是一次 RPC, 而往往一个普通的 Select 语句的执行，需要调用几十到几十万次 KV 的接口，这样的结果就是性能非常差，绝大部分时间都消耗在 RPC 上。为了解决这个问题，TiDB 实现了下推 API，把一部分简单的 SQL 层的执行逻辑下推到 KV 层执行，让 KV 层可以理解 Table 和 Column，可以批量读取多行结果，可以用 Where 里的 Expression 对结果进行过滤, 可以计算聚合函数，大幅减少了 RPC 次数和数据的传输量。TiDB 的下推 API 通过把 SQL 层的计算下推到 KV 层，大幅减少 RPC 次数和数据传输量，使性能得到数量级的提升。但是当我们一开始启用下推 API 的时候，发现了一个问题，就是当事务写入了数据，但是还未提交的时候，又执行了 Select 操作。这个时候，刚刚写入的未提交的脏数据读不到，得到的结果是错误的，比如我们在一个空表 t 执行：begin; insert t values (1); select * from t; 这时我们期待的结果是一条记录 “1”，但是启用下推 API 后得到的结果是空。导致这个问题的原因是我们的事务在提交之前，写入的数据是 buffer 在 SQL 层，并没有写入 KV, 而下推 API 直接从 KV 读取数据，得到的结果直接返回，所以得到了空的结果。但是既然 KV 层读取不到未提交的脏数据，那在启用下推 API 之前，是如何得到正确结果的呢？这就涉及到 SQL 层的 Buffer 实现。当初为了解决未提交事务的 Buffer 可见性问题，SQL 层实现了一个 UnionStore 的结构，UnionStore 对 Buffer 和 KV 层接口做了一个封装，事务对 KV 的读写都经过 UnionStore，当 UnionStore 遇到读请求时，会先在 Buffer 里找，Buffer 找不到时，才会调用 KV 层的接口，读取 KV 层的数据。所以相当于把 Buffer 和 KV 的数据做了一个 Merge，返回 Merge 后的正确结果。Buffer 的数据是用 goleveldb 的 MemDB 存储的，所以是有序的，当需要遍历数据的时候，UnionStore 会同时创建 Buffer 的 Iterator 和 KV 的 Iterator，遍历的算法类似 LevelDB，把两个 Iterator merge 成一个。UnionStore 的实现是基于 Key Value 的，但是下推 API 返回的结果是基于 Row 的，也就是说，我们虽然有脏数据 Buffer 和下推 API 返回的结果集, 但是我们没有办法把这两部分数据合并在一起, 所以我们为了绕过这个问题，加了一个判断条件，当事务写入了 Buffer，包含了脏数据以后，就不走下推 API，而是使用基础的 KV API。在我们刚刚开始启用下推 API 的时候，因为性能基准比较低，而且带脏数据的下推请求只占很小的一部分，所以我选择暂时绕过这个问题。但是当全面启用下推 API 以后，整体性能已经大幅提升，这时带脏数据的请求无法走下推 API 这个 worst case 问题就渐渐凸显出来。比如说，我们如果需要在一个事务里 UPDATE 多个行，就一定会遇到下推 API 无法使用，降级到基础 KV API 的问题。假设我们创建一个表，插入了两行数据：create table t (c int); insert t values (1), (4); 这时我们执行这样一个事务：begin; update t set c = 2 where c = 1; update t set c = 3 where c = 4; UPDATE 语句执行的过程分两步，第一步是先读取到需要更新的数据，第二步把更新的数据写入 Buffer。也就是 UPDATE 包含了一次 SELECT 请求。当第一个 UPDATE 语句执行的时候，因为没有脏数据，所以读请求会走下推 API，但是第一个 UPDATE 语句执行完后，事务就有了脏数据，再执行第二个 UPDATE 的时候，无法使用下推 API, 会导致性能大幅下降。解决这个问题的方案，最容易想到的是在 KV 层实现 UnionStore 相同的算法，当发送下推 API 请求时，把 Buffer 一并传下去。但是这个方案的缺点也很明显，就是计算和存储不在同一节点，不符合就近计算原则。脏数据是在 SQL 层生成并存储的，本来应该在 SQL 进行 Merge，但是却要传输到 KV 层去 Merge，如果 Buffer 的数据很多，传输 Buffer 带来的开销就会很大。最终我们设计实现了一个更好的方案 Union Scan，在不需要把 Buffer 传输到 KV 层，不修改 KV 层的情况下，解决了脏数据的可见性问题。下面是这个算法的简介 脏数据缓存在 SQL 层，要让它可见，一定是需要 Merge 的，当我们使用下推 API, 只拿到了一堆 Row，这时怎么 Merge 呢？如果我们不做 Merge，直接返回给用户结果集，错误表现的就是少了某些 row，多了某些 row，或某些 row 的数据是旧的。如果我们把 INSERT, UPDATE, DELETE 的修改操作，以 row 为单位记录下来，这样和下推 API 返回的结果就是同样的形式了，就可以很方便的做 Merge 的计算了。所以 Union Scan 的算法就是以 Row 为单位，把事务的修改操作保存起来，最终和下推 API 返回的结果集进行 Merge，返回给客户端。我们为每个事务在对某个 table 执行写操作时，创建一个 dirtyTable 保存这个事务的修改，dirtyTable 包含两个 map，一个是 addedRows，用来保存新写入的 row，另一个是 removedRows，用来保存删除的 row，对于不同的操作，我们需要对这两个 map 做不同的操作。对于 INSERT，我们需要把 row 添加到 addedRows 里。对于 DELETE，我们需要把 row 从 addedRows 里删掉，然后把 row 添加到 removedRows 里。对于 UPDATE，相当于先执行 DELETE, 再执行 INSERT。当我们从下推 API 得到了结果集之后，我们下面把它叫做快照结果集，Merge 的算法如下：对于每一条快照结果集里的 Row，在 removedRows 里查找，如果有，那么代表这一条结果已经被删掉，那么把它从结果集里删掉，得到过滤后的结果集。把 addedRows 里的所有 Row，放到一个 slice 里，并对这个 slice 用快照结果集相同的顺序排序，生成脏数据结果集。返回结果的时候，将过滤后的快照结果集与脏数据结果集进行 Merge。实现了 Union Scan 以后，所有的读请求都可以使用下推 API 加速，大幅提升了 worst case 的性能。"},
		{"url": "https://pingcap.com/meetup/memoir/meetup-2016-06-18/",
		"title": "TiDB 下推 API 实现细节 - Union Scan", 
		"content": "  TiDB 集群的架构分为上层的 SQL 层和底层的 KV 层，SQL 层通过调用 KV 层的 API 读写数据，由于 SQL 层的节点和 KV 层节点通常不在一台机器上，所以，每次调用 KV 的 API 都是一次 RPC, 而往往一个普通的 Select 语句的执行，需要调用几十到几十万次 KV 的接口，这样的结果就是性能非常差，绝大部分时间都消耗在 RPC 上。为了解决这个问题，TiDB 实现了下推 API，把一部分简单的 SQL 层的执行逻辑下推到 KV 层执行，让 KV 层可以理解 Table 和 Column，可以批量读取多行结果，可以用 Where 里的 Expression 对结果进行过滤, 可以计算聚合函数，大幅减少了 RPC 次数和数据的传输量。 TiDB 的下推 API 通过把 SQL 层的计算下推到 KV 层，大幅减少 RPC 次数和数据传输量，使性能得到数量级的提升。但是当我们一开始启用下推 API 的时候，发现了一个问题，就是当事务写入了数据，但是还未提交的时候，又执行了 Select 操作。这个时候，刚刚写入的未提交的脏数据读不到，得到的结果是错误的，比如我们在一个空表 t 执行：begin; insert t values (1); select * from t; 这时我们期待的结果是一条记录 “1”，但是启用下推 API 后得到的结果是空。导致这个问题的原因是我们的事务在提交之前，写入的数据是 buffer 在 SQL 层，并没有写入 KV, 而下推 API 直接从 KV 读取数据，得到的结果直接返回，所以得到了空的结果。但是既然 KV 层读取不到未提交的脏数据，那在启用下推 API 之前，是如何得到正确结果的呢？这就涉及到 SQL 层的 Buffer 实现。当初为了解决未提交事务的 Buffer 可见性问题，SQL 层实现了一个 UnionStore 的结构，UnionStore 对 Buffer 和 KV 层接口做了一个封装，事务对 KV 的读写都经过 UnionStore，当 UnionStore 遇到读请求时，会先在 Buffer 里找，Buffer 找不到时，才会调用 KV 层的接口，读取 KV 层的数据。所以相当于把 Buffer 和 KV 的数据做了一个 Merge，返回 Merge 后的正确结果。Buffer 的数据是用 goleveldb 的 MemDB 存储的，所以是有序的，当需要遍历数据的时候，UnionStore 会同时创建 Buffer 的 Iterator 和 KV 的 Iterator，遍历的算法类似 LevelDB，把两个 Iterator merge 成一个。UnionStore 的实现是基于 Key Value 的，但是下推 API 返回的结果是基于 Row 的，也就是说，我们虽然有脏数据 Buffer 和下推 API 返回的结果集, 但是我们没有办法把这两部分数据合并在一起, 所以我们为了绕过这个问题，加了一个判断条件，当事务写入了 Buffer，包含了脏数据以后，就不走下推 API，而是使用基础的 KV API。在我们刚刚开始启用下推 API 的时候，因为性能基准比较低，而且带脏数据的下推请求只占很小的一部分，所以我选择暂时绕过这个问题。但是当全面启用下推 API 以后，整体性能已经大幅提升，这时带脏数据的请求无法走下推 API 这个 worst case 问题就渐渐凸显出来。比如说，我们如果需要在一个事务里 UPDATE 多个行，就一定会遇到下推 API 无法使用，降级到基础 KV API 的问题。假设我们创建一个表，插入了两行数据：create table t (c int); insert t values (1), (4); 这时我们执行这样一个事务：begin; update t set c = 2 where c = 1; update t set c = 3 where c = 4; UPDATE 语句执行的过程分两步，第一步是先读取到需要更新的数据，第二步把更新的数据写入 Buffer。也就是 UPDATE 包含了一次 SELECT 请求。当第一个 UPDATE 语句执行的时候，因为没有脏数据，所以读请求会走下推 API，但是第一个 UPDATE 语句执行完后，事务就有了脏数据，再执行第二个 UPDATE 的时候，无法使用下推 API, 会导致性能大幅下降。解决这个问题的方案，最容易想到的是在 KV 层实现 UnionStore 相同的算法，当发送下推 API 请求时，把 Buffer 一并传下去。但是这个方案的缺点也很明显，就是计算和存储不在同一节点，不符合就近计算原则。脏数据是在 SQL 层生成并存储的，本来应该在 SQL 进行 Merge，但是却要传输到 KV 层去 Merge，如果 Buffer 的数据很多，传输 Buffer 带来的开销就会很大。最终我们设计实现了一个更好的方案 Union Scan，在不需要把 Buffer 传输到 KV 层，不修改 KV 层的情况下，解决了脏数据的可见性问题。下面是这个算法的简介 脏数据缓存在 SQL 层，要让它可见，一定是需要 Merge 的，当我们使用下推 API, 只拿到了一堆 Row，这时怎么 Merge 呢？如果我们不做 Merge，直接返回给用户结果集，错误表现的就是少了某些 row，多了某些 row，或某些 row 的数据是旧的。如果我们把 INSERT, UPDATE, DELETE 的修改操作，以 row 为单位记录下来，这样和下推 API 返回的结果就是同样的形式了，就可以很方便的做 Merge 的计算了。所以 Union Scan 的算法就是以 Row 为单位，把事务的修改操作保存起来，最终和下推 API 返回的结果集进行 Merge，返回给客户端。我们为每个事务在对某个 table 执行写操作时，创建一个 dirtyTable 保存这个事务的修改，dirtyTable 包含两个 map，一个是 addedRows，用来保存新写入的 row，另一个是 removedRows，用来保存删除的 row，对于不同的操作，我们需要对这两个 map 做不同的操作。对于 INSERT，我们需要把 row 添加到 addedRows 里。对于 DELETE，我们需要把 row 从 addedRows 里删掉，然后把 row 添加到 removedRows 里。对于 UPDATE，相当于先执行 DELETE, 再执行 INSERT。当我们从下推 API 得到了结果集之后，我们下面把它叫做快照结果集，Merge 的算法如下：对于每一条快照结果集里的 Row，在 removedRows 里查找，如果有，那么代表这一条结果已经被删掉，那么把它从结果集里删掉，得到过滤后的结果集。 把 addedRows 里的所有 Row，放到一个 slice 里，并对这个 slice 用快照结果集相同的顺序排序，生成脏数据结果集。返回结果的时候，将过滤后的快照结果集与脏数据结果集进行 Merge。实现了 Union Scan 以后，所有的读请求都可以使用下推 API 加速，大幅提升了 worst case 的性能。"},
		{"url": "https://pingcap.com/meetup/meetup-2016-06-04/",
		"title": "PingCAP 第11期 NewSQL Meetup", 
		"content": " PingCAP 第11期 NewSQL Meetup 2016-06-04 黄梦龙&amp;amp;张金鹏 PingCAP PingCAPPingCAP ![]() 微信号pingcap2015功能介绍PingCAP 专注于新型分布式数据库的研发，是知名开源数据库 TiDB (GitHub 总计10000+ stars ) 背后的团队，总部设在北京，是国内第一家开源的新型分布式关系型数据库公司、国内领先的大数据技术和解决方案提供商。NewSQL Meetup今天是 PingCAP 第11期 Meetup ，主题是黄梦龙分享的《 TiKV 的结构化存储模型优化》和张金鹏分享的《深入解析 LevelDB 》。▌黄梦龙《 TiKV 的结构化存储模型优化》目前 TiKV 的存储模型是简单的纯 Key-Value，在存储 SQL 结构化数据的过程中会产生比较严重的读写放大问题。我们计划为 TiKV 添加类似于 Hbase 的 ColumnFamily 机制，以使得 TiKV 与 TiDB 成为更加完美的搭档。大家对其中的实现细节，以及各种方案的优缺点进行了探讨。▌张金鹏《深入解析 LevelDB 》首先介绍了 LevelDB 的整体架构，以及 LSM Tree 这一数据库中非常经典的结构。之后对 LevelDB 的写和读的流程进行分析，同时介绍 LevelDB 的 snapshot 功能的实现原理，以及 iterator 内部实现，和 iterator 存在的潜在问题。最后介绍 LevelDB 的 compaction 过程，以及存在的问题。PingCAP Meetup"},
		{"url": "https://pingcap.com/meetup/meetup-2016-05-28/",
		"title": "PingCAP 第10期 NewSQL Meetup", 
		"content": " PingCAP 第10期 NewSQL Meetup 2016-05-28 刘奇&amp;amp;周昱行 PingCAP PingCAPPingCAP ![]() 微信号pingcap2015功能介绍PingCAP 专注于新型分布式数据库的研发，是知名开源数据库 TiDB (GitHub 总计10000+ stars ) 背后的团队，总部设在北京，是国内第一家开源的新型分布式关系型数据库公司、国内领先的大数据技术和解决方案提供商。NewSQL Meetup今天是 PingCAP 第10期 Meetup ，跟京东小伙伴就Raft group 中出现网络隔离时的 stale read 的问题做了充分讨论交流。之后进行的分享主题是《 TiKV 的网络模拟测试》和《 TiDB 的条件下推优化》。▌随机讨论Raft group 中出现网络隔离时，会有stale read 的问题。目前我们考虑采用 region leader 的方案，保证在出现网络隔离的情况下，也能保证读的正确性。大家对其中的实现细节，以及各种方案的优缺点进行了讨论。▌刘奇《 TiKV 的网络模拟测试》TiKV 如何做分布式系统测试。目前已经构建了一套测试框架，提供设置网络延迟、网络隔离、节点掉线等功能，用于构建测试用例。▌周昱行《 TiDB 的条件下推优化》使用基于 Row 的 Merge 算法，解决存在脏数据时，使用 TiDB 下推 API 优化的问题。TiDB 的下推 API 相比基础的 API 对读性能有着几个数量级的提升，任何无法使用下推 API 的操作的请求，性能都慢到完全无法接受的程度。但是之前的实现并不能保证所有读请求都可以走下推 API, 当事务有写操作以后，无法使用下推 API。无法使用的原因是，事务提交之前，事务内写入的数据是对事务自身是可见的，下推 API 只能读到已提交的数据，返回的结果是错误的。一个很常见的场景是在一个事务内 UPDATE 多个 Row，会退化到使用基础 KV API。本周 TiDB 的一个更新，通过设计实现了一种基于 Row 的 Merge 算法，解决了这个问题。小花絮： 赠送 PingCAP 家褶皱版美背 T 恤买家秀一只。大家周末愉快：PPingCAP Meetup"},
		{"url": "https://pingcap.com/meetup/meetup-2016-05-21/",
		"title": "PingCAP 第9期 NewSQL Meetup", 
		"content": " PingCAP 第9期 NewSQL Meetup 2016-05-21 韩飞&amp;amp;刘奇 PingCAP PingCAPPingCAP ![]() 微信号pingcap2015功能介绍PingCAP 专注于新型分布式数据库的研发，是知名开源数据库 TiDB (GitHub 总计10000+ stars ) 背后的团队，总部设在北京，是国内第一家开源的新型分布式关系型数据库公司、国内领先的大数据技术和解决方案提供商。NewSQL Meetup今天是 PingCAP 第9期 NewSQL Meetup ，分享主题是韩飞的《 SQL 子查询优化》和刘奇的《 TiKV MVCC 和 GC 实现》。▌韩飞 《 SQL 子查询优化》分享 SQL subqueries 的变换和优化问题。关联子查询的优化是 SQL 优化中很重要的一部分，一般的执行方式方式是 correlated execution，但是可以通过引入 Apply 算子形式化证明所有的子查询都可以改写成 Join 的不同形式。在分布式场景下，Join 可以比 correlated execution 有更多的优化空间。▌刘奇《 TiKV MVCC 和 GC 实现》详细分析了 TiKV 的 MVCC 机制, 事务模型，并进一步介绍了 percolator 事务模型的特点，以及对 GC 的影响。另外讲解了 TiKV 对 percolator 事务模型的改进, 以及 TiKV 的 GC 算法，和如何支持长时间的数据库备份和分析操作。PingCAP Meetup"},
		{"url": "https://pingcap.com/docs/overview/",
		"title": "About TiDB", 
		"content": " About TiDB TiDB introduction TiDB (The pronunciation is: /&amp;lsquo;taɪdiːbi:/ tai-D-B, etymology: titanium) is a Hybrid Transactional/Analytical Processing (HTAP) database. Inspired by the design of Google F1 and Google Spanner, TiDB features infinite horizontal scalability, strong consistency, and high availability. The goal of TiDB is to serve as a one-stop solution for online transactions and analyses. Horizontal scalability Compatible with MySQL protocol Automatic failover and high availability Consistent distributed transactions Online DDL Multiple storage engine support  Read the following three articles to understand TiDB techniques: Data Storage Computing Scheduling  Roadmap Read the Roadmap.Connect with us  Twitter: @PingCAP Reddit: https://www.reddit.com/r/TiDB/ Stack Overflow: https://stackoverflow.com/questions/tagged/tidb Mailing list: Google Group  TiDB architecture To better understand TiDB’s features, you need to understand the TiDB architecture.The TiDB cluster has three components: the TiDB server, the PD server, and the TiKV server.TiDB server The TiDB server is in charge of the following operations: Receiving the SQL requests Processing the SQL related logics Locating the TiKV address for storing and computing data through Placement Driver (PD) Exchanging data with TiKV Returning the result  The TiDB server is stateless. It does not store data and it is for computing only. TiDB is horizontally scalable and provides the unified interface to the outside through the load balancing components such as Linux Virtual Server (LVS), HAProxy, or F5.Placement Driver server The Placement Driver (PD) server is the managing component of the entire cluster and is in charge of the following three operations: Storing the metadata of the cluster such as the region location of a specific key. Scheduling and load balancing regions in the TiKV cluster, including but not limited to data migration and Raft group leader transfer. Allocating the transaction ID that is globally unique and monotonic increasing.  As a cluster, PD needs to be deployed to an odd number of nodes. Usually it is recommended to deploy to 3 online nodes at least.TiKV server The TiKV server is responsible for storing data. From an external view, TiKV is a distributed transactional Key-Value storage engine. Region is the basic unit to store data. Each Region stores the data for a particular Key Range which is a left-closed and right-open interval from StartKey to EndKey. There are multiple Regions in each TiKV node. TiKV uses the Raft protocol for replication to ensure the data consistency and disaster recovery. The replicas of the same Region on different nodes compose a Raft Group. The load balancing of the data among different TiKV nodes are scheduled by PD. Region is also the basic unit for scheduling the load balance.Features Horizontal scalability Horizontal scalability is the most important feature of TiDB. The scalability includes two aspects: the computing capability and the storage capacity. The TiDB server processes the SQL requests. As the business grows, the overall processing capability and higher throughput can be achieved by simply adding more TiDB server nodes. Data is stored in TiKV. As the size of the data grows, the scalability of data can be resolved by adding more TiKV server nodes. PD schedules data in Regions among the TiKV nodes and migrates part of the data to the newly added node. So in the early stage, you can deploy only a few service instances. For example, it is recommended to deploy at least 3 TiKV nodes, 3 PD nodes and 2 TiDB nodes. As business grows, more TiDB and TiKV instances can be added on-demand.High availability High availability is another important feature of TiDB. All of the three components, TiDB, TiKV and PD, can tolerate the failure of some instances without impacting the availability of the entire cluster. For each component, See the following for more details about the availability, the consequence of a single instance failure and how to recover.TiDB TiDB is stateless and it is recommended to deploy at least two instances. The front-end provides services to the outside through the load balancing components. If one of the instances is down, the Session on the instance will be impacted. From the application’s point of view, it is a single request failure but the service can be regained by reconnecting to the TiDB server. If a single instance is down, the service can be recovered by restarting the instance or by deploying a new one.PD PD is a cluster and the data consistency is ensured using the Raft protocol. If an instance is down but the instance is not a Raft Leader, there is no impact on the service at all. If the instance is a Raft Leader, a new Leader will be elected to recover the service. During the election which is approximately 3 seconds, PD cannot provide service. It is recommended to deploy three instances. If one of the instances is down, the service can be recovered by restarting the instance or by deploying a new one.TiKV TiKV is a cluster and the data consistency is ensured using the Raft protocol. The number of the replicas can be configurable and the default is 3 replicas. The load of TiKV servers are balanced through PD. If one of the node is down, all the Regions in the node will be impacted. If the failed node is the Leader of the Region, the service will be interrupted and a new election will be initiated. If the failed node is a Follower of the Region, the service will not be impacted. If a TiKV node is down for a period of time (the default value is 10 minutes), PD will move the data to another TiKV node."},
		{"url": "https://pingcap.com/docs/sql/aggregate-group-by-functions/",
		"title": "Aggregate (GROUP BY) Functions", 
		"content": " Aggregate (GROUP BY) Functions Aggregate (GROUP BY) function descriptions This section describes the supported MySQL group (aggregate) functions in TiDB.   Name Description     COUNT() Return a count of the number of rows returned   COUNT(DISTINCT) Return the count of a number of different values   SUM() Return the sum   AVG() Return the average value of the argument   MAX() Return the maximum value   MIN() Return the minimum value   GROUP_CONCAT() Return a concatenated string     Unless otherwise stated, group functions ignore NULL values. If you use a group function in a statement containing no GROUP BY clause, it is equivalent to grouping on all rows. For more information see TiDB handling of GROUP BY.  GROUP BY modifiers TiDB dose not support any GROUP BY modifiers currently. We&amp;rsquo;ll do it in the future. For more information, see #4250.TiDB handling of GROUP BY TiDB performs equivalent to MySQL with sql mode ONLY_FULL_GROUP_BY being disabled: permits the SELECT list, HAVING condition, or ORDER BY list to refer to non-aggregated columns even if the columns are not functionally dependent on GROUP BY columns.For example, this query is illegal in MySQL 5.7.5 with ONLY_FULL_GROUP_BY enabled because the non-aggregated column &amp;ldquo;b&amp;rdquo; in the SELECT list does not appear in the GROUP BY:drop table if exists t; create table t(a bigint, b bigint, c bigint); insert into t values(1, 2, 3), (2, 2, 3), (3, 2, 3); select a, b, sum(c) from t group by a; The preceding query is legal in TiDB. TiDB does not support SQL mode ONLY_FULL_GROUP_BY currently. We&amp;rsquo;ll do it in the future. For more inmormation, see #4248.Suppose that we execute the following query, expecting the results to be ordered by &amp;ldquo;c&amp;rdquo;:drop table if exists t; create table t(a bigint, b bigint, c bigint); insert into t values(1, 2, 1), (1, 2, 2), (1, 3, 1), (1, 3, 2); select distinct a, b from t order by c; To order the result, duplicates must be eliminated first. But to do so, which row should we keep? This choice influences the retained value of &amp;ldquo;c&amp;rdquo;, which in turn influences ordering and makes it arbitrary as well.In MySQL, a query that has DISTINCT and ORDER BY is rejected as invalid if any ORDER BY expression does not satisfy at least one of these conditions: - The expression is equal to one in the SELECT list - All columns referenced by the expression and belonging to the query&amp;rsquo;s selected tables are elements of the SELECT listBut in TiDB, the above query is legal, for more information see #4254.Another TiDB extension to standard SQL permits references in the HAVING clause to aliased expressions in the SELECT list. For example, the following query returns &amp;ldquo;name&amp;rdquo; values that occur only once in table &amp;ldquo;orders&amp;rdquo;:select name, count(name) from orders group by name having count(name) = 1; The TiDB extension permits the use of an alias in the HAVING clause for the aggregated column:select name, count(name) as c from orders group by name having c = 1; Standard SQL permits only column expressions in GROUP BY clauses, so a statement such as this is invalid because &amp;ldquo;FLOOR(value/100)&amp;rdquo; is a noncolumn expression:select id, floor(value/100) from tbl_name group by id, floor(value/100); TiDB extends standard SQL to permit noncolumn expressions in GROUP BY clauses and considers the preceding statement valid.Standard SQL also does not permit aliases in GROUP BY clauses. TiDB extends standard SQL to permit aliases, so another way to write the query is as follows:select id, floor(value/100) as val from tbl_name group by id, val; Detection of functional dependence TiDB does not support SQL mode ONLY_FULL_GROUP_BY and detection of functional dependence. We&amp;rsquo;ll do it in the future. For more information, see #4248."},
		{"url": "https://pingcap.com/docs/op-guide/ansible-deployment/",
		"title": "Ansible Deployment", 
		"content": " Ansible Deployment Overview Ansible is an IT automation tool. It can configure systems, deploy software, and orchestrate more advanced IT tasks such as continuous deployments or zero downtime rolling updates.TiDB-Ansible is a TiDB cluster deployment tool developed by PingCAP, based on Ansible playbook. TiDB-Ansible enables you to quickly deploy a new TiDB cluster which includes PD, TiDB, TiKV, and the cluster monitoring modules.You can use the TiDB-Ansible configuration file to set up the cluster topology, completing all operation tasks with one click, including: Initializing operating system parameters Deploying the components Rolling upgrade, including module survival detection Cleaning data Cleaning environment Configuring monitoring modules  Prepare Before you start, make sure that you have: Several target machines with the following requirements: 4 or more machines. At least 3 instances for TiKV. Do not deploy TiKV together with TiDB or PD on the same machine. See Software and Hardware Requirements. Recommended Operating system: CentOS 7.3 or later Linux x86_64 architecture (AMD64) ext4 filesystem  Use ext4 filesystem for your data disks. Mount ext4 filesystem with the nodelalloc mount option. See Mount the data disk ext4 filesystem with options. The network between machines. Turn off the firewalls and iptables when deploying and turn them on after the deployment. The same time and time zone for all machines with the NTP service on to synchronize the correct time. See How to check whether the NTP service is normal. Create a normal tidb user account as the user who runs the service. The tidb user can sudo to the root user without a password. See How to configure SSH mutual trust and sudo without password.    &amp;gt; Note: When you deploy TiDB using Ansible, use SSD disks for the data directory of TiKV and PD nodes. A Control Machine with the following requirements: The Control Machine can be one of the managed nodes. It is recommended to install CentOS 7.3 or later version of Linux operating system (Python 2.7 involved by default). The Control Machine must have access to the Internet in order to download TiDB and related packages. Configure mutual trust of ssh authorized_key. In the Control Machine, you can login to the deployment target machine using tidb user account without a password. See How to configure SSH mutual trust and sudo without password.   Install Ansible and dependencies in the Control Machine Use the following method to install Ansible on the Control Machine of CentOS 7 system. Installation from the EPEL source includes Ansible dependencies automatically (such as Jinja2==2.7.2 MarkupSafe==0.11). After installation, you can view the version using ansible --version. Note: Make sure that the Ansible version is Ansible 2.4 or later, otherwise a compatibility issue occurs. # yum install epel-release  # yum install ansible curl  # ansible --version  ansible 2.4.2.0 For other systems, see Install Ansible.Download TiDB-Ansible to the Control Machine Login to the Control Machine using the tidb user account and enter the /home/tidb directory. Use the following command to download the corresponding version of TiDB-Ansible from GitHub TiDB-Ansible project. The default folder name is tidb-ansible.Download the 1.0 version:cd /home/tidb git clone -b release-1.0 https://github.com/pingcap/tidb-ansible.git orDownload the master version:cd /home/tidb git clone https://github.com/pingcap/tidb-ansible.git  Note: For the production environment, download the 1.0 version to deploy TiDB. Orchestrate the TiDB cluster The file path of inventory.ini: tidb-ansible/inventory.iniThe standard cluster has 6 machines: 2 TiDB nodes, the first TiDB machine is used as a monitor 3 PD nodes 3 TiKV nodes  The cluster topology of single TiKV instance on a single machine    Name Host IP Services     node1 172.16.10.1 PD1, TiDB1   node2 172.16.10.2 PD2, TiDB2   node3 172.16.10.3 PD3   node4 172.16.10.4 TiKV1   node5 172.16.10.5 TiKV2   node6 172.16.10.6 TiKV3    [tidb_servers] 172.16.10.1 172.16.10.2 [pd_servers] 172.16.10.1 172.16.10.2 172.16.10.3 [tikv_servers] 172.16.10.4 172.16.10.5 172.16.10.6 [monitoring_servers] 172.16.10.1 [grafana_servers] 172.16.10.1 [monitored_servers] 172.16.10.1 172.16.10.2 172.16.10.3 172.16.10.4 172.16.10.5 172.16.10.6 The cluster topology of multiple TiKV instances on a single machine Take two TiKV instances as an example:   Name Host IP Services     node1 172.16.10.1 PD1, TiDB1   node2 172.16.10.2 PD2, TiDB2   node3 172.16.10.3 PD3   node4 172.16.10.4 TiKV1-1, TiKV1-2   node5 172.16.10.5 TiKV2-1, TiKV2-2   node6 172.16.10.6 TiKV3-1, TiKV3-2    [tidb_servers] 172.16.10.1 172.16.10.2 [pd_servers] 172.16.10.1 172.16.10.2 172.16.10.3 [tikv_servers] TiKV1-1 ansible_host=172.16.10.4 deploy_dir=/data1/deploy tikv_port=20171 labels=&amp;#34;host=tikv1&amp;#34; TiKV1-2 ansible_host=172.16.10.4 deploy_dir=/data2/deploy tikv_port=20172 labels=&amp;#34;host=tikv1&amp;#34; TiKV1-3 ansible_host=172.16.10.4 deploy_dir=/data3/deploy tikv_port=20173 labels=&amp;#34;host=tikv1&amp;#34; TiKV2-1 ansible_host=172.16.10.5 deploy_dir=/data1/deploy tikv_port=20171 labels=&amp;#34;host=tikv2&amp;#34; TiKV2-2 ansible_host=172.16.10.5 deploy_dir=/data2/deploy tikv_port=20172 labels=&amp;#34;host=tikv2&amp;#34; TiKV2-3 ansible_host=172.16.10.5 deploy_dir=/data3/deploy tikv_port=20173 labels=&amp;#34;host=tikv2&amp;#34; TiKV3-1 ansible_host=172.16.10.6 deploy_dir=/data1/deploy tikv_port=20171 labels=&amp;#34;host=tikv3&amp;#34; TiKV3-2 ansible_host=172.16.10.6 deploy_dir=/data2/deploy tikv_port=20172 labels=&amp;#34;host=tikv3&amp;#34; TiKV3-3 ansible_host=172.16.10.6 deploy_dir=/data3/deploy tikv_port=20173 labels=&amp;#34;host=tikv3&amp;#34; [monitoring_servers] 172.16.10.1 [grafana_servers] 172.16.10.1 [monitored_servers] 172.16.10.1 172.16.10.2 172.16.10.3 172.16.10.4 172.16.10.5 172.16.10.6 ...... [pd_servers:vars] location_labels = [&amp;#34;host&amp;#34;] Edit the parameters in the service configuration file: For multiple TiKV instances, edit the end-point-concurrency and block-cache-size parameters in tidb-ansible/conf/tikv.yml: end-point-concurrency: keep the number lower than CPU Vcores rocksdb defaultcf block-cache-size(GB): MEM * 80% / TiKV instance number * 30% rocksdb writecf block-cache-size(GB): MEM * 80% / TiKV instance number * 45% rocksdb lockcf block-cache-size(GB): MEM * 80% / TiKV instance number * 2.5% (128 MB at a minimum) raftdb defaultcf block-cache-size(GB): MEM * 80% / TiKV instance number * 2.5% (128 MB at a minimum)  If multiple TiKV instances are deployed on a same physical disk, edit the capacity parameter in conf/tikv.yml: capacity: (DISK - log space) / TiKV instance number (the unit is GB)   Description of inventory.ini variables Description of the deployment directory You can configure the deployment directory using the deploy_dir variable. The global variable is set to /home/tidb/deploy by default, and it applies to all services. If the data disk is mounted on the /data1 directory, you can set it to /data1/deploy. For example:## Global variables [all:vars] deploy_dir = /data1/deploy To set a deployment directory separately for a service, you can configure host variables when configuring the service host list. Take the TiKV node as an example and it is similar for other services. You must add the first column alias to avoid confusion when the services are mixedly deployed.TiKV1-1 ansible_host=172.16.10.4 deploy_dir=/data1/deploy Description of other variables    Variable Description     cluster_name the name of a cluster, adjustable   tidb_version the version of TiDB, configured by default in TiDB-Ansible branches   deployment_method the method of deployment, binary by default, Docker optional   process_supervision the supervision way of processes, systemd by default, supervise optional   timezone the timezone of the managed node, adjustable, Asia/Shanghai by default, used with the set_timezone variable   set_timezone to edit the timezone of the managed node, True by default; False means closing   enable_elk …"},
		{"url": "https://pingcap.com/docs/op-guide/root-ansible-deployment/",
		"title": "Ansible Deployment Using the Root User Account", 
		"content": " Ansible Deployment Using the Root User Account  Note: The remote Ansible user (the ansible_user in the incentory.ini file) can use the root user account to deploy TiDB, but it is not recommended. The following example uses the tidb user account as the user running the service.To deploy TiDB using a root user account, take the following steps: Edit inventory.ini as follows.Remove the code comments for ansible_user = root, ansible_become = true and ansible_become_user. Add comments for ansible_user = tidb.## Connection # ssh via root: ansible_user = root ansible_become = true ansible_become_user = tidb # ssh via normal user # ansible_user = tidb Connect to the network and download TiDB binary to the Control Machine.ansible-playbook local_prepare.yml Initialize the system environment and edit the kernel parameters.ansible-playbook bootstrap.yml  Note: If the service user does not exist, the initialization operation will automatically create the user. If the remote connection using the root user requires a password, use the -k (lower case) parameter. This applies to other playbooks as well:ansible-playbook bootstrap.yml -k Deploy the TiDB cluster.ansible-playbook deploy.yml -k Start the TiDB cluster.ansible-playbook start.yml -k  "},
		{"url": "https://pingcap.com/docs/op-guide/backup-restore/",
		"title": "Backup and Restore", 
		"content": " Backup and Restore About This document describes how to backup and restore the data of TiDB. Currently, this document only covers full backup and restoration.Here we assume that the TiDB service information is as follows:   Name Address Port User Password     TiDB 127.0.0.1 4000 root *    Use the following tools for data backup and restoration: mydumper: to export data from TiDB loader: to import data into TiDB  Download TiDB toolset (Linux) # Download the tool package. wget http://download.pingcap.org/tidb-enterprise-tools-latest-linux-amd64.tar.gz wget http://download.pingcap.org/tidb-enterprise-tools-latest-linux-amd64.sha256 # Check the file integrity. If the result is OK, the file is correct. sha256sum -c tidb-enterprise-tools-latest-linux-amd64.sha256 # Extract the package. tar -xzf tidb-enterprise-tools-latest-linux-amd64.tar.gz cd tidb-enterprise-tools-latest-linux-amd64 Full backup and restoration using mydumper/loader You can use mydumper to export data from MySQL and loader to import the data into TiDB. Note: Although TiDB also supports the official mysqldump tool from MySQL for data migration, it is not recommended to use it. Its performance is much lower than mydumper/loader and it takes much time to migrate large amounts of data. mydumper/loader is more powerful. For more information, see https://github.com/maxbube/mydumper. Best practices of full backup and restoration using mydumper/loader To quickly backup and restore data (especially large amounts of data), refer to the following recommendations: Keep the exported data file as small as possible and it is recommended keep it within 64M. You can use the -F parameter to set the value. You can adjust the -t parameter of loader based on the number and the load of TiKV instances. For example, if there are three TiKV instances, -t can be set to 3 * (1 ~ n). If the load of TiKV is too high and the log backoffer.maxSleep 15000ms is exceeded is displayed many times, decrease the value of -t; otherwise, increase it.  An example of restoring data and related configuration  The total size of the exported files is 214G. A single table has 8 columns and 2 billion rows. The cluster topology:  12 TiKV instances: 4 nodes, 3 TiKV instances per node 4 TiDB instances 3 PD instances  The configuration of each node:  CPU: Intel Xeon E5-2670 v3 @ 2.30GHz 48 vCPU [2 x 12 physical cores] Memory: 128G Disk: sda [raid 10, 300G] sdb[RAID 5, 2T] Operating System: CentOS 7.3  The -F parameter of mydumper is set to 16 and the -t parameter of loader is set to 64.  Results: It takes 11 hours to import all the data, which is 19.4G/hour.Backup data from TiDB Use mydumper to backup data from TiDB../bin/mydumper -h 127.0.0.1 -P 4000 -u root -t 16 -F 64 -B test -T t1,t2 --skip-tz-utc -o ./var/test In this command, -B test: means the data is exported from the test database. -T t1,t2: means only the t1 and t2 tables are exported. -t 16: means 16 threads are used to export the data. -F 64: means a table is partitioned into chunks and one chunk is 64MB. --skip-tz-utc: the purpose of adding this parameter is to ignore the inconsistency of time zone setting between MySQL and the data exporting machine and to disable automatic conversion.  Restore data into TiDB To restore data into TiDB, use loader to import the previously exported data. See Loader instructions for more information../bin/loader -h 127.0.0.1 -u root -P 4000 -t 32 -d ./var/test After the data is imported, you can view the data in TiDB using the MySQL client:mysql -h127.0.0.1 -P4000 -uroot mysql&amp;gt; show tables; +----------------+ | Tables_in_test | +----------------+ | t1 | | t2 | +----------------+  mysql&amp;gt; select * from t1; +----+------+ | id | age | +----+------+ | 1 | 1 | | 2 | 2 | | 3 | 3 | +----+------+  mysql&amp;gt; select * from t2; +----+------+ | id | name | +----+------+ | 1 | a | | 2 | b | | 3 | c | +----+------+"},
		{"url": "https://pingcap.com/docs/sql/bit-functions-and-operators/",
		"title": "Bit Functions and Operators", 
		"content": " Bit Functions and Operators In TiDB, the usage of bit functions and operators is similar to MySQL. See Bit Functions and Operators.Bit functions and operators   Name Description     BIT_COUNT() Return the number of bits that are set as 1   &amp;amp; Bitwise AND   ~ Bitwise inversion   | Bitwise OR   0 Bitwise XOR   &amp;lt;&amp;lt; Left shift   &amp;gt;&amp;gt; Right shift    "},
		{"url": "https://pingcap.com/docs-cn/sql/literal-value-bit-value/",
		"title": "Bit-value Literals", 
		"content": " Bit-value Literals 位值字面值用 b 或者 0b 做前缀，后接以 0 跟 1 组成的二进制数字。其中 0b 是区分大小写的，0B 是会报错的。合法的 Bit-value： b&amp;rsquo;01&amp;rsquo; B&amp;rsquo;01&amp;rsquo; 0b01  非法的 Bit-value： b&amp;rsquo;2&amp;rsquo; (2 不是二进制数值, 必须为 0 或 1) 0B01 (0B 必须是小写 0b)  默认情况，位值字面值是一个二进制字符串。Bit-value 是作为二进制返回的，所以输出到 MySQL Client 可能会显示不出来，如果要转换为可打印的字符，可以使用内建函数 BIN() 或者 HEX()：CREATE TABLE t (b BIT(8)); INSERT INTO t SET b = b&amp;#39;00010011&amp;#39;; INSERT INTO t SET b = b&amp;#39;1110&amp;#39;; INSERT INTO t SET b = b&amp;#39;100101&amp;#39;; mysql&amp;gt; SELECT b+0, BIN(b), HEX(b) FROM t; +------+--------+--------+ | b+0 | BIN(b) | HEX(b) | +------+--------+--------+ | 19 | 10011 | 13 | | 14 | 1110 | E | | 37 | 100101 | 25 | +------+--------+--------+ 3 rows in set (0.00 sec)"},
		{"url": "https://pingcap.com/recruit-cn/engineer/bizdev-cloud-engineer/",
		"title": "Bizdev &amp; Cloud Engineer", 
		"content": " Bizdev &amp;amp; Cloud Engineer 岗位职责  TiDB 基于 Kubernetes 平台自动化部署运维工具的开发 TiDB 与公有云 / 私有云平台整合  职位要求  扎实的编程能力，熟悉 C/C++/Go/Rust/Python 一种编程语言 对容器技术有较深入的了解 熟悉 Swarm/Mesos/Kubernetes 等容器编排系统中至少一种 熟练使用 Linux 具备大型分布式系统监控、分析和故障排查等相关经验 有国内外公有云平台使用和运维经验 良好的沟通能力和技巧  加分项  熟悉 Ansible / Saltstack 等自动化部署工具 为 Docker / Kubernetes 贡献过代码 熟悉 BGP，Overlay 网络  待遇 20K - 40K + 期权, 13薪 + 奖金, 优秀者可面议工作地点 北京，上海，广州，杭州，特别优秀可 remote"},
		{"url": "https://pingcap.com/recruit-cn/engineer/bizdev-fe-engineer/",
		"title": "Bizdev &amp; FE Engineer", 
		"content": " Bizdev &amp;amp; FE Engineer 岗位职责  负责为商业产品和工具等开发流畅酷炫富有科技范的前端界面 前端组件设计，框架定制和保证快速迭代的速度和质量，探索前端开发新规范和模式  职位要求  三年以上相关领域开发经验，扎实的编程能力 优秀的发现和解决问题能力，良好的沟通能力，具备团队合作精神 熟悉 JavaScript /TypeScript 和新语言规范和语法特性 如 ES2015 等 精通 webpack 构建，nodejs 脚本开发和常用 prettier，eslint, babel 等配置 熟悉 React/Angular/Vue 等现代 Web 前端框架使用和实现原理 熟悉富应用 SPA 开发模式，如单向数据流 Flux / Redux，响应式编程 rxjs / cyclejs  加分项  拥抱开源，对前沿技术有浓厚的热情和探索欲望，有开源项目经历 良好的适应和学习能力对自己不设限，挑战如：数据可视化，监控告警 Devops，商业/工具产品设计等方向 其他例如您熟悉 Electron、看过 Chromium 源代码、写过一些关于 JavaScript 技术的博客文章… 具体不限，我们愿闻其详  待遇 20K - 40K + 期权, 13薪 + 奖金, 优秀者可面议工作地址 北京"},
		{"url": "https://pingcap.com/recruit-cn/engineer/bizdev-sre-engineer/",
		"title": "Bizdev &amp; SRE Engineer", 
		"content": " Bizdev &amp;amp; SRE Engineer 岗位职责  管理维护公司内部各种资源，让一切自动化起来 Linux 系统调优和诊断工具开发  职位要求  以“折腾” Linux 为乐 掌握一门基础编程语言，如 C/C++ / Go / Rust / &amp;hellip; 熟练掌握一门脚本语言，如 shell / Python / Perl / &amp;hellip; 基于系统内核的诊断和调优，工具 熟悉 Linux kernel 和各个子系统(网络、存储，内存，调度、文件系统等)，熟悉常见的应用和系统 profile 工具。 熟悉 TCP / IP 基本原理 精通路由、交换、防火墙、四层交换等网络技术，有较强的网络安全意识 熟悉配置调试主流厂商如华为、Juniper 网络设备，有相关项目实施运维经验 责任心强、积极主动，抗压能力强，有良好的沟通能力和团队合作能力  加分项  熟悉 Systemtap、Perf 等分析调试工具优先考虑。 有 Cisco、H3C 网络认证者优先考虑。  待遇 20K - 40K + 期权, 13薪 + 奖金, 优秀者可面议工作地点 北京，上海，广州，杭州，特别优秀可 remote"},
		{"url": "https://pingcap.com/recruit-cn/engineer/bizdev-tools-engineer/",
		"title": "Bizdev &amp; Tools Engineer", 
		"content": " Bizdev &amp;amp; Tools Engineer 岗位职责  TiDB 商业工具开发，完善 TiDB 的周边生态，提升用户使用体验 建设高度智能的自动化测试系统，进行各种破坏性测试，验证 TiDB 的可靠性  职位要求  扎实的编程能力，熟悉 C/C++/Go/Rust 其中一种编程语言 熟悉大型分布式系统，具备冷静分析复杂问题能力 熟悉常用算法和数据结构 深入了解过操作系统和网络 良好的沟通能力和技巧，以及抗压能力 了解 Automated Reasoning / Static Analysis 等测试方法及工具  加分项  爱折腾，强烈的 Hack 精神 TopCoder, Codeforces 黄色以上  待遇 20K - 40K + 期权, 13薪 + 奖金, 优秀者可面议工作地点 北京，上海，广州，杭州，特别优秀可 remote"},
		{"url": "https://pingcap.com/blog-cn/",
		"title": "Blog-cns", 
		"content": ""},
		{"url": "https://pingcap.com/blog/",
		"title": "Blogs", 
		"content": ""},
		{"url": "https://pingcap.com/docs-cn/sql/literal-value-boolean/",
		"title": "Boolean Literals", 
		"content": " Boolean Literals 常量 TRUE 和 FALSE 等于 1 和 0，它是大小写不敏感的。mysql&amp;gt; SELECT TRUE, true, tRuE, FALSE, FaLsE, false; +------+------+------+-------+-------+-------+ | TRUE | true | tRuE | FALSE | FaLsE | false | +------+------+------+-------+-------+-------+ | 1 | 1 | 1 | 0 | 0 | 0 | +------+------+------+-------+-------+-------+ 1 row in set (0.00 sec)"},
		{"url": "https://pingcap.com/docs/sql/cast-functions-and-operators/",
		"title": "Cast Functions and Operators", 
		"content": " Cast Functions and Operators    Name Description     BINARY Cast a string to a binary string   CAST() Cast a value as a certain type   CONVERT() Cast a value as a certain type    Cast functions and operators enable conversion of values from one data type to another.For details, see here."},
		{"url": "https://pingcap.com/docs/sql/character-set-configuration/",
		"title": "Character Set Configuration", 
		"content": " Character Set Configuration Currently, TiDB does not support configuring the character set. The default character set is utf8.For more information, see Character Set Configuration in MySQL."},
		{"url": "https://pingcap.com/docs/sql/character-set-support/",
		"title": "Character Set Support", 
		"content": " Character Set Support A character set is a set of symbols and encodings. A collation is a set of rules for comparing characters in a character set.Currently, TiDB supports the following character sets:mysql&amp;gt; SHOW CHARACTER SET; +---------|---------------|-------------------|--------+ | Charset | Description | Default collation | Maxlen | +---------|---------------|-------------------|--------+ | utf8 | UTF-8 Unicode | utf8_bin | 3 | | utf8mb4 | UTF-8 Unicode | utf8mb4_bin | 4 | | ascii | US ASCII | ascii_bin | 1 | | latin1 | Latin1 | latin1_bin | 1 | | binary | binary | binary | 1 | +---------|---------------|-------------------|--------+ 5 rows in set (0.00 sec)  Note: In TiDB, utf8 is treated as utf8mb4. Each character set has at least one collation. Most of the character sets have several collations. You can use the following statement to display the available character sets:mysql&amp;gt; SHOW COLLATION WHERE Charset = &amp;#39;latin1&amp;#39;; +-------------------|---------|------|---------|----------|---------+ | Collation | Charset | Id | Default | Compiled | Sortlen | +-------------------|---------|------|---------|----------|---------+ | latin1_german1_ci | latin1 | 5 | | Yes | 1 | | latin1_swedish_ci | latin1 | 8 | Yes | Yes | 1 | | latin1_danish_ci | latin1 | 15 | | Yes | 1 | | latin1_german2_ci | latin1 | 31 | | Yes | 1 | | latin1_bin | latin1 | 47 | | Yes | 1 | | latin1_general_ci | latin1 | 48 | | Yes | 1 | | latin1_general_cs | latin1 | 49 | | Yes | 1 | | latin1_spanish_ci | latin1 | 94 | | Yes | 1 | +-------------------|---------|------|---------|----------|---------+ 8 rows in set (0.00 sec) The latin1 collations have the following meanings:   Collation Meaning     latin1_bin Binary according to latin1 encoding   latin1_danish_ci Danish/Norwegian   latin1_general_ci Multilingual (Western European)   latin1_general_cs Multilingual (ISO Western European), case sensitive   latin1_german1_ci German DIN-1 (dictionary order)   latin1_german2_ci German DIN-2 (phone book order)   latin1_spanish_ci Modern Spanish   latin1_swedish_ci Swedish/Finnish    Each character set has a default collation. For example, the default collation for utf8 is utf8_bin. Note: The collations in TiDB are case sensitive. Collation naming conventions The collation names in TiDB follow these conventions: The prefix of a collation is its corresponding character set, generally followed by one or more suffixes indicating other collation characteristic. For example, utf8_general_ci and latin1_swedish_ci are collations for the utf8 and latin1 character sets, respectively. The binary character set has a single collation, also named binary, with no suffixes. A language-specific collation includes a language name. For example, utf8_turkish_ci and utf8_hungarian_ci sort characters for the utf8 character set using the rules of Turkish and Hungarian, respectively. Collation suffixes indicate whether a collation is case and accent sensitive, or binary. The following table shows the suffixes used to indicate these characteristics.   Suffix Meaning     _ai Accent insensitive   _as Accent insensitive   _ci Case insensitive   _cs Case sensitive   _bin Binary      Note: For now, TiDB supports on some of the collations in the above table. Database character set and collation Each database has a character set and a collation. You can use the CREATE DATABASE statement to specify the database character set and collation:CREATE DATABASE db_name [[DEFAULT] CHARACTER SET charset_name] [[DEFAULT] COLLATE collation_name] Where DATABASE can be replaced with SCHEMA.Different databases can use different character sets and collations. Use the character_set_database and collation_database to see the character set and collation of the current database:mysql&amp;gt; create schema test1 character set utf8 COLLATE uft8_general_ci; Query OK, 0 rows affected (0.09 sec) mysql&amp;gt; use test1; Database changed mysql&amp;gt; SELECT @@character_set_database, @@collation_database; +--------------------------|----------------------+ | @@character_set_database | @@collation_database | +--------------------------|----------------------+ | utf8 | uft8_general_ci | +--------------------------|----------------------+ 1 row in set (0.00 sec) mysql&amp;gt; create schema test2 character set latin1 COLLATE latin1_general_ci; Query OK, 0 rows affected (0.09 sec) mysql&amp;gt; use test2; Database changed mysql&amp;gt; SELECT @@character_set_database, @@collation_database; +--------------------------|----------------------+ | @@character_set_database | @@collation_database | +--------------------------|----------------------+ | latin1 | latin1_general_ci | +--------------------------|----------------------+ 1 row in set (0.00 sec) You can also see the two values in INFORMATION_SCHEMA:SELECT DEFAULT_CHARACTER_SET_NAME, DEFAULT_COLLATION_NAME FROM INFORMATION_SCHEMA.SCHEMATA WHERE SCHEMA_NAME = &amp;#39;db_name&amp;#39;; Table character set and collation You can use the following statement to specify the character set and collation for tables:CREATE TABLE tbl_name (column_list) [[DEFAULT] CHARACTER SET charset_name] [COLLATE collation_name]] ALTER TABLE tbl_name [[DEFAULT] CHARACTER SET charset_name] [COLLATE collation_name] For example:mysql&amp;gt; CREATE TABLE t1(a int) CHARACTER SET utf8 COLLATE utf8_general_ci; Query OK, 0 rows affected (0.08 sec) The table character set and collation are used as the default values for column definitions if the column character set and collation are not specified in individual column definitions.Column character set and collation See the following table for the character set and collation syntax for columns:col_name {CHAR | VARCHAR | TEXT} (col_length) [CHARACTER SET charset_name] [COLLATE collation_name] col_name {ENUM | SET} (val_list) [CHARACTER SET charset_name] [COLLATE collation_name] Connection character sets and collations  The server character set and collation are the values of the character_set_server and collation_server system variables. The character set and collation of the default database are the values of the character_set_database and collation_database system variables. You can use character_set_connection and collation_connection to specify the character set and collation for each connection. The character_set_client variable is to set the client character set. Before returning the result, the character_set_results system variable indicates the character set in which the server returns query results to the client, including the metadata of the result.  You can use the following statement to specify a particular collation that is related to the client: SET NAMES &#39;charset_name&#39; [COLLATE &#39;collation_name&#39;]SET NAMES indicates what character set the client will use to send SQL statements to the server. SET NAMES utf8 indicates that all the requests from the client use utf8, as well as the results from the server.The SET NAMES &#39;charset_name&#39; statement is equivalent to the following statement combination:SET character_set_client = charset_name; SET character_set_results = charset_name; SET character_set_connection = charset_name; COLLATE is optional, if absent, the default collation of the charset_name is used. SET CHARACTER SET &#39;charset_name&#39;Similar to SET NAMES, the SET NAMES &#39;charset_name&#39; statement is equivalent to the following statement combination:SET character_set_client = charset_name; SET character_set_results = charset_name; SET collation_connection = @@collation_database;  For more information, see Connection Character Sets and Collations in MySQL."},
		{"url": "https://pingcap.com/docs/sql/comment-syntax/",
		"title": "Comment Syntax", 
		"content": " Comment Syntax TiDB supports three comment styles: Use # to comment a line. Use -- to comment a line, and this style requires at least one whitespace after --. Use /* */ to comment a block or multiple lines.  Example:mysql&amp;gt; SELECT 1+1; # This comment continues to the end of line +------+ | 1+1 | +------+ | 2 | +------+ 1 row in set (0.00 sec) mysql&amp;gt; SELECT 1+1; -- This comment continues to the end of line +------+ | 1+1 | +------+ | 2 | +------+ 1 row in set (0.00 sec) mysql&amp;gt; SELECT 1 /* this is an in-line comment */ + 1; +--------+ | 1 + 1 | +--------+ | 2 | +--------+ 1 row in set (0.01 sec) mysql&amp;gt; SELECT 1+ -&amp;gt; /* /*&amp;gt; this is a /*&amp;gt; multiple-line comment /*&amp;gt; */ -&amp;gt; 1; +-------+ | 1+ 1 | +-------+ | 2 | +-------+ 1 row in set (0.00 sec) mysql&amp;gt; SELECT 1+1--1; +--------+ | 1+1--1 | +--------+ | 3 | +--------+ 1 row in set (0.01 sec) Similar to MySQL, TiDB supports a variant of C comment style:/*! Specific code */ In this comment style, TiDB runs the statements in the comment. The syntax is used to make these SQL statements ignored in other databases and run only in TiDB.For example:SELECT /*! STRAIGHT_JOIN */ col1 FROM table1,table2 WHERE ... In TiDB, you can also use another version:SELECT STRAIGHT_JOIN col1 FROM table1,table2 WHERE ... If the server version number is specified in the comment, for example, /*!50110 KEY_BLOCK_SIZE=1024 */, in MySQL it means that the contents in this comment is processed only when the MySQL version is or higher than 5.1.10. But in TiDB, the version number does not work and all contents in the comment are processed.Another type of comment is specially treated as the Hint optimizer:SELECT /*+ hint */ FROM ...; Since Hint is involved in comments like /*+ xxx */, the MySQL client clears the comment by default in versions earlier than 5.7.7. To use Hint in those earlier versions, add the --comments option when you start the client. For example:mysql -h 127.0.0.1 -P 4000 -uroot --comments` Currently, TiDB supports the following specific types of Hint: TIDB_SMJ(t1, t2)   SELECT /*+ TIDB_SMJ(t1, t2) */ * from t1，t2 where t1.id = t2.id The Hint optimizer uses the Sort Merge Join algorithm, which usually consumes less memory but takes longer to run. This is recommended when the amount of data is too large, or the system memory is insufficient. TIDB_INLJ(t1, t2)  SELECT /*+ TIDB_INLJ(t1, t2) */ * from t1，t2 where t1.id = t2.id The Hint optimizer uses the Index Nested Loop Join algorithm. This algorithm is faster in some scenarios and consumes less system resources, while it may be slower in some other scenarios and consumes more system resources. For the scenarios that have a small result set (less than 10,000 lines) after the filtration of WHERE condition, you can try to use it. The parameter in TIDB_INLJ() is the candidate table of the driving table (outer table) when the query plan is created. In other words, TIDB_INLJ(t1) only uses t1 as the driving table to create the query plan.For more information, see Comment Syntax."},
		{"url": "https://pingcap.com/docs/sql/mysql-compatibility/",
		"title": "Compatibility with MySQL", 
		"content": " Compatibility with MySQL TiDB supports the majority of the MySQL grammar, including cross-row transactions, JOIN, subquery, and so on. You can connect to TiDB directly using your own MySQL client. If your existing business is developed based on MySQL, you can replace MySQL with TiDB to power your application without changing a single line of code in most cases.TiDB is compatible with most of the MySQL database management &amp;amp; administration tools such as PHPMyAdmin, Navicat, MySQL Workbench, and so on. It also supports the database backup tools, such as mysqldump and mydumper/myloader.However, in TiDB, the following MySQL features are not supported for the time being or are different:Unsupported features  Stored Procedures View Trigger The user-defined functions The FOREIGN KEY constraints The FULLTEXT indexes The Spatial indexes The Non-UTF-8 characters The JSON data type Add primary key Drop primary key  Features that are different from MySQL Auto-increment ID The auto-increment ID feature in TiDB is only guaranteed to be automatically incremental and unique but is not guaranteed to be allocated sequentially. Currently, TiDB is allocating IDs in batches. If data is inserted into multiple TiDB servers simultaneously, the allocated IDs are not sequential. Warning:If you use the auto-increment ID in a cluster with multiple TiDB servers, do not mix the default value and the custom value, because it reports an error in the following situation:In a cluster of two TiDB servers, namely TiDB A and TiDB B, TiDB A caches [1,5000] auto-increment ID, while TiDB B caches [5001,10000] auto-increment ID. Use the following statement to create a table with auto-increment ID:create table t(id int unique key auto_increment, c int); The statement is executed as follows: The client inserts a statement to TiDB B which sets the id to be 1 and the statement is executed successfully. The client inserts a record to TiDB A which sets the id set to the default value 1. In this case, it returns Duplicated Error.   Built-in functions TiDB supports most of the MySQL built-in functions, but not all. See TiDB SQL Grammar for the supported functions.DDL TiDB implements the asynchronous schema changes algorithm in F1. The Data Manipulation Language (DML) operations cannot be blocked during DDL the execution. Currently, the supported DDL includes: Create Database Drop Database Create Table Drop Table Add Index: Does not support creating muliple indexs at the same time. Drop Index Add Column:  Does not support creating muliple columns at the same time. Does not support setting a column as the primary key, or creating a unique index, or specifying auto_increment while adding it.  Drop Column: Does not support dropping the primary key column or index column. Alter Column Change/Modify Column  Supports changing/modifying the types among the following integer types: TinyInt，SmallInt，MediumInt，Int，BigInt. Supports changing/modifying the types among the following string types: Char，Varchar，Text，TinyText，MediumText，LongText Support changing/modifying the types among the following string types: Blob，TinyBlob，MediumBlob，LongBlob.   Note: The change/modifying column operation cannot make the length of the original type become shorter and it cannot change the unsigned/charset/collate attributes of the column.- Supports changing the following type definitions: default value，comment，null，not null and OnUpdate, but does not support changing from null to not null. - Supports parsing the `LOCK [=] {DEFAULT|NONE|SHARED|EXCLUSIVE}` syntax, but there is no actual operation.  Truncate Table Rename Table Create Table Like  Transaction TiDB implements an optimistic transaction model. Unlike MySQL, which uses row-level locking to avoid write conflict, in TiDB, the write conflict is checked only in the commit process during the execution of the statements like Update, Insert, Delete, and so on.Note: On the business side, remember to check the returned results of commit because even there is no error in the execution, there might be errors in the commit process.Load data  Syntax:LOAD DATA LOCAL INFILE &amp;#39;file_name&amp;#39; INTO TABLE table_name {FIELDS | COLUMNS} TERMINATED BY &amp;#39;string&amp;#39; ENCLOSED BY &amp;#39;char&amp;#39; ESCAPED BY &amp;#39;char&amp;#39; LINES STARTING BY &amp;#39;string&amp;#39; TERMINATED BY &amp;#39;string&amp;#39; (col_name ...); Currently, the supported ESCAPED BY characters are: //. TransactionWhen TiDB is in the execution of loading data, by default, a record with 20,000 rows of data is seen as a transaction for persistent storage. If a load data operation inserts more than 20,000 rows, it will be divided into multiple transactions to commit. If an error occurs in one transaction, this transaction in process will not be committed. However, transactions before that are committed successfully. In this case, a part of the load data operation is successfully inserted, and the rest of the data insertion fails. But MySQL treats a load data operation as a transaction, one error leads to the failure of the entire load data operation.  "},
		{"url": "https://pingcap.com/docs/op-guide/configuration/",
		"title": "Configuration Flags", 
		"content": " Configuration Flags TiDB, TiKV and PD are configurable using command-line flags and environment variables.TiDB The default TiDB ports are 4000 for client requests and 10080 for status report.--binlog-socket  The TiDB services use the unix socket file for internal connections, such as the PUMP service Default: `` You can use &amp;ldquo;/tmp/pump.sock&amp;rdquo; to accept the communication of PUMP unix socket file.  --cross-join  To enable (true) or disable (false) the cross join without any equal conditions Default: true The value can be true or false. By default, true is to enable join without any equal conditions (the Where field). If you set the value to false, the server refuses to run the join statement.  --host  The host address that the TiDB server monitors Default: &amp;ldquo;0.0.0.0&amp;rdquo; The TiDB server monitors this address. The &amp;ldquo;0.0.0.0&amp;rdquo; monitors all network cards. If you have multiple network cards, specify the network card that provides service, such as 192.168.100.113.  --join-concurrency int  The number of goroutine when join-concurrency executes join concurrently Default: 5 The number depends on the amount of data and data distribution, usually the larger the better, and a larger number means a larger CPU overhead.  -L  The log level Default: &amp;ldquo;info&amp;rdquo; You can choose from debug, info, warn, error, or fatal.  --lease  The schema lease time in seconds Default: &amp;ldquo;10&amp;rdquo; This is the schema lease time that is used in online schema changes. The value will affect the DDL statement running time. Do not change it unless you understand the internal mechanism.  --log-file  The log file Default: &amp;ldquo;&amp;rdquo; If this flag is not set, logs will be written to stderr. Otherwise, logs will be stored in the log file which will be automatically rotated every day.  --metrics-addr  The Prometheus pushgateway address Default: &amp;ldquo;&amp;rdquo; Leaving it empty stops the Prometheus client from pushing. The format is:--metrics-addr=192.168.100.115:9091  --metrics-interval  The Prometheus client push interval in seconds Default: 0 Setting the value to 0 stops the Prometheus client from pushing.  -P  The monitoring port of TiDB services Default: &amp;ldquo;4000&amp;rdquo; The TiDB server accepts MySQL client requests from this port.  --path  The path to the data directory for local storage engines like &amp;ldquo;goleveldb&amp;rdquo; and &amp;ldquo;BoltDB&amp;rdquo; Do not set --path for the &amp;ldquo;memory&amp;rdquo; storage engine. For the distributed storage engine like TiKV, --path specifies the actual PD address. Assuming that you deploy the PD server on 192.168.100.113:2379, 192.168.100.114:2379 and 192.168.100.115:2379, the value of --path is &amp;ldquo;192.168.100.113:2379, 192.168.100.114:2379, 192.168.100.115:2379&amp;rdquo;. Default: &amp;ldquo;/tmp/tidb&amp;rdquo;  --perfschema  To enable(true) or disable(false) the performance schema Default: false The value can be (true) or (false). (true) is to enable and (false) is to disable. The Performance Schema provides a way to inspect internal execution of the server at runtime. See performance schema for more information. If you enable the performance schema, the performance is affected.  --privilege  To enable(true) or disable(false) the privilege check(for debugging) Default: true The value can be (true) or (false). (true) is to enable and (false) is to disable. This option is deprecated and will be removed.  --proxy-protocol-networks  The proxy server&amp;rsquo;s IP addresses that allowed by PROXY Protocol. Default: &amp;ldquo;&amp;rdquo; (empty string) The value can be IP address (192.168.1.50) or CIDR (192.168.1.0/24), if more than one address (or CIDR) required, use , to split. * means any IP addresses. Leaving it empty disable PROXY Protocol.  --proxy-protocol-header-timeout  PROXY Protocol header read timeout. Default: 5 (seconds) The value set timeout for the PROXY protocol header read. The unit is second. You should not set this value to 0.  --query-log-max-len int  The maximum length of SQL statements recorded in the log Default: 2048 Overlong requests are truncated when output to the log.  --report-status  To enable(true) or disable(false) the status report and pprof tool Default: true The value can be (true) or (false). (true) is to enable metrics and pprof. (false) is to disable metrics and pprof.  --retry-limit int  The maximum number of retries when a transaction meets conflicts Default: 10 A large number of retries affects the TiDB cluster performance.  --run-ddl  To see whether the tidb-server runs DDL statements, and set when the number of tidb-server is over two in the cluster Default: true The value can be (true) or (false). (true) indicates the tidb-server runs DDL itself. (false) indicates the tidb-server does not run DDL itself.  --skip-grant-table  To enable anyone to connect without a password and with all privileges Default: false The value can be (true) or (false). This option is usually used to reset password, and enabling it requires the root privileges.  --slow-threshold int  The SQL statements with a larger value of this parameter are recorded. Default: 300 The value can only be an integer (int), and the unit is millisecond.  --socket string  The TiDB services use the unix socket file for external connections. Default: &amp;ldquo;&amp;rdquo; You can use &amp;ldquo;/tmp/tidb.sock&amp;rdquo; to open the unix socket file.  --ssl-ca  The path to a file in PEM format that contains a list of trusted SSL certificate authorities. Default: &amp;ldquo;&amp;rdquo; When this option is specified along with --ssl-cert and --ssl-key, the server verifies the client&amp;rsquo;s certificate via this CA list if the client provides its certificate accordingly. The secure connection will be established without client verification if the client does not provide a certificate even when this option is set.  --ssl-cert  The path to an SSL certificate file in PEM format to use for establishing a secure connection. Default: &amp;ldquo;&amp;rdquo; When this option is specified along with --ssl-key, the server permits but does not require secure connections. If the specified certificate or key is not valid, the server still starts normally but does not permit secure connections.  --ssl-key  The path to an SSL key file in PEM format to use for establishing a secure connection, namely the private key of the certificate you specified by --ssl-cert. Default: &amp;ldquo;&amp;rdquo; Currently TiDB does not support keys protected by a passphrase.  --status  The status report port for TiDB server Default: &amp;ldquo;10080&amp;rdquo; This is used to get server internal data. The data includes prometheus metrics and pprof. Prometheus metrics can be got through &amp;ldquo;http://host:status_port/metrics&amp;quot;. Pprof data can be got through &amp;ldquo;http://host:status_port/debug/pprof&amp;quot;.  --statsLease string  Scan the full table incrementally, and analyze information like the data amount and index of the table Default: 3s Before you use --statsLease string, run analyze table name manually. The statistics are updated automatically and stored in TiKV, taking up some memory.  --store  The storage engine type Human-readable name for this member. Default: &amp;ldquo;goleveldb&amp;rdquo; You can choose from &amp;ldquo;memory&amp;rdquo;, &amp;ldquo;goleveldb&amp;rdquo;, &amp;ldquo;BoltDB&amp;rdquo; or &amp;ldquo;TiKV&amp;rdquo;. The first three are all local storage engines. TiKV is a distributed storage engine.  --tcp-keep-alive  keepalive is enabled in the tcp layer of TiDB. Default: false  Placement Driver (PD) --advertise-client-urls  The advertise URL list for client traffic from outside Default: ${client-urls} If the client cannot connect to PD through the default listening client URLs, you must manually set the advertise client URLs explicitly. For example, the internal IP address of Docker is 172.17.0.1, while the IP address of the host is 192.168.100.113 and the port mapping is set to -p 2379:2379. In this case, you can set --advertise-client-urls to &amp;ldquo;http://192.168.100.113:2379&amp;quot;. The client can find this service through …"},
		{"url": "https://pingcap.com/docs/community/",
		"title": "Connect with us", 
		"content": " Connect with us  Twitter: @PingCAP Reddit: https://www.reddit.com/r/TiDB/ Stack Overflow: https://stackoverflow.com/questions/tagged/tidb Mailing list: Google Group  "},
		{"url": "https://pingcap.com/docs/sql/connection-and-APIs/",
		"title": "Connectors and APIs", 
		"content": " Connectors and APIs Database Connectors provide connectivity to the TiDB server for client programs. APIs provide low-level access to the MySQL protocol and MySQL resources. Both Connectors and the APIs enable you to connect and execute MySQL statements from another language or environment, including ODBC, Java (JDBC), Perl, Python, PHP, Ruby and C.TiDB is compatible with all Connectors and APIs of MySQL (5.6, 5.7), including: MySQL Connector/C MySQL Connector/C++ MySQL Connector/J MySQL Connector/Net MySQL Connector/ODBC MySQL Connector/Python MySQL C API MySQL PHP API MySQL Perl API MySQL Python API MySQL Ruby APIs MySQL Tcl API MySQL Eiffel Wrapper Mysql Go API  Connect to TiDB using MySQL Connectors Oracle develops the following APIs and TiDB is compatible with all of them: MySQL Connector/C: a standalone replacement for the libmysqlclient, to be used for C applications MySQL Connector/C++：to enable C++ applications to connect to MySQL MySQL Connector/J：to enable Java applications to connect to MySQL using the standard JDBC API MySQL Connector/Net：to enable .Net applications to connect to MySQL; MySQL for Visual Studio uses this; support Microsoft Visual Studio 2012, 2013, 2015 and 2017 versions MySQL Connector/ODBC：the standard ODBC API; support Windows, Unix, and OS X platforms MySQL Connector/Python：to enable Python applications to connect to MySQL, compliant with the Python DB API version 2.0  Connect to TiDB using MySQL C API If you use C language programs to connect to TiDB, you can connect to libmysqlclient directly and use the MySQL C API. This is one of the major connection methods using C language, widely used by various clients and APIs, including Connector/C.Connect to TiDB using third-party MySQL APIs The third-party APIs are not developed by Oracle. The following table lists the commonly used third-party APIs:   Environment API Type Notes     Ada GNU Ada MySQL Bindings libmysqlclient See MySQL Bindings for GNU Ada   C C API libmysqlclient See Section 27.8, “MySQL C API”   C Connector/C Replacement for libmysqlclient See MySQL Connector/C Developer Guide   C++ Connector/C++ libmysqlclient See MySQL Connector/C++ Developer Guide    MySQL++ libmysqlclient See MySQL++ Web site    MySQL wrapped libmysqlclient See MySQL wrapped   Go go-sql-driver Native Driver See Mysql Go API   Cocoa MySQL-Cocoa libmysqlclient Compatible with the Objective-C Cocoa environment. See http://mysql-cocoa.sourceforge.net/   D MySQL for D libmysqlclient See MySQL for D   Eiffel Eiffel MySQL libmysqlclient See Section 27.14, “MySQL Eiffel Wrapper”   Erlang erlang-mysql-driver libmysqlclient See erlang-mysql-driver   Haskell Haskell MySQL Bindings Native Driver See Brian O&amp;rsquo;Sullivan&amp;rsquo;s pure Haskell MySQL bindings    hsql-mysql libmysqlclient See MySQL driver for Haskell    Java/JDBC Connector/J Native Driver See MySQL Connector/J 5.1 Developer Guide   Kaya MyDB libmysqlclient See MyDB   Lua LuaSQL libmysqlclient See LuaSQL   .NET/Mono Connector/Net Native Driver See MySQL Connector/Net Developer Guide   Objective Caml OBjective Caml MySQL Bindings libmysqlclient See MySQL Bindings for Objective Caml   Octave Database bindings for GNU Octave libmysqlclient See Database bindings for GNU Octave   ODBC Connector/ODBC libmysqlclient See MySQL Connector/ODBC Developer Guide   Perl DBI/DBD::mysql libmysqlclient See Section 27.10, “MySQL Perl API”    Net::MySQL Native Driver See Net::MySQL at CPAN   PHP mysql, ext/mysqlinterface (deprecated) libmysqlclient See Original MySQL API    mysqli, ext/mysqliinterface libmysqlclient See MySQL Improved Extension    PDO_MYSQL libmysqlclient See MySQL Functions (PDO_MYSQL)    PDO mysqlnd Native Driver    Python Connector/Python Native Driver See MySQL Connector/Python Developer Guide   Python Connector/Python C Extension libmysqlclient See MySQL Connector/Python Developer Guide    MySQLdb libmysqlclient See Section 27.11, “MySQL Python API”   Ruby MySQL/Ruby libmysqlclient Uses libmysqlclient. See Section 27.12.1, “The MySQL/Ruby API”    Ruby/MySQL Native Driver See Section 27.12.2, “The Ruby/MySQL API”   Scheme Myscsh libmysqlclient See Myscsh   SPL sql_mysql libmysqlclient See sql_mysql for SPL   Tcl MySQLtcl libmysqlclient See Section 27.13, “MySQL Tcl API”    Connector versions supported by TiDB    Connector Connector Version     Connector/C 6.1.0 GA   Connector/C++ 1.0.5 GA   Connector/J 5.1.8   Connector/Net 6.9.9 GA   Connector/Net 6.8.8 GA   Connector/ODBC 5.1   Connector/ODBC 3.51 (Unicode not supported)   Connector/Python 2.0   Connector/Python 1.2    "},
		{"url": "https://pingcap.com/docs/sql/control-flow-functions/",
		"title": "Control Flow Functions", 
		"content": " Control Flow Functions    Name Description     CASE Case operator   IF() If/else construct   IFNULL() Null if/else construct   NULLIF() Return NULL if expr1 = expr2    "},
		{"url": "https://pingcap.com/docs/op-guide/location-awareness/",
		"title": "Cross-Region Deployment", 
		"content": " Cross-Region Deployment Overview PD schedules according to the topology of the TiKV cluster to maximize the TiKV&amp;rsquo;s capability for disaster recovery.Before you begin, see Ansible Deployment (Recommended) and Docker Deployment.TiKV reports the topological information TiKV reports the topological information to PD according to the startup parameter or configuration of TiKV.Assuming that the topology has three structures: zone &amp;gt; rack &amp;gt; host, use lables to specify the following information:Startup parameter:tikv-server --labels zone=&amp;lt;zone&amp;gt;,rack=&amp;lt;rack&amp;gt;,host=&amp;lt;host&amp;gt; Configuration:[server] labels = &amp;#34;zone=&amp;lt;zone&amp;gt;,rack=&amp;lt;rack&amp;gt;,host=&amp;lt;host&amp;gt;&amp;#34; PD understands the TiKV topology PD gets the topology of TiKV cluster through the PD configuration.[replication] max-replicas = 3 location-labels = [&amp;#34;zone&amp;#34;, &amp;#34;rack&amp;#34;, &amp;#34;host&amp;#34;] location-labels needs to correspond to the TiKV labels name so that PD can understand that the labels represents the TiKV topology.PD schedules based on the TiKV topology PD makes optimal schedulings according to the topological information. You just need to care about what kind of topology can achieve the desired effect.If you use 3 replicas and hope that everything still works well when a data zone hangs up, you need at least 4 data zones. (Theoretically, three data zones are feasible but the current implementation cannot guarantee.)Assume that we have 4 data zones, each zone has 2 racks and each rack has 2 hosts. We can start 2 TiKV instances on each host:# zone=z1 tikv-server --labels zone=z1,rack=r1,host=h1 tikv-server --labels zone=z1,rack=r1,host=h2 tikv-server --labels zone=z1,rack=r2,host=h1 tikv-server --labels zone=z1,rack=r2,host=h2 # zone=z2 tikv-server --labels zone=z2,rack=r1,host=h1 tikv-server --labels zone=z2,rack=r1,host=h2 tikv-server --labels zone=z2,rack=r2,host=h1 tikv-server --labels zone=z2,rack=r2,host=h2 # zone=z3 tikv-server --labels zone=z3,rack=r1,host=h1 tikv-server --labels zone=z3,rack=r1,host=h2 tikv-server --labels zone=z3,rack=r2,host=h1 tikv-server --labels zone=z3,rack=r2,host=h2 # zone=z4 tikv-server --labels zone=z4,rack=r1,host=h1 tikv-server --labels zone=z4,rack=r1,host=h2 tikv-server --labels zone=z4,rack=r2,host=h1 tikv-server --labels zone=z4,rack=r2,host=h2 In other words, 16 TiKV instances are distributed across 4 data zones, 8 racks and 16 machines.In this case, PD will schedule different replicas of each datum to different data zones. - If one of the data zones hangs up, everything still works well. - If the data zone cannot recover within a period of time, PD will remove the replica from this data zone.To sum up, PD maximizes the disaster recovery of the cluster according to the current topology. Therefore, if you want to reach a certain level of disaster recovery, deploy many machines in different sites according to the topology. The number of machines must be more than the number of max-replicas."},
		{"url": "https://pingcap.com/recruit-cn/engineer/dba/",
		"title": "DBA", 
		"content": " DBA 岗位职责  负责上线用户和 POC 用户 TiDB 集群的日常运行维护，包括配置管理、升级、扩容、备份，数据迁移等工作 对用户进行培训，介绍 TiDB 的原理和使用指导，总结并传授最佳实践 负责用户的 TiDB 集群的监控，性能分析、问题跟踪与管理； 7x24小时响应故障处理。  职位要求  三年以上 MySQL/postgreSQL/Oracle 运维相关工作经验 精通 MySQL 数据库配置、备份、优化、监控，擅长使用自动化运维脚本 熟悉大规模 Linux 环境下数据库系统运营和维护 高度的责任心、良好的沟通技巧和团队合作精神  待遇 15K - 25K , 13薪 + 奖金, 优秀者可面议工作地点 北京，上海，广州，杭州"},
		{"url": "https://pingcap.com/docs-cn/sql/ddl/",
		"title": "DDL", 
		"content": " 数据定义语言 DDL（Data Definition Language）用于定义和管理数据库以及数据库中各种对象的语句。CREATE DATABASE 语法 CREATE {DATABASE | SCHEMA} [IF NOT EXISTS] db_name [create_specification] ... create_specification: [DEFAULT] CHARACTER SET [=] charset_name | [DEFAULT] COLLATE [=] collation_name CREATE DATABASE 用于创建数据库，并可以指定数据库的默认属性（如数据库默认字符集,校验规则。CREATE SCHEMA 跟 CREATE DATABASE 操作效果一样。当创建已存在的数据库且不指定使用 IF NOT EXISTS 时会报错。create_specification 选项用于指定数据库具体的 CHARACTER SET 和 COLLATE。目前这个选项只是语法支持。DROP DATABASE 语法 DROP {DATABASE | SCHEMA} [IF EXISTS] db_name DROP DATABASE 用于删除指定数据库以及它其中的所用表格。IF EXISTS 用于防止当数据库不存在时发生错误。CREATE TABLE 语法 CREATE TABLE [IF NOT EXISTS] tbl_name (create_definition,...) [table_options] CREATE TABLE [IF NOT EXISTS] tbl_name { LIKE old_tbl_name | (LIKE old_tbl_name) } create_definition: col_name column_definition | [CONSTRAINT [symbol]] PRIMARY KEY [index_type] (index_col_name,...) [index_option] ... | {INDEX|KEY} [index_name] [index_type] (index_col_name,...) [index_option] ... | [CONSTRAINT [symbol]] UNIQUE [INDEX|KEY] [index_name] [index_type] (index_col_name,...) [index_option] ... | {FULLTEXT} [INDEX|KEY] [index_name] (index_col_name,...) [index_option] ... | [CONSTRAINT [symbol]] FOREIGN KEY [index_name] (index_col_name,...) reference_definition column_definition: data_type [NOT NULL | NULL] [DEFAULT default_value] [AUTO_INCREMENT] [UNIQUE [KEY] | [PRIMARY] KEY] [COMMENT &amp;#39;string&amp;#39;] [reference_definition] | data_type [GENERATED ALWAYS] AS (expression) [VIRTUAL | STORED] [UNIQUE [KEY]] [COMMENT comment] [NOT NULL | NULL] [[PRIMARY] KEY] data_type: BIT[(length)] | TINYINT[(length)] [UNSIGNED] [ZEROFILL] | SMALLINT[(length)] [UNSIGNED] [ZEROFILL] | MEDIUMINT[(length)] [UNSIGNED] [ZEROFILL] | INT[(length)] [UNSIGNED] [ZEROFILL] | INTEGER[(length)] [UNSIGNED] [ZEROFILL] | BIGINT[(length)] [UNSIGNED] [ZEROFILL] | REAL[(length,decimals)] [UNSIGNED] [ZEROFILL] | DOUBLE[(length,decimals)] [UNSIGNED] [ZEROFILL] | FLOAT[(length,decimals)] [UNSIGNED] [ZEROFILL] | DECIMAL[(length[,decimals])] [UNSIGNED] [ZEROFILL] | NUMERIC[(length[,decimals])] [UNSIGNED] [ZEROFILL] | DATE | TIME[(fsp)] | TIMESTAMP[(fsp)] | DATETIME[(fsp)] | YEAR | CHAR[(length)] [BINARY] [CHARACTER SET charset_name] [COLLATE collation_name] | VARCHAR(length) [BINARY] [CHARACTER SET charset_name] [COLLATE collation_name] | BINARY[(length)] | VARBINARY(length) | TINYBLOB | BLOB | MEDIUMBLOB | LONGBLOB | TINYTEXT [BINARY] [CHARACTER SET charset_name] [COLLATE collation_name] | TEXT [BINARY] [CHARACTER SET charset_name] [COLLATE collation_name] | MEDIUMTEXT [BINARY] [CHARACTER SET charset_name] [COLLATE collation_name] | LONGTEXT [BINARY] [CHARACTER SET charset_name] [COLLATE collation_name] | ENUM(value1,value2,value3,...) [CHARACTER SET charset_name] [COLLATE collation_name] | SET(value1,value2,value3,...) [CHARACTER SET charset_name] [COLLATE collation_name] | JSON index_col_name: col_name [(length)] [ASC | DESC] index_type: USING {BTREE | HASH} index_option: KEY_BLOCK_SIZE [=] value | index_type | COMMENT &amp;#39;string&amp;#39; reference_definition: REFERENCES tbl_name (index_col_name,...) [MATCH FULL | MATCH PARTIAL | MATCH SIMPLE] [ON DELETE reference_option] [ON UPDATE reference_option] reference_option: RESTRICT | CASCADE | SET NULL | NO ACTION | SET DEFAULT table_options: table_option [[,] table_option] ... table_option: AUTO_INCREMENT [=] value | AVG_ROW_LENGTH [=] value | [DEFAULT] CHARACTER SET [=] charset_name | CHECKSUM [=] {0 | 1} | [DEFAULT] COLLATE [=] collation_name | COMMENT [=] &amp;#39;string&amp;#39; | COMPRESSION [=] {&amp;#39;ZLIB&amp;#39;|&amp;#39;LZ4&amp;#39;|&amp;#39;NONE&amp;#39;} | CONNECTION [=] &amp;#39;connect_string&amp;#39; | DELAY_KEY_WRITE [=] {0 | 1} | ENGINE [=] engine_name | KEY_BLOCK_SIZE [=] value | MAX_ROWS [=] value | MIN_ROWS [=] value | ROW_FORMAT [=] {DEFAULT|DYNAMIC|FIXED|COMPRESSED|REDUNDANT|COMPACT} | STATS_PERSISTENT [=] {DEFAULT|0|1} CREATE TABLE 用于创建一个表。目前不支持临时表，不支持 CHECK 约束，不支持创建表的同时从其它表导入数据功能。 在语法上也支持一些 Partition_options，但是并不完全，就不做列举了。 使用 IF NOT EXIST 时，即使创建的表已经存在，也不会报错，如果不指定时，则报错。 使用 LIKE 基于一个表的定义创建一个空表，包括这个表中的列属性和索引属性。 create_definition 中 FULLTEXT 和 FOREIGN KEY 目前只是语法上支持 data_type 请参考数据类型章节。 index_col_name 中 [ASC | DESC] 目前只是语法上支持。 index_type 目前只是语法上支持。 index_option 中 KEY_BLOCK_SIZE 目前只是语法上支持。 table_option 目前支持的只有 AUTO_INCREMENT，CHARACTER SET 和 COMMENT，其它只是语法上支持。具体内容参考下表，各个子句之间用逗号隔开。   参数 含义 举例     AUTO_INCREMENT 自增字段初始值 AUTO_INCREMENT = 5   CHARACTER SET 指定该表的字符串编码。目前支持 UTF8MB4 CHARACTER SET = &amp;lsquo;utf8mb4&amp;rsquo;   COMMENT 注释信息 COMMENT = &amp;lsquo;comment info&amp;rsquo;     AUTO_INCREMENT 说明 TiDB 的自增 ID (AUTO_INCREMENT ID) 只保证自增且唯一，并不保证连续分配。TiDB 目前采用批量分配的方式，所以如果在多台 TiDB 上同时插入数据，分配的自增 ID 会不连续。允许给整型类型的字段指定 AUTO_INCREMENT，且一个表只允许一个属性为 AUTO_INCREMENT 的字段。DROP TABLE 语法 DROP TABLE [IF EXISTS] tbl_name [, tbl_name] ... 可以同时删除多个表，表之间用 , 隔开。当删除不存在的表时且不指定使用 IF EXISTS 时会报错。TRUNCATE TABLE 语法 TRUNCATE [TABLE] tbl_name TRUNCATE TABLE 用于清除指定表中所有数据，但是保留表结构。此操作于删除指定表全表数据的操作类似，但是操作的执行速度会远快于删除全表的速度，且不受表内数据行数影响。 注意：使用此语句后，原先表内的 AUTO_INCREMENT 的值不会记录，会被重新计数。 RENAME TABLE 语法 RENAME TABLE tbl_name TO new_tbl_name RENAME TABLE 用于对一个表进行重命名。这个语句等价于如下的 ALTER TABLE 语句：ALTER TABLE old_table RENAME new_table; ALTER TABLE 语法 ALTER TABLE tbl_name [alter_specification] alter_specification: table_options | ADD [COLUMN] col_name column_definition [FIRST | AFTER col_name] | ADD [COLUMN] (col_name column_definition,...) | ADD {INDEX|KEY} [index_name] [index_type] (index_col_name,...) [index_option] ... | ADD [CONSTRAINT [symbol]] PRIMARY KEY [index_type] (index_col_name,...) [index_option] ... | ADD [CONSTRAINT [symbol]] UNIQUE [INDEX|KEY] [index_name] [index_type] (index_col_name,...) [index_option] ... | ADD FULLTEXT [INDEX|KEY] [index_name] (index_col_name,...) [index_option] ... | ADD [CONSTRAINT [symbol]] FOREIGN KEY [index_name] (index_col_name,...) reference_definition | ALTER [COLUMN] col_name {SET DEFAULT literal | DROP DEFAULT} | CHANGE [COLUMN] old_col_name new_col_name column_definition [FIRST|AFTER col_name] | {DISABLE|ENABLE} KEYS | DROP [COLUMN] col_name | DROP {INDEX|KEY} index_name | DROP PRIMARY KEY | DROP FOREIGN KEY fk_symbol | LOCK [=] {DEFAULT|NONE|SHARED|EXCLUSIVE} | MODIFY [COLUMN] col_name column_definition [FIRST | AFTER col_name] | RENAME [TO|AS] new_tbl_name | {WITHOUT|WITH} VALIDATION index_col_name: col_name [(length)] [ASC | DESC] index_type: USING {BTREE | HASH} index_option: KEY_BLOCK_SIZE [=] value | index_type | COMMENT &amp;#39;string&amp;#39; table_options: table_option [[,] table_option] ... table_option: AVG_ROW_LENGTH [=] value | [DEFAULT] CHARACTER SET [=] charset_name | CHECKSUM [=] {0 | 1} | [DEFAULT] COLLATE [=] collation_name | COMMENT [=] &amp;#39;string&amp;#39; | COMPRESSION [=] {&amp;#39;ZLIB&amp;#39;|&amp;#39;LZ4&amp;#39;|&amp;#39;NONE&amp;#39;} | CONNECTION [=] &amp;#39;connect_string&amp;#39; | DELAY_KEY_WRITE [=] {0 | 1} | ENGINE [=] engine_name | KEY_BLOCK_SIZE [=] value | MAX_ROWS [=] value | MIN_ROWS [=] value | ROW_FORMAT [=] {DEFAULT|DYNAMIC|FIXED|COMPRESSED|REDUNDANT|COMPACT} | STATS_PERSISTENT [=] {DEFAULT|0|1} ALTER TABLE 用于修改已存在的表的结构，比如：修改表及表属性、新增或删除列、创建或删除索引、修改列及属性等. 以下是几个字段类型的描述: index_col_name、index_type 和 index_option 可以参考 CREATE INDEX 语法. table_option 目前支持都只是语法上支持。  下面介绍一下具体操作类型的支持情况。 ADD/DROP INDEX/COLUMN 操作目前不支持同时创建或删除多个索引或列。 ADD/DROP PRIMARY KEY 操作目前不支持。 DROP COLUMN 操作目前不支持删除的列为主键列或索引列。 ADD COLUMN 操作目前不支持同时将新添加的列设为主键或唯一索引，也不支持将此列设成 AUTO_INCREMENT 属性。 CHANGE/MODIFY COLUMN 操作目前支持部分语法，细节如下： 在修改类型方面，只支持整数类型之间修改，字符串类型之间修改和 Blob 类型之间的修改，且只能使原类型长度变长。此外，不能改变列的 unsigned/charset/collate 属性。这里的类型分类如下： 具体支持的整型类型有：TinyInt，SmallInt，MediumInt，Int，BigInt。 具体支持的字符串类型有：Char，Varchar，Text，TinyText，MediumText，LongText。 具体支持的 Blob 类型有：Blob，TinyBlob，MediumBlob，LongBlob。 在修改类型定义方面，支持的包括 default value，comment，null，not null 和 OnUpdate，但是不支持从 null 到 not null 的修改。 不支持对 enum 类型的列进行修改  LOCK [=] {DEFAULT|NONE|SHARED|EXCLUSIVE} 目前只是语法支持。  CREATE INDEX 语法 CREATE [UNIQUE] INDEX index_name [index_type] ON tbl_name (index_col_name,...) [index_option] ... index_col_name: col_name [(length)] [ASC | DESC] index_option: KEY_BLOCK_SIZE [=] value | …"},
		{"url": "https://pingcap.com/docs/sql/ddl/",
		"title": "Data Definition Statements", 
		"content": " Data Definition Statements DDL (Data Definition Language) is used to define the database structure or schema, and to manage the database and statements of various objects in the database.CREATE DATABASE syntax CREATE {DATABASE | SCHEMA} [IF NOT EXISTS] db_name [create_specification] ... create_specification: [DEFAULT] CHARACTER SET [=] charset_name | [DEFAULT] COLLATE [=] collation_name The CREATE DATABASE statement is used to create a database, and to specify the default properties of the database, such as the default character set and validation rules. CREATE SCHEMA is a synonym for CREATE DATABASE.If you create an existing database and does not specify IF NOT EXISTS, an error is displayed.The create_specification option is used to specify the specific CHARACTER SET and COLLATE in the database. Currently, the option is only supported in syntax.DROP DATABASE syntax DROP {DATABASE | SCHEMA} [IF EXISTS] db_name The DROP DATABASE statement is used to delete the specified database and its tables.The IF EXISTS statement is used to prevent an error if the database does not exist.CREATE TABLE syntax CREATE TABLE [IF NOT EXISTS] tbl_name (create_definition,...) [table_options] CREATE TABLE [IF NOT EXISTS] tbl_name { LIKE old_tbl_name | (LIKE old_tbl_name) } create_definition: col_name column_definition | [CONSTRAINT [symbol]] PRIMARY KEY [index_type] (index_col_name,...) [index_option] ... | {INDEX|KEY} [index_name] [index_type] (index_col_name,...) [index_option] ... | [CONSTRAINT [symbol]] UNIQUE [INDEX|KEY] [index_name] [index_type] (index_col_name,...) [index_option] ... | {FULLTEXT} [INDEX|KEY] [index_name] (index_col_name,...) [index_option] ... | [CONSTRAINT [symbol]] FOREIGN KEY [index_name] (index_col_name,...) reference_definition column_definition: data_type [NOT NULL | NULL] [DEFAULT default_value] [AUTO_INCREMENT] [UNIQUE [KEY] | [PRIMARY] KEY] [COMMENT &amp;#39;string&amp;#39;] [reference_definition] | data_type [GENERATED ALWAYS] AS (expression) [VIRTUAL | STORED] [UNIQUE [KEY]] [COMMENT comment] [NOT NULL | NULL] [[PRIMARY] KEY] data_type: BIT[(length)] | TINYINT[(length)] [UNSIGNED] [ZEROFILL] | SMALLINT[(length)] [UNSIGNED] [ZEROFILL] | MEDIUMINT[(length)] [UNSIGNED] [ZEROFILL] | INT[(length)] [UNSIGNED] [ZEROFILL] | INTEGER[(length)] [UNSIGNED] [ZEROFILL] | BIGINT[(length)] [UNSIGNED] [ZEROFILL] | REAL[(length,decimals)] [UNSIGNED] [ZEROFILL] | DOUBLE[(length,decimals)] [UNSIGNED] [ZEROFILL] | FLOAT[(length,decimals)] [UNSIGNED] [ZEROFILL] | DECIMAL[(length[,decimals])] [UNSIGNED] [ZEROFILL] | NUMERIC[(length[,decimals])] [UNSIGNED] [ZEROFILL] | DATE | TIME[(fsp)] | TIMESTAMP[(fsp)] | DATETIME[(fsp)] | YEAR | CHAR[(length)] [BINARY] [CHARACTER SET charset_name] [COLLATE collation_name] | VARCHAR(length) [BINARY] [CHARACTER SET charset_name] [COLLATE collation_name] | BINARY[(length)] | VARBINARY(length) | TINYBLOB | BLOB | MEDIUMBLOB | LONGBLOB | TINYTEXT [BINARY] [CHARACTER SET charset_name] [COLLATE collation_name] | TEXT [BINARY] [CHARACTER SET charset_name] [COLLATE collation_name] | MEDIUMTEXT [BINARY] [CHARACTER SET charset_name] [COLLATE collation_name] | LONGTEXT [BINARY] [CHARACTER SET charset_name] [COLLATE collation_name] | ENUM(value1,value2,value3,...) [CHARACTER SET charset_name] [COLLATE collation_name] | SET(value1,value2,value3,...) [CHARACTER SET charset_name] [COLLATE collation_name] | JSON index_col_name: col_name [(length)] [ASC | DESC] index_type: USING {BTREE | HASH} index_option: KEY_BLOCK_SIZE [=] value | index_type | COMMENT &amp;#39;string&amp;#39; reference_definition: REFERENCES tbl_name (index_col_name,...) [MATCH FULL | MATCH PARTIAL | MATCH SIMPLE] [ON DELETE reference_option] [ON UPDATE reference_option] reference_option: RESTRICT | CASCADE | SET NULL | NO ACTION | SET DEFAULT table_options: table_option [[,] table_option] ... table_option: AUTO_INCREMENT [=] value | AVG_ROW_LENGTH [=] value | [DEFAULT] CHARACTER SET [=] charset_name | CHECKSUM [=] {0 | 1} | [DEFAULT] COLLATE [=] collation_name | COMMENT [=] &amp;#39;string&amp;#39; | COMPRESSION [=] {&amp;#39;ZLIB&amp;#39;|&amp;#39;LZ4&amp;#39;|&amp;#39;NONE&amp;#39;} | CONNECTION [=] &amp;#39;connect_string&amp;#39; | DELAY_KEY_WRITE [=] {0 | 1} | ENGINE [=] engine_name | KEY_BLOCK_SIZE [=] value | MAX_ROWS [=] value | MIN_ROWS [=] value | ROW_FORMAT [=] {DEFAULT|DYNAMIC|FIXED|COMPRESSED|REDUNDANT|COMPACT} | STATS_PERSISTENT [=] {DEFAULT|0|1} The CREATE TABLE statement is used to create a table. Currently, it does not support temporary tables, CHECK constraints, or importing data from other tables while creating tables. It supports some of the Partition_options in syntax. When you create an existing table and if you specify IF NOT EXIST, it does not report an error. Otherwise, it reports an error. Use LIKE to create an empty table based on the definition of another table including its column and index properties. The FULLTEXT and FOREIGN KEY in create_definition are currently only supported in syntax. For the data_type, see Data Types. The [ASC | DESC] in index_col_name is currently only supported in syntax. The index_type is currently only supported in syntax. The KEY_BLOCK_SIZE in index_option is currently only supported in syntax. The table_option currently only supports AUTO_INCREMENT, CHARACTER SET and COMMENT, while the others are only supported in syntax. The clauses are separated by a comma ,. See the following table for details:   Parameters Description Example     AUTO_INCREMENT The initial value of the increment field AUTO_INCREMENT = 5   CHARACTER SET To specify the string code for the table; currently only support UTF8MB4 CHARACTER SET = &amp;lsquo;utf8mb4&amp;rsquo;   COMMENT The comment information COMMENT = &amp;lsquo;comment info&amp;rsquo;     AUTO_INCREMENT description The TiDB automatic increment ID (AUTO_INCREMENT ID) only guarantees automatic increment and uniqueness and does not guarantee continuous allocation. Currently, TiDB adopts bulk allocation. If you insert data into multiple TiDB servers at the same time, the allocated automatic increment ID is not continuous.You can specify the AUTO_INCREMENT for integer fields. A table only supports one field with the AUTO_INCREMENT property.DROP TABLE syntax DROP TABLE [IF EXISTS] tbl_name [, tbl_name] ... You can delete multiple tables at the same time. The tables are separated by a comma ,.If you delete a table that does not exist and does not specify the use of IF EXISTS, an error is displayed.TRUNCATE TABLE syntax TRUNCATE [TABLE] tbl_name The TRUNCATE TABLE statement is used to clear all the data in the specified table but keeps the table structure.This operation is similar to deleting all the data of a specified table, but it is much faster and is not affected by the number of rows in the table. Note: If you use the TRUNCATE TABLE statement, the value of AUTO_INCREMENT in the original table is reset to its starting value. RENAME TABLE syntax RENAME TABLE tbl_name TO new_tbl_name The RENAME TABLE statement is used to rename a table.This statement is equivalent to the following ALTER TABLE statement:ALTER TABLE old_table RENAME new_table; ALTER TABLE syntax ALTER TABLE tbl_name [alter_specification] alter_specification: table_options | ADD [COLUMN] col_name column_definition [FIRST | AFTER col_name] | ADD [COLUMN] (col_name column_definition,...) | ADD {INDEX|KEY} [index_name] [index_type] (index_col_name,...) [index_option] ... | ADD [CONSTRAINT [symbol]] PRIMARY KEY [index_type] (index_col_name,...) [index_option] ... | ADD [CONSTRAINT [symbol]] UNIQUE [INDEX|KEY] [index_name] [index_type] (index_col_name,...) [index_option] ... | ADD FULLTEXT [INDEX|KEY] [index_name] (index_col_name,...) [index_option] ... | ADD [CONSTRAINT [symbol]] FOREIGN KEY [index_name] (index_col_name,...) reference_definition | ALTER [COLUMN] col_name {SET DEFAULT literal | DROP DEFAULT} | CHANGE [COLUMN] old_col_name new_col_name column_definition [FIRST|AFTER col_name] | {DISABLE|ENABLE} KEYS | DROP [COLUMN] col_name | DROP {INDEX|KEY} index_name | DROP PRIMARY KEY | DROP …"},
		{"url": "https://pingcap.com/docs/sql/admin/",
		"title": "Database Administration Statements", 
		"content": " Database Administration Statements TiDB manages the database using a number of statements, including granting privileges, modifying system variables, and querying database status.Privilege management See Privilege Management.SET statement The SET statement has multiple functions and forms.Assign values to variables SET variable_assignment [, variable_assignment] ... variable_assignment: user_var_name = expr | param_name = expr | local_var_name = expr | [GLOBAL | SESSION] system_var_name = expr | [@@global. | @@session. | @@] system_var_name = expr You can use the above syntax to assign values to variables in TiDB, which include system variables and user-defined variables. All user-defined variables are session variables. The system variables set using @@global. or GLOBAL are global variables, otherwise session variables. For more information, see The System Variables.SET CHARACTER statement and SET NAMES SET {CHARACTER SET | CHARSET} {&amp;#39;charset_name&amp;#39; | DEFAULT} SET NAMES {&amp;#39;charset_name&amp;#39; [COLLATE &amp;#39;collation_name&amp;#39;] | DEFAULT} This statement sets three session system variables (character_set_client, character_set_results and character_set_connection) as given character set. Currently, the value of character_set_connection differs from MySQL and is set as the value of character_set_database in MySQL.Set the password SET PASSWORD [FOR user] = password_option password_option: { &amp;#39;auth_string&amp;#39; | PASSWORD(&amp;#39;auth_string&amp;#39;) } This statement is used to set user passwords. For more information, see Privilege Management.Set the isolation level SET SESSION TRANSACTION ISOLATION LEVEL READ COMMITTED; This statement is used to set the transaction isolation level. For more information, see Transaction Isolation Level.SHOW statement TiDB supports part of SHOW statements, used to view the Database/Table/Column information and the internal status of the database. Currently supported statements:# Supported and similar to MySQL SHOW CHARACTER SET [like_or_where] SHOW COLLATION [like_or_where] SHOW [FULL] COLUMNS FROM tbl_name [FROM db_name] [like_or_where] SHOW CREATE {DATABASE|SCHEMA} db_name SHOW CREATE TABLE tbl_name SHOW DATABASES [like_or_where] SHOW GRANTS FOR user SHOW INDEX FROM tbl_name [FROM db_name] SHOW PRIVILEGES SHOW [FULL] PROCESSLIST SHOW [GLOBAL | SESSION] STATUS [like_or_where] SHOW TABLE STATUS [FROM db_name] [like_or_where] SHOW [FULL] TABLES [FROM db_name] [like_or_where] SHOW [GLOBAL | SESSION] VARIABLES [like_or_where] SHOW WARNINGS # Supported to improve compatibility but return null results SHOW ENGINE engine_name {STATUS | MUTEX} SHOW [STORAGE] ENGINES SHOW PLUGINS SHOW PROCEDURE STATUS [like_or_where] SHOW TRIGGERS [FROM db_name] [like_or_where] SHOW EVENTS SHOW FUNCTION STATUS [like_or_where] # TiDB-specific statements for viewing statistics SHOW STATS_META [like_or_where] SHOW STATS_HISTOGRAMS [like_or_where] SHOW STATS_BUCKETS [like_or_where] like_or_where: LIKE &amp;#39;pattern&amp;#39; | WHERE expr  Note: To view statistics using the SHOW statement, see View Statistics. For more information about the SHOW statement, see SHOW Syntax in MySQL.   ADMIN statement This statement is a TiDB extension syntax, used to view the status of TiDB.ADMIN SHOW DDL ADMIN SHOW DDL JOBS ADMIN CANCEL DDL JOBS &amp;#39;job_id&amp;#39; [, &amp;#39;job_id&amp;#39;] ...  ADMIN SHOW DDL: To view the currently running DDL jobs. ADMIN SHOW DDL JOBS: To view all the results in the current DDL job queue (including tasks that are running and waiting to be run) and the last ten results in the completed DDL job queue. ADMIN CANCEL DDL JOBS &#39;job_id&#39; [, &#39;job_id&#39;] ...: To cancel the currently running DDL jobs and return whether the corresponding jobs are successfully cancelled. If the operation fails to cancel the jobs, specific reasons are displayed.    Note: This operation can cancel multiple DDL jobs at the same time. You can get the ID of DDL jobs using the ADMIN SHOW DDL JOBS statement. If the jobs you want to cancel are finished, the cancellation operation fails.   "},
		{"url": "https://pingcap.com/docs/sql/date-and-time-functions/",
		"title": "Date and Time Functions", 
		"content": " Date and Time Functions The usage of date and time functions is similar to MySQL. For more information, see here.Date/Time functions   Name Description     ADDDATE() Add time values (intervals) to a date value   ADDTIME() Add time   CONVERT_TZ() Convert from one time zone to another   CURDATE() Return the current date   CURRENT_DATE(), CURRENT_DATE Synonyms for CURDATE()   CURRENT_TIME(), CURRENT_TIME Synonyms for CURTIME()   CURRENT_TIMESTAMP(), CURRENT_TIMESTAMP Synonyms for NOW()   CURTIME() Return the current time   DATE() Extract the date part of a date or datetime expression   DATE_ADD() Add time values (intervals) to a date value   DATE_FORMAT() Format date as specified   DATE_SUB() Subtract a time value (interval) from a date   DATEDIFF() Subtract two dates   DAY() Synonym for DAYOFMONTH()   DAYNAME() Return the name of the weekday   DAYOFMONTH() Return the day of the month (0-31)   DAYOFWEEK() Return the weekday index of the argument   DAYOFYEAR() Return the day of the year (1-366)   EXTRACT() Extract part of a date   FROM_DAYS() Convert a day number to a date   FROM_UNIXTIME() Format Unix timestamp as a date   GET_FORMAT() Return a date format string   HOUR() Extract the hour   LAST_DAY Return the last day of the month for the argument   LOCALTIME(), LOCALTIME Synonym for NOW()   LOCALTIMESTAMP, LOCALTIMESTAMP() Synonym for NOW()   MAKEDATE() Create a date from the year and day of year   MAKETIME() Create time from hour, minute, second   MICROSECOND() Return the microseconds from argument   MINUTE() Return the minute from the argument   MONTH() Return the month from the date passed   MONTHNAME() Return the name of the month   NOW() Return the current date and time   PERIOD_ADD() Add a period to a year-month   PERIOD_DIFF() Return the number of months between periods   QUARTER() Return the quarter from a date argument   SEC_TO_TIME() Converts seconds to &amp;lsquo;HH:MM:SS&amp;rsquo; format   SECOND() Return the second (0-59)   STR_TO_DATE() Convert a string to a date   SUBDATE() Synonym for DATE_SUB() when invoked with three arguments   SUBTIME() Subtract times   SYSDATE() Return the time at which the function executes   TIME() Extract the time portion of the expression passed   TIME_FORMAT() Format as time   TIME_TO_SEC() Return the argument converted to seconds   TIMEDIFF() Subtract time   TIMESTAMP() With a single argument, this function returns the date or datetime expression; with two arguments, the sum of the arguments   TIMESTAMPADD() Add an interval to a datetime expression   TIMESTAMPDIFF() Subtract an interval from a datetime expression   TO_DAYS() Return the date argument converted to days   TO_SECONDS() Return the date or datetime argument converted to seconds since Year 0   UNIX_TIMESTAMP() Return a Unix timestamp   UTC_DATE() Return the current UTC date   UTC_TIME() Return the current UTC time   UTC_TIMESTAMP() Return the current UTC date and time   WEEK() Return the week number   WEEKDAY() Return the weekday index   WEEKOFYEAR() Return the calendar week of the date (1-53)   YEAR() Return the year   YEARWEEK() Return the year and week    For details, see here."},
		{"url": "https://pingcap.com/docs-cn/sql/literal-value-date-and-time-literals/",
		"title": "Date 和 Time 字面值", 
		"content": " Date and Time Literals Date 跟 Time 字面值有几种格式，例如用字符串表示，或者直接用数字表示。在 TiDB 里面，当 TiDB 期望一个 Date 的时候，它会把 &#39;2017-08-24&#39;， &#39;20170824&#39;，20170824 当做是 Date。TiDB 的 Date 值有以下几种格式： &#39;YYYY-MM-DD&#39; 或者 &#39;YY-MM-DD&#39;，这里的 - 分隔符并不是严格的，可以是任意的标点符号。比如 &#39;2017-08-24&#39;，&#39;2017&amp;amp;08&amp;amp;24&#39;， &#39;2012@12^31&#39; 都是一样的。唯一需要特别对待的是 &amp;lsquo;.&amp;rsquo; 号，它被当做是小数点，用于分隔整数和小数部分。 Date 和 Time 部分可以被 &amp;rsquo;T&amp;rsquo; 分隔，它的作用跟空格符是一样的，例如 2017-8-24 10:42:00 跟 2017-8-24T10:42:00 是一样的。 &#39;YYYYMMDDHHMMSS&#39; 或者 &#39;YYMMDDHHMMSS&#39;，例如 &#39;20170824104520&#39; 和 &#39;170824104520&#39; 被当做是 &#39;2017-08-24 10:45:20&#39;，但是如果你提供了一个超过范围的值，例如&#39;170824304520&#39;，那这就不是一个有效的 Date 字面值。 YYYYMMDDHHMMSS 或者 YYMMDDHHMMSS 注意这里没有单引号或者双引号，是一个数字。例如 20170824104520表示为 &#39;2017-08-24 10:45:20&#39;。  DATETIME 或者 TIMESTAMP 值可以接一个小数部分，用来表示微秒（精度最多到小数点后 6 位），用小数点 . 分隔。Dates 如果 year 部分只有两个数字，这是有歧义的（推荐使用四个数字的格式），TiDB 会尝试用以下的规则来解释： year 值如果在 70-99 范围，那么被转换成 1970-1999。 year 值如果在 00-69 范围，那么被转换成 2000-2069。   对于小于 10 的 month 或者 day 值，&#39;2017-8-4&#39; 跟 &#39;2017-08-04&#39; 是一样的。对于 Time 也是一样，比如 &#39;2017-08-24 1:2:3&#39; 跟 &#39;2017-08-24 01:02:03&#39;是一样的。在需要 Date 或者 Time 的语境下, 对于数值，TiDB 会根据数值的长度来选定指定的格式： 6 个数字，会被解释为 YYMMDD。 12 个数字，会被解释为 YYMMDDHHMMSS。 8 个数字，会解释为 YYYYMMDD。 14 个数字，会被解释为 YYYYMMDDHHMMSS。  对于 Time 类型，TiDB 用以下格式来表示： &#39;D HH:MM:SS&#39;，或者 &#39;HH:MM:SS&#39;，&#39;HH:MM&#39;，&#39;D HH:MM&#39;，&#39;D HH&#39;，&#39;SS&#39;，这里的 D 表示 days，合法的范围是 0-34。 数值 HHMMSS，例如 231010 被解释为&#39;23:10:10&#39;。 数值 SS，MMSS，HHMMSS 都是可以被当做 Time。  Time 类型的小数点也是 .，精度最多小数点后 6 位。更多细节。"},
		{"url": "https://pingcap.com/docs/op-guide/binary-deployment/",
		"title": "Deploy TiDB Using the Binary", 
		"content": " Deploy TiDB Using the Binary Overview A complete TiDB cluster contains PD, TiKV, and TiDB. To start the database service, follow the order of PD -&amp;gt; TiKV -&amp;gt; TiDB. To stop the database service, follow the order of stopping TiDB -&amp;gt; TiKV -&amp;gt; PD.Before you start, see TiDB architecture and Software and Hardware Requirements.This document describes the binary deployment of three scenarios: To quickly understand and try TiDB, see Single node cluster deployment. To try TiDB out and explore the features, see Multiple nodes cluster deployment for test. To deploy and use TiDB in production, see Multiple nodes cluster deployment.  TiDB components and default ports TiDB database components (required) See the following table for the default ports for the TiDB components:   Component Default Port Protocol Description     ssh 22 TCP sshd service   TiDB 4000 TCP the communication port for the application and DBA tools   TiDB 10080 TCP the communication port to report TiDB status   TiKV 20160 TCP the TiKV communication port   PD 2379 TCP the communication port between TiDB and PD   PD 2380 TCP the inter-node communication port within the PD cluster    TiDB database components (optional) See the following table for the default ports for the optional TiDB components:   Component Default Port Protocol Description     Prometheus 9090 TCP the communication port for the Prometheus service   Pushgateway 9091 TCP the aggregation and report port for TiDB, TiKV, and PD monitor   Node_exporter 9100 TCP the communication port to report the system information of every TiDB cluster node   Grafana 3000 TCP the port for the external Web monitoring service and client (Browser) access   alertmanager 9093 TCP the port for the alert service    Configure and check the system before installation Operating system    Configuration Description     Supported Platform See the Software and Hardware Requirements   File System The ext4 file system is recommended in TiDB Deployment   Swap Space The Swap Space is recommended to close in TiDB Deployment   Disk Block Size Set the size of the system disk Block to 4096    Network and firewall    Configuration Description     Firewall / Port Check whether the ports required by TiDB are accessible between the nodes    Operating system parameters    Configuration Description     Nice Limits For system users, set the default value of nice in TiDB to 0   min_free_kbytes The setting for vm.min_free_kbytes in sysctl.conf needs to be high enough   User Open Files Limit For database administrators, set the number of TiDB open files to 1000000   System Open File Limits Set the number of system open files to 1000000   User Process Limits For TiDB users, set the nproc value to 4096 in limits.conf   Address Space Limits For TiDB users, set the space to unlimited in limits.conf   File Size Limits For TiDB users, set the fsize value to unlimited in limits.conf   Disk Readahead Set the value of the readahead data disk to 4096 at a minimum   NTP service Configure the NTP time synchronization service for each node   SELinux Turn off the SELinux service for each node   CPU Frequency Scaling It is recommended to turn on CPU overclocking   Transparent Hugepages For Red Hat 7+ and CentOS 7+ systems, it is required to set the Transparent Hugepages to always   I/O Scheduler Set the I/O Scheduler of data disks to the deadline mode   vm.swappiness Set vm.swappiness = 0     Note: To adjust the operating system parameters, contact your system administrator. Database running user    Configuration Description     LANG environment Set LANG = en_US.UTF8   TZ time zone Set the TZ time zone of all nodes to the same value    Create the database running user account In the Linux environment, create TiDB on each installation node as a database running user, and set up the SSH mutual trust between cluster nodes. To create a running user and open SSH mutual trust, contact the system administrator. Here is an example:# useradd tidb # usermod -a -G tidb tidb # su - tidb Last login: Tue Aug 22 12:06:23 CST 2017 on pts/2 -bash-4.2$ ssh-keygen -t rsa Generating public/private rsa key pair. Enter file in which to save the key (/home/tidb/.ssh/id_rsa): Created directory &amp;#39;/home/tidb/.ssh&amp;#39;. Enter passphrase (empty for no passphrase): Enter same passphrase again: Your identification has been saved in /home/tidb/.ssh/id_rsa. Your public key has been saved in /home/tidb/.ssh/id_rsa.pub. The key fingerprint is: 5a:00:e6:df:9e:40:25:2c:2d:e2:6e:ee:74:c6:c3:c1 tidb@t001 The key&amp;#39;s randomart image is: +--[ RSA 2048]----+ | oo. . | | .oo.oo | | . ..oo | | .. o o | | . E o S | | oo . = . | | o. * . o | | ..o . | | .. | +-----------------+ -bash-4.2$ cd .ssh -bash-4.2$ cat id_rsa.pub &amp;gt;&amp;gt; authorized_keys -bash-4.2$ chmod 644 authorized_keys -bash-4.2$ ssh-copy-id -i ~/.ssh/id_rsa.pub 192.168.1.100 Download the official binary package TiDB provides the official binary installation package that supports Linux. For the operating system, it is recommended to use Redhat 7.3+, CentOS 7.3+ and higher versions.Operating system: Linux (Redhat 7+, CentOS 7+) # Download the package. wget http://download.pingcap.org/tidb-latest-linux-amd64.tar.gz wget http://download.pingcap.org/tidb-latest-linux-amd64.sha256 # Check the file integrity. If the result is OK, the file is correct. sha256sum -c tidb-latest-linux-amd64.sha256 # Extract the package. tar -xzf tidb-latest-linux-amd64.tar.gz cd tidb-latest-linux-amd64 Single node cluster deployment After downloading the TiDB binary package, you can run and test the TiDB cluster on a standalone server. Follow the steps below to start PD, TiKV and TiDB: Start PD../bin/pd-server --data-dir=pd   --log-file=pd.log Start TiKV../bin/tikv-server --pd=&amp;#34;127.0.0.1:2379&amp;#34;   --data-dir=tikv   --log-file=tikv.log Start TiDB../bin/tidb-server --store=tikv   --path=&amp;#34;127.0.0.1:2379&amp;#34;   --log-file=tidb.log Use the official MySQL client to connect to TiDB.mysql -h 127.0.0.1 -P 4000 -u root -D test  Multiple nodes cluster deployment for test If you want to test TiDB but have a limited number of nodes, you can use one PD instance to test the entire cluster.Assuming that you have four nodes, you can deploy 1 PD instance, 3 TiKV instances, and 1 TiDB instance. See the following table for details:   Name Host IP Services     Node1 192.168.199.113 PD1, TiDB   Node2 192.168.199.114 TiKV1   Node3 192.168.199.115 TiKV2   Node4 192.168.199.116 TiKV3    Follow the steps below to start PD, TiKV and TiDB: Start PD on Node1../bin/pd-server --name=pd1   --data-dir=pd1   --client-urls=&amp;#34;http://192.168.199.113:2379&amp;#34;   --peer-urls=&amp;#34;http://192.168.199.113:2380&amp;#34;   --initial-cluster=&amp;#34;pd1=http://192.168.199.113:2380&amp;#34;   --log-file=pd.log Start TiKV on Node2, Node3 and Node4../bin/tikv-server --pd=&amp;#34;192.168.199.113:2379&amp;#34;   --addr=&amp;#34;192.168.199.114:20160&amp;#34;   --data-dir=tikv1   --log-file=tikv.log ./bin/tikv-server --pd=&amp;#34;192.168.199.113:2379&amp;#34;   --addr=&amp;#34;192.168.199.115:20160&amp;#34;   --data-dir=tikv2   --log-file=tikv.log ./bin/tikv-server --pd=&amp;#34;192.168.199.113:2379&amp;#34;   --addr=&amp;#34;192.168.199.116:20160&amp;#34;   --data-dir=tikv3   --log-file=tikv.log Start TiDB on Node1../bin/tidb-server --store=tikv   --path=&amp;#34;192.168.199.113:2379&amp;#34;   --log-file=tidb.log Use the official MySQL client to connect to TiDB.mysql -h 192.168.199.113 -P 4000 -u root -D test  Multiple nodes cluster deployment For the production environment, multiple nodes cluster deployment is recommended. Before you begin, see Software and Hardware Requirements.Assuming that you have six nodes, you can deploy 3 PD instances, 3 TiKV instances, and 1 TiDB instance. See the following table for details:   Name Host IP Services     Node1 192.168.199.113 PD1, TiDB   Node2 192.168.199.114 PD2   Node3 192.168.199.115 PD3   Node4 192.168.199.116 TiKV1   Node5 192.168.199.117 TiKV2   Node6 192.168.199.118 TiKV3    Follow the steps below to start PD, TiKV, and TiDB: Start PD on …"},
		{"url": "https://pingcap.com/docs/",
		"title": "Docs", 
		"content": ""},
		{"url": "https://pingcap.com/docs-cn/",
		"title": "Docs-cns", 
		"content": ""},
		{"url": "https://pingcap.com/docs/op-guide/security/",
		"title": "Enable TLS Authentication", 
		"content": " Enable TLS Authentication Overview This document describes how to enable TLS authentication in the TiDB cluster. The TLS authentication includes the following two conditions: The mutual authentication between TiDB components, including the authentication among TiDB, TiKV and PD, between TiKV Control and TiKV, between PD Control and PD, between TiKV peers, and between PD peers. Once enabled, the mutual authentication applies to all components, and it does not support applying to only part of the components. The one-way and mutual authentication between the TiDB server and the MySQL Client.   Note: The authentication between the MySQL Client and the TiDB server uses one set of certificates, while the authentication among TiDB components uses another set of certificates. Enable mutual TLS authentication among TiDB components Prepare certificates It is recommended to prepare a separate server certificate for TiDB, TiKV and PD, and make sure that they can authenticate each other. The clients of TiDB, TiKV and PD share one client certificate.You can use multiple tools to generate self-signed certificates, such as openssl, easy-rsa and cfssl.See an example of generating self-signed certificates using cfssl.Configure certificates To enable mutual authentication among TiDB components, configure the certificates of TiDB, TiKV and PD as follows.TiDB Configure in the configuration file or command line arguments:[security] # Path of file that contains list of trusted SSL CAs for connection with cluster components. cluster-ssl-ca = &amp;#34;/path/to/ca.pem&amp;#34; # Path of file that contains X509 certificate in PEM format for connection with cluster components. cluster-ssl-cert = &amp;#34;/path/to/tidb-server.pem&amp;#34; # Path of file that contains X509 key in PEM format for connection with cluster components. cluster-ssl-key = &amp;#34;/path/to/tidb-server-key.pem&amp;#34; TiKV Configure in the configuration file or command line arguments, and set the corresponding URL to https:[security] # set the path for certificates. Empty string means disabling secure connections. ca-path = &amp;#34;/path/to/ca.pem&amp;#34; cert-path = &amp;#34;/path/to/client.pem&amp;#34; key-path = &amp;#34;/path/to/client-key.pem&amp;#34; PD Configure in the configuration file or command line arguments, and set the corresponding URL to https:[security] # Path of file that contains list of trusted SSL CAs. If set, following four settings shouldn&amp;#39;t be empty cacert-path = &amp;#34;/path/to/ca.pem&amp;#34; # Path of file that contains X509 certificate in PEM format. cert-path = &amp;#34;/path/to/server.pem&amp;#34; # Path of file that contains X509 key in PEM format. key-path = &amp;#34;/path/to/server-key.pem&amp;#34; Now mutual authentication among TiDB components is enabled.When you connect the server using the client, it is required to specify the client certificate. For example:./pd-ctl -u https://127.0.0.1:2379 --cacert /path/to/ca.pem --cert /path/to/pd-client.pem --key /path/to/pd-client-key.pem ./tikv-ctl --host=&amp;#34;127.0.0.1:20160&amp;#34; --ca-path=&amp;#34;/path/to/ca.pem&amp;#34; --cert-path=&amp;#34;/path/to/client.pem&amp;#34; --key-path=&amp;#34;/path/to/clinet-key.pem&amp;#34; Enable TLS authentication between the MySQL client and TiDB server Prepare certificates mysql_ssl_rsa_setup --datadir=certs Configure one-way authentication Configure in the configuration file or command line arguments of TiDB:[security] # Path of file that contains list of trusted SSL CAs. ssl-ca = &amp;#34;&amp;#34; # Path of file that contains X509 certificate in PEM format. ssl-cert = &amp;#34;/path/to/certs/server.pem&amp;#34; # Path of file that contains X509 key in PEM format. ssl-key = &amp;#34;/path/to/certs/server-key.pem&amp;#34; Configure in the MySQL client:mysql -u root --host 127.0.0.1 --port 4000 --ssl-mode=REQUIRED Configure mutual authentication Configure in the configuration file or command line arguments of TiDB:[security] # Path of file that contains list of trusted SSL CAs for connection with mysql client. ssl-ca = &amp;#34;/path/to/certs/ca.pem&amp;#34; # Path of file that contains X509 certificate in PEM format for connection with mysql client. ssl-cert = &amp;#34;/path/to/certs/server.pem&amp;#34; # Path of file that contains X509 key in PEM format for connection with mysql client. ssl-key = &amp;#34;/path/to/certs/server-key.pem&amp;#34; Specify the client certificate in the client:mysql -u root --host 127.0.0.1 --port 4000 --ssl-cert=/path/to/certs/client-cert.pem --ssl-key=/path/to/certs/client-key.pem --ssl-ca=/path/to/certs/ca.pem --ssl-mode=VERIFY_IDENTITY"},
		{"url": "https://pingcap.com/docs/sql/encryption-and-compression-functions/",
		"title": "Encryption and Compression Functions", 
		"content": " Encryption and Compression Functions    Name Description     MD5() Calculate MD5 checksum   PASSWORD() (deprecated 5.7.6) Calculate and return a password string   RANDOM_BYTES() Return a random byte vector   SHA1(), SHA() Calculate an SHA-1 160-bit checksum   SHA2() Calculate an SHA-2 checksum   AES_DECRYPT() Decrypt using AES   AES_ENCRYPT() Encrypt using AES   COMPRESS() Return result as a binary string   UNCOMPRESS() Uncompress a string compressed   UNCOMPRESSED_LENGTH() Return the length of a string before compression   CREATE_ASYMMETRIC_PRIV_KEY() Create private key   CREATE_ASYMMETRIC_PUB_KEY() Create public key   CREATE_DH_PARAMETERS() Generate shared DH secret   CREATE_DIGEST() Generate digest from string   ASYMMETRIC_DECRYPT() Decrypt ciphertext using private or public key   ASYMMETRIC_DERIVE() Derive symmetric key from asymmetric keys   ASYMMETRIC_ENCRYPT() Encrypt cleartext using private or public key   ASYMMETRIC_SIGN() Generate signature from digest   ASYMMETRIC_VERIFY() Verify that signature matches digest    "},
		{"url": "https://pingcap.com/docs/sql/error/",
		"title": "Error Codes and Troubleshooting", 
		"content": " Error Codes and Troubleshooting This document describes the problems encountered during the use of TiDB and provides the solutions.Error codes TiDB is compatible with the error codes in MySQL, and in most cases returns the same error code as MySQL. In addition, TiDB has the following unique error codes:   Error code Description Solution     9001 The PD request timed out. Check the state/monitor/log of the PD server and the network between the TiDB server and the PD server.   9002 The TiKV request timed out. Check the state/monitor/log of the TiKV server and the network between the TiDB server and the TiKV server.   9003 The TiKV server is busy and this usually occurs when the workload is too high. Check the state/monitor/log of the TiKV server.   9004 This error occurs when a large number of transactional conflicts exist in the database. Check the code of application.   9005 A certain Raft Group is not available, such as the number of replicas is not enough. This error usually occurs when the TiKV server is busy or the TiKV node is down. Check the state/monitor/log of the TiKV server.   9006 The interval of GC Life Time is too short and the data that should be read by the long transactions might be cleared. Extend the interval of GC Life Time.   9500 A single transaction is too large. See here for the solution.    Troubleshooting See the troubleshooting and FAQ documents."},
		{"url": "https://pingcap.com/docs/sql/expression-syntax/",
		"title": "Expression Syntax", 
		"content": " Expression Syntax The following rules define the expression syntax in TiDB. You can find the definition in parser/parser.y. The syntax parsing in TiDB is based on Yacc.Expression: singleAtIdentifier assignmentEq Expression | Expression logOr Expression | Expression &amp;#34;XOR&amp;#34; Expression | Expression logAnd Expression | &amp;#34;NOT&amp;#34; Expression | Factor IsOrNotOp trueKwd | Factor IsOrNotOp falseKwd | Factor IsOrNotOp &amp;#34;UNKNOWN&amp;#34; | Factor Factor: Factor IsOrNotOp &amp;#34;NULL&amp;#34; | Factor CompareOp PredicateExpr | Factor CompareOp singleAtIdentifier assignmentEq PredicateExpr | Factor CompareOp AnyOrAll SubSelect | PredicateExpr PredicateExpr: PrimaryFactor InOrNotOp &amp;#39;(&amp;#39; ExpressionList &amp;#39;)&amp;#39; | PrimaryFactor InOrNotOp SubSelect | PrimaryFactor BetweenOrNotOp PrimaryFactor &amp;#34;AND&amp;#34; PredicateExpr | PrimaryFactor LikeOrNotOp PrimaryExpression LikeEscapeOpt | PrimaryFactor RegexpOrNotOp PrimaryExpression | PrimaryFactor PrimaryFactor: PrimaryFactor &amp;#39;|&amp;#39; PrimaryFactor | PrimaryFactor &amp;#39;&amp;amp;&amp;#39; PrimaryFactor | PrimaryFactor &amp;#34;&amp;lt;&amp;lt;&amp;#34; PrimaryFactor | PrimaryFactor &amp;#34;&amp;gt;&amp;gt;&amp;#34; PrimaryFactor | PrimaryFactor &amp;#39;+&amp;#39; PrimaryFactor | PrimaryFactor &amp;#39;-&amp;#39; PrimaryFactor | PrimaryFactor &amp;#39;*&amp;#39; PrimaryFactor | PrimaryFactor &amp;#39;/&amp;#39; PrimaryFactor | PrimaryFactor &amp;#39;%&amp;#39; PrimaryFactor | PrimaryFactor &amp;#34;DIV&amp;#34; PrimaryFactor | PrimaryFactor &amp;#34;MOD&amp;#34; PrimaryFactor | PrimaryFactor &amp;#39;^&amp;#39; PrimaryFactor | PrimaryExpression PrimaryExpression: Operand | FunctionCallKeyword | FunctionCallNonKeyword | FunctionCallAgg | FunctionCallGeneric | Identifier jss stringLit | Identifier juss stringLit | SubSelect | &amp;#39;!&amp;#39; PrimaryExpression | &amp;#39;~&amp;#39; PrimaryExpression | &amp;#39;-&amp;#39; PrimaryExpression | &amp;#39;+&amp;#39; PrimaryExpression | &amp;#34;BINARY&amp;#34; PrimaryExpression | PrimaryExpression &amp;#34;COLLATE&amp;#34; StringName "},
		{"url": "https://pingcap.com/recruit-cn/campus/frontend-engineer/",
		"title": "Front End Engineer", 
		"content": " Front End Engineer 职位描述 这是一个对我们这种做「后端」的公司非常重要的岗位，直接关系到我们能够提供一个什么「样子」的解决方案给我们的客户。 我们需要把技术上复杂的算法和逻辑隐藏起来，让开发者没有心智负担的使用，而不是终日面对冰冷的命令行接口狂敲。一个现代的商用基础软件，流畅优雅的 UI/UE 必不可少，我们对设计和交互的偏执等同于分布式算法和测试的偏执，不可分割。我们在后边的一切工作和炫酷的技术，都需要同样炫酷的前端来落地。我们在 enjoy 这个「造物」的过程，希望邀你一起。哦，对了，我们的技术栈： Bootstrap Vue.js AngularJS ReactJS HighChart Gulp Less  我们对于前端工程师没有其他别的要求，就是对于「美」有所追求，充满好奇心：）待遇 15K - 20K + 期权, 13薪 + 奖金联系方式： hire@pingcap.com工作地点 北京"},
		{"url": "https://pingcap.com/docs/sql/functions-and-operators-reference/",
		"title": "Function and Operator Reference", 
		"content": " Function and Operator Reference The usage of the functions and operators in TiDB is similar to MySQL. See Functions and Operators in MySQL.In SQL statements, expressions can be used on the ORDER BY and HAVING clauses of the SELECT statement, the WHERE clause of SELECT/DELETE/UPDATE statements, and SET statements.You can write expressions using literals, column names, NULL, built-in functions, operators and so on."},
		{"url": "https://pingcap.com/docs-cn/sql/aggregate-group-by-functions/",
		"title": "GROUP BY 聚合函数", 
		"content": " GROUP BY 聚合函数 GROUP BY 聚合函数功能描述 本节介绍 TiDB 中支持的 MySQL GROUP BY 聚合函数。   函数名 功能描述     COUNT() 返回检索到的行的数目   COUNT(DISTINCT) 返回不同值的数目   SUM() 返回和   AVG() 返回平均值   MAX() 返回最大值   MIN() 返回最小值   GROUP_CONCAT() 返回连接的字符串     Note: 除非另有说明，否则组函数默认忽略 NULL 值。 如果在不包含 GROUP BY 子句的语句中使用组函数，则相当于对所有行进行分组。详情参阅 TiDB 中的 GROUP BY。   GROUP BY 修饰符 TiDB 目前不支持任何 GROUP BY 修饰符，将来会提供支持，详情参阅 #4250。TiDB 中的 GROUP BY 当 SQL 模式 ONLY_FULL_GROUP_BY 被禁用时，TiDB 与 MySQL 等效：允许 SELECT 列表、HAVING 条件或 ORDER BY 列表引用非聚合列，即使这些列在功能上不依赖于 GROUP BY 列。例如，在 MySQL 5.7.5 中使用 ONLY_FULL_GROUP_BY 的查询是不合规的，因为 SELECT 列表中的非聚合列 &amp;ldquo;b&amp;rdquo; 在 GROUP BY 中不显示：drop table if exists t; create table t(a bigint, b bigint, c bigint); insert into t values(1, 2, 3), (2, 2, 3), (3, 2, 3); select a, b, sum(c) from t group by a; 上述查询在 TiDB 中是合规的。TiDB 目前不支持 SQL 模式 ONLY_FULL_GROUP_BY，将来会提供支持，详情参阅 #4248。假设我们执行以下查询，希望结果按 c 排序:drop table if exists t; create table t(a bigint, b bigint, c bigint); insert into t values(1, 2, 1), (1, 2, 2), (1, 3, 1), (1, 3, 2); select distinct a, b from t order by c; 要对结果进行排序，必须先清除重复。但选择保留哪一行会影响 c 的保留值，也会影响排序，并使其具有任意性。在 MySQL 中，ORDER BY 表达式需至少满足以下条件之一，否则 DISTINCT 和 ORDER BY 查询将因不合规而被拒绝： 表达式等同于 SELECT 列表中的一个。 表达式引用并属于查询选择表的所有列都是 SELECT 列表的元素。  但是在 TiDB 中，上述查询是合规的，详情参阅 #4254。TiDB 中另一个标准 SQL 的扩展允许 HAVING 子句中的引用使用 SELECT 列表中的别名表达式。例如：以下查询返回在 orders 中只出现一次的 name 值：select name, count(name) from orders group by name having count(name) = 1; 这个 TiDB 扩展允许在聚合列的 HAVING 子句中使用别名：select name, count(name) as c from orders group by name having c = 1; 标准 SQL 只支持 GROUP BY 子句中的列表达式，以下语句不合规，因为 FLOOR(value/100) 是一个非列表达式：select id, floor(value/100) from tbl_name group by id, floor(value/100); TiDB 对标准 SQL 的扩展支持 GROUP BY 子句中非列表达式，认为上述语句合规。标准 SQL 也不支持 GROUP BY 子句中使用别名。TiDB 对标准 SQL 的扩展支持使用别名，查询的另一种写法如下：select id, floor(value/100) as val from tbl_name group by id, val; 函数依赖检测 TiDB 不支持 SQL 模式 ONLY_FULL_GROUP_BY 和函数依赖检测，将来会提供支持，详情参阅 #4248。"},
		{"url": "https://pingcap.com/docs/op-guide/generate-self-signed-certificates/",
		"title": "Generate Self-signed Certificates", 
		"content": " Generate Self-signed Certificates Overview This document describes how to generate self-signed certificates using cfssl.Assume that the topology of the instance cluster is as follows:   Name Host IP Services     node1 172.16.10.1 PD1, TiDB1   node2 172.16.10.2 PD2, TiDB2   node3 172.16.10.3 PD3   node4 172.16.10.4 TiKV1   node5 172.16.10.5 TiKV2   node6 172.16.10.6 TiKV3    Download cfssl Assume that the host is x86_64 Linux:mkdir ~/bin curl -s -L -o ~/bin/cfssl https://pkg.cfssl.org/R1.2/cfssl_linux-amd64 curl -s -L -o ~/bin/cfssljson https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64 chmod +x ~/bin/{cfssl,cfssljson} export PATH=$PATH:~/bin Initialize the certificate authority To make it easy for modification later, generate the default configuration of cfssl:mkdir ~/cfssl cd ~/cfssl cfssl print-defaults config &amp;gt; ca-config.json cfssl print-defaults csr &amp;gt; ca-csr.json Generate certificates Certificates description  tidb-server certificate: used by TiDB to authenticate TiDB for other components and clients tikv-server certificate: used by TiKV to authenticate TiKV for other components and clients pd-server certificate: used by PD to authenticate PD for other components and clients client certificate: used to authenticate the clients from PD, TiKV and TiDB, such as pd-ctl, tikv-ctl and pd-recover  Configure the CA option Edit ca-config.json according to your need:{ &amp;#34;signing&amp;#34;: { &amp;#34;default&amp;#34;: { &amp;#34;expiry&amp;#34;: &amp;#34;43800h&amp;#34; }, &amp;#34;profiles&amp;#34;: { &amp;#34;server&amp;#34;: { &amp;#34;expiry&amp;#34;: &amp;#34;43800h&amp;#34;, &amp;#34;usages&amp;#34;: [ &amp;#34;signing&amp;#34;, &amp;#34;key encipherment&amp;#34;, &amp;#34;server auth&amp;#34;, &amp;#34;client auth&amp;#34; ] }, &amp;#34;client&amp;#34;: { &amp;#34;expiry&amp;#34;: &amp;#34;43800h&amp;#34;, &amp;#34;usages&amp;#34;: [ &amp;#34;signing&amp;#34;, &amp;#34;key encipherment&amp;#34;, &amp;#34;client auth&amp;#34; ] } } } } Edit ca-csr.json according to your need:{ &amp;#34;CN&amp;#34;: &amp;#34;My own CA&amp;#34;, &amp;#34;key&amp;#34;: { &amp;#34;algo&amp;#34;: &amp;#34;rsa&amp;#34;, &amp;#34;size&amp;#34;: 2048 }, &amp;#34;names&amp;#34;: [ { &amp;#34;C&amp;#34;: &amp;#34;CN&amp;#34;, &amp;#34;L&amp;#34;: &amp;#34;Beijing&amp;#34;, &amp;#34;O&amp;#34;: &amp;#34;PingCAP&amp;#34;, &amp;#34;ST&amp;#34;: &amp;#34;Beijing&amp;#34; } ] } Generate the CA certificate cfssl gencert -initca ca-csr.json | cfssljson -bare ca - The command above generates the following files:ca-key.pem ca.csr ca.pem Generate the server certificate The IP address of all components and 127.0.0.1 are included in hostname.echo &amp;#39;{&amp;#34;CN&amp;#34;:&amp;#34;tidb-server&amp;#34;,&amp;#34;hosts&amp;#34;:[&amp;#34;&amp;#34;],&amp;#34;key&amp;#34;:{&amp;#34;algo&amp;#34;:&amp;#34;rsa&amp;#34;,&amp;#34;size&amp;#34;:2048}}&amp;#39; | cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=server -hostname=&amp;#34;172.16.10.1,172.16.10.2,127.0.0.1&amp;#34; - | cfssljson -bare tidb-server echo &amp;#39;{&amp;#34;CN&amp;#34;:&amp;#34;tikv-server&amp;#34;,&amp;#34;hosts&amp;#34;:[&amp;#34;&amp;#34;],&amp;#34;key&amp;#34;:{&amp;#34;algo&amp;#34;:&amp;#34;rsa&amp;#34;,&amp;#34;size&amp;#34;:2048}}&amp;#39; | cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=server -hostname=&amp;#34;172.16.10.4,172.16.10.5,172.16.10.6,127.0.0.1&amp;#34; - | cfssljson -bare tikv-server echo &amp;#39;{&amp;#34;CN&amp;#34;:&amp;#34;pd-server&amp;#34;,&amp;#34;hosts&amp;#34;:[&amp;#34;&amp;#34;],&amp;#34;key&amp;#34;:{&amp;#34;algo&amp;#34;:&amp;#34;rsa&amp;#34;,&amp;#34;size&amp;#34;:2048}}&amp;#39; | cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=server -hostname=&amp;#34;172.16.10.1,172.16.10.2,172.16.10.3,127.0.0.1&amp;#34; - | cfssljson -bare pd-server The command above generates the following files:tidb-server-key.pem tikv-server-key.pem pd-server-key.pem tidb-server.csr tikv-server.csr pd-server.csr tidb-server.pem tikv-server.pem pd-server.pem Generate the client certificate echo &amp;#39;{&amp;#34;CN&amp;#34;:&amp;#34;client&amp;#34;,&amp;#34;hosts&amp;#34;:[&amp;#34;&amp;#34;],&amp;#34;key&amp;#34;:{&amp;#34;algo&amp;#34;:&amp;#34;rsa&amp;#34;,&amp;#34;size&amp;#34;:2048}}&amp;#39; | cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=client -hostname=&amp;#34;&amp;#34; - | cfssljson -bare client The command above generates the following files:client-key.pem client.csr client.pem"},
		{"url": "https://pingcap.com/docs/sql/information-functions/",
		"title": "Information Functions", 
		"content": " Information Functions In TiDB, the usage of information functions is similar to MySQL. For more information, see Information Functions.Information function descriptions    Name Description     CONNECTION_ID() Return the connection ID (thread ID) for the connection   CURRENT_USER(), CURRENT_USER Return the authenticated user name and host name   DATABASE() Return the default (current) database name   FOUND_ROWS() For a SELECT with a LIMIT clause, the number of the rows that are returned if there is no LIMIT clause   LAST_INSERT_ID() Return the value of the AUTOINCREMENT column for the last INSERT   SCHEMA() Synonym for DATABASE()   SESSION_USER() Synonym for USER()   SYSTEM_USER() Synonym for USER()   USER() Return the user name and host name provided by the client   VERSION() Return a string that indicates the MySQL server version   TIDB_VERSION Return a string that indicates the TiDB server version    "},
		{"url": "https://pingcap.com/recruit-cn/campus/infrastructure-engineer/",
		"title": "Infrastructure Engineer", 
		"content": " Infrastructure Engineer 职位描述 如果你： 内心不安，喜欢挑战和创新 熟悉分布式系统，大数据或者数据库领域 想和简单有爱的 PingCAP 的工程师们一起做世界级的开源项目  那么你就是我们要找的人。在分布式数据库领域有很多迷人的问题需要去解决，如果你对任何一个问题感到无比的好奇，想要深挖究竟，都可以来和我们聊聊: 想深入理解业界最前沿的分布式数据库 Spanner 的设计和思考，如何从 0 到 1 落地实现 如何设计和实现世界前沿的分布式 SQL 优化器，让一个复杂的 SQL 查询变的无比轻快智能 如何在成千上万台集群规模的情况下，实现无阻塞的表结构变更操作，而不影响任何在线的业务 如何实现一个高效的分布式事务管理器，让 ACID 事务在大规模并发的分布式存场景下依然可以高效可靠 如何基于一致性的 Raft 协议实现快速稳定的数据复制和自动故障恢复，确保数据安全 如何在一个 PR 提交之后，快速验证千万级别的 tests 是否全部通过，性能有没有显著提升  &amp;hellip; &amp;hellip;待遇 15K - 20K + 期权, 13薪 + 奖金联系方式： hire@pingcap.com工作地点 北京，上海，广州，杭州"},
		{"url": "https://pingcap.com/recruit-cn/campus/infrastructure-engineer-intern/",
		"title": "Infrastructure Engineer Intern", 
		"content": " Infrastructure Engineer Intern 职位描述 你能从工作中学习到什么？ 如何构建一个分布式关系数据库 如何将其包装成为一套完整的商业产品 亲身参与以上过程，并实践你所掌握的开发技术 成为未来具有全球影响力的开源分布式数据库产品的早期贡献者  要求： 熟悉常用的开发语言，熟悉 Golang/Rust 优先 熟悉分布式系统/数据库系统优先 有开源项目实践经历优先 实习优秀者可获得正式工作机会，并有期权  待遇 250 * 8小时，水果零食，购书补助等等联系方式： hire@pingcap.com工作地点 北京，上海，广州，杭州"},
		{"url": "https://pingcap.com/docs/sql/statistics/",
		"title": "Introduction to Statistics", 
		"content": " Introduction to Statistics Based on the statistics, the TiDB optimizer chooses the most efficient query execution plan. The statistics collect table-level and column-level information. The statistics of a table include the total number of rows and the number of updated rows. The statistics of a column include the number of different values, the number of NULL, and the histogram of the column.Collect statistics Manual collection You can run the ANALYZE statement to collect statistics.Syntax:ANALYZE TABLE TableNameList &amp;gt; The statement collects statistics of all the tables in `TableNameList`. ANALYZE TABLE TableName INDEX IndexNameList &amp;gt; The statement collects statistics of the index columns on all `IndexNameList` in `TableName`.  Automatic update For the INSERT, DELETE, or UPDATE statements, TiDB automatically updates the number of rows and updated rows. TiDB persists this information regularly and the update cycle is 5 * stats-lease. The default value of stats-lease is 3s. If you specify the value as 0, it does not update automatically.Control ANALYZE concurrency When you run the ANALYZE statement, you can adjust the concurrency using the following parameters, to control its effect on the system.tidb_build_stats_concurrency Currently, when you run the ANALYZE statement, the task is divided into multiple small tasks. Each task only works on one column or index. You can use the tidb_build_stats_concurrency parameter to control the number of simultaneous tasks. The default value is 4.tidb_distsql_scan_concurrency When you analyze regular columns, you can use the tidb_distsql_scan_concurrency parameter to control the number of Region to be read at one time. The default value is 10.tidb_index_serial_scan_concurrency When you analyze index columns, you can use the tidb_index_serial_scan_concurrency parameter to control the number of Region to be read at one time. The default value is 1.View statistics You can view the statistics status using the following statements.Metadata of tables You can use the SHOW STATS_META statement to view the total number of rows and the number of updated rows.Syntax:SHOW STATS_META [ShowLikeOrWhere] &amp;gt; The statement returns the total number of rows and the number of updated rows. You can use `ShowLikeOrWhere` to filter the information you need. Currently, the SHOW STATS_META statement returns the following 5 columns:   Syntax Element Description     db_name database name   table_name table name   update_time the time of the update   modify_count the number of modified rows   row_count the total number of rows    Metadata of columns You can use the SHOW STATS_HISTOGRAMS statement to view the number of different values and the number of NULL in all the columns.Syntax:SHOW STATS_HISTOGRAMS [ShowLikeOrWhere] &amp;gt; The statement returns the number of different values and the number of `NULL` in all the columns. You can use `ShowLikeOrWhere` to filter the information you need. Currently, the SHOW STATS_HISTOGRAMS statement returns the following 7 columns:   Syntax Element Description     db_name database name   table_name table name   column_name column name   is_index whether it is an index column or not   update_time the time of the update   distinct_count the number of different values   null_count the number of NULL    Buckets of histogram You can use the SHOW STATS_BUCKETS statement to view each bucket of the histogram.Syntax:SHOW STATS_BUCKETS [ShowLikeOrWhere] &amp;gt; The statement returns information about all the buckets. You can use `ShowLikeOrWhere` to filter the information you need. Currently, the SHOW STATS_BUCKETS statement returns the following 9 columns:   Syntax Element Description     db_name database name   table_name table name   column_name column name   is_index whether it is an index column or not   bucket_id the ID of a bucket   count the number of all the values that falls on the bucket and the previous buckets   repeats the occurrence number of the maximum value   lower_bound the minimum value   upper_bound the maximum value    Delete statistics You can run the DROP STATS statement to delete statistics.Syntax:DROP STATS TableName &amp;gt; The statement deletes statistics of all the tables in `TableName`。"},
		{"url": "https://pingcap.com/docs/sql/json-functions/",
		"title": "JSON Functions", 
		"content": " JSON Functions    Function Name and Syntactic Sugar Description     JSON_EXTRACT(json_doc, path[, path] &amp;hellip;) Return data from a JSON document, selected from the parts of the document matched by the path arguments   JSON_UNQUOTE(json_val) Unquote JSON value and return the result as a utf8mb4 string   JSON_TYPE(json_val) Return a utf8mb4 string indicating the type of a JSON value   JSON_SET(json_doc, path, val[, path, val] &amp;hellip;) Insert or update data in a JSON document and return the result   JSON_INSERT(json_doc, path, val[, path, val] &amp;hellip;) Insert data into a JSON document and return the result   JSON_REPLACE(json_doc, path, val[, path, val] &amp;hellip;) Replace existing values in a JSON document and return the result   JSON_REMOVE(json_doc, path[, path] &amp;hellip;) Remove data from a JSON document and return the result   JSON_MERGE(json_doc, json_doc[, json_doc] &amp;hellip;) Merge two or more JSON documents and return the merged result   JSON_OBJECT(key, val[, key, val] &amp;hellip;) Evaluate a (possibly empty) list of key-value pairs and return a JSON object containing those pairs   JSON_ARRAY([val[, val] &amp;hellip;]) Evaluate a (possibly empty) list of values and return a JSON array containing those values   -&amp;gt; Return value from JSON column after evaluating path; the syntactic sugar of JSON_EXTRACT(doc, path_literal)   -&amp;gt;&amp;gt; Return value from JSON column after evaluating path and unquoting the result; the syntactic sugar of JSON_UNQUOTE(JSONJSON_EXTRACT(doc, path_literal))    "},
		{"url": "https://pingcap.com/docs/sql/json-functions-generated-column/",
		"title": "JSON Functions and Generated Column", 
		"content": " JSON Functions and Generated Column About To be compatible with MySQL 5.7 or later and better support the document store, TiDB supports JSON in the latest version. In TiDB, a document is a set of Key-Value pairs, encoded as a JSON object. You can use the JSON datatype in a TiDB table and create indexes for the JSON document fields using generated columns. In this way, you can flexibly deal with the business scenarios with uncertain schema and are no longer limited by the read performance and the lack of support for transactions in traditional document databases.JSON functions The support for JSON in TiDB mainly refers to the user interface of MySQL 5.7. For example, you can create a table that includes a JSON field to store complex information:CREATE TABLE person ( id INT NOT NULL AUTO_INCREMENT PRIMARY KEY, name VARCHAR(255) NOT NULL, address_info JSON ); When you insert data into a table, you can deal with those data with uncertain schema like this:INSERT INTO person (name, address_info) VALUES (&amp;#34;John&amp;#34;, &amp;#39;{&amp;#34;city&amp;#34;: &amp;#34;Beijing&amp;#34;}&amp;#39;); You can insert JSON data into the table by inserting a legal JSON string into the column corresponding to the JSON field. TiDB will then parse the text and save it in a more compact and easy-to-access binary form.You can also convert other data type into JSON using CAST:INSERT INTO person (name, address_info) VALUES (&amp;#34;John&amp;#34;, CAST(&amp;#39;{&amp;#34;city&amp;#34;: &amp;#34;Beijing&amp;#34;}&amp;#39; AS JSON)); INSERT INTO person (name, address_info) VALUES (&amp;#34;John&amp;#34;, CAST(&amp;#39;123&amp;#39; AS JSON)); INSERT INTO person (name, address_info) VALUES (&amp;#34;John&amp;#34;, CAST(123 AS JSON)); Now, if you want to query all the users living in Beijing from the table, you can simply use the following SQL statement:SELECT id, name FROM person WHERE JSON_EXTRACT(address_info, &amp;#39;$.city&amp;#39;) = &amp;#39;Beijing&amp;#39;); TiDB supports the JSON_EXTRACT function which is exactly the same as in MySQL. The function is to extract the city field from the address_info document. The second argument is a &amp;ldquo;path expression&amp;rdquo; and is used to specify which field to extract. See the following few examples to help you understand the &amp;ldquo;path expression&amp;rdquo;:SET @person = &amp;#39;{&amp;#34;name&amp;#34;:&amp;#34;John&amp;#34;,&amp;#34;friends&amp;#34;:[{&amp;#34;name&amp;#34;:&amp;#34;Forest&amp;#34;,&amp;#34;age&amp;#34;:16},{&amp;#34;name&amp;#34;:&amp;#34;Zhang San&amp;#34;,&amp;#34;gender&amp;#34;:&amp;#34;male&amp;#34;}]}&amp;#39;; SELECT JSON_EXTRACT(@person, &amp;#39;$.name&amp;#39;); -- gets &amp;#34;John&amp;#34; SELECT JSON_EXTRACT(@person, &amp;#39;$.friends[0].age&amp;#39;); -- gets 16 SELECT JSON_EXTRACT(@person, &amp;#39;$.friends[1].gender&amp;#39;); -- gets &amp;#34;male&amp;#34; SELECT JSON_EXTRACT(@person, &amp;#39;$.friends[2].name&amp;#39;); -- gets NULL In addition to inserting and querying data, TiDB also supports editing JSON. In general, TiDB currently supports the following JSON functions in MySQL 5.7: JSON_EXTRACT JSON_ARRAY JSON_OBJECT JSON_SET JSON_REPLACE JSON_INSERT JSON_REMOVE JSON_TYPE JSON_UNQUOTE  You can get the general use of these functions directly from the function name. These functions in TiDB behave the same as in MySQL 5.7. For more information, see the JSON Functions document of MySQL 5.7. If you are a user of MySQL 5.7, you can migrate to TiDB seamlessly.Currently TiDB does not support all the JSON functions in MySQL 5.7. This is because our preliminary goal is to provide complete support for MySQL X Plugin, which covers the majority of JSON functions used to insert, select, update and delete data. More functions will be supported if necessary.Index JSON using generated column The full table scan is executed when you query a JSON field. When you run the EXPLAIN statement in TiDB, the results show that it is full table scan. Then, can you index the JSON field?First, this type of index is wrong:CREATE TABLE person ( id INT NOT NULL AUTO_INCREMENT PRIMARY KEY, name VARCHAR(255) NOT NULL, address_info JSON, KEY (address_info) ); This is not because of technical impossibility but because the direct comparison of JSON itself is meaningless. Although we can agree on some comparison rules, such as ARRAY is bigger than all OBJECT, it is useless. Therefore, as what is done in MySQL 5.7, TiDB prohibits the direct creation of index on JSON field, but you can index the fields in the JSON document in the form of generated column:CREATE TABLE person ( id INT NOT NULL AUTO_INCREMENT PRIMARY KEY, name VARCHAR(255) NOT NULL, address_info JSON, city VARCHAR(64) AS (JSON_EXTRACT(address_info, &amp;#39;$.city&amp;#39;)) VIRTUAL, KEY (city) ); In this table, the city column is a generated column. As the name implies, the column is generated by other columns in the table, and cannot be assigned a value when inserted or updated. For generating a column, you can specify it as VIRTUAL to prevent it from being explicitly saved in the record, but by other columns when needed. This is particularly useful when the column is wide and you need to save storage space. With this generated column, you can create an index on it, and it looks the same with other regular columns. In query, you can run the following statements:SELECT name, id FROM person WHERE city = &amp;#39;Beijing&amp;#39;; In this way, you can create an index. Note: In the JSON document, if the field in the specified path does not exist, the result of JSON_EXTRACT will be NULL. The value of the generated column with index is also NULL. If this is not what you want to see, you can add a NOT NULL constraint on the generated column. In this way, when the value of the city field is NULL after you insert data, it can be detected. Limitations The current limitations of JSON and generated column are as follows: You cannot add the generated column in the storage type of STORED through ALTER TABLE. You cannot create an index on the generated column through ALTER TABLE.  The above functions and some other JSON functions are under development."},
		{"url": "https://pingcap.com/docs-cn/sql/json-functions-generated-column/",
		"title": "JSON 函数及 Generated Column", 
		"content": " JSON 函数及 Generated Column 概述 为了在功能上兼容 MySQL 5.7 及以上，同时更好地支持文档类型存储，我们在最新版本的 TiDB 中加入了 JSON 的支持。TiDB 所支持的文档是指以 JSON 为编码类型的键值对的组合。用户可以在 TiDB 的表中使用 JSON 类型的字段，同时以生成列（generated column）的方式为 JSON 文档内部的字段建立索引。基于此，用户可以很灵活地处理那些 schema 不确定的业务，同时不必受限于传统文档数据库糟糕的读性能及匮乏的事务支持。JSON功能介绍 TiDB 的 JSON 主要参考了 MySQL 5.7 的用户接口。例如，可以创建一个表，包含一个 JSON 字段来存储那些复杂的信息：CREATE TABLE person ( id INT NOT NULL AUTO_INCREMENT PRIMARY KEY, name VARCHAR(255) NOT NULL, address_info JSON ); 当我们向表中插入数据时，便可以这样处理那些模式不确定的数据了：INSERT INTO person (name, address_info) VALUES (&amp;#34;John&amp;#34;, &amp;#39;{&amp;#34;city&amp;#34;: &amp;#34;Beijing&amp;#34;}&amp;#39;); 就这么简单！直接在 JSON 字段对应的位置上，放一个合法的 JSON 字符串，就可以向表中插入 JSON 了。TiDB 会解析这个文本，然后以一种更加紧凑、易于访问的二进制形式来保存。当然，你也可以将其他类型的数据用 CAST 转换为 JSON：INSERT INTO person (name, address_info) VALUES (&amp;#34;John&amp;#34;, CAST(&amp;#39;{&amp;#34;city&amp;#34;: &amp;#34;Beijing&amp;#34;}&amp;#39; AS JSON)); INSERT INTO person (name, address_info) VALUES (&amp;#34;John&amp;#34;, CAST(&amp;#39;123&amp;#39; AS JSON)); INSERT INTO person (name, address_info) VALUES (&amp;#34;John&amp;#34;, CAST(123 AS JSON)); 现在，如果我们想查询表中所有居住在北京的用户，该怎么做呢？需要把数据全拉回来，然后在业务层进行过滤吗？不需要，和 MongoDB 等文档数据库相同，我们有在服务端支持用户各种复杂组合查询条件的能力。你可以这样写 SQL：SELECT id, name FROM person WHERE JSON_EXTRACT(address_info, &amp;#39;$.city&amp;#39;) = &amp;#39;Beijing&amp;#39;); TiDB 支持 JSON_EXTRACT 函数，该函数与 MySQL 5.7 中 JSON_EXTRACT 的用法完全相同。这个函数的意思就是，从 address_info 这个文档中取出名为 city 这个字段。它的第二个参数是一个“路径表达式”，我们由此可以指定到底要取出哪个字段。关于路径表达式的完整语法描述比较复杂，我们还是通过几个简单的例子来了解其用法：SET @person = &amp;#39;{&amp;#34;name&amp;#34;:&amp;#34;John&amp;#34;,&amp;#34;friends&amp;#34;:[{&amp;#34;name&amp;#34;:&amp;#34;Forest&amp;#34;,&amp;#34;age&amp;#34;:16},{&amp;#34;name&amp;#34;:&amp;#34;Zhang San&amp;#34;,&amp;#34;gender&amp;#34;:&amp;#34;male&amp;#34;}]}&amp;#39;; SELECT JSON_EXTRACT(@person, &amp;#39;$.name&amp;#39;); -- gets &amp;#34;John&amp;#34; SELECT JSON_EXTRACT(@person, &amp;#39;$.friends[0].age&amp;#39;); -- gets 16 SELECT JSON_EXTRACT(@person, &amp;#39;$.friends[1].gender&amp;#39;); -- gets &amp;#34;male&amp;#34; SELECT JSON_EXTRACT(@person, &amp;#39;$.friends[2].name&amp;#39;); -- gets NULL 除了插入、查询外，对 JSON 的修改也是支持的。总的来说，目前我们支持的 MySQL 5.7 的 JSON 函数如下表所示： JSON_EXTRACT JSON_ARRAY JSON_OBJECT JSON_SET JSON_REPLACE JSON_INSERT JSON_REMOVE JSON_TYPE JSON_UNQUOTE  直接从名字上，我们便能得出这些函数的大致用途，而且它们的语义也与 MySQL 5.7 完全一致，因此，想要查询它们具体的用法，我们可以直接查阅 MySQL 5.7 的相关文档。MySQL 5.7 的用户可以无缝迁移至 TiDB。熟悉 MySQL 5.7 的用户会发现，TiDB 尚未完全支持所有 MySQL 5.7 中的 JSON 函数。这是因为我们的一期目标是能够提供完备的 MySQL X Plugin 支持即可，而这已经涵盖大部分常用的 JSON 增删改查的功能了。如有需要，我们会继续完善对其他函数的支持。使用生成列对 JSON 建索引 在有了上述的知识铺垫后，您可能会发现我们在查询 JSON 中的一个字段时，走的是全表扫描。使用 TiDB 的 EXPLAIN 语句时，一个比 MySQL 完备得多的结果会告诉我们，的确是全表扫描。那么，我们能否对 JSON 字段进行索引呢？首先，这种索引是错误的：CREATE TABLE person ( id INT NOT NULL AUTO_INCREMENT PRIMARY KEY, name VARCHAR(255) NOT NULL, address_info JSON, KEY (address_info) ); 这并非是因为技术上无法支持，而是因为对 JSON 的直接比较，本身就是没有意义的 —— 尽管我们可以人为地约定一些比较规则，比如 ARRAY 比所有的 OBJECT 都大 —— 但是这并没有什么用处。因此，正如 MySQL 5.7 所做的那样，我们禁止了直接在 JSON 字段上创建索引，而是通过生成列的方式，支持了对 JSON 文档内的某一字段建立索引：CREATE TABLE person ( id INT NOT NULL AUTO_INCREMENT PRIMARY KEY, name VARCHAR(255) NOT NULL, address_info JSON, city VARCHAR(64) AS (JSON_EXTRACT(address_info, &amp;#39;$.city&amp;#39;)) VIRTUAL, KEY (city) ); 这个表中，city 列就是一个 生成列。顾名思义，该列由表中其他的列生成，而不能显式地在插入或更新时为它赋一个值。对于生成列，用户还可以指定其为 VIRTUAL 来避免它被显式地保存在记录中，而是在需要地时候再由其他列来生成，这对于列比较宽且需要节约存储空间地情况尤为有用。有了这个生成列，我们就可以在它上面建立索引了，在用户看来与常规的列便没什么两样，是不是很简单呢？而查询的时候，我们可以：SELECT name, id FROM person WHERE city = &amp;#39;Beijing&amp;#39;; 这样，便可以走索引了！另外，需要注意的是，如果 JSON 文档中指定路径下的字段不存在，那么 JSON_EXTRACT 的结果会是 NULL ，这时，带有索引的生成列的值也就为 NULL 了。因此，如果这是用户不希望看到的，那也可以在生成列上增加 NOT NULL 约束，这样，当插入新的纪录算出来的 city 字段为 NULL 时，便可以检查出来了。目前的一些限制 目前 JSON 及生成列仍然有一些限制： 不能 ALTER TABLE 增加 STORED 存储方式的生成列； 不能 ALTER TABLE 在生成列上增加索引；  这些功能，包括其他一些 JSON 函数的实现尚在开发过程中。"},
		{"url": "https://pingcap.com/docs-cn/sql/json-functions/",
		"title": "JSON 相关的函数和语法糖", 
		"content": " JSON 相关的函数和语法糖    函数或语法糖 功能描述     JSON_EXTRACT(json_doc, path[, path] &amp;hellip;) 从 JSON 文档中解出某一路径对应的子文档   JSON_UNQUOTE(json_val) 去掉 JSON 文档外面的引号   JSON_TYPE(json_val) 检查某 JSON 文档内部内容的类型   JSON_SET(json_doc, path, val[, path, val] &amp;hellip;) 在 JSON 文档中为某一路径设置子文档   JSON_INSERT(json_doc, path, val[, path, val] &amp;hellip;) 在 JSON 文档中在某一路径下插入子文档   JSON_REPLACE(json_doc, path, val[, path, val] &amp;hellip;) 替换 JSON 文档中的某一路径下的子文档   JSON_REMOVE(json_doc, path[, path] &amp;hellip;) 移除 JSON 文档中某一路径下的子文档   JSON_MERGE(json_doc, json_doc[, json_doc] &amp;hellip;) 将多个 JSON 文档合并成一个文档，其类型为数组   JSON_OBJECT(key, val[, key, val] &amp;hellip;) 根据一系列 K/V 对创建一个 JSON 文档   JSON_ARRAY([val[, val] &amp;hellip;]) 根据一系列元素创建一个 JSON 文档   -&amp;gt; JSON_EXTRACT(doc, path_literal) 的语法糖   -&amp;gt;&amp;gt; JSON_UNQUOTE(JSONJSON_EXTRACT(doc, path_literal)) 的语法糖    "},
		{"url": "https://pingcap.com/docs/op-guide/dashboard-overview-info/",
		"title": "Key Metrics", 
		"content": " Key Metrics If you use Ansible to deploy TiDB cluster, you can deploy the monitoring system at the same time. See Overview of the Monitoring Framework for more information.The Grafana dashboard is divided into four sub dashboards: node_export, PD, TiKV, and TiDB. There are a lot of metics there to help you diagnose. For routine operations, some of the key metrics are displayed on the Overview dashboard so that you can get the overview of the status of the components and the entire cluster. See the following section for their descriptions:Key metrics description    Service Panel Name Description Normal Range     PD Storage Capacity the total storage capacity of the TiDB cluster    PD Current Storage Size the occupied storage capacity of the TiDB cluster    PD Store Status &amp;ndash; up store the number of TiKV nodes that are up    PD Store Status &amp;ndash; down store the number of TiKV nodes that are down 0. If the number is bigger than 0, it means some node(s) are not down.   PD Store Status &amp;ndash; offline store the number of TiKV nodes that are manually offline    PD Store Status &amp;ndash; Tombstone store the number of TiKV nodes that are Tombstone    PD Current storage usage the storage occupancy rate of the TiKV cluster If it exceeds 80%, you need to consider adding more TiKV nodes.   PD 99% completed cmds duration seconds the 99th percentile duration to complete a pd-server request less than 5ms   PD average completed cmds duration seconds the average duration to complete a pd-server request less than 50ms   PD leader balance ratio the leader ratio difference of the nodes with the biggest leader ratio and the smallest leader ratio It is less than 5% for a balanced situation. It becomes bigger when a node is restarting.   PD region balance ratio the region ratio difference of the nodes with the biggest region ratio and the smallest region ratio It is less than 5% for a balanced situation. It becomes bigger when adding or removing a node.   TiDB handle requests duration seconds the response time to get TSO from PD less than 100ms   TiDB tidb server QPS the QPS of the cluster application specific   TiDB connection count the number of connections from application servers to the database Application specific. If the number of connections hops, you need to find out the reasons. If it drops to 0, you can check if the network is broken; if it surges, you need to check the application.   TiDB statement count the number of different types of statement within a given time application specific   TiDB Query Duration 99th percentile the 99th percentile query time    TiKV 99% &amp;amp; 99.99% scheduler command duration the 99th percentile and 99.99th percentile scheduler command duration For 99%, it is less than 50ms; for 99.99%, it is less than 100ms.   TiKV 95% &amp;amp; 99.99% storage async_request duration the 95th percentile and 99.99th percentile Raft command duration For 95%, it is less than 50ms; for 99.99%, it is less than 100ms.   TiKV server report failure message There might be an issue with the network or the message might not come from this cluster. If there are large amount of messages which contains unreachable, there might be an issue with the network. If the message contains store not match, the message does not come from this cluster.   TiKV Vote the frequency of the Raft vote Usually, the value only changes when there is a split. If the value of Vote remains high for a long time, the system might have a severe issue and some nodes are not working.   TiKV 95% and 99% coprocessor request duration the 95th percentile and the 99th percentile coprocessor request duration Application specific. Usually, the value does not remain high.   TiKV Pending task the number of pending tasks Except for PD worker, it is not normal if the value is too high.   TiKV stall RocksDB stall time If the value is bigger than 0, it means that RocksDB is too busy, and you need to pay attention to IO and CPU usage.   TiKV channel full The channel is full and the threads are too busy. If the value is bigger than 0, the threads are too busy.   TiKV 95% send message duration seconds the 95th percentile message sending time less than 50ms   TiKV leader/region the number of leader/region per TiKV server application specific    "},
		{"url": "https://pingcap.com/docs-cn/sql/keywords-and-reserved-words/",
		"title": "Keywords and Reserved Words", 
		"content": " Keywords and Reserved Words 关键字在 SQL 中有特殊的意义， 例如 SELECT， UPDATE， DELETE，在作为表名跟函数名的时候，需要特殊对待，例如作为表名，保留字需要被反引号包住：mysql&amp;gt; CREATE TABLE select (a INT); ERROR 1105 (HY000): line 0 column 19 near &amp;#34; (a INT)&amp;#34; (total length 27) mysql&amp;gt; CREATE TABLE `select` (a INT); Query OK, 0 rows affected (0.09 sec) BEGIN 和 END 是关键字， 但不是保留字，所以不需要反引号：mysql&amp;gt; CREATE TABLE `select` (BEGIN int, END int); Query OK, 0 rows affected (0.09 sec) 有一种特殊情况， 如果使用了限定符 .，那么也不需要用反引号：mysql&amp;gt; CREATE TABLE test.select (BEGIN int, END int); Query OK, 0 rows affected (0.08 sec) 下表列出了在 TiDB 中的关键字跟保留字，保留字用 &amp;reg; 来标识：   ACTION ADD &amp;reg; ADDDATE     ADMIN AFTER ALL &amp;reg;   ALTER &amp;reg; ALWAYS ANALYZE&amp;reg;   AND &amp;reg; ANY AS &amp;reg;   ASC &amp;reg; ASCII AUTO_INCREMENT   AVG AVG_ROW_LENGTH BEGIN   BETWEEN &amp;reg; BIGINT &amp;reg; BINARY &amp;reg;   BINLOG BIT BIT_XOR   BLOB &amp;reg; BOOL BOOLEAN   BOTH &amp;reg; BTREE BY &amp;reg;   BYTE CASCADE &amp;reg; CASE &amp;reg;   CAST CHANGE &amp;reg; CHAR &amp;reg;   CHARACTER &amp;reg; CHARSET CHECK &amp;reg;   CHECKSUM COALESCE COLLATE &amp;reg;   COLLATION COLUMN &amp;reg; COLUMNS   COMMENT COMMIT COMMITTED   COMPACT COMPRESSED COMPRESSION   CONNECTION CONSISTENT CONSTRAINT &amp;reg;   CONVERT &amp;reg; COUNT CREATE &amp;reg;   CROSS &amp;reg; CURRENT_DATE &amp;reg; CURRENT_TIME &amp;reg;   CURRENT_TIMESTAMP &amp;reg; CURRENT_USER &amp;reg; CURTIME   DATA DATABASE &amp;reg; DATABASES &amp;reg;   DATE DATE_ADD DATE_SUB   DATETIME DAY DAY_HOUR &amp;reg;   DAY_MICROSECOND &amp;reg; DAY_MINUTE &amp;reg; DAY_SECOND &amp;reg;   DDL DEALLOCATE DEC   DECIMAL &amp;reg; DEFAULT &amp;reg; DELAY_KEY_WRITE   DELAYED &amp;reg; DELETE &amp;reg; DESC &amp;reg;   DESCRIBE &amp;reg; DISABLE DISTINCT &amp;reg;   DISTINCTROW &amp;reg; DIV &amp;reg; DO   DOUBLE &amp;reg; DROP &amp;reg; DUAL &amp;reg;   DUPLICATE DYNAMIC ELSE &amp;reg;   ENABLE ENCLOSED END   ENGINE ENGINES ENUM   ESCAPE ESCAPED EVENTS   EXCLUSIVE EXECUTE EXISTS   EXPLAIN &amp;reg; EXTRACT FALSE &amp;reg;   FIELDS FIRST FIXED   FLOAT &amp;reg; FLUSH FOR &amp;reg;   FORCE &amp;reg; FOREIGN &amp;reg; FORMAT   FROM &amp;reg; FULL FULLTEXT &amp;reg;   FUNCTION GENERATED &amp;reg; GET_FORMAT   GLOBAL GRANT &amp;reg; GRANTS   GROUP &amp;reg; GROUP_CONCAT HASH   HAVING &amp;reg; HIGH_PRIORITY &amp;reg; HOUR   HOUR_MICROSECOND &amp;reg; HOUR_MINUTE &amp;reg; HOUR_SECOND &amp;reg;   IDENTIFIED IF &amp;reg; IGNORE &amp;reg;   IN &amp;reg; INDEX &amp;reg; INDEXES   INFILE &amp;reg; INNER &amp;reg; INSERT &amp;reg;   INT &amp;reg; INTEGER &amp;reg; INTERVAL &amp;reg;   INTO &amp;reg; IS &amp;reg; ISOLATION   JOBS JOIN &amp;reg; JSON   KEY &amp;reg; KEY_BLOCK_SIZE KEYS &amp;reg;   KILL &amp;reg; LEADING &amp;reg; LEFT &amp;reg;   LESS LEVEL LIKE &amp;reg;   LIMIT &amp;reg; LINES &amp;reg; LOAD &amp;reg;   LOCAL LOCALTIME &amp;reg; LOCALTIMESTAMP &amp;reg;   LOCK &amp;reg; LONGBLOB &amp;reg; LONGTEXT &amp;reg;   LOW_PRIORITY &amp;reg; MAX MAX_ROWS   MAXVALUE &amp;reg; MEDIUMBLOB &amp;reg; MEDIUMINT &amp;reg;   MEDIUMTEXT &amp;reg; MICROSECOND MIN   MIN_ROWS MINUTE MINUTE_MICROSECOND &amp;reg;   MINUTE_SECOND &amp;reg; MIN MIN_ROWS   MINUTE MINUTE_MICROSECOND MINUTE_SECOND   MOD &amp;reg; MODE MODIRY   MONTH NAMES NATIONAL   NATURAL &amp;reg; NO NO_WRITE_TO_BINLOG &amp;reg;   NONE NOT &amp;reg; NOW   NULL &amp;reg; NUMERIC &amp;reg; NVARCHAR &amp;reg;   OFFSET ON &amp;reg; ONLY   OPTION &amp;reg; OR &amp;reg; ORDER &amp;reg;   OUTER &amp;reg; PARTITION &amp;reg; PARTITIONS   PASSWORD PLUGINS POSITION   PRECISION &amp;reg; PREPARE PRIMARY &amp;reg;   PRIVILEGES PROCEDURE &amp;reg; PROCESS   PROCESSLIST QUARTER QUERY   QUICK RANGE &amp;reg; READ &amp;reg;   REAL &amp;reg; REDUNDANT REFERENCES &amp;reg;   REGEXP &amp;reg; RENAME &amp;reg; REPEAT &amp;reg;   REPEATABLE REPLACE &amp;reg; RESTRICT &amp;reg;   REVERSE REVOKE &amp;reg; RIGHT &amp;reg;   RLIKE &amp;reg; ROLLBACK ROW   ROW_COUNT ROW_FORMAT SCHEMA   SCHEMAS SECOND SECOND_MICROSECOND &amp;reg;   SELECT &amp;reg; SERIALIZABLE SESSION   SET &amp;reg; SHARE SHARED   SHOW &amp;reg; SIGNED SMALLINT &amp;reg;   SNAPSHOT SOME SQL_CACHE   SQL_CALC_FOUND_ROWS &amp;reg; SQL_NO_CACHE START   STARTING &amp;reg; STATS STATS_BUCKETS   STATS_HISTOGRAMS STATS_META STATS_PERSISTENT   STATUS STORED &amp;reg; SUBDATE   SUBSTR SUBSTRING SUM   SUPER TABLE &amp;reg; TABLES   TERMINATED &amp;reg; TEXT THAN   THEN &amp;reg; TIDB TIDB_INLJ   TIDB_SMJ TIME TIMESTAMP   TIMESTAMPADD TIMESTAMPDIFF TINYBLOB &amp;reg;   TINYINT &amp;reg; TINYTEXT &amp;reg; TO &amp;reg;   TRAILING &amp;reg; TRANSACTION TRIGGER &amp;reg;   TRIGGERS TRIM TRUE &amp;reg;   TRUNCATE UNCOMMITTED UNION &amp;reg;   UNIQUE &amp;reg; UNKNOWN UNLOCK &amp;reg;   UNSIGNED &amp;reg; UPDATE &amp;reg; USE &amp;reg;   USER USING &amp;reg; UTC_DATE &amp;reg;   UTC_TIME &amp;reg; UTC_TIMESTAMP &amp;reg; VALUE   VALUES &amp;reg; VARBINARY &amp;reg; VARCHAR &amp;reg;   VARIABLES VIEW VIRTUAL &amp;reg;   WARNINGS WEEK WHEN &amp;reg;   WHERE &amp;reg; WITH &amp;reg; WRITE &amp;reg;   XOR &amp;reg; YEAR YEAR_MONTH &amp;reg;   ZEROFILL &amp;reg;      "},
		{"url": "https://pingcap.com/docs/sql/keywords-and-reserved-words/",
		"title": "Keywords and Reserved Words", 
		"content": " Keywords and Reserved Words Keywords are words that have significance in SQL. Certain keywords, such as SELECT, UPDATE, or DELETE, are reserved and require special treatment for use as identifiers such as table and column names. For example, as table names, the reserved words must be quoted with backquotes:mysql&amp;gt; CREATE TABLE select (a INT); ERROR 1105 (HY000): line 0 column 19 near &amp;#34; (a INT)&amp;#34; (total length 27) mysql&amp;gt; CREATE TABLE `select` (a INT); Query OK, 0 rows affected (0.09 sec) The BEGIN and END are keywords but not reserved words, so you do not need to quote them with backquotes:mysql&amp;gt; CREATE TABLE `select` (BEGIN int, END int); Query OK, 0 rows affected (0.09 sec) Exception: A word that follows a period . qualifier does not need to be quoted with backquotes either:mysql&amp;gt; CREATE TABLE test.select (BEGIN int, END int); Query OK, 0 rows affected (0.08 sec) The following table lists the keywords and reserved words in TiDB. The reserved words are labelled with &amp;reg;.   ACTION ADD &amp;reg; ADDDATE     ADMIN AFTER ALL &amp;reg;   ALTER &amp;reg; ALWAYS ANALYZE&amp;reg;   AND &amp;reg; ANY AS &amp;reg;   ASC &amp;reg; ASCII AUTO_INCREMENT   AVG AVG_ROW_LENGTH BEGIN   BETWEEN &amp;reg; BIGINT &amp;reg; BINARY &amp;reg;   BINLOG BIT BIT_XOR   BLOB &amp;reg; BOOL BOOLEAN   BOTH &amp;reg; BTREE BY &amp;reg;   BYTE CASCADE &amp;reg; CASE &amp;reg;   CAST CHANGE &amp;reg; CHAR &amp;reg;   CHARACTER &amp;reg; CHARSET CHECK &amp;reg;   CHECKSUM COALESCE COLLATE &amp;reg;   COLLATION COLUMN &amp;reg; COLUMNS   COMMENT COMMIT COMMITTED   COMPACT COMPRESSED COMPRESSION   CONNECTION CONSISTENT CONSTRAINT &amp;reg;   CONVERT &amp;reg; COUNT CREATE &amp;reg;   CROSS &amp;reg; CURRENT_DATE &amp;reg; CURRENT_TIME &amp;reg;   CURRENT_TIMESTAMP &amp;reg; CURRENT_USER &amp;reg; CURTIME   DATA DATABASE &amp;reg; DATABASES &amp;reg;   DATE DATE_ADD DATE_SUB   DATETIME DAY DAY_HOUR &amp;reg;   DAY_MICROSECOND &amp;reg; DAY_MINUTE &amp;reg; DAY_SECOND &amp;reg;   DDL DEALLOCATE DEC   DECIMAL &amp;reg; DEFAULT &amp;reg; DELAY_KEY_WRITE   DELAYED &amp;reg; DELETE &amp;reg; DESC &amp;reg;   DESCRIBE &amp;reg; DISABLE DISTINCT &amp;reg;   DISTINCTROW &amp;reg; DIV &amp;reg; DO   DOUBLE &amp;reg; DROP &amp;reg; DUAL &amp;reg;   DUPLICATE DYNAMIC ELSE &amp;reg;   ENABLE ENCLOSED END   ENGINE ENGINES ENUM   ESCAPE ESCAPED EVENTS   EXCLUSIVE EXECUTE EXISTS   EXPLAIN &amp;reg; EXTRACT FALSE &amp;reg;   FIELDS FIRST FIXED   FLOAT &amp;reg; FLUSH FOR &amp;reg;   FORCE &amp;reg; FOREIGN &amp;reg; FORMAT   FROM &amp;reg; FULL FULLTEXT &amp;reg;   FUNCTION GENERATED &amp;reg; GET_FORMAT   GLOBAL GRANT &amp;reg; GRANTS   GROUP &amp;reg; GROUP_CONCAT HASH   HAVING &amp;reg; HIGH_PRIORITY &amp;reg; HOUR   HOUR_MICROSECOND &amp;reg; HOUR_MINUTE &amp;reg; HOUR_SECOND &amp;reg;   IDENTIFIED IF &amp;reg; IGNORE &amp;reg;   IN &amp;reg; INDEX &amp;reg; INDEXES   INFILE &amp;reg; INNER &amp;reg; INSERT &amp;reg;   INT &amp;reg; INTEGER &amp;reg; INTERVAL &amp;reg;   INTO &amp;reg; IS &amp;reg; ISOLATION   JOBS JOIN &amp;reg; JSON   KEY &amp;reg; KEY_BLOCK_SIZE KEYS &amp;reg;   KILL &amp;reg; LEADING &amp;reg; LEFT &amp;reg;   LESS LEVEL LIKE &amp;reg;   LIMIT &amp;reg; LINES &amp;reg; LOAD &amp;reg;   LOCAL LOCALTIME &amp;reg; LOCALTIMESTAMP &amp;reg;   LOCK &amp;reg; LONGBLOB &amp;reg; LONGTEXT &amp;reg;   LOW_PRIORITY &amp;reg; MAX MAX_ROWS   MAXVALUE &amp;reg; MEDIUMBLOB &amp;reg; MEDIUMINT &amp;reg;   MEDIUMTEXT &amp;reg; MICROSECOND MIN   MIN_ROWS MINUTE MINUTE_MICROSECOND &amp;reg;   MINUTE_SECOND &amp;reg; MIN MIN_ROWS   MINUTE MINUTE_MICROSECOND MINUTE_SECOND   MOD &amp;reg; MODE MODIRY   MONTH NAMES NATIONAL   NATURAL &amp;reg; NO NO_WRITE_TO_BINLOG &amp;reg;   NONE NOT &amp;reg; NOW   NULL &amp;reg; NUMERIC &amp;reg; NVARCHAR &amp;reg;   OFFSET ON &amp;reg; ONLY   OPTION &amp;reg; OR &amp;reg; ORDER &amp;reg;   OUTER &amp;reg; PARTITION &amp;reg; PARTITIONS   PASSWORD PLUGINS POSITION   PRECISION &amp;reg; PREPARE PRIMARY &amp;reg;   PRIVILEGES PROCEDURE &amp;reg; PROCESS   PROCESSLIST QUARTER QUERY   QUICK RANGE &amp;reg; READ &amp;reg;   REAL &amp;reg; REDUNDANT REFERENCES &amp;reg;   REGEXP &amp;reg; RENAME &amp;reg; REPEAT &amp;reg;   REPEATABLE REPLACE &amp;reg; RESTRICT &amp;reg;   REVERSE REVOKE &amp;reg; RIGHT &amp;reg;   RLIKE &amp;reg; ROLLBACK ROW   ROW_COUNT ROW_FORMAT SCHEMA   SCHEMAS SECOND SECOND_MICROSECOND &amp;reg;   SELECT &amp;reg; SERIALIZABLE SESSION   SET &amp;reg; SHARE SHARED   SHOW &amp;reg; SIGNED SMALLINT &amp;reg;   SNAPSHOT SOME SQL_CACHE   SQL_CALC_FOUND_ROWS &amp;reg; SQL_NO_CACHE START   STARTING &amp;reg; STATS STATS_BUCKETS   STATS_HISTOGRAMS STATS_META STATS_PERSISTENT   STATUS STORED &amp;reg; SUBDATE   SUBSTR SUBSTRING SUM   SUPER TABLE &amp;reg; TABLES   TERMINATED &amp;reg; TEXT THAN   THEN &amp;reg; TIDB TIDB_INLJ   TIDB_SMJ TIME TIMESTAMP   TIMESTAMPADD TIMESTAMPDIFF TINYBLOB &amp;reg;   TINYINT &amp;reg; TINYTEXT &amp;reg; TO &amp;reg;   TRAILING &amp;reg; TRANSACTION TRIGGER &amp;reg;   TRIGGERS TRIM TRUE &amp;reg;   TRUNCATE UNCOMMITTED UNION &amp;reg;   UNIQUE &amp;reg; UNKNOWN UNLOCK &amp;reg;   UNSIGNED &amp;reg; UPDATE &amp;reg; USE &amp;reg;   USER USING &amp;reg; UTC_DATE &amp;reg;   UTC_TIME &amp;reg; UTC_TIMESTAMP &amp;reg; VALUE   VALUES &amp;reg; VARBINARY &amp;reg; VARCHAR &amp;reg;   VARIABLES VIEW VIRTUAL &amp;reg;   WARNINGS WEEK WHEN &amp;reg;   WHERE &amp;reg; WITH &amp;reg; WRITE &amp;reg;   XOR &amp;reg; YEAR YEAR_MONTH &amp;reg;   ZEROFILL &amp;reg;      "},
		{"url": "https://pingcap.com/docs/sql/literal-values/",
		"title": "Literal Values", 
		"content": " Literal Values String literals A string is a sequence of bytes or characters, enclosed within either single quote &#39; or double quote &amp;quot; characters. For example:&amp;#39;example string&amp;#39; &amp;#34;example string&amp;#34; Quoted strings placed next to each other are concatenated to a single string. The following lines are equivalent:&amp;#39;a string&amp;#39; &amp;#39;a&amp;#39; &amp;#39; &amp;#39; &amp;#39;string&amp;#39; &amp;#34;a&amp;#34; &amp;#39; &amp;#39; &amp;#34;string&amp;#34; If the ANSI_QUOTES SQL MODE is enabled, string literals can be quoted only within single quotation marks because a string quoted within double quotation marks is interpreted as an identifier.A binary string is a string of bytes. Each binary string has a character set and collation named binary. A non-binary string is a string of characters. It has a character set other than binary and a collation that is compatible with the character set.For both types of strings, comparisons are based on the numeric values of the string unit. For binary strings, the unit is the byte. For non-binary strings, the unit is the character and some character sets support multibyte characters.A string literal may have an optional character set introducer and COLLATE clause, to designate it as a string that uses a specific character set and collation. TiDB only supports this in syntax, but does not process it.[_charset_name]&amp;#39;string&amp;#39; [COLLATE collation_name] For example:SELECT _latin1&amp;#39;string&amp;#39;; SELECT _binary&amp;#39;string&amp;#39;; SELECT _utf8&amp;#39;string&amp;#39; COLLATE utf8_bin; You can use N&amp;rsquo;literal&amp;rsquo; (or n&amp;rsquo;literal&amp;rsquo;) to create a string in the national character set. The following statements are equivalent:SELECT N&amp;#39;some text&amp;#39;; SELECT n&amp;#39;some text&amp;#39;; SELECT _utf8&amp;#39;some text&amp;#39;; Escape characters: 0: An ASCII NUL (X&amp;rsquo;00&amp;rsquo;) character &#39;: A single quote (&amp;lsquo;) character &amp;quot;: A double quote (&amp;ldquo;)character b: A backspace character n: A newline (linefeed) character r: A carriage return character t: A tab character z: ASCII 26 (Ctrl + Z) : A backslash  character %: A % character _: A _ character  You can use the following ways to include quote characters within a string: A &#39; inside a string quoted with &#39; may be written as &#39;&#39;. A &amp;quot; inside a string quoted with &amp;quot; may be written as &amp;quot;&amp;quot;. Precede the quote character by an escape character . A &#39; inside a string quoted with &amp;quot; needs no special treatment, and a &amp;quot; inside a string quoted with &#39; needs no special treatment either.  For more information, see String Literals in MySQL.Numeric literals Numeric literals include integer and DECIMAL literals and floating-point literals.Integer may include . as a decimal separator. Numbers may be preceded by - or + to indicate a negative or positive value respectively.Exact-value numeric literals can be represented as 1, .2, 3.4, -5, -6.78, +9.10.Numeric literals can also be represented in scientific notation, such as 1.2E3, 1.2E-3, -1.2E3, -1.2E-3.For more information, see Numeric Literals in MySQL.NULL values The NULL value means “no data”. NULL can be written in any letter case. A synonym is N (case sensitive).Be aware that the NULL value is different from values such as 0 for numeric types or the empty string &#39;&#39; for string types.Hexadecimal literals Hexadecimal literal values are written using X&#39;val&#39; or 0xval notation, where val contains hexadecimal digits. A leading 0x is case sensitive and cannot be written as 0X.Legal hexadecimal literals:X&amp;#39;ac12&amp;#39; X&amp;#39;12AC&amp;#39; x&amp;#39;ac12&amp;#39; x&amp;#39;12AC&amp;#39; 0xac12 0x12AC Illegal hexadecimal literals:X&amp;#39;1z&amp;#39; (z is not a hexadecimal legal digit) 0X12AC (0X must be written as 0x) Hexadecimal literals written using X&#39;val&#39; notation must contain an even number of digits. To avoid the syntax error, pad the value with a leading zero:mysql&amp;gt; select X&amp;#39;aff&amp;#39;; ERROR 1105 (HY000): line 0 column 13 near &amp;#34;&amp;#34;hex literal: invalid hexadecimal format, must even numbers, but 3 (total length 13) mysql&amp;gt; select X&amp;#39;0aff&amp;#39;; +---------+ | X&amp;#39;0aff&amp;#39; | +---------+ | | +---------+ 1 row in set (0.00 sec) By default, a hexadecimal literal is a binary string.To convert a string or a number to a string in hexadecimal format, use the HEX() function:mysql&amp;gt; SELECT HEX(&amp;#39;TiDB&amp;#39;); +-------------+ | HEX(&amp;#39;TiDB&amp;#39;) | +-------------+ | 54694442 | +-------------+ 1 row in set (0.01 sec) mysql&amp;gt; SELECT X&amp;#39;54694442&amp;#39;; +-------------+ | X&amp;#39;54694442&amp;#39; | +-------------+ | TiDB | +-------------+ 1 row in set (0.00 sec) Date and time literals Date and time values can be represented in several formats, such as quoted strings or as numbers. When TiDB expects a date, it interprets any of &#39;2015-07-21&#39;, &#39;20150721&#39; and 20150721 as a date.TiDB supports the following formats for date values: As a string in either &#39;YYYY-MM-DD&#39; or &#39;YY-MM-DD&#39; format. The - delimiter is &amp;ldquo;relaxed&amp;rdquo; in syntax. Any punctuation character may be used as the delimiter between date parts. For example, &#39;2017-08-24&#39;, &#39;2017&amp;amp;08&amp;amp;24&#39; and &#39;2012@12^31&#39; are equivalent. The only delimiter recognized is the . character, which is treated as a decimal point to separate the integer and fractional parts. The date and time parts can be separated by T other than a space. For example, 2017-8-24 10:42:00 and 2017-8-24T10:42:00 are equivalent. As a string with no delimiters in either &#39;YYYYMMDDHHMMSS&#39; or &#39;YYMMDDHHMMSS&#39; format. For example, &#39;20170824104520&#39; and &#39;170824104520&#39; are interpreted as &#39;2017-08-24 10:45:20&#39;. But &#39;170824304520&#39; is illegal because the hour part exceeds the legal range. As a number in either YYYYMMDDHHMMSS or YYMMDDHHMMSS format, without single quotation marks or double quotation marks. For example, 20170824104520 is interpreted as &#39;2017-08-24 10:45:20&#39;.  A DATETIME or TIMESTAMP value can include a trailing fractional seconds part in up to microseconds (6 digits) precision. The fractional part should always be separated from the rest of the time by a decimal point.Dates containing two-digit year values are ambiguous. It is recommended to use the four-digit format. TiDB interprets two-digit year values using the following rules: Year values in the range of 70-99 are converted to 1970-1999. Year values in the range of 00-69 are converted to 2000-2069.  For values specified as strings that include date part delimiters, it is unnecessary to specify two digits for month or day values that are less than 10. &#39;2017-8-4&#39; is the same as &#39;2017-08-04&#39;. Similarly, for values specified as strings that include time part delimiters, it is unnecessary to specify two digits for hour, minute, or second values that are less than 10. &#39;2017-08-24 1:2:3&#39; is the same as &#39;2017-08-24 01:02:03&#39;.In TiDB, the date or time values specified as numbers are interpreted according their length: 6 digits: YYMMDD 12 digits: YYMMDDHHMMSS 8 digits: YYYYMMDD 14 digits: YYYYMMDDHHMMSS  TiDB supports the following formats for time values: As a string in &#39;D HH:MM:SS&#39; format. You can also use one of the following “relaxed” syntaxes: &#39;HH:MM:SS&#39;, &#39;HH:MM&#39;, &#39;D HH:MM&#39;, &#39;D HH&#39;, or &#39;SS&#39;. Here D represents days and the legal value range is 0-34. As a number in &#39;HHMMSS&#39; format. For example, 231010 is interpreted as &#39;23:10:10&#39;. A number in any of the SS, MMSS or HHMMSS format can be treated as time.  The time value can also include a trailing fractional part in up to 6 digits precision. The . character represents the decimal point.For more information, see Date and Time Literals in MySQL.Boolean literals The constants TRUE and FALSE evaluate to 1 and 0 respectively, which are not case sensitive.mysql&amp;gt; SELECT TRUE, true, tRuE, FALSE, FaLsE, false; +------+------+------+-------+-------+-------+ | TRUE | true | tRuE | FALSE | FaLsE | false | +------+------+------+-------+-------+-------+ | 1 | 1 | 1 | 0 | 0 | 0 | +------+------+------+-------+-------+-------+ 1 row in set (0.00 sec) Bit-value literals Bit-value literals are written using b&#39;val&#39; or 0bval notation. The val is a binary value …"},
		{"url": "https://pingcap.com/docs/tools/loader/",
		"title": "Loader Instructions", 
		"content": " Loader Instructions What is Loader? Loader is a data import tool to load data to TiDB.Download the Binary.Why did we develop Loader? Since tools like mysqldump will take us days to migrate massive amounts of data, we used the mydumper/myloader suite of Percona to multi-thread export and import data. During the process, we found that mydumper works well. However, as myloader lacks functions of error retry and savepoint, it is inconvenient for us to use. Therefore, we developed loader, which reads the output data files of mydumper and imports data to TiDB through mysql protocol.What can Loader do?  Multi-thread import data Support table level concurrent import and scattered hot spot write Support concurrent import of a single large table and scattered hot spot write Support mydumper data format Support error retry Support savepoint Improve the speed of importing data through system variable  Usage  Note: - Do not import the mysql system database from the MySQL instance to the downstream TiDB instance. - If mydumper uses the -m parameter, the data is exported without the table structure and the loader can not import the data. - If you use the default checkpoint-schema parameter, after importing the data of a database, run drop database tidb_loader before you begin to import the next database. - It is recommended to specify the checkpoint-schema = &amp;quot;tidb_loader&amp;quot; parameter when importing data. Parameter description -L string: the log level setting, which can be set as debug, info, warn, error, fatal (default: &amp;#34;info&amp;#34;) -P int: the port of TiDB (default: 4000) -V boolean: prints version and exit -c string: config file -checkpoint-schema string: the database name of checkpoint. In the execution process, loader will constantly update this database. After recovering from an interruption, loader will get the process of the last run through this database. (default: &amp;#34;tidb_loader&amp;#34;) -d string: the storage directory of data that need to import (default: &amp;#34;./&amp;#34;) -h string: the host of TiDB (default: &amp;#34;127.0.0.1&amp;#34;) -p string: the account and password of TiDB -pprof-addr string: the pprof address of Loader. It tunes the perfomance of Loader (default: &amp;#34;:10084&amp;#34;) -t int: the number of thread,increase this as TiKV nodes increase (default: 16) -u string: the user name of TiDB (default: &amp;#34;root&amp;#34;) Configuration file Apart from command line parameters, you can also use configuration files. The format is shown as below:# Loader log level, which can be set as &amp;#34;debug&amp;#34;, &amp;#34;info&amp;#34;, &amp;#34;warn&amp;#34;, &amp;#34;error&amp;#34; and &amp;#34;fatal&amp;#34; (default: &amp;#34;info&amp;#34;) log-level = &amp;#34;info&amp;#34; # Loader log file log-file = &amp;#34;loader.log&amp;#34; # Directory of the dump to import (default: &amp;#34;./&amp;#34;) dir = &amp;#34;./&amp;#34; # Loader pprof address, used to tune the performance of Loader (default: &amp;#34;127.0.0.1:10084&amp;#34;) pprof-addr = &amp;#34;127.0.0.1:10084&amp;#34; # The checkpoint data is saved to TiDB, and the schema name is defined here. checkpoint-schema = &amp;#34;tidb_loader&amp;#34; # Number of threads restoring concurrently for worker pool (default: 16). Each worker restore one file at a time. pool-size = 16 # The target database information [db] host = &amp;#34;127.0.0.1&amp;#34; user = &amp;#34;root&amp;#34; password = &amp;#34;&amp;#34; port = 4000 # The sharding synchronising rules support wildcharacter. # 1. The asterisk character (*, also called &amp;#34;star&amp;#34;) matches zero or more characters, # for example, &amp;#34;doc*&amp;#34; matches &amp;#34;doc&amp;#34; and &amp;#34;document&amp;#34; but not &amp;#34;dodo&amp;#34;; # asterisk character must be in the end of the wildcard word, # and there is only one asterisk in one wildcard word. # 2. The question mark &amp;#39;?&amp;#39; matches exactly one character. # [[route-rules]] # pattern-schema = &amp;#34;shard_db_*&amp;#34; # pattern-table = &amp;#34;shard_table_*&amp;#34; # target-schema = &amp;#34;shard_db&amp;#34; # target-table = &amp;#34;shard_table&amp;#34; Usage example Command line parameter:./bin/loader -d ./test -h 127.0.0.1 -u root -P 4000 Or use configuration file &amp;ldquo;config.toml&amp;rdquo;:./bin/loader -c=config.toml FAQ The scenario of synchronising data from sharded tables Loader supports importing data from sharded tables into one table within one database according to the route-rules. Before synchronising, check the following items: Whether the sharding rules can be represented using the route-rules syntax. Whether the sharded tables contain monotone increasing primary keys, or whether there are conflicts in the unique indexes or the primary keys after the combination.  To combine tables, start the route-rules parameter in the configuration file of Loader: To use the table combination function, it is required to fill the pattern-schema and target-schema. If the pattern-table and target-table are NULL, the table name is not combined or converted.  [[route-rules]] pattern-schema = &amp;#34;example_db&amp;#34; pattern-table = &amp;#34;table_*&amp;#34; target-schema = &amp;#34;example_db&amp;#34; target-table = &amp;#34;table&amp;#34;"},
		{"url": "https://pingcap.com/docs-cn/tools/loader/",
		"title": "Loader 使用文档", 
		"content": " Loader 使用文档 Loader 是什么 是由 PingCAP 开发的数据导入工具，可以用于向 TiDB 中导入数据。Binary 下载为什么我们要做这个东西 当数据量比较大的时候，如果用 mysqldump 这样的工具迁移数据会比较慢。我们尝试了 Percona 的 mydumper/myloader 套件，能够多线程导出和导入数据。在使用过程中，mydumper 问题不大，但是 myloader 由于缺乏出错重试、断点续传这样的功能，使用起来很不方便。所以我们开发了 loader，能够读取 mydumper 的输出数据文件，通过 mysql protocol 向 TiDB/MySQL 中导入数据。Loader 有哪些优点  多线程导入 支持表级别的并发导入，分散写入热点 支持对单个大表并发导入，分散写入热点 支持 mydumper 数据格式 出错重试 断点续导 通过 system variable 优化 TiDB 导入数据速度  使用方法 注意事项 请勿使用 loader 导入 MySQL 实例中 mysql 系统数据库到下游 TiDB。如果 mydumper 使用 -m 参数，会导出不带表结构的数据，这时 loader 无法导入数据。如果使用默认的 checkpoint-schema 参数，在导完一个 database 数据库后，请 drop database tidb_loader 后再开始导入下一个 database。推荐数据库开始导入的时候，明确指定 checkpoint-schema = &amp;quot;tidb_loader&amp;quot; 参数。参数说明 -L string log 级别设置，可以设置为 debug, info, warn, error, fatal (默认为 &amp;#34;info&amp;#34;) -P int TiDB/MySQL 的端口 (默认为 4000) -V 打印 loader 版本 -c string 指定配置文件启动 loader -checkpoint-schema string checkpoint 数据库名，loader 在运行过程中会不断的更新这个数据库，在中断并恢复后，会通过这个库获取上次运行的进度 (默认为 &amp;#34;tidb_loader&amp;#34;) -d string 需要导入的数据存放路径 (default &amp;#34;./&amp;#34;) -h string TiDB 服务 host IP (default &amp;#34;127.0.0.1&amp;#34;) -p string TiDB 账户密码 -pprof-addr string Loader 的 pprof 地址，用于对 Loader 进行性能调试 (默认为 &amp;#34;:10084&amp;#34;) -t int 线程数 (默认为 16). 每个线程同一时刻只能操作一个数据文件。 -u string TiDB 的用户名 (默认为 &amp;#34;root&amp;#34;) 配置文件 除了使用命令行参数外，还可以使用配置文件来配置，配置文件的格式如下：# 日志输出等级；可以设置为 debug, info, warn, error, fatal (默认为 &amp;#34;info&amp;#34;) log-level = &amp;#34;info&amp;#34; # 指定 loader 日志目录 log-file = &amp;#34;loader.log&amp;#34; # 需要导入的数据存放路径 (default &amp;#34;./&amp;#34;) dir = &amp;#34;./&amp;#34; # Loader 的 pprof 地址，用于对 Loader 进行性能调试 (默认为 &amp;#34;:10084&amp;#34;) pprof-addr = &amp;#34;127.0.0.1:10084&amp;#34; # checkpoint 数据库名，loader 在运行过程中会不断的更新这个数据库，在中断并恢复后， # 会通过这个库获取上次运行的进度 (默认为 &amp;#34;tidb_loader&amp;#34;) checkpoint-schema = &amp;#34;tidb_loader&amp;#34; # 线程数 (默认为 16). 每个线程同一时刻只能操作一个数据文件。 pool-size = 16 # 目标数据库信息 [db] host = &amp;#34;127.0.0.1&amp;#34; user = &amp;#34;root&amp;#34; password = &amp;#34;&amp;#34; port = 4000 # sharding 同步规则，采用 wildcharacter # 1. 星号字符 (*) 可以匹配零个或者多个字符, # 例子, doc* 匹配 doc 和 document, 但是和 dodo 不匹配; # 星号只能放在 pattern 结尾，并且一个 pattern 中只能有一个 # 2. 问号字符 (?) 匹配任一一个字符 # [[route-rules]] # pattern-schema = &amp;#34;shard_db_*&amp;#34; # pattern-table = &amp;#34;shard_table_*&amp;#34; # target-schema = &amp;#34;shard_db&amp;#34; # target-table = &amp;#34;shard_table&amp;#34; 使用示例 通过命令行参数：./bin/loader -d ./test -h 127.0.0.1 -u root -P 4000 或者使用配置文件 &amp;ldquo;config.toml&amp;rdquo;:./bin/loader -c=config.toml FAQ 合库合表场景案例说明 根据配置文件的 route-rules 可以支持将分库分表的数据导入到同一个库同一个表中，但是在开始前需要检查分库分表规则 + 是否可以利用 route-rules 的语义规则表示 + 分表中是否包含唯一递增主键，或者合并后数据上有冲突的唯一索引或者主键loader 需要配置文件中开启 route-rules 参数以提供合库合表功能 + 如果使用该功能，pattern-schema 与 target-schema 必须填写 + 如果 pattern-table 与 target-table 为空，将不进行表名称合并或转换[[route-rules]] pattern-schema = &amp;#34;example_db&amp;#34; pattern-table = &amp;#34;table_*&amp;#34; target-schema = &amp;#34;example_db&amp;#34; target-table = &amp;#34;table&amp;#34;"},
		{"url": "https://pingcap.com/docs/op-guide/migration/",
		"title": "Migrate Data from MySQL to TiDB", 
		"content": " Migrate Data from MySQL to TiDB Use the mydumper / loader tool to export and import all the data You can use mydumper to export data from MySQL and loader to import the data into TiDB. Note: Although TiDB also supports the official mysqldump tool from MySQL for data migration, it is not recommended to use it. Its performance is much lower than mydumper / loader and it takes much time to migrate large amounts of data. mydumper/loader is more powerful. For more information, see https://github.com/maxbube/mydumper. Export data from MySQL Use the mydumper tool to export data from MySQL by using the following command:./bin/mydumper -h 127.0.0.1 -P 3306 -u root -t 16 -F 64 -B test -T t1,t2 --skip-tz-utc -o ./var/test In this command, -B test: means the data is exported from the test database. -T t1,t2: means only the t1 and t2 tables are exported. -t 16: means 16 threads are used to export the data. -F 64: means a table is partitioned into chunks and one chunk is 64MB. --skip-tz-utc: the purpose of adding this parameter is to ignore the inconsistency of time zone setting between MySQL and the data exporting machine and to disable automatic conversion.   Note: On the Cloud platforms which require the super privilege, such as on the Aliyun platform, add the --no-locks parameter to the command. If not, you might get the error message that you don&amp;rsquo;t have the privilege. Import data to TiDB Use loader to import the data from MySQL to TiDB. See Loader instructions for more information../bin/loader -h 127.0.0.1 -u root -P 4000 -t 32 -d ./var/test After the data is imported, you can view the data in TiDB using the MySQL client:mysql -h127.0.0.1 -P4000 -uroot mysql&amp;gt; show tables; +----------------+ | Tables_in_test | +----------------+ | t1 | | t2 | +----------------+  mysql&amp;gt; select * from t1; +----+------+ | id | age | +----+------+ | 1 | 1 | | 2 | 2 | | 3 | 3 | +----+------+  mysql&amp;gt; select * from t2; +----+------+ | id | name | +----+------+ | 1 | a | | 2 | b | | 3 | c | +----+------+ Best practice To migrate data quickly, especially for huge amount of data, you can refer to the following recommendations. Keep the exported data file as small as possible and it is recommended keep it within 64M. You can use the -F parameter to set the value. You can adjust the -t parameter of loader based on the number and the load of TiKV instances. For example, if there are three TiKV instances, -t can be set to 3 * (1 ~ n). If the load of TiKV is too high and the log backoffer.maxSleep 15000ms is exceeded is displayed many times, decrease the value of -t; otherwise, increase it.  A sample and the configuration  The total size of the exported files is 214G. A single table has 8 columns and 2 billion rows. The cluster topology:  12 TiKV instances: 4 nodes, 3 TiKV instances per node 4 TiDB instances 3 PD instances  The configuration of each node:  CPU: Intel Xeon E5-2670 v3 @ 2.30GHz 48 vCPU [2 x 12 physical cores] Memory: 128G Disk: sda [raid 10, 300G] sdb[RAID 5, 2T] Operating System: CentOS 7.3  The -F parameter of mydumper is set to 16 and the -t parameter of loader is set to 64.  Results: It takes 11 hours to import all the data, which is 19.4G/hour.Use the syncer tool to import data incrementally (optional) The previous section introduces how to import all the history data from MySQL to TiDB using mydumper/loader. But this is not applicable if the data in MySQL is updated after the migration and it is expected to import the updated data quickly.Therefore, TiDB provides the syncer tool for an incremental data import from MySQL to TiDB.See Download the TiDB enterprise toolset to download the syncer tool.Download the TiDB enterprise toolset (Linux) # Download the enterprise tool package. wget http://download.pingcap.org/tidb-enterprise-tools-latest-linux-amd64.tar.gz wget http://download.pingcap.org/tidb-enterprise-tools-latest-linux-amd64.sha256 # Check the file integrity. If the result is OK, the file is correct. sha256sum -c tidb-enterprise-tools-latest-linux-amd64.sha256 # Extract the package. tar -xzf tidb-enterprise-tools-latest-linux-amd64.tar.gz cd tidb-enterprise-tools-latest-linux-amd64 Assuming the data from t1 and t2 is already imported to TiDB using mydumper/loader. Now we hope that any updates to these two tables are synchronised to TiDB in real time.Obtain the position to synchronise The data exported from MySQL contains a metadata file which includes the position information. Take the following metadata information as an example:Started dump at: 2017-04-28 10:48:10 SHOW MASTER STATUS: Log: mysql-bin.000003 Pos: 930143241 GTID: Finished dump at: 2017-04-28 10:48:11 The position information (Pos: 930143241) needs to be stored in the syncer.meta file for syncer to synchronize:# cat syncer.meta binlog-name = &amp;#34;mysql-bin.000003&amp;#34; binlog-pos = 930143241  Note: The syncer.meta file only needs to be configured once when it is first used. The position will be automatically updated when binlog is synchronised. Start syncer The config.toml file for syncer:log-level = &amp;#34;info&amp;#34; server-id = 101 # The file path for meta: meta = &amp;#34;./syncer.meta&amp;#34; worker-count = 16 batch = 10 # The testing address for pprof. It can also be used by Prometheus to pull the syncer metrics. status-addr = &amp;#34;:10081&amp;#34; skip-sqls = [&amp;#34;ALTER USER&amp;#34;, &amp;#34;CREATE USER&amp;#34;] # Support whitelist filter. You can specify the database and table to be synchronised. For example: # Synchronise all the tables of db1 and db2: replicate-do-db = [&amp;#34;db1&amp;#34;,&amp;#34;db2&amp;#34;] # Synchronise db1.table1. [[replicate-do-table]] db-name =&amp;#34;db1&amp;#34; tbl-name = &amp;#34;table1&amp;#34; # Synchronise db3.table2. [[replicate-do-table]] db-name =&amp;#34;db3&amp;#34; tbl-name = &amp;#34;table2&amp;#34; # Support regular expressions. Start with &amp;#39;~&amp;#39; to use regular expressions. # To synchronise all the databases that start with `test`: replicate-do-db = [&amp;#34;~^test.*&amp;#34;] # The sharding synchronising rules support wildcharacter. # 1. The asterisk character (*, also called &amp;#34;star&amp;#34;) matches zero or more characters, # for example, &amp;#34;doc*&amp;#34; matches &amp;#34;doc&amp;#34; and &amp;#34;document&amp;#34; but not &amp;#34;dodo&amp;#34;; # asterisk character must be in the end of the wildcard word, # and there is only one asterisk in one wildcard word. # 2. The question mark ? matches exactly one character. #[[route-rules]] #pattern-schema = &amp;#34;route_*&amp;#34; #pattern-table = &amp;#34;abc_*&amp;#34; #target-schema = &amp;#34;route&amp;#34; #target-table = &amp;#34;abc&amp;#34; #[[route-rules]] #pattern-schema = &amp;#34;route_*&amp;#34; #pattern-table = &amp;#34;xyz_*&amp;#34; #target-schema = &amp;#34;route&amp;#34; #target-table = &amp;#34;xyz&amp;#34; [from] host = &amp;#34;127.0.0.1&amp;#34; user = &amp;#34;root&amp;#34; password = &amp;#34;&amp;#34; port = 3306 [to] host = &amp;#34;127.0.0.1&amp;#34; user = &amp;#34;root&amp;#34; password = &amp;#34;&amp;#34; port = 4000 Start syncer:./bin/syncer -config config.toml 2016/10/27 15:22:01 binlogsyncer.go:226: [info] begin to sync binlog from position (mysql-bin.000003, 1280) 2016/10/27 15:22:01 binlogsyncer.go:130: [info] register slave for master server 127.0.0.1:3306 2016/10/27 15:22:01 binlogsyncer.go:552: [info] rotate to (mysql-bin.000003, 1280) 2016/10/27 15:22:01 syncer.go:549: [info] rotate binlog to (mysql-bin.000003, 1280) Insert data into MySQL INSERT INTO t1 VALUES (4, 4), (5, 5); Log in TiDB and view the data mysql -h127.0.0.1 -P4000 -uroot -p mysql&amp;gt; select * from t1; +----+------+ | id | age | +----+------+ | 1 | 1 | | 2 | 2 | | 3 | 3 | | 4 | 4 | | 5 | 5 | +----+------+ syncer outputs the current synchronised data statistics every 30 seconds:2017/06/08 01:18:51 syncer.go:934: [info] [syncer]total events = 15, total tps = 130, recent tps = 4, master-binlog = (ON.000001, 11992), master-binlog-gtid=53ea0ed1-9bf8-11e6-8bea-64006a897c73:1-74, syncer-binlog = (ON.000001, 2504), syncer-binlog-gtid = 53ea0ed1-9bf8-11e6-8bea-64006a897c73:1-17 2017/06/08 01:19:21 syncer.go:934: [info] [syncer]total events = 15, total tps = 191, recent tps = 2, master-binlog = …"},
		{"url": "https://pingcap.com/docs/op-guide/migration-overview/",
		"title": "Migration Overview", 
		"content": " Migration Overview Overview This document describes how to migrate data from MySQL to TiDB in detail.See the following for the assumed MySQL and TiDB server information:   Name Address Port User Password     MySQL 127.0.0.1 3306 root *   TiDB 127.0.0.1 4000 root *    Scenarios  To import all the history data. This needs the following tools: Checker: to check if the shema is compatible with TiDB. Mydumper: to export data from MySQL. Loader: to import data to TiDB.  To incrementally synchronise data after all the history data is imported. This needs the following tools: Checker: to check if the shema is compatible with TiDB. Mydumper: to export data from MySQL. Loader: to import data to TiDB. Syncer: to incrementally synchronize data from MySQL to TiDB. Note: To incrementally synchronize data from MySQL to TiDB, the binary logging (binlog) must be enabled and must use the row format in MySQL.   Enable binary logging (binlog) in MySQL Before using the syncer tool, make sure: + Binlog is enabled in MySQL. See Setting the Replication Master Configuration. Binlog must use the row format which is the recommended binlog format in MySQL 5.7. It can be configured using the following statement:SET GLOBAL binlog_format = ROW;  Use the checker tool to check the schema Before migrating, you can use the checker tool in TiDB to check if TiDB supports the table schema of the data to be migrated. If the checker fails to check a certain table schema, it means that the table is not currently supported by TiDB and therefore the data in the table cannot be migrated.See Download the TiDB toolset to download the checker tool.Download the TiDB toolset (Linux) # Download the tool package. wget http://download.pingcap.org/tidb-enterprise-tools-latest-linux-amd64.tar.gz wget http://download.pingcap.org/tidb-enterprise-tools-latest-linux-amd64.sha256 # Check the file integrity. If the result is OK, the file is correct. sha256sum -c tidb-enterprise-tools-latest-linux-amd64.sha256 # Extract the package. tar -xzf tidb-enterprise-tools-latest-linux-amd64.tar.gz cd tidb-enterprise-tools-latest-linux-amd64 A sample to use the checker tool  Create several tables in the test database in MySQL and insert data.USE test; CREATE TABLE t1 (id INT, age INT, PRIMARY KEY(id)) ENGINE=InnoDB; CREATE TABLE t2 (id INT, name VARCHAR(256), PRIMARY KEY(id)) ENGINE=InnoDB; INSERT INTO t1 VALUES (1, 1), (2, 2), (3, 3); INSERT INTO t2 VALUES (1, &amp;#34;a&amp;#34;), (2, &amp;#34;b&amp;#34;), (3, &amp;#34;c&amp;#34;); Use the checker tool to check all the tables in the test database../bin/checker -host 127.0.0.1 -port 3306 -user root test 2016/10/27 13:11:49 checker.go:48: [info] Checking database test 2016/10/27 13:11:49 main.go:37: [info] Database DSN: root:@tcp(127.0.0.1:3306)/test?charset=utf8 2016/10/27 13:11:49 checker.go:63: [info] Checking table t1 2016/10/27 13:11:49 checker.go:69: [info] Check table t1 succ 2016/10/27 13:11:49 checker.go:63: [info] Checking table t2 2016/10/27 13:11:49 checker.go:69: [info] Check table t2 succ Use the checker tool to check one of the tables in the test database.Note: Assuming you need to migrate the t1 table only in this sample../bin/checker -host 127.0.0.1 -port 3306 -user root test t1 2016/10/27 13:13:56 checker.go:48: [info] Checking database test 2016/10/27 13:13:56 main.go:37: [info] Database DSN: root:@tcp(127.0.0.1:3306)/test?charset=utf8 2016/10/27 13:13:56 checker.go:63: [info] Checking table t1 2016/10/27 13:13:56 checker.go:69: [info] Check table t1 succ Check database succ!  A sample of a table that cannot be migrated  Create the following t_error table in MySQL:CREATE TABLE t_error ( a INT NOT NULL, PRIMARY KEY (a)) ENGINE=InnoDB TABLESPACE ts1 PARTITION BY RANGE (a) PARTITIONS 3 ( PARTITION P1 VALUES LESS THAN (2), PARTITION P2 VALUES LESS THAN (4) TABLESPACE ts2, PARTITION P3 VALUES LESS THAN (6) TABLESPACE ts3); Use the checker tool to check the table. If the following error is displayed, the t_error table cannot be migrated../bin/checker -host 127.0.0.1 -port 3306 -user root test t_error 2017/08/04 11:14:35 checker.go:48: [info] Checking database test 2017/08/04 11:14:35 main.go:39: [info] Database DSN: root:@tcp(127.0.0.1:3306)/test?charset=utf8 2017/08/04 11:14:35 checker.go:63: [info] Checking table t1 2017/08/04 11:14:35 checker.go:67: [error] Check table t1 failed with err: line 3 column 29 near &amp;#34; ENGINE=InnoDB DEFAULT CHARSET=latin1 /*!50100 PARTITION BY RANGE (a) (PARTITION P1 VALUES LESS THAN (2) ENGINE = InnoDB, PARTITION P2 VALUES LESS THAN (4) TABLESPACE = ts2 ENGINE = InnoDB, PARTITION P3 VALUES LESS THAN (6) TABLESPACE = ts3 ENGINE = InnoDB) */&amp;#34; (total length 354) github.com/pingcap/tidb/parser/yy_parser.go:96: github.com/pingcap/tidb/parser/yy_parser.go:109: /home/jenkins/workspace/build_tidb_tools_master/go/src/github.com/pingcap/tidb-tools/checker/checker.go:122: parse CREATE TABLE `t1` ( `a` int(11) NOT NULL, PRIMARY KEY (`a`) ) /*!50100 TABLESPACE ts1 */ ENGINE=InnoDB DEFAULT CHARSET=latin1 /*!50100 PARTITION BY RANGE (a) (PARTITION P1 VALUES LESS THAN (2) ENGINE = InnoDB, PARTITION P2 VALUES LESS THAN (4) TABLESPACE = ts2 ENGINE = InnoDB, PARTITION P3 VALUES LESS THAN (6) TABLESPACE = ts3 ENGINE = InnoDB) */ error /home/jenkins/workspace/build_tidb_tools_master/go/src/github.com/pingcap/tidb-tools/checker/checker.go:114: 2017/08/04 11:14:35 main.go:83: [error] Check database test with 1 errors and 0 warnings.  "},
		{"url": "https://pingcap.com/docs/sql/miscellaneous-functions/",
		"title": "Miscellaneous Functions", 
		"content": " Miscellaneous Functions    Name Description     ANY_VALUE() Suppress ONLY_FULL_GROUP_BY value rejection   SLEEP() Sleep for a number of seconds   UUID() Return a Universal Unique Identifier (UUID)   VALUES() Defines the values to be used during an INSERT   INET_ATON() Return the numeric value of an IP address   INET_NTOA() Return the IP address from a numeric value   INET6_ATON() Return the numeric value of an IPv6 address   INET6_NTOA() Return the IPv6 address from a numeric value   IS_IPV4() Whether argument is an IPv4 address   IS_IPV4_COMPAT() Whether argument is an IPv4-compatible address   IS_IPV4_MAPPED() Whether argument is an IPv4-mapped address   IS_IPV6() Whether argument is an IPv6 address   GET_LOCK() Get a named lock   RELEASE_LOCK() Releases the named lock    "},
		{"url": "https://pingcap.com/docs/op-guide/monitor/",
		"title": "Monitor a TiDB Cluster", 
		"content": " Monitor a TiDB Cluster Currently there are two types of interfaces to monitor the state of the TiDB cluster: Using the HTTP interface to get the internal information of a component, which is called the component state interface. Using Prometheus to record the detailed information of the various operations in the components, which is called the Metrics interface.  The component state interface You can use this type of interface to monitor the basic information of the component. This interface can act as the interface to monitor Keepalive. In addition, the interface of the Placement Driver (PD) can get the details of the entire TiKV cluster.TiDB server The HTTP interface of TiDB is: http://host:port/statusThe default port number is: 10080 which can be set using the --status flag.The interface can be used to get the current TiDB server state and to determine whether the server is alive. The result is returned in the following JSON format:curl http://127.0.0.1:10080/status { connections: 0, version: &amp;#34;5.5.31-TiDB-1.0&amp;#34;, git_hash: &amp;#34;b99521846ff6f71f06e2d49a3f98fa1c1d93d91b&amp;#34; } In this example, connection: the current number of clients connected to the TiDB server version: the TiDB version number git_hash: the Git Hash of the current TiDB code  PD server The API address of PD is: http://${host}:${port}/pd/api/v1/${api_name}The default port number is: 2379.See PD API doc for detailed information about various API names.The interface can be used to get the state of all the TiKV servers and the information about load balancing. It is the most important and frequently-used interface to get the state information of all the TiKV nodes. See the following example for the the information about a single-node TiKV cluster:curl http://127.0.0.1:2379/pd/api/v1/stores { &amp;#34;count&amp;#34;: 1 // the number of the TiKV node &amp;#34;stores&amp;#34;: [ // the list of the TiKV node // the detailed information about the single TiKV node { &amp;#34;store&amp;#34;: { &amp;#34;id&amp;#34;: 1, &amp;#34;address&amp;#34;: &amp;#34;127.0.0.1:22161&amp;#34;, &amp;#34;state&amp;#34;: 0 }, &amp;#34;status&amp;#34;: { &amp;#34;store_id&amp;#34;: 1, // the ID of the node &amp;#34;capacity&amp;#34;: 1968874332160, // the total capacity &amp;#34;available&amp;#34;: 1264847716352, // the available capacity &amp;#34;region_count&amp;#34;: 1, // the count of Regions in this node &amp;#34;sending_snap_count&amp;#34;: 0, &amp;#34;receiving_snap_count&amp;#34;: 0, &amp;#34;start_ts&amp;#34;: &amp;#34;2016-10-24T19:54:00.110728339+08:00&amp;#34;, // the starting timestamp &amp;#34;last_heartbeat_ts&amp;#34;: &amp;#34;2016-10-25T10:52:54.973669928+08:00&amp;#34;, // the timestamp of the last heartbeat &amp;#34;total_region_count&amp;#34;: 1, // the count of the total Regions &amp;#34;leader_region_count&amp;#34;: 1, // the count of the Leader Regions &amp;#34;uptime&amp;#34;: &amp;#34;14h58m54.862941589s&amp;#34; }, &amp;#34;scores&amp;#34;: [ 100, 35 ] } ] } The metrics interface You can use this type of interface to monitor the state and performance of the entire cluster. The metrics data is displayed in Prometheus and Grafana. See Use Prometheus and Grafana for how to set up the monitoring system.You can get the following metrics for each component:TiDB server  query processing time to monitor the latency and throughput the DDL process monitoring TiKV client related monitoring PD client related monitoring  PD server  the total number of times that the command executes the total number of times that a certain command fails the duration that a command succeeds the duration that a command fails the duration that a command finishes and returns result  TiKV server  Garbage Collection (GC) monitoring the total number of times that the TiKV command executes the duration that Scheduler executes commands the total number of times of the Raft propose command the duration that Raft executes commands the total number of times that Raft commands fail the total number of times that Raft processes the ready state  Use Prometheus and Grafana The deployment architecture See the following diagram for the deployment architecture: Note: You must add the Prometheus Pushgateway addresses to the startup parameters of the TiDB, PD and TiKV components. Set up the monitoring system See the following links for your reference: Prometheus Push Gateway: https://github.com/prometheus/pushgateway Prometheus Server: https://github.com/prometheus/prometheus#install Grafana: http://docs.grafana.org  Configuration Configure TiDB, PD and TiKV  TiDB: Set the two parameters: --metrics-addr and --metrics-interval. Set the Push Gateway address as the --metrics-addr parameter. Set the push frequency as the --metrics-interval parameter. The unit is s, and the default value is 15.  PD: update the toml configuration file with the Push Gateway address and the the push frequency:[metric] # prometheus client push interval, set &amp;#34;0s&amp;#34; to disable prometheus. interval = &amp;#34;15s&amp;#34; # prometheus pushgateway address, leaves it empty will disable prometheus. address = &amp;#34;host:port&amp;#34; TiKV: update the toml configuration file with the Push Gateway address and the the push frequency. Set the job field as &amp;ldquo;tikv&amp;rdquo;.[metric] # the Prometheus client push interval. Setting the value to 0s stops Prometheus client from pushing. interval = &amp;#34;15s&amp;#34; # the Prometheus pushgateway address. Leaving it empty stops Prometheus client from pushing. address = &amp;#34;host:port&amp;#34; # the Prometheus client push job name. Note: A node id will automatically append, e.g., &amp;#34;tikv_1&amp;#34;. job = &amp;#34;tikv&amp;#34;  Configure PushServer Generally, it does not need to be configured. You can use the default port: 9091.Configure Prometheus Add the Push Gateway address to the yaml configuration file:scrape_configs: # The job name is added as a label `job=&amp;lt;job_name&amp;gt;` to any timeseries scraped from this config. - job_name: &amp;#39;TiDB&amp;#39; # Override the global default and scrape targets from this job every 5 seconds. scrape_interval: 5s honor_labels: true static_configs: - targets: [&amp;#39;host:port&amp;#39;] # use the Push Gateway address labels: group: &amp;#39;production&amp;#39; Configure Grafana Create a Prometheus data source  Login the Grafana Web interface. The default address is: http://localhost:3000 The default account name: admin The password for the default account: admin  Click the Grafana logo to open the sidebar menu. Click &amp;ldquo;Data Sources&amp;rdquo; in the sidebar. Click &amp;ldquo;Add data source&amp;rdquo;. Specify the data source information: Specify the name for the data source. For Type, select Prometheus. For Url, specify the Prometheus address. Specify other fields as needed.  Click &amp;ldquo;Add&amp;rdquo; to save the new data source.  Create a Grafana dashboard  Click the Grafana logo to open the sidebar menu. On the sidebar menu, click &amp;ldquo;Dashboards&amp;rdquo; -&amp;gt; &amp;ldquo;Import&amp;rdquo; to open the &amp;ldquo;Import Dashboard&amp;rdquo; window. Click &amp;ldquo;Upload .json File&amp;rdquo; to upload a JSON file ( Download TiDB Grafana Config ). Click &amp;ldquo;Save &amp;amp; Open&amp;rdquo;. A Prometheus dashboard is created.  "},
		{"url": "https://pingcap.com/docs-cn/sql/literal-value-null-values/",
		"title": "NULL Values", 
		"content": " NULL Values NULL 代表数据为空，它是大小写不敏感的，与 N(大小写敏感) 同义。需要注意的是 NULL 跟 0 并不一样，跟空字符串 &#39;&#39; 也不一样。"},
		{"url": "https://pingcap.com/docs/sql/numeric-functions-and-operators/",
		"title": "Numeric Functions and Operators", 
		"content": " Numeric Functions and Operators Arithmetic operators    Name Description     + Addition operator   - Minus operator   * Multiplication operator   / Division operator   DIV Integer division   %, MOD Modulo operator   - Change the sign of the argument    Mathematical functions    Name Description     POW() Return the argument raised to the specified power   POWER() Return the argument raised to the specified power   EXP() Raise to the power of   SQRT() Return the square root of the argument   LN() Return the natural logarithm of the argument   LOG() Return the natural logarithm of the first argument   LOG2() Return the base-2 logarithm of the argument   LOG10() Return the base-10 logarithm of the argument   PI() Return the value of pi   TAN() Return the tangent of the argument   COT() Return the cotangent   SIN() Return the sine of the argument   COS() Return the cosine   ATAN() Return the arc tangent   ATAN2(), ATAN() Return the arc tangent of the two arguments   ASIN() Return the arc sine   ACOS() Return the arc cosine   RADIANS() Return argument converted to radians   DEGREES() Convert radians to degrees   MOD() Return the remainder   ABS() Return the absolute value   CEIL() Return the smallest integer value not less than the argument   CEILING() Return the smallest integer value not less than the argument   FLOOR() Return the largest integer value not greater than the argument   ROUND() Round the argument   RAND() Return a random floating-point value   SIGN() Return the sign of the argument   CONV() Convert numbers between different number bases   TRUNCATE() Truncate to specified number of decimal places   CRC32() Compute a cyclic redundancy check value    "},
		{"url": "https://pingcap.com/recruit-cn/engineer/ops-engineer/",
		"title": "OPS Engineer", 
		"content": " OPS Engineer 岗位职责  维护 TiDB 集群在用户生产系统中平稳运行，包括产品部署、配置管理、系统监控和线上诊断，及时应对和处理相关模块的线上问题 负责公司内部软硬件资源管理和内部系统配置管理 研究分布式系统前沿技术，改进系统的服务和运维架构，提升系统可靠性和可运维性 探索、研究新的运维自动化技术和方向  职位要求  两年以上互联网公司运维经验，至少熟练掌握 Python/shell/php 等1种脚本语言 有 web server、分布式系统、负载均衡，系统监控等运维经验，熟悉 nginx，lvs，keepalived，zabbix，mysql，redis 等常用开源系统的搭建，配置，优化 有自动化运维经验，熟悉 puppet/ansible/saltstack 对容器有一定的了解，有 docker 使用经验 高度的责任心、良好的沟通技巧和团队合作精神  待遇 15K - 25K , 13薪 + 奖金, 优秀者可面议工作地点 北京，上海，广州，杭州"},
		{"url": "https://pingcap.com/docs/op-guide/offline-ansible-deployment/",
		"title": "Offline Deployment Using Ansible", 
		"content": " Offline Deployment Using Ansible Prepare Before you start, make sure that you have: A download machine The machine must have access to the Internet in order to download TiDB-Ansible, TiDB and related packages. For Linux operating system, it is recommended to install CentOS 7.3 or later.  Several target machines and one Control Machine For system requirements and configuration, see Prepare the environment. It is acceptable without access to the Internet.   Install Ansible and dependencies in the Control Machine  Install Ansible offline on the CentOS 7 system: Download the Ansible offline installation package to the Control Machine. # tar -xzvf ansible-2.4-rpms.el7.tar.gz  # cd ansible-2.4-rpms.el7  # rpm -ivh PyYAML*rpm libyaml*rpm python-babel*rpm python-backports*rpm python-backports-ssl_match_hostname*rpm python-cffi*rpm python-enum34*rpm python-httplib2*rpm python-idna*rpm python-ipaddress*rpm python-jinja2*rpm python-markupsafe*rpm python-paramiko*rpm python-passlib*rpm python-ply*rpm python-pycparser*rpm python-setuptools*rpm python-six*rpm python2-cryptography*rpm python2-jmespath*rpm python2-pyasn1*rpm sshpass*rpm  # rpm -ivh ansible-2.4.2.0-2.el7.noarch.rpm After Ansible is installed, you can view the version using ansible --version.# ansible --version  ansible 2.4.2.0  Download TiDB-Ansible and TiDB packages on the download machine  Install Ansible on the download machine.Use the following method to install Ansible online on the download machine installed with the CentOS 7 system. Installing using the EPEL source automatically installs the related Ansible dependencies (such as Jinja2==2.7.2 MarkupSafe==0.11). After Ansible is installed, you can view the version using ansible --version.# yum install epel-release # yum install ansible curl # ansible --version  ansible 2.4.2.0  Note: Make sure that the version of Ansible is 2.4 or later, otherwise compatibility problem might occur. Download TiDB-Ansible.Use the following command to download the corresponding version of TiDB-Ansible from the GitHub TiDB-Ansible project. The default folder name is tidb-ansible.Download the 1.0 (GA) version:git clone -b release-1.0 https://github.com/pingcap/tidb-ansible.git ORDownload the master version:git clone https://github.com/pingcap/tidb-ansible.git  Note: For production environment, download TiDB-Ansible 1.0 to deploy TiDB. Run the local_prepare.yml playbook, and download TiDB binary online to the download machine.cd tidb-ansible ansible-playbook local_prepare.yml After running the above command, copy the tidb-ansible folder to the /home/tidb directory of the Control Machine. The ownership authority of the file must be the tidb user.  Orchestrate the TiDB cluster See Orchestrate the TiDB cluster.Deploy the TiDB cluster  See Deploy the TiDB cluster. You do not need to run the ansible-playbook local_prepare.yml playbook again.  Test the cluster See Test the cluster."},
		{"url": "https://pingcap.com/docs/op-guide/monitor-overview/",
		"title": "Overview of the TiDB Monitoring Framework", 
		"content": " Overview of the Monitoring Framework The TiDB monitoring framework adopts two open source projects: Prometheus and Grafana. TiDB uses Prometheus to store the monitoring and performance metrics and Grafana to visualize these metrics.About Prometheus in TiDB As a time series database, Prometheus has a multi-dimensional data model and flexible query language. As one of the most popular open source projects, many companies and organizations have adopted Prometheus, and the project has a very active community. PingCAP is one of the active developers and adoptors of Prometheus for monitoring and alerting in TiDB, TiKV and PD.Prometheus consists of multiple components. Currently, TiDB uses the following of them: The Prometheus Server to scrape and store time series data. The client libraries to customize necessary metrics in the application. A push GateWay to receive the data from Client Push for the Prometheus main server. An AlertManager for the alerting mechanism.  The diagram is as follows:About Grafana in TiDB Grafana is an open source project for analysing and visualizing metrics. TiDB uses Grafana to display the performance metrics as follows:"},
		{"url": "https://pingcap.com/docs/tools/pd-control/",
		"title": "PD Control User Guide", 
		"content": " PD Control User Guide As a command line tool of PD, PD Control obtains the state information of the cluster and tunes the cluster.Source code compiling  Go Version 1.7 or later In the PD root directory, use the make command to compile and generate bin/pd-ctl   Note: Generally, you don&amp;rsquo;t need to compile source code as the PD Control tool already exists in the released Binary or Docker. However, dev users can refer to the above instruction for compiling source code. Usage Single-command mode:./pd-ctl store -d -u http://127.0.0.1:2379 Interactive mode:./pd-ctl -u http://127.0.0.1:2379 Use environment variables:export PD_ADDR=http://127.0.0.1:2379 ./pd-ctl Use TLS to encrypt:./pd-ctl -u https://127.0.0.1:2379 --cacert=&amp;#34;path/to/ca&amp;#34; --cert=&amp;#34;path/to/cert&amp;#34; --key=&amp;#34;path/to/key&amp;#34; Command line flags --pd,-u  PD address Default address: http://127.0.0.1:2379 Enviroment variable: PD_ADDR  --detach,-d  Use single command line mode (not entering readline) Default: false  &amp;ndash;cacert  Specify the path to the certificate file of the trusted CA in PEM format Default: &amp;ldquo;&amp;rdquo;  &amp;ndash;cert  Specify the path to the certificate of SSL in PEM format Default: &amp;ldquo;&amp;rdquo;  &amp;ndash;key  Specify the path to the certificate key file of SSL in PEM format, which is the private key of the certificate specified by --cert Default: &amp;ldquo;&amp;rdquo;  &amp;ndash;version,-V  Print the version information and exit Default: false  Command cluster Use this command to view the basic information of the cluster.Usage:&amp;gt;&amp;gt; cluster // To show the cluster information { &amp;#34;id&amp;#34;: 6493707687106161130, &amp;#34;max_peer_count&amp;#34;: 3 } config [show | set &amp;lt;option&amp;gt; &amp;lt;value&amp;gt;] Use this command to view or modify the configuration information.Usage:&amp;gt;&amp;gt; config show //　Display the config information of the scheduler { &amp;#34;max-snapshot-count&amp;#34;: 3, &amp;#34;max-pending-peer-count&amp;#34;: 16, &amp;#34;max-store-down-time&amp;#34;: &amp;#34;1h0m0s&amp;#34;, &amp;#34;leader-schedule-limit&amp;#34;: 64, &amp;#34;region-schedule-limit&amp;#34;: 16, &amp;#34;replica-schedule-limit&amp;#34;: 24, &amp;#34;tolerant-size-ratio&amp;#34;: 2.5, &amp;#34;schedulers-v2&amp;#34;: [ { &amp;#34;type&amp;#34;: &amp;#34;balance-region&amp;#34;, &amp;#34;args&amp;#34;: null }, { &amp;#34;type&amp;#34;: &amp;#34;balance-leader&amp;#34;, &amp;#34;args&amp;#34;: null }, { &amp;#34;type&amp;#34;: &amp;#34;hot-region&amp;#34;, &amp;#34;args&amp;#34;: null } ] } &amp;gt;&amp;gt; config show all // Display all config information &amp;gt;&amp;gt; config show namespace ts1 // Display the config information of the namespace named ts1 { &amp;#34;leader-schedule-limit&amp;#34;: 64, &amp;#34;region-schedule-limit&amp;#34;: 16, &amp;#34;replica-schedule-limit&amp;#34;: 24, &amp;#34;max-replicas&amp;#34;: 3, } &amp;gt;&amp;gt; config show replication // Display the config information of replication { &amp;#34;max-replicas&amp;#34;: 3, &amp;#34;location-labels&amp;#34;: &amp;#34;&amp;#34; }  leader-schedule-limit controls the number of tasks scheduling the leader at the same time. This value affects the speed of leader balance. A larger value means a higher speed and setting the value to 0 closes the scheduling. Usually the leader scheduling has a small load, and you can increase the value in need.&amp;gt;&amp;gt; config set leader-schedule-limit 4 // 4 tasks of leader scheduling at the same time at most region-schedule-limit controls the number of tasks scheduling the region at the same time. This value affects the speed of region balance. A larger value means a higher speed and setting the value to 0 closes the scheduling. Usually the region scheduling has a large load, so do not set a too large value.&amp;gt;&amp;gt; config set region-schedule-limit 2 // 2 tasks of region scheduling at the same time at most replica-schedule-limit controls the number of tasks scheduling the replica at the same time. This value affects the scheduling speed when the node is down or removed. A larger value means a higher speed and setting the value to 0 closes the scheduling. Usually the replica scheduling has a large load, so do not set a too large value.&amp;gt;&amp;gt; config set replica-schedule-limit 4 // 4 tasks of replica scheduling at the same time at most  The configuration above is global. You can also tune the configuration by configuring different namespaces. The global configuration is used if the corresponding configuration of the namespace is not set. Note: The configuration of the namespace only supports editing leader-schedule-limit, region-schedule-limit, replica-schedule-limit and max-replicas. ```bash &amp;gt;&amp;gt; config set namespace ts1 leader-schedule-limit 4 // 4 tasks of leader scheduling at the same time at most for the namespace named ts1 &amp;gt;&amp;gt; config set namespace ts2 region-schedule-limit 2 // 2 tasks of region scheduling at the same time at most for the namespace named ts2 ``` config delete namespace &amp;lt;name&amp;gt; [&amp;lt;option&amp;gt;] Use this command to delete the configuration of namespace.Usage:After you configure the namespace, if you want it to continue to use global configuration, delete the configuration information of the namespace using the following command:&amp;gt;&amp;gt; config delete namespace ts1 // Delete the configuration of the namespace named ts1 If you want to use global configuration only for a certain configuration of the namespace, use the following command:&amp;gt;&amp;gt; config delete namespace region-schedule-limit ts2 // Delete the region-schedule-limit configuration of the namespace named ts2 health Use this command to view the health information of the cluster.Usage:&amp;gt;&amp;gt; health // Display the health information {&amp;#34;health&amp;#34;: &amp;#34;true&amp;#34;} hot [read | write | store] Use this command to view the hot spot information of the cluster.Usage:&amp;gt;&amp;gt; hot read // Display hot spot for the read operation &amp;gt;&amp;gt; hot write // Display hot spot for the write operation &amp;gt;&amp;gt; hot store // Display hot spot for all the read and write operations label [store] Use this command to view the label information of the cluster.Usage:&amp;gt;&amp;gt; label // Display all labels &amp;gt;&amp;gt; label store zone cn // Display all stores including the &amp;#34;zone&amp;#34;:&amp;#34;cn&amp;#34; label member [leader | delete] Use this command to view the PD members or remove a specified member.Usage:&amp;gt;&amp;gt; member // Display the information of all members { &amp;#34;members&amp;#34;: [......] } &amp;gt;&amp;gt; member leader show // Display the information of the leader { &amp;#34;name&amp;#34;: &amp;#34;pd&amp;#34;, &amp;#34;addr&amp;#34;: &amp;#34;http://192.168.199.229:2379&amp;#34;, &amp;#34;id&amp;#34;: 9724873857558226554 } &amp;gt;&amp;gt; member delete name pd2 // Delete &amp;#34;pd2&amp;#34; Success! &amp;gt;&amp;gt; member delete id 1319539429105371180 // Delete a node using id Success! operator [show | add | remove] Use this command to view and control the scheduling operation.Usage:&amp;gt;&amp;gt; operator show // Display all operators &amp;gt;&amp;gt; operator show admin // Display all admin operators &amp;gt;&amp;gt; operator show leader // Display all leader operators &amp;gt;&amp;gt; operator show region // Display all region operators &amp;gt;&amp;gt; operator add add-peer 1 2 // Add a replica of region 1 on store 2 &amp;gt;&amp;gt; operator remove remove-peer 1 2 // Remove a replica of region 1 on store 2 &amp;gt;&amp;gt; operator add transfer-leader 1 2 // Schedule the leader of region 1 to store 2 &amp;gt;&amp;gt; operator add transfer-region 1 2 3 4 // Schedule region 1 to store 2,3,4 &amp;gt;&amp;gt; operator add transfer-peer 1 2 3 // Schedule the replica of region 1 on store 2 to store 3 &amp;gt;&amp;gt; operator remove 1 // Remove the scheduling operation of region 1 ping Use this command to view the time that ping PD takes.Usage:&amp;gt;&amp;gt; ping time: 43.12698ms region &amp;lt;region_id&amp;gt; Use this command to view the region information.Usage:&amp;gt;&amp;gt; region //　Display the information of all regions { &amp;#34;count&amp;#34;: 1, &amp;#34;regions&amp;#34;: [......] } &amp;gt;&amp;gt; region 2 // Display the information of the region with the id of 2 { &amp;#34;region&amp;#34;: { &amp;#34;id&amp;#34;: 2, ...... } &amp;#34;leader&amp;#34;: { ...... } } region key [--format=raw|pb|proto|protobuf] &amp;lt;key&amp;gt; Use this command to query the region that a specific key resides in. It supports the raw and protobuf formats.Raw format …"},
		{"url": "https://pingcap.com/docs-cn/tools/pd-control/",
		"title": "PD Control 使用说明", 
		"content": " PD Control 使用说明 PD Control 是 PD 的命令行工具，用于获取集群状态信息和调整集群。源码编译  Go Version 1.7 以上 在 PD 项目根目录使用 make 命令进行编译，生成 bin/pd-ctl  简单例子 单命令模式：./pd-ctl store -d -u http://127.0.0.1:2379 交互模式：./pd-ctl -u http://127.0.0.1:2379 使用环境变量：export PD_ADDR=http://127.0.0.1:2379 ./pd-ctl 使用TLS加密：./pd-ctl -u https://127.0.0.1:2379 --cacert=&amp;#34;path/to/ca&amp;#34; --cert=&amp;#34;path/to/cert&amp;#34; --key=&amp;#34;path/to/key&amp;#34; 命令行参数(flags) --pd,-u  指定 PD 的地址 默认地址: http://127.0.0.1:2379 环境变量: PD_ADDR  --detach,-d  使用单命令行模式(不进入 readline ) 默认值: false  &amp;ndash;cacert  指定 PEM 格式的受信任 CA 的证书文件路径 默认值: &amp;ldquo;&amp;rdquo;  &amp;ndash;cert  指定 PEM 格式的 SSL 证书文件路径 默认值: &amp;ldquo;&amp;rdquo;  &amp;ndash;key  指定 PEM 格式的 SSL 证书密钥文件路径，即 --cert 所指定的证书的私钥 默认值: &amp;ldquo;&amp;rdquo;  &amp;ndash;version,-V  打印版本信息并退出 默认值: false  命令(command) cluster 用于显示集群基本信息。示例：&amp;gt;&amp;gt; cluster // 显示 cluster 的信息 { &amp;#34;id&amp;#34;: 6493707687106161130, &amp;#34;max_peer_count&amp;#34;: 3 } config [show | set &amp;lt;option&amp;gt; &amp;lt;value&amp;gt;] 用于显示或调整配置信息。示例：&amp;gt;&amp;gt; config show //　显示 scheduler 的相关 config 信息 { &amp;#34;max-snapshot-count&amp;#34;: 3, &amp;#34;max-pending-peer-count&amp;#34;: 16, &amp;#34;max-store-down-time&amp;#34;: &amp;#34;1h0m0s&amp;#34;, &amp;#34;leader-schedule-limit&amp;#34;: 64, &amp;#34;region-schedule-limit&amp;#34;: 16, &amp;#34;replica-schedule-limit&amp;#34;: 24, &amp;#34;tolerant-size-ratio&amp;#34;: 2.5, &amp;#34;schedulers-v2&amp;#34;: [ { &amp;#34;type&amp;#34;: &amp;#34;balance-region&amp;#34;, &amp;#34;args&amp;#34;: null }, { &amp;#34;type&amp;#34;: &amp;#34;balance-leader&amp;#34;, &amp;#34;args&amp;#34;: null }, { &amp;#34;type&amp;#34;: &amp;#34;hot-region&amp;#34;, &amp;#34;args&amp;#34;: null } ] } &amp;gt;&amp;gt; config show all // 显示所有的 config 信息 &amp;gt;&amp;gt; config show namespace ts1 // 显示名为 ts1 的 namespace 的相关 config 信息 { &amp;#34;leader-schedule-limit&amp;#34;: 64, &amp;#34;region-schedule-limit&amp;#34;: 16, &amp;#34;replica-schedule-limit&amp;#34;: 24, &amp;#34;max-replicas&amp;#34;: 3, } &amp;gt;&amp;gt; config show replication // 显示 replication 的相关 config 信息 { &amp;#34;max-replicas&amp;#34;: 3, &amp;#34;location-labels&amp;#34;: &amp;#34;&amp;#34; } 通过调整 leader-schedule-limit 可以控制同时进行 leader 调度的任务个数。 这个值主要影响 leader balance 的速度，值越大调度得越快，设置为 0 则关闭调度。 Leader 调度的开销较小，需要的时候可以适当调大。&amp;gt;&amp;gt; config set leader-schedule-limit 4 // 最多同时进行 4 个 leader 调度 通过调整 region-schedule-limit 可以控制同时进行 region 调度的任务个数。 这个值主要影响 region balance 的速度，值越大调度得越快，设置为 0 则关闭调度。 Region 调度的开销较大，所以这个值不宜调得太大。&amp;gt;&amp;gt; config set region-schedule-limit 2 // 最多同时进行 2 个 region 调度 通过调整 replica-schedule-limit 可以控制同时进行 replica 调度的任务个数。 这个值主要影响节点挂掉或者下线的时候进行调度的速度，值越大调度得越快，设置为 0 则关闭调度。 Replica 调度的开销较大，所以这个值不宜调得太大。&amp;gt;&amp;gt; config set replica-schedule-limit 4 // 最多同时进行 4 个 replica 调度 以上对配置的修改是全局性的，还可以通过对不同 namespace 的配置，进行细化调整。当 namespace 未设置相应配置时，使用全局配置。注：namespace 的配置只支持对 leader-schedule-limit，region-schedule-limit，replica-schedule-limit，max-replicas 的调整，否则不生效。&amp;gt;&amp;gt; config set namespace ts1 leader-schedule-limit 4 // 设置名为 ts1 的 namespace 最多同时进行 4 个 leader 调度 &amp;gt;&amp;gt; config set namespace ts2 region-schedule-limit 2 // 设置名为 ts2 的 namespace 最多同时进行 2 个 region 调度 config delete namespace &amp;lt;name&amp;gt; [&amp;lt;option&amp;gt;] 用于删除 namespace 的配置信息。示例：在对 namespace 相关配置进行设置后，若想让该 namespace 继续使用全局配置，可删除该 namespace 的配置信息，之后便使用全局配置。&amp;gt;&amp;gt; config delete namespace ts1 // 删除名为 ts1 的 namespace 的相关配置 若只想让 namespace 中的某项配置使用全局配置而不影响其他配置，则可使用如下命令：&amp;gt;&amp;gt; config delete namespace region-schedule-limit ts2 // 删除名为 ts2 的 namespace 的 region-schedule-limit 配置 health 用于显示集群健康信息。示例：&amp;gt;&amp;gt; health // 显示健康信息 {&amp;#34;health&amp;#34;: &amp;#34;true&amp;#34;} hot [read | write | store] 用于显示集群热点信息。示例：&amp;gt;&amp;gt; hot read // 显示读热点信息 &amp;gt;&amp;gt; hot write // 显示写热点信息 &amp;gt;&amp;gt; hot store // 显示所有 store 的读写信息 label [store] 用于显示集群标签信息示例：&amp;gt;&amp;gt; label // 显示所有 label &amp;gt;&amp;gt; label store zone cn // 显示所有包含 label 为 &amp;#34;zone&amp;#34;:&amp;#34;cn&amp;#34; 的 store member [leader | delete] 用于显示 PD 成员信息或删除指定成员。示例：&amp;gt;&amp;gt; member // 显示所有成员的信息 { &amp;#34;members&amp;#34;: [......] } &amp;gt;&amp;gt; member leader show // 显示 leader 的信息 { &amp;#34;name&amp;#34;: &amp;#34;pd&amp;#34;, &amp;#34;addr&amp;#34;: &amp;#34;http://192.168.199.229:2379&amp;#34;, &amp;#34;id&amp;#34;: 9724873857558226554 } &amp;gt;&amp;gt; member delete name pd2 // 下线 &amp;#34;pd2&amp;#34; Success! &amp;gt;&amp;gt; member delete id 1319539429105371180 // 使用 id 下线节点 Success! operator [show | add | remove] 用于显示和控制调度操作。示例：&amp;gt;&amp;gt; operator show // 显示所有的 operators &amp;gt;&amp;gt; operator show admin // 显示所有的 admin operators &amp;gt;&amp;gt; operator show leader // 显示所有的 leader operators &amp;gt;&amp;gt; operator show region // 显示所有的 region operators &amp;gt;&amp;gt; operator add add-peer 1 2 // 在 store 2 上新增 region 1 的一个副本 &amp;gt;&amp;gt; operator remove remove-peer 1 2 // 移除 store 2 上的 region 1 的一个副本 &amp;gt;&amp;gt; operator add transfer-leader 1 2 // 把 region 1 的 leader 调度到 store 2 &amp;gt;&amp;gt; operator add transfer-region 1 2 3 4 // 把 region 1 调度到 store 2,3,4 &amp;gt;&amp;gt; operator add transfer-peer 1 2 3 // 把 region 1 在 store 2 上的副本调度到 store 3 &amp;gt;&amp;gt; operator remove 1 // 把 region 1 的调度操作删掉 ping 用于显示ping PD 所需要花费的时间示例：&amp;gt;&amp;gt; ping time: 43.12698ms region &amp;lt;region_id&amp;gt; 用于显示 region 信息。示例：&amp;gt;&amp;gt; region //　显示所有 region 信息 { &amp;#34;count&amp;#34;: 1, &amp;#34;regions&amp;#34;: [......] } &amp;gt;&amp;gt; region 2 // 显示 region id 为 2 的信息 { &amp;#34;region&amp;#34;: { &amp;#34;id&amp;#34;: 2, ...... } &amp;#34;leader&amp;#34;: { ...... } } region key [&amp;ndash;format=raw|pb|proto|protobuf] &amp;lt;key&amp;gt; 用于查询某个 key 在哪个 region 上，支持 raw 和 protobuf 格式。Raw 格式（默认）示例：&amp;gt;&amp;gt; region key abc { &amp;#34;region&amp;#34;: { &amp;#34;id&amp;#34;: 2, ...... } } Protobuf 格式示例：&amp;gt;&amp;gt; region key --format=pb t200000000000000000000377035_r200000000000000377017U320000000000000000372 { &amp;#34;region&amp;#34;: { &amp;#34;id&amp;#34;: 2, ...... } } scheduler [show | add | remove] 用于显示和控制调度策略。示例：&amp;gt;&amp;gt; scheduler show // 显示所有的 schedulers &amp;gt;&amp;gt; scheduler add grant-leader-scheduler 1 // 把 store 1 上的所有 region 的 leader 调度到 store 1 &amp;gt;&amp;gt; scheduler add evict-leader-scheduler 1 // 把 store 1 上的所有 region 的 leader 从 store 1 调度出去 &amp;gt;&amp;gt; scheduler add shuffle-leader-scheduler // 随机交换不同 store 上的 leader &amp;gt;&amp;gt; scheduler add shuffle-region-scheduler // 随机调度不同 store 上的 region &amp;gt;&amp;gt; scheduler remove grant-leader-scheduler-1 // 把对应的 scheduler 删掉 store [delete | label | weight] &amp;lt;store_id&amp;gt; 用于显示 store 信息或者删除指定 store。示例：&amp;gt;&amp;gt; store // 显示所有 store 信息 { &amp;#34;count&amp;#34;: 3, &amp;#34;stores&amp;#34;: [...] } &amp;gt;&amp;gt; store 1 // 获取 store id 为 1 的 store ...... &amp;gt;&amp;gt; store delete 1 // 下线 store id 为 1 的 store ...... &amp;gt;&amp;gt; store label 1 zone cn // 设置 store id 为 1 的 store 的键为 &amp;#34;zone&amp;#34; 的 label 的值为 &amp;#34;cn&amp;#34; &amp;gt;&amp;gt; store weight 1 5 10 // 设置 store id 为 1 的 store 的 leader weight 为 5，region weight 为 10 table_ns [create | add | remove | set_store | rm_store | set_meta | rm_meta] 用于显示 table 的 namespace 的相关信息示例：&amp;gt;&amp;gt; table_ns add ts1 1 // 将 table id 为 1 的 table 添加到名为 ts1 的 namespace &amp;gt;&amp;gt; table_ns create ts1 // 添加名为 ts1 的 namespace &amp;gt;&amp;gt; table_ns remove ts1 1 // 将 table id 为 1 的 table 从名为 ts1 的 namespace 中移除 &amp;gt;&amp;gt; table_ns rm_meta ts1 // 将 meta 信息从名为 ts1 的 namespace 中移除 &amp;gt;&amp;gt; table_ns rm_store 1 ts1 // 将 store id 为 1 的 table 从名为 ts1 的 namespace 中移除 &amp;gt;&amp;gt; table_ns set_meta ts1 // 将 meta 信息添加到名为 ts1 的 namespace &amp;gt;&amp;gt; table_ns set_store 1 ts1 // 将 store id 为 1 的 table 添加到名为 ts1 的 namespace tso 用于解析 TSO 到物理时间和逻辑时间示例：&amp;gt;&amp;gt; tso 395181938313123110 // 解析 TSO system: 2017-10-09 05:50:59 +0800 CST logic: 120102"},
		{"url": "https://pingcap.com/recruit-cn/market/pr-manager/",
		"title": "PR经理", 
		"content": " PR经理 岗位职责  负责公司媒体资源开拓、谈判、合作和后期运营 负责公关稿件的撰写、公关传播方案、市场推广宣传方案的策划，提高品牌美誉度 策划主持公关专题活动，协调处理各方面的关系 分析评估公关及活动效果、检测市场反应，及时优化调整活动策略  职位要求  本科及以上，新闻专业出身，有良好的写作能力，2年以上科技媒体或商业媒体的采编工作经验 熟悉各类型的媒体，具有一定的媒体资源 熟悉公关宣传活动策划和执行流程 对各类人士的接触及交往能力较好，能够维护良好人际关系 对科技行业，互联网创业，企业级服务，数据库、开源等领域的发展趋势有浓厚兴趣 英文好为加分项  待遇 15K - 30K ，13薪 + 奖金，优秀者可面议工作地点 北京"},
		{"url": "https://pingcap.com/",
		"title": "PingCAP Site", 
		"content": ""},
		{"url": "https://pingcap.com/docs/releases/prega/",
		"title": "Pre-GA release notes", 
		"content": " Pre-GA Release Notes On August 30, 2017, TiDB Pre-GA is released! This release is focused on MySQL compatibility, SQL optimization, stability, and performance.TiDB:  The SQL query optimizer:  Adjust the cost model Use index scan to handle the where clause with the compare expression which has different types on each side Support the Greedy algorithm based Join Reorder  Many enhancements have been introduced to be more compatible with MySQL Support Natural Join Support the JSON type (Experimental), including the query, update and index of the JSON fields Prune the useless data to reduce the consumption of the executor memory Support configuring prioritization in the SQL statements and automatically set the prioritization for some of the statements according to the query type Completed the expression refactor and the speed is increased by about 30%  Placement Driver (PD):  Support manually changing the leader of the PD cluster  TiKV:  Use dedicated Rocksdb instance to store Raft log Use DeleteRange to speed up the deleting of replicas Coprocessor now supports more pushdown operators Improve the performance and stability  TiDB Connector for Spark Beta Release:  Implement the predicates pushdown Implement the aggregation pushdown Implement range pruning Capable of running full set of TPC+H except for one query that needs view support  "},
		{"url": "https://pingcap.com/docs/sql/precision-math/",
		"title": "Precision Math", 
		"content": " Precision Math The precision math support in TiDB is consistent with MySQL. For more information, see Precision Math in MySQL.Numeric types The scope of precision math for exact-value operations includes the exact-value data types (integer and DECIMAL types) and exact-value numeric literals. Approximate-value data types and numeric literals are handled as floating-point numbers.Exact-value numeric literals have an integer part or fractional part, or both. They may be signed. Examples: 1, .2, 3.4, -5, -6.78, +9.10.Approximate-value numeric literals are represented in scientific notation (power-of-10) with a mantissa and exponent. Either or both parts may be signed. Examples: 1.2E3, 1.2E-3, -1.2E3, -1.2E-3.Two numbers that look similar might be treated differently. For example, 2.34 is an exact-value (fixed-point) number, whereas 2.34E0 is an approximate-value (floating-point) number.The DECIMAL data type is a fixed-point type and the calculations are exact. The FLOAT and DOUBLE data types are floating-point types and calculations are approximate.DECIMAL data type characteristics This section discusses the following topics of the characteristics of the DECIMAL data type (and its synonyms): Maximum number of digits Storage format Storage requirements  The declaration syntax for a DECIMAL column is DECIMAL(M,D). The ranges of values for the arguments are as follows: M is the maximum number of digits (the precision). 1&amp;lt;= M &amp;lt;= 65. D is the number of digits to the right of the decimal point (the scale). 1 &amp;lt;= D &amp;lt;= 30 and D must be no larger than M.  The maximum value of 65 for M means that calculations on DECIMAL values are accurate up to 65 digits. This limit of 65 digits of precision also applies to exact-value numeric literals.Values for DECIMAL columns are stored using a binary format that packs 9 decimal digits into 4 bytes. The storage requirements for the integer and fractional parts of each value are determined separately. Each multiple of 9 digits requires 4 bytes, and any remaining digits left over require some fraction of 4 bytes. The storage required for remaining digits is given by the following table.   Leftover Digits Number of Bytes     0 0   1–2 1   3–4 2   5–6 3   7–9 4    For example, a DECIMAL(18,9) column has 9 digits on each side of the decimal point, so the integer part and the fractional part each require 4 bytes. A DECIMAL(20,6) column has 14 integer digits and 6 fractional digits. The integer digits require 4 bytes for 9 of the digits and 3 bytes for the remaining 5 digits. The 6 fractional digits require 3 bytes.DECIMAL columns do not store a leading + character or - character or leading 0 digits. If you insert +0003.1 into a DECIMAL(5,1) column, it is stored as 3.1. For negative numbers, a literal - character is not stored.DECIMAL columns do not permit values larger than the range implied by the column definition. For example, a DECIMAL(3,0) column supports a range of -999 to 999. A DECIMAL(M,D) column permits at most M - D digits to the left of the decimal point.For more information about the internal format of the DECIMAL values, see mydecimal.go in TiDB souce code.Expression handling For expressions with precision math, TiDB uses the exact-value numbers as given whenever possible. For example, numbers in comparisons are used exactly as given without a change in value. In strict SQL mode, if you add an exact data type into a column, a number is inserted with its exact value if it is within the column range. When retrieved, the value is the same as what is inserted. If strict SQL mode is not enabled, truncation for INSERT is permitted in TiDB.How to handle a numeric expression depends on the values of the expression: If the expression contains any approximate values, the result is approximate. TiDB evaluates the expression using floating-point arithmetic. If the expression contains no approximate values are present, which means only exact values are contained, and if any exact value contains a fractional part, the expression is evaluated using DECIMAL exact arithmetic and has a precision of 65 digits. Otherwise, the expression contains only integer values. The expression is exact. TiDB evaluates the expression using integer arithmetic and has a precision the same as BIGINT (64 bits).  If a numeric expression contains strings, the strings are converted to double-precision floating-point values and the result of the expression is approximate.Inserts into numeric columns are affected by the SQL mode. The following discussions mention strict mode and ERROR_FOR_DIVISION_BY_ZERO. To turn on all the restrictions, you can simply use the TRADITIONAL mode, which includes both strict mode values and ERROR_FOR_DIVISION_BY_ZERO:SET sql_mode = &amp;#39;TRADITIONAL`; If a number is inserted into an exact type column (DECIMAL or integer), it is inserted with its exact value if it is within the column range. For this number: - If the value has too many digits in the fractional part, rounding occurs and a warning is generated. - If the value has too many digits in the integer part, it is too large and is handled as follows: - If strict mode is not enabled, the value is truncated to the nearest legal value and a warning is generated. - If strict mode is enabled, an overflow error occurs.To insert strings into numeric columns, TiDB handles the conversion from string to number as follows if the string has nonnumeric contents: In strict mode, a string (including an empty string) that does not begin with a number cannot be used as a number. An error, or a warning occurs. A string that begins with a number can be converted, but the trailing nonnumeric portion is truncated. In strict mode, if the truncated portion contains anything other than spaces, an error, or a warning occurs.  By default, the result of the division by 0 is NULL and no warning. By setting the SQL mode appropriately, division by 0 can be restricted. If you enable the ERROR_FOR_DIVISION_BY_ZERO SQL mode, TiDB handles division by 0 differently: In strict mode, inserts and updates are prohibited, and an error occurs. If it&amp;rsquo;s not in the strict mode, a warning occurs.  In the following SQL statement:INSERT INTO t SET i = 1/0; The following results are returned in different SQL modes:   sql_mode Value Result     &amp;ldquo; No warning, no error; i is set to NULL.   strict No warning, no error; i is set to NULL.   ERROR_FOR_DIVISION_BY_ZERO Warning, no error; i is set to NULL.   strict, ERROR_FOR_DIVISION_BY_ZERO Error; no row is inserted.    Rounding behavior The result of the ROUND() function depends on whether its argument is exact or approximate: For exact-value numbers, the ROUND() function uses the “round half up” rule. For approximate-value numbers, the results in TiDB differs from that in MySQL:TiDB &amp;gt; SELECT ROUND(2.5), ROUND(25E-1); +------------+--------------+ | ROUND(2.5) | ROUND(25E-1) | +------------+--------------+ | 3 | 3 | +------------+--------------+ 1 row in set (0.00 sec)  For inserts into a DECIMAL or integer column, the rounding uses round half away from zero.TiDB &amp;gt; CREATE TABLE t (d DECIMAL(10,0)); Query OK, 0 rows affected (0.01 sec) TiDB &amp;gt; INSERT INTO t VALUES(2.5),(2.5E0); Query OK, 2 rows affected, 2 warnings (0.00 sec) TiDB &amp;gt; SELECT d FROM t; +------+ | d | +------+ | 3 | | 3 | +------+ 2 rows in set (0.00 sec)"},
		{"url": "https://pingcap.com/docs/sql/prepare/",
		"title": "Prepared SQL Statement Syntax", 
		"content": " Prepared SQL Statement Syntax TiDB supports server-side Prepared statements, which can reduce the load of statement parsing and query optimization and improve execution efficiency. You can use Prepared statements in two ways: application programs and SQL statements.Use application programs Most MySQL Drivers support Prepared statements, such as MySQL Connector/C. You can call the Prepared statement API directly through the Binary protocol.Use SQL statements You can also implement Prepared statements using PREPARE, EXECUTE and DEALLOCATE PREPARE. This approach is not as efficient as the application programs, but you do not need to write a program.PREPARE statement PREPARE stmt_name FROM preparable_stmt The PREPARE statement preprocesses preparable_stmt (syntax parsing, semantic check and query optimization) and names the result as stmt_name. The following operations can refer to it using stmt_name. Processed statements can be executed using the EXECUTE statement or released using the DEALLOCATE PREPARE statement.EXECUTE statement EXECUTE stmt_name [USING @var_name [, @var_name] ...] The EXECUTE statement executes the prepared statements named as stmt_name. If parameters exist in the prepared statements, use the User Variable list in the USING clause to assign values to parameters.DEALLOCATE PREPARE statement {DEALLOCATE | DROP} PREPARE stmt_name The DEALLOCATE PREPARE statement is used to delete the result of the prepared statements returned by PREPARE.For more information, see MySQL Prepared Statement Syntax."},
		{"url": "https://pingcap.com/docs-cn/sql/prepare/",
		"title": "Prepared SQL 语句语法", 
		"content": " Prepared SQL 语句语法 TiDB 支持服务器端的 Prepared 语句，这种方式可以降低语句解析以及查询优化的开销，提高执行效率。有两种方式可以使用 Prepared 语句：通过应用程序 大多数 MySQL Driver 都支持 Prepared 语句，比如 MySQL Connector/C。这种方式可以通过 Binary 协议直接调用 Prepared 语句 API。通过 SQL 语句 通过 PREPARE，EXECUTE 以及 DEALLOCATE PREPARE 这三个语句也可以实现 Prepared 语句，这种方式不如第一种方式效率高，但是不需要写程序即可使用。PREPARE 语句 PREPARE stmt_name FROM preparable_stmt PREPARE 语句对 preparable_stmt 做预处理（语法解析、语义检查、查询优化）并将其处理结果命名为 stmt_name，后面的操作可以通过 stmt_name 来引用。处理好的语句可以通过 EXECUTE 语句执行或者是通过 DEALLOCATE PREPARE 语句释放。EXECUTE 语句 EXECUTE stmt_name [USING @var_name [, @var_name] ...] EXECUTE 语句执行名字为 stmt_name 的预处理语句。如果预处理语句中有参数，则可以通过 USING 子句中的 User Variable 列表给参数赋值。DEALLOCATE PREPARE 语句 {DEALLOCATE | DROP} PREPARE stmt_name DEALLOCATE PREPARE 语句删除 PREPARE 产生的预处理语句结果。更多信息请参考 MySQL Prepared Statement Syntax。"},
		{"url": "https://pingcap.com/docs/sql/privilege/",
		"title": "Privilege Management", 
		"content": " Privilege Management Privilege management overview TiDB&amp;rsquo;s privilege management system is implemented according to the privilege management system in MySQL. It supports most of the syntaxes and privilege types in MySQL. If you find any inconsistency with MySQL, feel free to open an issue.Examples User account operation TiDB user account names consist of a user name and a host name. The account name syntax is &#39;user_name&#39;@&#39;host_name&#39;. The user_name is case sensitive. The host_name can be a host name or an IP address. The % and _ wildcard characters are permitted in host name or IP address values. For example, a host value of &#39;%&#39; matches any host name and &#39;192.168.1.%&#39; matches every host on a subnet.  Create user The CREATE USER statement creates new MySQL accounts.create user &amp;#39;test&amp;#39;@&amp;#39;127.0.0.1&amp;#39; identified by &amp;#39;xxx&amp;#39;; If the host name is not specified, you can log in from any IP address. If the password is not specified, it is empty by default:create user &amp;#39;test&amp;#39;; Equals:create user &amp;#39;test&amp;#39;@&amp;#39;%&amp;#39; identified by &amp;#39;&amp;#39;; Required Privilege: To use CREATE USER, you must have the global CREATE USER privilege.Change the password You can use the SET PASSWORD syntax to assign or modify a password to a user account.set password for &amp;#39;root&amp;#39;@&amp;#39;%&amp;#39; = &amp;#39;xxx&amp;#39;; Required Privilege: Operations that assign or modify passwords are permitted only to users with the CREATE USER privilege.Drop user The DROP USER statement removes one or more MySQL accounts and their privileges. It removes the user record entries in the mysql.user table and the privilege rows for the account from all grant tables.drop user &amp;#39;test&amp;#39;@&amp;#39;%&amp;#39;; Required Privilege: To use DROP USER, you must have the global CREATE USER privilege.Reset the root password If you forget the root password, you can skip the privilege system and use the root privilege to reset the password.To reset the root password, Start TiDB with a special startup option (root privilege required)：sudo ./tidb-server -skip-grant-table=true Use the root account to log in and reset the password:mysql -h 127.0.0.1 -P 4000 -u root  Privilege-related operations Grant privileges The GRANT statement grants privileges to the user accounts.For example, use the following statement to grant the xxx user the privilege to read the test database.grant Select on test.* to &amp;#39;xxx&amp;#39;@&amp;#39;%&amp;#39;; Use the following statement to grant the xxx user all privileges on all databases:grant all privileges on *.* to &amp;#39;xxx&amp;#39;@&amp;#39;%&amp;#39;; If the granted user does not exist, TiDB will automatically create a user.mysql&amp;gt; select * from mysql.user where user=&amp;#39;xxxx&amp;#39;; Empty set (0.00 sec) mysql&amp;gt; grant all privileges on test.* to &amp;#39;xxxx&amp;#39;@&amp;#39;%&amp;#39; identified by &amp;#39;yyyyy&amp;#39;; Query OK, 0 rows affected (0.00 sec) mysql&amp;gt; select user,host from mysql.user where user=&amp;#39;xxxx&amp;#39;; +------|------+ | user | host | +------|------+ | xxxx | % | +------|------+ 1 row in set (0.00 sec) In this example, xxxx@% is the user that is automatically created. Note: Granting privileges to a database or table does not check if the database or table exists. mysql&amp;gt; select * from test.xxxx; ERROR 1146 (42S02): Table &amp;#39;test.xxxx&amp;#39; doesn&amp;#39;t exist mysql&amp;gt; grant all privileges on test.xxxx to xxxx; Query OK, 0 rows affected (0.00 sec) mysql&amp;gt; select user,host from mysql.tables_priv where user=&amp;#39;xxxx&amp;#39;; +------|------+ | user | host | +------|------+ | xxxx | % | +------|------+ 1 row in set (0.00 sec) You can use fuzzy matching to grant privileges to databases and tables.mysql&amp;gt; grant all privileges on `te%`.* to genius; Query OK, 0 rows affected (0.00 sec) mysql&amp;gt; select user,host,db from mysql.db where user=&amp;#39;genius&amp;#39;; +--------|------|-----+ | user | host | db | +--------|------|-----+ | genius | % | te% | +--------|------|-----+ 1 row in set (0.00 sec) In this example, because of the % in te%, all the databases starting with te are granted the privilege.Revoke privileges The REVOKE statement enables system administrators to revoke privileges from the user accounts.The REVOKE statement corresponds with the REVOKE statement:revoke all privileges on `test`.* from &amp;#39;genius&amp;#39;@&amp;#39;localhost&amp;#39;;  Note: To revoke privileges, you need the exact match. If the matching result cannot be found, an error will be displayed: ``` mysql&amp;gt; revoke all privileges on `te%`.* from &amp;#39;genius&amp;#39;@&amp;#39;%&amp;#39;; ERROR 1141 (42000): There is no such grant defined for user &amp;#39;genius&amp;#39; on host &amp;#39;%&amp;#39; ``` About fuzzy matching, escape, string and identifier:mysql&amp;gt; grant all privileges on `te%`.* to &amp;#39;genius&amp;#39;@&amp;#39;localhost&amp;#39;; Query OK, 0 rows affected (0.00 sec) This example uses exact match to find the database named te%. Note that the % uses the  escape character so that % is not considered as a wildcard.A string is enclosed in single quotation marks(&amp;ldquo;), while an identifier is enclosed in backticks (``). See the differences below:mysql&amp;gt; grant all privileges on &amp;#39;test&amp;#39;.* to &amp;#39;genius&amp;#39;@&amp;#39;localhost&amp;#39;; ERROR 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near &amp;#39;&amp;#39;test&amp;#39;.* to &amp;#39;genius&amp;#39;@&amp;#39;localhost&amp;#39;&amp;#39; at line 1 mysql&amp;gt; grant all privileges on `test`.* to &amp;#39;genius&amp;#39;@&amp;#39;localhost&amp;#39;; Query OK, 0 rows affected (0.00 sec) If you want to use special keywords as table names, enclose them in backticks (``). For example:mysql&amp;gt; create table `select` (id int); Query OK, 0 rows affected (0.27 sec) Check privileges granted to user You can use the show grant statement to see what privileges are granted to a user.show grants for &amp;#39;root&amp;#39;@&amp;#39;%&amp;#39;; To be more precise, you can check the privilege information in the Grant table. For example, you can use the following steps to check if the test@% user has the Insert privilege on db1.t: Check if test@% has global Insert privilege:select Insert from mysql.user where user=&amp;#39;test&amp;#39; and host=&amp;#39;%&amp;#39;; If not, check if test@% has database-level Insert privilege at db1:select Insert from mysql.db where user=&amp;#39;test&amp;#39; and host=&amp;#39;%&amp;#39;; If the result is still empty, check whether test@% has table-level Insert privilege at db1.t:select tables_priv from mysql.tables_priv where user=&amp;#39;test&amp;#39; and host=&amp;#39;%&amp;#39; and db=&amp;#39;db1&amp;#39;;  Implementation of the privilege system Grant table The following system tables are special because all the privilege-related data is stored in them: mysql.user (user account, global privilege) mysql.db (database-level privilege) mysql.tables_priv (table-level privilege) mysql.columns_priv (column-level privilege)  These tables contain the effective range and privilege information of the data. For example, in the mysql.user table:mysql&amp;gt; select User,Host,Select_priv,Insert_priv from mysql.user limit 1; +------|------|-------------|-------------+ | User | Host | Select_priv | Insert_priv | +------|------|-------------|-------------+ | root | % | Y | Y | +------|------|-------------|-------------+ 1 row in set (0.00 sec) In this record, Host and User determine that the connection request sent by the root user from any host (%) can be accepted. Select_priv and Insert_priv mean that the user has global Select and Insert privilege. The effective range in the mysql.user table is global.Host and User in mysql.db determine which databases users can access. The effective range is the database.In theory, all privilege-related operations can be done directly by the CRUD operations on the grant table.On the implementation level, only a layer of syntactic sugar is added. For example, you can use the following command to remove a user:delete from mysql.user where user=&amp;#39;test&amp;#39;; However, it’s not recommended to manually modify the grant table.Connection verification When the client sends a connection …"},
		{"url": "https://pingcap.com/docs/op-guide/history-read/",
		"title": "Reading Data from History Versions", 
		"content": " Reading Data From History Versions This document describes how TiDB reads data from the history versions, how TiDB manages the data versions, as well as an example to show how to use the feature.Feature description TiDB implements a feature to read history data using the standard SQL interface directly without special clients or drivers. By using this feature, - Even when data is updated or removed, its history versions can be read using the SQL interface. - Even if the table structure changes after the data is updated, TiDB can use the old structure to read the history data.How TiDB reads data from history versions The tidb_snapshot system variable is introduced to support reading history data. About the tidb_snapshot variable: The variable is valid in the Session scope. Its value can be modified using the Set statement. The data type for the variable is text. The variable is to record time in the following format: “2016-10-08 16:45:26.999”. Generally, the time can be set to seconds like in “2016-10-08 16:45:26”. When the variable is set, TiDB creates a Snapshot using its value as the timestamp, just for the data structure and there is no any overhead. After that, all the Select operations will read data from this Snapshot.   Note: Because the timestamp in TiDB transactions is allocated by Placement Driver (PD), the version of the stored data is also marked based on the timestamp allocated by PD. When a Snapshot is created, the version number is based on the value of the tidb_snapshot variable. If there is a large difference between the local time of the TiDB server and the PD server, use the time of the PD server. After reading data from history versions, you can read data from the latest version by ending the current Session or using the Set statement to set the value of the tidb_snapshot variable to &amp;ldquo;&amp;rdquo; (empty string).How TiDB manages the data versions TiDB implements Multi-Version Concurrency Control (MVCC) to manage data versions. The history versions of data are kept because each update / removal creates a new version of the data object instead of updating / removing the data object in-place. But not all the versions are kept. If the versions are older than a specific time, they will be removed completely to reduce the storage occupancy and the performance overhead caused by too many history versions.In TiDB, Garbage Collection (GC) runs periodically to remove the obsolete data versions. GC is triggered in the following way: There is a gc_worker goroutine running in the background of each TiDB server. In a cluster with multiple TiDB servers, one of the gc_worker goroutines will be automatically selected to be the leader. The leader is responsible for maintaining the GC state and sends GC commands to each TiKV region leader.The running record of GC is recorded in the system table of mysql.tidb as follows and can be monitored and configured using the SQL statements：mysql&amp;gt; select variable_name, variable_value from mysql.tidb; +-----------------------+----------------------------+ | variable_name | variable_value | +-----------------------+----------------------------+ | bootstrapped | True | | tikv_gc_leader_uuid | 55daa0dfc9c0006 | | tikv_gc_leader_desc | host:pingcap-pc5 pid:10549 | | tikv_gc_leader_lease | 20160927-13:18:28 +0800 CST| | tikv_gc_run_interval | 10m0s | | tikv_gc_life_time | 10m0s | | tikv_gc_last_run_time | 20160927-13:13:28 +0800 CST| | tikv_gc_safe_point | 20160927-13:03:28 +0800 CST| +-----------------------+----------------------------+ 7 rows in set (0.00 sec) Pay special attention to the following two rows: tikv_gc_life_time: This row is to configure the retention time of the history version and its default value is 10m. You can use SQL statements to configure it. For example, if you want all the data within one day to be readable, set this row to 24h by using the update mysql.tidb set variable_value=&#39;24h&#39; where variable_name=&#39;tikv_gc_life_time&#39; statement. The format is: &amp;ldquo;24h&amp;rdquo;, &amp;ldquo;2h30m&amp;rdquo;, &amp;ldquo;2.5h&amp;rdquo;. The unit of time can be: &amp;ldquo;h&amp;rdquo;, &amp;ldquo;m&amp;rdquo;, &amp;ldquo;s&amp;rdquo;.   Note: If your data is updated very frequently, the following issues might occur if the value of the tikv_gc_life_time is set to be too large like in days or months: The more versions of the data, the more disk storage is occupied. A large amount of the history versions might slow down the query, especially the range queries like select count(*) from t. If the value of the tikv_gc_life_time variable is suddenly changed to be smaller while the database is running, it might lead to the removal of large amounts of history data and cause huge I/O burden. tikv_gc_safe_point: This row records the current safePoint. You can safely create the Snapshot to read the history data using the timestamp that is later than the safePoint. The safePoint automatically updates every time GC runs.   Example  At the initial stage, create a table and insert several rows of data:mysql&amp;gt; create table t (c int); Query OK, 0 rows affected (0.01 sec) mysql&amp;gt; insert into t values (1), (2), (3); Query OK, 3 rows affected (0.00 sec) View the data in the table:mysql&amp;gt; select * from t; +------+ | c | +------+ | 1 | | 2 | | 3 | +------+ 3 rows in set (0.00 sec) View the timestamp of the table:mysql&amp;gt; select now(); +---------------------+ | now() | +---------------------+ | 2016-10-08 16:45:26 | +---------------------+ 1 row in set (0.00 sec) Update the data in one row:mysql&amp;gt; update t set c=22 where c=2; Query OK, 1 row affected (0.00 sec) Make sure the data is updated:mysql&amp;gt; select * from t; +------+ | c | +------+ | 1 | | 22 | | 3 | +------+ 3 rows in set (0.00 sec) Set the tidb_snapshot variable whose scope is Session. The variable is set so that the latest version before the value can be read. Note: In this example, the value is set to be the time before the update operation. mysql&amp;gt; set @@tidb_snapshot=&amp;#34;2016-10-08 16:45:26&amp;#34;; Query OK, 0 rows affected (0.00 sec) Result: The read from the following statement is the data before the update operation, which is the history data.mysql&amp;gt; select * from t; +------+ | c | +------+ | 1 | | 2 | | 3 | +------+ 3 rows in set (0.00 sec) Set the tidb_snapshot variable to be &amp;ldquo;&amp;rdquo; (empty string) and you can read the data from the latest version:mysql&amp;gt; set @@tidb_snapshot=&amp;#34;&amp;#34;; Query OK, 0 rows affected (0.00 sec)mysql&amp;gt; select * from t; +------+ | c | +------+ | 1 | | 22 | | 3 | +------+ 3 rows in set (0.00 sec)  "},
		{"url": "https://pingcap.com/docs/releases/rn/",
		"title": "Release Notes", 
		"content": " TiDB Release Notes  1.0.8 1.0.7 1.1 Alpha 1.0.6 1.0.5 1.0.4 1.0.3 1.0.2 1.0.1 1.0 Pre-GA RC4 RC3 RC2 RC1  "},
		{"url": "https://pingcap.com/docs/op-guide/horizontal-scale/",
		"title": "Scale a TiDB cluster", 
		"content": " Scale a TiDB cluster Overview The capacity of a TiDB cluster can be increased or reduced without affecting online services.The following part shows you how to add or delete PD, TiKV or TiDB nodes.About pd-ctl usage, please refer to PD Control User Guide.PD Assume we have three PD servers with the following details:   Name ClientUrls PeerUrls     pd1 http://host1:2379 http://host1:2380   pd2 http://host2:2379 http://host2:2380   pd3 http://host3:2379 http://host3:2380    Get the information about the existing PD nodes through pd-ctl:./pd-ctl -u http://host1:2379 &amp;gt;&amp;gt; member Add a node dynamically Add a new PD server to the current PD cluster by using the parameter join. To add pd4, you just need to specify the client url of any PD server in the PD cluster in the parameter --join, like:./bin/pd-server --name=pd4   --client-urls=&amp;#34;http://host4:2379&amp;#34;   --peer-urls=&amp;#34;http://host4:2380&amp;#34;   --join=&amp;#34;http://host1:2379&amp;#34; Delete a node dynamically Delete pd4 through pd-ctl:./pd-ctl -u http://host1:2379 &amp;gt;&amp;gt; member delete pd4 Migrate a node dynamically If you want to migrate a node to a new machine, you need to, first of all, add a node on the new machine and then delete the node on the old machine. As you can just migrate one node at a time, if you want to migrate multiple nodes, you need to repeat the above steps until you have migrated all nodes. After completing each step, you can verify the process by checking the information of all nodes.TiKV Get the information about the existing TiKV nodes through pd-ctl:./pd-ctl -u http://host1:2379 &amp;gt;&amp;gt; store Add a node dynamically It is very easy to add a new TiKV server dynamically. You just need to start a TiKV server on the new machine. The newly started TiKV server will automatically register in the existing PD of the cluster. To reduce the pressure of the existing TiKV servers, PD loads balance automatically, which means PD gradually migrates some data to the new TiKV server.Delete a node dynamically To delete (make it offline) a TiKV server safely, you need to inform PD in advance. After that, PD is able to migrate the data on this TiKV server to other TiKV servers, ensuring that data have enough replicas.Assume that you need to delete the TiKV server with a store id 1, you can complete this through pd-ctl:./pd-ctl -u http://host1:2379 &amp;gt;&amp;gt; store delete 1 Then you can check the state of this TiKV:./pd-ctl -u http://host1:2379 &amp;gt;&amp;gt; store 1 { &amp;#34;store&amp;#34;: { &amp;#34;id&amp;#34;: 1, &amp;#34;address&amp;#34;: &amp;#34;127.0.0.1:21060&amp;#34;, &amp;#34;state&amp;#34;: 1, &amp;#34;state_name&amp;#34;: &amp;#34;Offline&amp;#34; }, &amp;#34;status&amp;#34;: { ... } } You can verify the state of this store using state_name: state_name=Up: This store is in service. state_name=Disconnected: The heartbeats of this store cannot be detected currently, which might be caused by a failure or network interruption. state_name=Down: PD does not receive heartbeats from the TiKV store for more than an hour (the time can be configured using max-down-time). At this time, PD adds a replica for the data on this store. state_name=Offline: This store is shutting down, but the store is still in service. state_name=Tombstone: This store is shut down and has no data on it, so the instance can be deleted.  Migrate a node dynamically To migrate TiKV servers to a new machine, you also need to add nodes on the new machine and then make all nodes on the old machine offline. In the process of migration, you can add all machines in the new cluster to the existing cluster, then make old nodes offline one by one. To verify whether a node has been made offline, you can check the state information of the node in process. After verifying, you can make the next node offline.TiDB TiDB is a stateless server, which means it can be added or deleted directly. It should be noted that if you deploy a proxy (such as HAProxy) in front of TiDB, you need to update the proxy configuration and reload it."},
		{"url": "https://pingcap.com/docs-cn/sql/schema-object-names/",
		"title": "Schema Object Names", 
		"content": " Schema Object Names 在 TiDB 中，包括 database，table，index，column，alias 等等都被认为是 identifier (标识符，之后阐述用英文).在 TiDB 中，identifier可以被反引号 (`) 包裹，为了阐述方便，我们叫这种情况为 被引用。identifier 也可以不被 ` 包裹。 但是如果一个 identifier 存在一个特殊符号或者是一个保留关键字，那么你必须要 引用 它。mysql&amp;gt; SELECT * FROM `table` WHERE `table`.id = 20; 如果ANSI_QUOTES sql mode 被设置了，那么我们认为被双引号 &amp;quot; 包裹的字符串为 identifier。mysql&amp;gt; CREATE TABLE &amp;#34;test&amp;#34; (a varchar(10)); ERROR 1105 (HY000): line 0 column 19 near &amp;#34; (a varchar(10))&amp;#34; (total length 35) mysql&amp;gt; SET SESSION sql_mode=&amp;#39;ANSI_QUOTES&amp;#39;; Query OK, 0 rows affected (0.00 sec) mysql&amp;gt; CREATE TABLE &amp;#34;test&amp;#34; (a varchar(10)); Query OK, 0 rows affected (0.09 sec) 如果你需要在被引用的 identifier 中使用反引号这个字符，那你需要重复两次，例如你需要创建一个表为 a`b：mysql&amp;gt; CREATE TABLE `a``b` (a int); 在 select 语句中，alias 语句可以用 identifier 或者字符串：mysql&amp;gt; SELECT 1 AS `identifier`, 2 AS &amp;#39;string&amp;#39;; +------------+--------+ | identifier | string | +------------+--------+ | 1 | 2 | +------------+--------+ 1 row in set (0.00 sec) 更多细节Identifier Qualifiers Object Names (对象名字) 可以被限定也可以不用。例如你可以在创建表的时候不指定 database names：CREATE TABLE t (i int); 但是如果你之前没有设定过默认的数据库，会报 ERROR 1046 (3D000): No database selected 错误。当然你也可以指定数据库限定名：CREATE TABLE test.t (i int); 对于 . 左右两端可以出现空格，table_name.col_name 等于 table_name . col_name。如果你要引用这个 identifier，那么请使用：`table_name`.`col_name`  而不是：`table_name.col_name` 更多细节"},
		{"url": "https://pingcap.com/docs/sql/schema-object-names/",
		"title": "Schema Object Names", 
		"content": " Schema Object Names Some objects names in TiDB, including database, table, index, column, alias, etc., are known as identifiers.In TiDB, you can quote or unquote an identifier. If an identifier contains special characters or is a reserved word, you must quote it whenever you refer to it. To quote, use the backtick (`) to wrap the identifier. For example:mysql&amp;gt; SELECT * FROM `table` WHERE `table`.id = 20; If the ANSI_QUOTES SQL mode is enabled, you can also quote identifiers within double quotation marks(&amp;ldquo;):mysql&amp;gt; CREATE TABLE &amp;#34;test&amp;#34; (a varchar(10)); ERROR 1105 (HY000): line 0 column 19 near &amp;#34; (a varchar(10))&amp;#34; (total length 35) mysql&amp;gt; SET SESSION sql_mode=&amp;#39;ANSI_QUOTES&amp;#39;; Query OK, 0 rows affected (0.00 sec) mysql&amp;gt; CREATE TABLE &amp;#34;test&amp;#34; (a varchar(10)); Query OK, 0 rows affected (0.09 sec) The quote characters can be included within an identifier. Double the character if the character to be included within the identifier is the same as that used to quote the identifier itself. For example, the following statement creates a table named a`b:mysql&amp;gt; CREATE TABLE `a``b` (a int); In a SELECT statement, a quoted column alias can be specified using an identifier or a string quoting characters:mysql&amp;gt; SELECT 1 AS `identifier`, 2 AS &amp;#39;string&amp;#39;; +------------+--------+ | identifier | string | +------------+--------+ | 1 | 2 | +------------+--------+ 1 row in set (0.00 sec) For more information, see MySQL Schema Object Names.Identifier qualifiers Object names can be unqualified or qualified. For example, the following statement creates a table using the unqualified name t:CREATE TABLE t (i int); If there is no default database, the ERROR 1046 (3D000): No database selected is displayed. You can also use the qualified name test.t:CREATE TABLE test.t (i int); The qualifier character is a separate token and need not be contiguous with the associated identifiers. For example, there can be white spaces around ., and table_name.col_name and table_name . col_name are equivalent.To quote this identifier, use:`table_name`.`col_name`  Instead of`table_name.col_name` For more information, see MySQL Identifier Qualifiers."},
		{"url": "https://pingcap.com/recruit/engineer/software-engineer-for-tidb/",
		"title": "Software Engineer for TiDB", 
		"content": " Software Engineer for TiDB Minimum qualifications  BA/BS degree in Computer Science or related technical field or equivalent practical experience. 2 years of professional software development experience. Experience with one or more programming languages including C, C++, Java and Go.  Preferred qualifications  Experience with Unix/Linux environments. Experience with designing and deploying large scale distributed systems. Experience in concurrency, multithreading and synchronization. Experience with database internals, database language theories, database design, SQL and database programming. Understanding of technologies such as virtualization, load balancing, networking, massive data storage, Hadoop, MapReduce and security.  Responsibilities  TiDB kernel development, including but not limited to SQL statement parsing and execution, Query optimization, distributed computing framework. Design, development and testing of TiDB to ensure the stability and reliability of the service. Performance optimization of TiDB  "},
		{"url": "https://pingcap.com/recruit/engineer/software-engineer-for-tikv/",
		"title": "Software Engineer for TiKV", 
		"content": " Software Engineer for TiKV Qualifications  Solid development experience on distributed systems, understanding distributed transactions and consensus algorithms like Paxos or Raft. Development experience on high performance services, knowledge of performance testing and system optimisation based on relevant profile tools such as FlameGraph. Testing experience on distributed systems, knowledge of how to create corner cases of distributed environments and verification of system stability. Familiar with Rust or C or C++. Knowledge of Go is a plus.  Responsibilities  TiKV kernel development, including but not limited to core functions of Raft, distributed transactions and Coprocessor TiKV distributed testing, creating corner cases to ensure the system stability and reliability Performance optimization of TiKV, including but not limited to the optimization of applications and Linux system Placement Driver development and optimization of cluster management  "},
		{"url": "https://pingcap.com/docs/op-guide/recommendation/",
		"title": "Software and Hardware Requirements", 
		"content": " Software and Hardware Requirements About As an open source distributed NewSQL database with high performance, TiDB can be deployed in the Intel architecture server and major virtualization environments and runs well. TiDB supports most of the major hardware networks and Linux operating systems.Linux OS version requirements    Linux OS Platform Version     Red Hat Enterprise Linux 7.3 and above   CentOS 7.3 and above   Oracle Enterprise Linux 7.3 and above   Ubuntu LTS 16.04 and above     Note: For Oracle Enterprise Linux, TiDB supports the Red Hat Compatible Kernel (RHCK) and does not support the Unbreakable Enterprise Kernel provided by Oracle Enterprise Linux. The support for the Linux operating systems above include the deployment and operation in physical servers as well as in major virtualized environments like VMware, KVM and XEM.   Server requirements You can deploy and run TiDB on the 64-bit generic hardware server platform in the Intel x86-64 architecture. The requirements and recommendations about server hardware configuration for development, testing and production environments are as follows:Development and testing environments    Component CPU Memory Local Storage Network Instance Number (Minimum Requirement)     TiDB 8 core+ 16 GB+ SAS, 200 GB+ Gigabit network card 1 (can be deployed on the same machine with PD)   PD 8 core+ 16 GB+ SAS, 200 GB+ Gigabit network card 1 (can be deployed on the same machine with TiDB)   TiKV 8 core+ 32 GB+ SAS, 200 GB+ Gigabit network card 3       Total Server Number 4     Note: In the test environment, the TiDB and PD can be deployed on the same server. For performance-related testing, do not use low-performance storage and network hardware configuration, in order to guarantee the correctness of the test result.   Production environment    Component CPU Memory Hard Disk Type Network Instance Number (Minimum Requirement)     TiDB 16 core+ 48 GB+ SAS 10 Gigabit network card (2 preferred) 2   PD 8 core+ 16 GB+ SSD 10 Gigabit network card (2 preferred) 3   TiKV 16 core+ 48 GB+ SSD 10 Gigabit network card (2 preferred) 3   Monitor 8 core+ 16 GB+ SAS Gigabit network card 1       Total Server Number 9     Note: In the production environment, you can deploy and run TiDB and PD on the same server. If you have a higher requirement for performance and reliability, try to deploy them separately. It is strongly recommended to use higher configuration in the production environment. It is recommended to keep the size of TiKV hard disk within 800G in case it takes too long to restore data when the hard disk is damaged.   Network requirements As an open source distributed NewSQL database, TiDB requires the following network port configuration to run. Based on the TiDB deployment in actual environments, the administrator can enable relevant ports in the network side and host side.   Component Default Port Description     TiDB 4000 the communication port for the application and DBA tools   TiDB 10080 the communication port to report TiDB status   TiKV 20160 the TiKV communication port   PD 2379 the communication port between TiDB and PD   PD 2380 the inter-node communication port within the PD cluster   Prometheus 9090 the communication port for the Prometheus service   Pushgateway 9091 the aggregation and report port for TiDB, TiKV, and PD monitor   Node_exporter 9100 the communication port to report the system information of every TiDB cluster node   Grafana 3000 the port for the external Web monitoring service and client (Browser) access    Web browser requirements Based on the Prometheus and Grafana platform, TiDB provides a visual data monitoring solution to monitor the TiDB cluster status. To visit the Grafana monitor interface, it is recommended to use a higher version of Microsoft IE, Google Chrome or Mozilla Firefox."},
		{"url": "https://pingcap.com/docs/sql/string-functions/",
		"title": "String Functions", 
		"content": " String Functions    Name Description     ASCII() Return numeric value of left-most character   CHAR() Return the character for each integer passed   BIN() Return a string containing binary representation of a number   HEX() Return a hexadecimal representation of a decimal or string value   OCT() Return a string containing octal representation of a number   UNHEX() Return a string containing hex representation of a number   TO_BASE64() Return the argument converted to a base-64 string   FROM_BASE64() Decode to a base-64 string and return result   LOWER() Return the argument in lowercase   LCASE() Synonym for LOWER()   UPPER() Convert to uppercase   UCASE() Synonym for UPPER()   LPAD() Return the string argument, left-padded with the specified string   RPAD() Append string the specified number of times   TRIM() Remove leading and trailing spaces   LTRIM() Remove leading spaces   RTRIM() Remove trailing spaces   BIT_LENGTH() Return length of argument in bits   CHAR_LENGTH() Return number of characters in argument   CHARACTER_LENGTH() Synonym for CHAR_LENGTH()   LENGTH() Return the length of a string in bytes   OCTET_LENGTH() Synonym for LENGTH()   INSERT() Insert a substring at the specified position up to the specified number of characters   REPLACE() Replace occurrences of a specified string   SUBSTR() Return the substring as specified   SUBSTRING() Return the substring as specified   SUBSTRING_INDEX() Return a substring from a string before the specified number of occurrences of the delimiter   MID() Return a substring starting from the specified position   LEFT() Return the leftmost number of characters as specified   RIGHT() Return the specified rightmost number of characters   INSTR() Return the index of the first occurrence of substring   LOCATE() Return the position of the first occurrence of substring   POSITION() Synonym for LOCATE()   REPEAT() Repeat a string the specified number of times   CONCAT() Return concatenated string   CONCAT_WS() Return concatenate with separator   REVERSE() Reverse the characters in a string   SPACE() Return a string of the specified number of spaces   FIELD() Return the index (position) of the first argument in the subsequent arguments   ELT() Return string at index number   EXPORT_SET() Return a string such that for every bit set in the value bits, you get an on string and for every unset bit, you get an off string   MAKE_SET() Return a set of comma-separated strings that have the corresponding bit in bits set   FIND_IN_SET() Return the index position of the first argument within the second argument   FORMAT() Return a number formatted to specified number of decimal places   ORD() Return character code for leftmost character of the argument   QUOTE() Escape the argument for use in an SQL statement   SOUNDEX() Return a soundex string   SOUNDS LIKE Compare sounds    String comparison functions    Name Description     LIKE Simple pattern matching   NOT LIKE Negation of simple pattern matching   STRCMP() Compare two strings   MATCH Perform full-text search    Regular expressions    Name Description     REGEXP Pattern matching using regular expressions   RLIKE Synonym for REGEXP   NOT REGEXP Negation of REGEXP    "},
		{"url": "https://pingcap.com/docs/tools/syncer/",
		"title": "Syncer User Guide", 
		"content": " Syncer User Guide Syncer architecture Download the TiDB toolset (Linux) # Download the tool package. wget http://download.pingcap.org/tidb-enterprise-tools-latest-linux-amd64.tar.gz wget http://download.pingcap.org/tidb-enterprise-tools-latest-linux-amd64.sha256 # Check the file integrity. If the result is OK, the file is correct. sha256sum -c tidb-enterprise-tools-latest-linux-amd64.sha256 # Extract the package. tar -xzf tidb-enterprise-tools-latest-linux-amd64.tar.gz cd tidb-enterprise-tools-latest-linux-amd64 Where to deploy Syncer Syncer can be deployed to any of the machines that can connect to MySQL or TiDB cluster. But it is recommended to be deployed to the TiDB cluster.Enable binary logging (binlog) in MySQL Before using the syncer tool, make sure: Binlog is enabled in MySQL. See Setting the Replication Master Configuration. Binlog must use the row format which is the recommended binlog format in MySQL 5.7. It can be configured using the following statement:SET GLOBAL binlog_format = ROW;  Use the syncer tool to import data incrementally 1. Obtain the position to synchronise Set the meta file for syncer. Assuming the meta file is syncer.meta file:# cat syncer.meta binlog-name = &amp;#34;mysql-bin.000003&amp;#34; binlog-pos = 930143241 binlog-gtid = &amp;#34;2bfabd22-fff7-11e6-97f7-f02fa73bcb01:1-23,61ccbb5d-c82d-11e6-ac2e-487b6bd31bf7:1-4&amp;#34;  Note: The syncer.meta file only needs to be configured once when it is first used. The position will be automatically updated when binlog is synchronised. If you use the binlog position to synchronise, you only need to configure binlog-name and binlog-pos; if you use binlog-gtid to synchronise, you only need to configure binlog-gtid.   2. Start syncer The config.toml file for syncer:log-level = &amp;#34;info&amp;#34; server-id = 101 # The file path for meta: meta = &amp;#34;./syncer.meta&amp;#34; worker-count = 16 batch = 10 # The testing address for pprof. It can also be used by Prometheus to pull the syncer metrics. status-addr = &amp;#34;:10081&amp;#34; # Notice: skip-sqls is abandoned, and use skip-ddls instead. # skip-ddls skips DDL statements, and supports regular expressions. If the DDL sqls are incompatible with TiDB, skip them using these rules. # skip-ddls = [&amp;#34;^CREATEs+USER&amp;#34;] # Notice: skip-events is abandoned, and use skip-dmls instead. # skip-dmls skips DML statements. The type value can be &amp;#39;insert&amp;#39;, &amp;#39;update&amp;#39; and &amp;#39;delete&amp;#39;. # skip &amp;#39;delete&amp;#39; statements in foo.bar table. # [[skip-dmls]] # db-name = &amp;#34;foo&amp;#34; # tbl-name = &amp;#34;bar&amp;#34; # type = &amp;#34;delete&amp;#34; #  # skip all delete statements. # [[skip-dmls]] # type = &amp;#34;delete&amp;#34; #  # skip all delete statements in all foo.* tables. # [[skip-dmls]] # db-name = &amp;#34;foo&amp;#34; # type = &amp;#34;delete&amp;#34; # Support whitelist filter. You can specify the database and table to be synchronised. For example: # Synchronise all the tables of db1 and db2: # replicate-do-db = [&amp;#34;db1&amp;#34;,&amp;#34;db2&amp;#34;] # Synchronise db1.table1. # [[replicate-do-table]] # db-name =&amp;#34;db1&amp;#34; # tbl-name = &amp;#34;table1&amp;#34; # Synchronise db3.table2. # [[replicate-do-table]] # db-name =&amp;#34;db3&amp;#34; # tbl-name = &amp;#34;table2&amp;#34; # Support regular expressions. Start with &amp;#39;~&amp;#39; to use regular expressions. # To synchronise all the databases that start with `test`: # replicate-do-db = [&amp;#34;~^test.*&amp;#34;] # The sharding synchronising rules support wildcharacter. # 1. The asterisk character (*, also called &amp;#34;star&amp;#34;) matches zero or more characters, # for example, &amp;#34;doc*&amp;#34; matches &amp;#34;doc&amp;#34; and &amp;#34;document&amp;#34; but not &amp;#34;dodo&amp;#34;; # asterisk character must be in the end of the wildcard word, # and there is only one asterisk in one wildcard word. # 2. The question mark &amp;#39;?&amp;#39; matches exactly one character. #[[route-rules]] #pattern-schema = &amp;#34;route_*&amp;#34; #pattern-table = &amp;#34;abc_*&amp;#34; #target-schema = &amp;#34;route&amp;#34; #target-table = &amp;#34;abc&amp;#34; #[[route-rules]] #pattern-schema = &amp;#34;route_*&amp;#34; #pattern-table = &amp;#34;xyz_*&amp;#34; #target-schema = &amp;#34;route&amp;#34; #target-table = &amp;#34;xyz&amp;#34; [from] host = &amp;#34;127.0.0.1&amp;#34; user = &amp;#34;root&amp;#34; password = &amp;#34;&amp;#34; port = 3306 [to] host = &amp;#34;127.0.0.1&amp;#34; user = &amp;#34;root&amp;#34; password = &amp;#34;&amp;#34; port = 4000 Start syncer:./bin/syncer -config config.toml 2016/10/27 15:22:01 binlogsyncer.go:226: [info] begin to sync binlog from position (mysql-bin.000003, 1280) 2016/10/27 15:22:01 binlogsyncer.go:130: [info] register slave for master server 127.0.0.1:3306 2016/10/27 15:22:01 binlogsyncer.go:552: [info] rotate to (mysql-bin.000003, 1280) 2016/10/27 15:22:01 syncer.go:549: [info] rotate binlog to (mysql-bin.000003, 1280) 3. Insert data into MySQL INSERT INTO t1 VALUES (4, 4), (5, 5); 4. Log in TiDB and view the data: mysql -h127.0.0.1 -P4000 -uroot -p mysql&amp;gt; select * from t1; +----+------+ | id | age | +----+------+ | 1 | 1 | | 2 | 2 | | 3 | 3 | | 4 | 4 | | 5 | 5 | +----+------+ syncer outputs the current synchronised data statistics every 30 seconds:2017/06/08 01:18:51 syncer.go:934: [info] [syncer]total events = 15, total tps = 130, recent tps = 4, master-binlog = (ON.000001, 11992), master-binlog-gtid=53ea0ed1-9bf8-11e6-8bea-64006a897c73:1-74, syncer-binlog = (ON.000001, 2504), syncer-binlog-gtid = 53ea0ed1-9bf8-11e6-8bea-64006a897c73:1-17 2017/06/08 01:19:21 syncer.go:934: [info] [syncer]total events = 15, total tps = 191, recent tps = 2, master-binlog = (ON.000001, 11992), master-binlog-gtid=53ea0ed1-9bf8-11e6-8bea-64006a897c73:1-74, syncer-binlog = (ON.000001, 2504), syncer-binlog-gtid = 53ea0ed1-9bf8-11e6-8bea-64006a897c73:1-35 You can see that by using syncer, the updates in MySQL are automatically synchronised in TiDB.Support synchronising data from sharded tables Syncer supports importing data from sharded tables into one table within one database according to the route-rules.For example,You just need to start syncer in all the MySQL instances and set the following route-rules:[[route-rules]] pattern-schema = &amp;#34;example_db&amp;#34; pattern-table = &amp;#34;table_*&amp;#34; target-schema = &amp;#34;example_db&amp;#34; target-table = &amp;#34;table&amp;#34;  Note: But before synchronising, you need to check: If the sharding rules can be represented using the route-rules syntax; If the sharded tables contain monotone increasing primary keys, or if there are conflicts in the unique indexes or the primary keys after the combination.   Monitor The syncer monitoring scheme contains the following components: Prometheus, a time series database, to store the monitoring and performance metrics Grafana, an open source project for analysing and visualising metrics, to display the performance metrics. AlertManager, for the alerting mechanism  For more information, see the following diagram:"},
		{"url": "https://pingcap.com/docs-cn/tools/syncer/",
		"title": "Syncer 使用文档", 
		"content": " Syncer 使用文档 syncer 架构 下载 TiDB 工具集 (Linux) # 下载 tool 压缩包 wget http://download.pingcap.org/tidb-enterprise-tools-latest-linux-amd64.tar.gz wget http://download.pingcap.org/tidb-enterprise-tools-latest-linux-amd64.sha256 # 检查文件完整性，返回 ok 则正确 sha256sum -c tidb-enterprise-tools-latest-linux-amd64.sha256 # 解开压缩包 tar -xzf tidb-enterprise-tools-latest-linux-amd64.tar.gz cd tidb-enterprise-tools-latest-linux-amd64 Syncer 部署位置 Syncer 可以部署在任一台可以连通对应的 MySQL 和 TiDB 集群的机器上，推荐部署在 TiDB 集群。syncer 增量导入数据示例 使用前请详细阅读syncer 同步前预检查设置同步开始的 position 设置 syncer 的 meta 文件, 这里假设 meta 文件是 syncer.meta:# cat syncer.meta binlog-name = &amp;#34;mysql-bin.000003&amp;#34; binlog-pos = 930143241 binlog-gtid = &amp;#34;2bfabd22-fff7-11e6-97f7-f02fa73bcb01:1-23,61ccbb5d-c82d-11e6-ac2e-487b6bd31bf7:1-4&amp;#34;  注意： syncer.meta 只需要第一次使用的时候配置，后续 syncer 同步新的 binlog 之后会自动将其更新到最新的 position 注意： 如果使用 binlog position 同步则只需要配置 binlog-name binlog-pos; 使用 gtid 同步则需要设置 gtid，且启动 syncer 时带有 --enable-gtid  启动 syncer syncer 的命令行参数说明:Usage of syncer: -L string 日志等级: debug, info, warn, error, fatal (默认为 &amp;#34;info&amp;#34;) -V 输出 syncer 版本；默认 false -auto-fix-gtid 当 mysql master/slave 切换时，自动修复 gtid 信息；默认 false -b int batch 事务大小 (默认 10) -c int syncer 处理 batch 线程数 (默认 16) -config string 指定相应配置文件启动 sycner 服务；如 `--config config.toml` -enable-gtid 使用 gtid 模式启动 syncer；默认 false，开启前需要上游 MySQL 开启 GTID 功能 -log-file string 指定日志文件目录；如 `--log-file ./syncer.log` -log-rotate string 指定日志切割周期, hour/day (默认 &amp;#34;day&amp;#34;) -meta string 指定 syncer 上游 meta 信息文件 (默认与配置文件相同目录下 &amp;#34;syncer.meta&amp;#34;) -server-id int 指定 MySQL slave sever-id (默认 101) -status-addr string 指定 syncer metric 信息; 如 `--status-addr 127:0.0.1:10088` syncer 的配置文件 config.toml:log-level = &amp;#34;info&amp;#34; server-id = 101 ## meta 文件地址 meta = &amp;#34;./syncer.meta&amp;#34; worker-count = 16 batch = 10 ## pprof 调试地址, Prometheus 也可以通过该地址拉取 syncer metrics ## 将 127.0.0.1 修改为相应主机 IP 地址 status-addr = &amp;#34;127.0.0.1:10086&amp;#34; # 注意: skip-sqls 已经废弃, 请使用 skip-ddls. # skip-ddls 可以跳过与 TiDB 不兼容的 DDL 语句，支持正则语法。 # skip-ddls = [&amp;#34;^CREATEs+USER&amp;#34;] # 注意: skip-events 已经废弃, 请使用 skip-dmls  # skip-dmls 用于跳过 DML 语句. type 字段取值为 &amp;#39;insert&amp;#39;, &amp;#39;update&amp;#39;, &amp;#39;delete&amp;#39;。 # 下面的例子为跳过 foo.bar 表的所有 delete 语句。 # [[skip-dmls]] # db-name = &amp;#34;foo&amp;#34; # tbl-name = &amp;#34;bar&amp;#34; # type = &amp;#34;delete&amp;#34; #  # 下面的例子为跳过所有表的 delete 语句。 # [[skip-dmls]] # type = &amp;#34;delete&amp;#34; #  # 下面的例子为跳过 foo 库中所有表的 delete 语句。  # [[skip-dmls]] # db-name = &amp;#34;foo&amp;#34; # type = &amp;#34;delete&amp;#34; ## 指定要同步数据库名；支持正则匹配，表达式语句必须以 `~` 开始 #replicate-do-db = [&amp;#34;~^b.*&amp;#34;,&amp;#34;s1&amp;#34;] ## 指定要同步的 db.table 表 ## db-name 与 tbl-name 不支持 `db-name =&amp;#34;dbname，dbname2&amp;#34;` 格式 #[[replicate-do-table]] #db-name =&amp;#34;dbname&amp;#34; #tbl-name = &amp;#34;table-name&amp;#34; #[[replicate-do-table]] #db-name =&amp;#34;dbname1&amp;#34; #tbl-name = &amp;#34;table-name1&amp;#34; ## 指定要同步的 db.table 表；支持正则匹配，表达式语句必须以 `~` 开始 #[[replicate-do-table]] #db-name =&amp;#34;test&amp;#34; #tbl-name = &amp;#34;~^a.*&amp;#34; ## 指定**忽略**同步数据库；支持正则匹配，表达式语句必须以 `~` 开始 #replicate-ignore-db = [&amp;#34;~^b.*&amp;#34;,&amp;#34;s1&amp;#34;] ## 指定**忽略**同步数据库 ## db-name &amp;amp; tbl-name 不支持 `db-name =&amp;#34;dbname，dbname2&amp;#34;` 语句格式 #[[replicate-ignore-table]] #db-name = &amp;#34;your_db&amp;#34; #tbl-name = &amp;#34;your_table&amp;#34; ## 指定要**忽略**同步数据库名；支持正则匹配，表达式语句必须以 `~` 开始 #[[replicate-ignore-table]] #db-name =&amp;#34;test&amp;#34; #tbl-name = &amp;#34;~^a.*&amp;#34; # sharding 同步规则，采用 wildcharacter # 1. 星号字符 (*) 可以匹配零个或者多个字符, # 例子, doc* 匹配 doc 和 document, 但是和 dodo 不匹配; # 星号只能放在 pattern 结尾，并且一个 pattern 中只能有一个 # 2. 问号字符 (?) 匹配任一一个字符 #[[route-rules]] #pattern-schema = &amp;#34;route_*&amp;#34; #pattern-table = &amp;#34;abc_*&amp;#34; #target-schema = &amp;#34;route&amp;#34; #target-table = &amp;#34;abc&amp;#34; #[[route-rules]] #pattern-schema = &amp;#34;route_*&amp;#34; #pattern-table = &amp;#34;xyz_*&amp;#34; #target-schema = &amp;#34;route&amp;#34; #target-table = &amp;#34;xyz&amp;#34; [from] host = &amp;#34;127.0.0.1&amp;#34; user = &amp;#34;root&amp;#34; password = &amp;#34;&amp;#34; port = 3306 [to] host = &amp;#34;127.0.0.1&amp;#34; user = &amp;#34;root&amp;#34; password = &amp;#34;&amp;#34; port = 4000 启动 syncer:./bin/syncer -config config.toml 2016/10/27 15:22:01 binlogsyncer.go:226: [info] begin to sync binlog from position (mysql-bin.000003, 1280) 2016/10/27 15:22:01 binlogsyncer.go:130: [info] register slave for master server 127.0.0.1:3306 2016/10/27 15:22:01 binlogsyncer.go:552: [info] rotate to (mysql-bin.000003, 1280) 2016/10/27 15:22:01 syncer.go:549: [info] rotate binlog to (mysql-bin.000003, 1280) 在 MySQL 插入新的数据 INSERT INTO t1 VALUES (4, 4), (5, 5); 登录到 TiDB 查看：mysql -h127.0.0.1 -P4000 -uroot -p mysql&amp;gt; select * from t1; +----+------+ | id | age | +----+------+ | 1 | 1 | | 2 | 2 | | 3 | 3 | | 4 | 4 | | 5 | 5 | +----+------+ syncer 每隔 30s 会输出当前的同步统计，如下2017/06/08 01:18:51 syncer.go:934: [info] [syncer]total events = 15, total tps = 130, recent tps = 4, master-binlog = (ON.000001, 11992), master-binlog-gtid=53ea0ed1-9bf8-11e6-8bea-64006a897c73:1-74, syncer-binlog = (ON.000001, 2504), syncer-binlog-gtid = 53ea0ed1-9bf8-11e6-8bea-64006a897c73:1-17 2017/06/08 01:19:21 syncer.go:934: [info] [syncer]total events = 15, total tps = 191, recent tps = 2, master-binlog = (ON.000001, 11992), master-binlog-gtid=53ea0ed1-9bf8-11e6-8bea-64006a897c73:1-74, syncer-binlog = (ON.000001, 2504), syncer-binlog-gtid = 53ea0ed1-9bf8-11e6-8bea-64006a897c73:1-35 可以看到，使用 syncer，我们就能自动的将 MySQL 的更新同步到 TiDB。FAQ 指定数据库同步  通过实际案例描述 syncer 同步数据库参数优先级关系。 如果使用 route-rules 规则，请移步 sharding 同步支持 优先级：replicate-do-db &amp;ndash;&amp;gt; replicate-do-table &amp;ndash;&amp;gt; replicate-ignore-db &amp;ndash;&amp;gt; replicate-ignore-table  # 指定同步 ops 数据库 # 指定同步以 ti 开头的数据库 replicate-do-db = [&amp;#34;ops&amp;#34;,&amp;#34;~^ti.*&amp;#34;] # china 数据库下有 guangzhou / shanghai / beijing 等多张表，只同步 shanghai 与 beijing 表。 # 指定同步 china 数据库下 shanghai 表 [[replicate-do-table]] db-name =&amp;#34;china&amp;#34; tbl-name = &amp;#34;shanghai&amp;#34; # 指定同步 china 数据库下 beijing 表 [[replicate-do-table]] db-name =&amp;#34;china&amp;#34; tbl-name = &amp;#34;beijing&amp;#34; # ops 数据库下有 ops_user / ops_admin / weekly 等数据表，只需要同步 ops_user 表。 # 因 replicate-do-db 优先级比 replicate-do-table 高，所以此处设置只同步 ops_user 表无效，实际工作会同步 ops 整个数据库 [[replicate-do-table]] db-name =&amp;#34;ops&amp;#34; tbl-name = &amp;#34;ops_user&amp;#34; # history 数据下有 2017_01 2017_02 ... 2017_12 / 2016_01 2016_02 ... 2016_12 等多张表,只需要同步 2017 年的数据表 [[replicate-do-table]] db-name =&amp;#34;history&amp;#34; tbl-name = &amp;#34;~^2017_.*&amp;#34; # 忽略同步 ops 与 fault 数据库 # 忽略同步以 www 开头的数据库 ## 因 replicate-do-db 优先级比 replicate-ignore-db 高，所以此处忽略同步 ops 不生效。 replicate-ignore-db = [&amp;#34;ops&amp;#34;,&amp;#34;fault&amp;#34;,&amp;#34;~^www&amp;#34;] # fault 数据库下有 faults / user_feedback / ticket 等数据表 # 忽略同步 user_feedback 数据表 # 因 replicate-ignore-db 优先级比 replicate-ignore-table 高，所以此处设置只同步 user_feedback 表无效，实际工作会同步 fault 整个数据库 [[replicate-ignore-table]] db-name = &amp;#34;fault&amp;#34; tbl-name = &amp;#34;user_feedback&amp;#34; # order 数据下有 2017_01 2017_02 ... 2017_12 / 2016_01 2016_02 ... 2016_12 等多张表,忽略 2016 年的数据表 [[replicate-ignore-table]] db-name =&amp;#34;order&amp;#34; tbl-name = &amp;#34;~^2016_.*&amp;#34; sharding 同步支持 根据配置文件的 route-rules 可以支持将分库分表的数据导入到同一个库同一个表中，但是在开始前需要检查分库分表规则 + 是否可以利用 route-rule 的语义规则表示 + 分表中是否包含唯一递增主键，或者合并后数据上有冲突的唯一索引或者主键 + 暂时对 ddl 支持不完善分库分表同步示例  则只需要在所有 mysql 实例下面，启动 syncer, 并且设置以下 route-rules replicate-do-db &amp;amp; replicate-ignore-db 与 route-rules 同时使用场景下，replicate-do-db &amp;amp; replicate-ignore-db 需要指定 route-rules 中 target-schema &amp;amp; target-table 内容  # 场景如下: # 数据库A 下有 order_2016 / history_2016 等多个数据库 # 数据库B 下有 order_2017 / history_2017 等多个数据库 # 指定同步数据库A order_2016 数据库，数据表如下 2016_01 2016_02 ... 2016_12  # 指定同步数据表B order_2017 数据库，数据表如下 2017_01 2017_02 ... 2017_12 # 表内使用 order_id 作为主键，数据之间主键不冲突 # 忽略同步 history_2016 与 history_2017 数据库 # 目标库需要为 order ，目标数据表为 order_2017 / order_2016 # syncer 获取到上游数据后，发现 route-rules 规则启用，先做合库合表操作，再进行 do-db &amp;amp; do-table 判定 ## 此处需要设置 target-schema &amp;amp; target-table 判定需要同步的数据库 [[replicate-do-table]] db-name =&amp;#34;order&amp;#34; tbl-name = &amp;#34;order_2016&amp;#34; [[replicate-do-table]] db-name =&amp;#34;order&amp;#34; tbl-name = &amp;#34;order_2017&amp;#34; [[route-rules]] pattern-schema = &amp;#34;order_2016&amp;#34; pattern-table = &amp;#34;2016_??&amp;#34; target-schema = &amp;#34;order&amp;#34; …"},
		{"url": "https://pingcap.com/docs/sql/tidb-specific/",
		"title": "The Proprietary System Variables and Syntaxes in TiDB", 
		"content": " The Proprietary System Variables and Syntaxes in TiDB On the basis of MySQL variables and syntaxes, TiDB has defined some specific system variables and syntaxes to optimize performance.System variable Variables can be set with the SET statement, for example:set @@tidb_distsql_scan_concurrency = 10If you need to set the global variable, run:set @@global.tidb_distsql_scan_concurrency = 10tidb_distsql_scan_concurrency Scope: SESSION | GLOBAL Default value: 10 This variable is used to set the concurrency of the scan operation. Use a bigger value in OLAP scenarios, and a smaller value in OLTP scenarios. For OLAP scenarios, the maximum value cannot exceed the number of CPU cores of all the TiKV nodes.tidb_index_lookup_size Scope: SESSION | GLOBAL Default value: 20000 This variable is used to set the batch size of index lookup operation. Use a bigger value in OLAP scenarios, and a smaller value in OLTP scenarios.tidb_index_lookup_concurrency Scope: SESSION | GLOBAL Default value: 4 This variable is used to set the concurrency of the index lookup operation. Use a bigger value in OLAP scenarios, and a smaller value in OLTP scenarios.tidb_index_serial_scan_concurrency Scope: SESSION | GLOBAL Default value: 1 This variable is used to set the concurrency of the serial scan operation. Use a bigger value in OLAP scenarios, and a smaller value in OLTP scenarios.Optimizer hint On the basis of MySQL’s Optimizer Hint Syntax, TiDB adds some proprietary Hint syntaxes. When using the Hint syntax, the TiDB optimizer will try to use the specific algorithm, which performs better than the default algorithm in some scenarios.The Hint syntax is included in comments like /*+ xxx */, and in MySQL client versions earlier than 5.7.7, the comment is removed by default. If you want to use the Hint syntax in these earlier versions, add the --comments option when starting the client. For example: mysql -h 127.0.0.1 -P 4000 -uroot --comments.TIDB_SMJ(t1, t2) SELECT /*+ TIDB_SMJ(t1, t2) */ * from t1，t2 where t1.id = t2.idThis variable is used to remind the optimizer to use the Sort Merge Join algorithm. This algorithm takes up less memory, but takes longer to execute. It is recommended if the data size is too large, or there’s insufficient system memory.TIDB_INLJ(t1, t2) SELECT /*+ TIDB_INLJ(t1, t2) */ * from t1，t2 where t1.id = t2.idThis variable is used to remind the optimizer to use the Index Nested Loop Join algorithm. In some scenarios, this algorithm runs faster and takes up fewer system resources, but may be slower and takes up more system resources in some other scenarios. You can try to use this algorithm in scenarios where the result-set is less than 10,000 rows after the outer table is filtered by the WHERE condition. The parameter in TIDB_INLJ() is the candidate table for the driving table (external table) when generating the query plan. That means, TIDB_INLJ (t1) will only consider using t1 as the driving table to create a query plan."},
		{"url": "https://pingcap.com/docs/sql/variable/",
		"title": "The System Variables", 
		"content": " The System Variables The system variables in MySQL are the system parameters that modify the operation of the database runtime. These variables have two types of scope, Global Scope and Session Scope. TiDB supports all the system variables in MySQL 5.7. Most of the variables are only supported for compatibility and do not affect the runtime behaviors.Set the system variables You can use the SET statement to change the value of the system variables. Before you change, consider the scope of the variable. For more information, see MySQL Dynamic System Variables.Set Global variables Add the GLOBAL keyword before the variable or use @@global. as the modifier:SET GLOBAL autocommit = 1; SET @@global.autocommit = 1; Set Session Variables Add the SESSION keyword before the variable, use @@session. as the modifier, or use no modifier:SET SESSION autocommit = 1; SET @@session.autocommit = 1; SET @@autocommit = 1;  Note: LOCAL and @@local. are the synonyms for SESSION and @@session. The fully supported MySQL system variables in TiDB The following MySQL system variables are fully supported in TiDB and have the same behaviors as in MySQL.   Name Scope Description     autocommit GLOBAL | SESSION whether automatically commit a transaction   sql_mode GLOBAL | SESSION support some of the MySQL SQL modes   time_zone GLOBAL | SESSION the time zone of the database   tx_isolation GLOBAL | SESSION the isolation level of a transaction    The proprietary system variables and syntaxes in TiDB See The Proprietary System Variables and Syntax in TiDB."},
		{"url": "https://pingcap.com/docs/sql/server-command-option/",
		"title": "The TiDB Command Options", 
		"content": " The TiDB Command Options TiDB startup options When you star TiDB processes, you can specify some program options.TiDB supports a lot of startup options. Run the following command to get a brief introduction:./tidb-server --help Run the following command to get the version:./tidb-server -V The complete descriptions of startup options are as follows.-L  Log level Default: &amp;ldquo;info&amp;rdquo; Optional values: debug, info, warn, error or fatal  -P  TiDB service monitor port Default: &amp;ldquo;4000&amp;rdquo; TiDB uses this port to accept requests from the MySQL client  --binlog-socket  TiDB uses the unix socket file to accept the internal connection, such as the PUMP service. Default: &amp;ldquo;&amp;rdquo; For example, use &amp;ldquo;/tmp/pump.sock&amp;rdquo; to accept the PUMP unix socket file communication.  --config  TiDB configuration files Default: &amp;ldquo;&amp;rdquo; The file path of the configuration files  --lease  The lease time of schema; unit: second Default: &amp;ldquo;10&amp;rdquo; The lease of schema is mainly used in online schema changes. This value affects the actual execution time of the DDL statement. In most cases, you do not need to change this value unless you clearly understand the internal implementation mechanism of TiDB DDL.  --host  TiDB service monitor host Default: &amp;ldquo;0.0.0.0&amp;rdquo; TiDB service monitors this host. The 0.0.0.0 port monitors the address of all network cards. You can specify the network card that provides external service, such as 192.168.100.113.  --log-file  Log file Default: &amp;ldquo;&amp;rdquo; If the option is not set, the log is output to &amp;ldquo;stderr&amp;rdquo;; if set, the log is output to the corresponding file. In the small hours of every day, the log automatically rotates to use a new file, renames and backups the previous file.  --metrics-addr  The address of Prometheus Push Gateway Default: &amp;ldquo;&amp;rdquo; If the option value is null, TiDB does not push the statistics to Push Gateway. The option format is like --metrics-addr=192.168.100.115:9091.  --metrics-intervel  The time interval that the statistics are pushed to Prometheus Push Gateway Default: 15s If you set the option value to 0, the statistics are not pushed to Push Gateway. --metrics-interval=2 means the statistics are pushed to Push Gateway every two seconds.  --path  For the local storage engines such as &amp;ldquo;goleveldb&amp;rdquo; or &amp;ldquo;BoltDB&amp;rdquo;, path specifies the actual data storage path. For the &amp;ldquo;memory&amp;rdquo; storage engine, it is not necessary to set path. For the &amp;ldquo;TiKV&amp;rdquo; storage engine, path specifies the actual PD address. For example, if the PD is deployed on 192.168.100.113:2379, 192.168.100.114:2379 and 192.168.100.115:2379, the path is &amp;ldquo;192.168.100.113:2379, 192.168.100.114:2379, 192.168.100.115:2379&amp;rdquo;.  --report-status  Enable (true) or disable (false) the status monitor port Default: true The value is either true or false. The true value means opening the status monitor port. The false value means closing the status monitor port. The status monitor port is used to report some internal service information to the external.  --run-ddl  Whether the TiDB server runs DDL statements; set the option when more than two TiDB servers are in the cluster Default: true The value is either true or false. The true value means the TiDB server runs DDL statements. The false value means the TiDB server does not run DDL statements.  --socket string  TiDB uses the unix socket file to accept the external connection. Default: &amp;ldquo;&amp;rdquo; For example, use &amp;ldquo;/tmp/tidb.sock&amp;rdquo; to open the unix socket file.  --status  The status monitor port of TiDB Default: &amp;ldquo;10080&amp;rdquo; This port is used to display the internal data of TiDB, including the Prometheus statistics and pprof. Access the Prometheus statistics at http://host:status_port/metrics. Access the pprof data at http://host:status_port/debug/pprof.  --store  To specify the storage engine used by the bottom layer of TiDB Default: &amp;ldquo;mocktikv&amp;rdquo; Optional values: &amp;ldquo;memory&amp;rdquo;, &amp;ldquo;goleveldb&amp;rdquo;, &amp;ldquo;boltdb&amp;rdquo;, &amp;ldquo;mocktikv&amp;rdquo; or &amp;ldquo;tikv&amp;rdquo; (TiKV is a distributed storage engine, while the others are local storage engines) For example, use tidb-server --store=memory to start a TiDB server with a pure memory engine  TiDB server configuration files When you start the TiDB server, you can specify the server&amp;rsquo;s configuration file using --config path. For overlapped options in configuration, the priority of command options is higher than configuration files.See an example of the configuration file.The complete descriptions of startup options are as follows.host Same as the &amp;ldquo;host&amp;rdquo; startup optionport Same as the &amp;ldquo;P&amp;rdquo; startup optionpath Same as the &amp;ldquo;path&amp;rdquo; startup optionsocket Same as the &amp;ldquo;socket&amp;rdquo; startup optionbinlog-socket Same as the &amp;ldquo;binlog-socket&amp;rdquo; startup optionrun-ddl Same as the &amp;ldquo;run-ddl&amp;rdquo; startup optioncross-join  Default: true When you execute join on tables without any conditions on both sides, the statement can be run by default. But if you set the value to false, the server does not run such join statement.  join-concurrency  The goroutine number when the join-concurrency runs join Default: 5 To view the amount of data and data distribution; generally the more the better; a larger value indicates a larger CPU is needed  query-log-max-len  To record the maximum length of SQL statements in the log Default: 2048 The overlong request is truncated when it is output to the log  slow-threshold int  To record the SQL statement that has a larger value than this option Default: 300 It is required that the value is an integer (int); unit: millisecond  slow-query-file  The slow query log file Default: &amp;ldquo;&amp;rdquo; The value is the file name. If a non-null string is specified, the slow query log is redirected to the corresponding file.  retry-limit  The maximum number of commit retries when the transaction meets a conflict Default: 10 Setting a large number of retries can affect the performance of the TiDB cluster  skip-grant-table  Allow anyone to connect without a password, and all operations do not check privileges Default: false The value is either true or false. The machine&amp;rsquo;s root privilege is required to enable this option, which is used to reset the password when forgotten.  stats-lease  Scan the full table incrementally, and analyze the data amount and indexes of the table Default: &amp;ldquo;3s&amp;rdquo; To use this option, you need to manually run analyze table name. Update the statistics automatically and store data in TiKV persistently, taking up some memory.  tcp-keep-alive  To Enable keepalive in the tcp layer of TiDB Default: false  ssl-cert  The file path of SSL certificate in PEM format Default: &amp;ldquo;&amp;rdquo; If this option and the --ssl-key option are set at the same time, the client can (not required) securely connect to TiDB using TLS. If the specified certificate or private key is invalid, TiDB starts as usual but does not support encrypted connections.  ssl-key  The file path of SSL certificate keys in PEM format, or the private keys specified by --ssl-cert Default: &amp;ldquo;&amp;rdquo; Currently, you cannot load a password-protected private key in TiDB.  ssl-ca  The file path of the trusted CA certificate in PEM format Default: &amp;ldquo;&amp;rdquo; If this option and the --ssl-cert, --ssl-key options are set at the same time, TiDB authenticates the client certificate based on the trusted CA list specified by the option when the client presents the certificate. If the authentication fails, the connection stops. If this option is set but the client does not present the certificate, the encrypted connection continues but the client certificate is not authenticated.  "},
		{"url": "https://pingcap.com/docs/sql/tidb-server/",
		"title": "The TiDB Server", 
		"content": " The TiDB Server TiDB service TiDB refers to the TiDB database management system. This document describes the basic management functions of the TiDB cluster.TiDB cluster startup configuration You can set the service parameters using the command line or the configuration file, or both. The priority of the command line parameters is higher than the configuration file. If the same parameter is set in both ways, TiDB uses the value set using command line parameters. For more information, see The TiDB Command Options.TiDB system variable TiDB is compatible with MySQL system variables, and defines some unique system variables to adjust the database behavior. For more information, see The Proprietary System Variables and Syntaxes in TiDB.TiDB system table Similar to MySQL, TiDB also has system tables that store the information needed when TiDB runs. For more information, see The TiDB System Database.TiDB data directory The TiDB data is stored in the storage engine and the data directory depends on the storage engine used. For more information about how to choose the storage engine, see the TiDB startup parameters document.When you use the local storage engine, the data is stored on the local hard disk and the directory location is controlled by the path parameter.When you use the TiKV storage engine, the data is stored on the TiKV node and the directory location is controlled by the data-dir parameter.TiDB server logs The three components of the TiDB cluster (tidb-server, tikv-server and pd-server) outputs the logs to standard errors by default. In each of the three components, you can set the --log-file parameter (or the configuration item in the configuration file) and output the log into a file.You can adjust the log behavior using the configuration file. For more details, see the configuration file description of each component. For example, the tidb-server log configuration item."},
		{"url": "https://pingcap.com/docs/sql/system-database/",
		"title": "The TiDB System Database", 
		"content": " The TiDB System Database The TiDB System Database is similar to MySQL, which contains tables that store information required by the server when it runs.Grant system tables These system tables contain grant information about user accounts and their privileges: user: user accounts, global privileges, and other non-privilege columns db: database-level privileges tables_priv: table-level privileges columns_priv: column-level privileges  Server-side help system tables Currently, the help_topic is NULL.Statistics system tables  stats_buckets: the buckets of statistics stats_histograms: the histograms of statistics stats_meta: the meta information of tables, such as the total number of rows and updated rows  GC worker system tables  gc_delete_range: to record the data to be deleted  Miscellaneous system tables  GLOBAL_VARIABLES: global system variable table tidb: to record the version information when TiDB executes bootstrap  INFORMATION_SCHEMA tables To be compatible with MySQL, TiDB supports INFORMATION_SCHEMA tables. Some third-party software queries information in these tables. Currently, most INFORMATION_SCHEMA tables in TiDB are NULL.CHARACTER_SETS table The CHARACTER_SETS table provides information about character sets. But it contains dummy data. By default, TiDB only supports utf8mb4.mysql&amp;gt; select * from CHARACTER_SETS; +--------------------|----------------------|-----------------------|--------+ | CHARACTER_SET_NAME | DEFAULT_COLLATE_NAME | DESCRIPTION | MAXLEN | +--------------------|----------------------|-----------------------|--------+ | ascii | ascii_general_ci | US ASCII | 1 | | binary | binary | Binary pseudo charset | 1 | | latin1 | latin1_swedish_ci | cp1252 West European | 1 | | utf8 | utf8_general_ci | UTF-8 Unicode | 3 | | utf8mb4 | utf8mb4_general_ci | UTF-8 Unicode | 4 | +--------------------|----------------------|-----------------------|--------+ 5 rows in set (0.00 sec) COLLATIONS table The COLLATIONS table is similar to the CHARACTER_SETS table.COLLATION_CHARACTER_SET_APPLICABILITY table NULL.COLUMNS table The COLUMNS table provides information about columns in tables. The information in this table is not accurate. To query information, it is recommended to use the SHOW statement:SHOW COLUMNS FROM table_name [FROM db_name] [LIKE &amp;#39;wild&amp;#39;] COLUMNS_PRIVILEGE table NULL.ENGINES table The ENGINES table provides information about storage engines. But it contains dummy data only. In the production environment, use the TiKV engine for TiDB.EVENTS table NULL.FILES table NULL.GLOBAL_STATUS table NULL.GLOBAL_VARIABLES table NULL.KEY_COLUMN_USAGE table The KEY_COLUMN_USAGE table describes the key constraints of the columns, such as the primary key constraint.OPTIMIZER_TRACE table NULL.PARAMETERS table NULL.PARTITIONS table NULL.PLUGINS table NULL.PROFILING table NULL.REFERENTIAL_CONSTRAINTS table NULL.ROUTINES table NULL.SCHEMATA table The SCHEMATA table provides information about databases. The table data is equivalent to the result of the SHOW DATABASES statement.mysql&amp;gt; select * from SCHEMATA; +--------------|--------------------|----------------------------|------------------------|----------+ | CATALOG_NAME | SCHEMA_NAME | DEFAULT_CHARACTER_SET_NAME | DEFAULT_COLLATION_NAME | SQL_PATH | +--------------|--------------------|----------------------------|------------------------|----------+ | def | INFORMATION_SCHEMA | utf8 | utf8_bin | NULL | | def | mysql | utf8 | utf8_bin | NULL | | def | PERFORMANCE_SCHEMA | utf8 | utf8_bin | NULL | | def | test | utf8 | utf8_bin | NULL | +--------------|--------------------|----------------------------|------------------------|----------+ 4 rows in set (0.00 sec) SCHEMA_PRIVILEGES table NULL.SESSION_STATUS table NULL.SESSION_VARIABLES table The SESSION_VARIABLES table provides information about session variables. The table data is similar to the result of the SHOW SESSION VARIABLES statement.STATISTICS table The STATISTICS table provides information about table indexes.mysql&amp;gt; desc statistics; +---------------|---------------------|------|------|---------|-------+ | Field | Type | Null | Key | Default | Extra | +---------------|---------------------|------|------|---------|-------+ | TABLE_CATALOG | varchar(512) | YES | | NULL | | | TABLE_SCHEMA | varchar(64) | YES | | NULL | | | TABLE_NAME | varchar(64) | YES | | NULL | | | NON_UNIQUE | varchar(1) | YES | | NULL | | | INDEX_SCHEMA | varchar(64) | YES | | NULL | | | INDEX_NAME | varchar(64) | YES | | NULL | | | SEQ_IN_INDEX | bigint(2) UNSIGNED | YES | | NULL | | | COLUMN_NAME | varchar(21) | YES | | NULL | | | COLLATION | varchar(1) | YES | | NULL | | | CARDINALITY | bigint(21) UNSIGNED | YES | | NULL | | | SUB_PART | bigint(3) UNSIGNED | YES | | NULL | | | PACKED | varchar(10) | YES | | NULL | | | NULLABLE | varchar(3) | YES | | NULL | | | INDEX_TYPE | varchar(16) | YES | | NULL | | | COMMENT | varchar(16) | YES | | NULL | | | INDEX_COMMENT | varchar(1024) | YES | | NULL | | +---------------|---------------------|------|------|---------|-------+ The following statements are equivalent:SELECT * FROM INFORMATION_SCHEMA.STATISTICS WHERE table_name = &amp;#39;tbl_name&amp;#39; AND table_schema = &amp;#39;db_name&amp;#39; SHOW INDEX FROM tbl_name FROM db_name TABLES table The TABLES table provides information about tables in databases.The following statements are equivalent:SELECT table_name FROM INFORMATION_SCHEMA.TABLES WHERE table_schema = &amp;#39;db_name&amp;#39; [AND table_name LIKE &amp;#39;wild&amp;#39;] SHOW TABLES FROM db_name [LIKE &amp;#39;wild&amp;#39;] TABLESPACES table NULL.TABLE_CONSTRAINTS table The TABLE_CONSTRAINTS table describes which tables have constraints. The CONSTRAINT_TYPE value can be UNIQUE, PRIMARY KEY, or FOREIGN KEY. The UNIQUE and PRIMARY KEY information is similar to the result of the SHOW INDEX statement.  TABLE_PRIVILEGES table NULL.TRIGGERS table NULL.USER_PRIVILEGES table The USER_PRIVILEGES table provides information about global privileges. This information comes from the mysql.user grant table.mysql&amp;gt; desc USER_PRIVILEGES; +----------------|--------------|------|------|---------|-------+ | Field | Type | Null | Key | Default | Extra | +----------------|--------------|------|------|---------|-------+ | GRANTEE | varchar(81) | YES | | NULL | | | TABLE_CATALOG | varchar(512) | YES | | NULL | | | PRIVILEGE_TYPE | varchar(64) | YES | | NULL | | | IS_GRANTABLE | varchar(3) | YES | | NULL | | +----------------|--------------|------|------|---------|-------+ 4 rows in set (0.00 sec) VIEWS table NULL. Currently, TiDB does not support views."},
		{"url": "https://pingcap.com/docs-cn/releases/ga/",
		"title": "TiDB 1.0 release notes", 
		"content": " TiDB 1.0 release notes 10 月 16 日，TiDB 发布 GA 版（TiDB 1.0）。该版本对 MySQL 兼容性、SQL 优化器、系统稳定性、性能做了大量的工作。TiDB:  SQL 查询优化器 调整代价模型 Analyze 下推 函数签名下推  优化内部数据格式，减小中间结果大小 提升 MySQL 兼容性 支持 NO_SQL_CACHE 语法，控制存储引擎对缓存的使用 重构 Hash Aggregator 算子，降低内存使用 支持 Stream Aggragator 算子  PD:  支持基于读流量的热点调度 支持设置 Store 权重，以及基于权重的调度  TiKV:  Coprocessor 支持更多下推函数 支持取样操作下推 支持手动触发数据 Compact，用于快速回收空间 提升性能和稳定性 增加 Debug API，方便调试  TiSpark Beta Release:  支持可配置框架 支持 ThriftSever/JDBC 和 Spark SQL 脚本入口  源码地址 源码地址鸣谢 特别感谢参与项目的企业和团队  Archon Mobike SpeedyCloud UCloud 腾讯云 韩国三星研究院  感谢以下组织/个人提供出色的开源软件/服务：  Asta Xie CNCF CoreOS Databricks Docker Github Grafana gRPC Jepsen Kubernetes Namazu Prometheus RedHat RocksDB Team Rust Team  感谢社区个人贡献者 TiDB Contributor  8cbx Akihiro Suda aliyx alston111111 andelf Andy Librian Arthur Yang astaxie Bai, Yang bailaohe Bin Liu Blame cosmos Breezewish Carlos Ferreira Ce Gao Changjian Zhang Cheng Lian Cholerae Hu Chu Chao coldwater Cole R Lawrence cuiqiu cuiyuan Cwen Dagang David Chen David Ding dawxy dcadevil Deshi Xiao Di Tang disksing dongxu dreamquster Drogon Du Chuan Dylan Wen eBoyy Eric Romano Ewan Chou Fiisio follitude Fred Wang follitude fud fudali gaoyangxiaozhu Gogs goroutine Gregory Ian Guanqun Lu Guilherme Hübner Franco Haibin Xie Han Fei Hiroaki Nakamura hiwjd Hongyuan Wang Hu Ming Hu Ziming Huachao Huang HuaiyuXu Huxley Hu iamxy Ian insion iroi44 Ivan.Yang Jack Yu jacky liu Jan Mercl Jason W Jay Jay Lee Jianfei Wang Jiaxing Liang Jie Zhou jinhelin Jonathan Boulle Karl Ostendorf knarfeh Kuiba leixuechun li Li Shihai Liao Qiang Light lijian Lilian Lee Liqueur Librazy Liu Cong Liu Shaohui liubo0127 liyanan lkk2003rty Louis louishust luckcolors Lynn Mae Huang maiyang maxwell mengshangqi Michael Belenchenko mo2zie morefreeze MQ mxlxm Neil Shen netroby ngaut Nicole Nie nolouch onlymellb overvenus PaladinTyrion paulg Priya Seth qgxiaozhan qhsong Qiannan qiuyesuifeng queenypingcap qupeng Rain Li ranxiaolong Ray Rick Yu shady ShawnLi Shen Li Sheng Tang Shirly Shuai Li ShuNing ShuYu Wang siddontang silenceper Simon J Mudd Simon Xia skimmilk6877 sllt soup Sphinx Steffen sumBug sunhao2017 Tao Meng Tao Zhou tennix tiancaiamao TianGuangyu Tristan Su ueizhou UncP Unknwon v01dstar Van WangXiangUSTC wangyisong1996 weekface wegel Wei Fu Wenbin Xiao Wenting Li Wenxuan Shi winkyao woodpenker wuxuelian Xiang Li xiaojian cai Xuanjia Yang Xuanwo XuHuaiyu Yang Zhexuan Yann Autissier Yanzhe Chen Yiding Cui Yim youyouhu Yu Jun Yuwen Shen Zejun Li Zhang Yuning zhangjinpeng1987 ZHAO Yijun ZhengQian ZhengQianFang zhengwanbo Zhe-xuan Yang ZhiFeng Hu Zhiyuan Zheng Zhou Tao Zhoubirdblue zhouningnan Ziyi Yan zs634134578 zyguan zz-jason qiukeren hawkingrei wangyanjun zxylvlp  "},
		{"url": "https://pingcap.com/docs/releases/ga/",
		"title": "TiDB 1.0 release notes", 
		"content": " TiDB 1.0 Release Notes On October 16, 2017, TiDB 1.0 is now released! This release is focused on MySQL compatibility, SQL optimization, stability, and performance.TiDB:  The SQL query optimizer:  Adjust the cost model Analyze pushdown Function signature pushdown  Optimize the internal data format to reduce the interim data size Enhance the MySQL compatibility Support the NO_SQL_CACHE syntax and limit the cache usage in the storage engine Refactor the Hash Aggregator operator to reduce the memory usage Support the Stream Aggregator operator  PD:  Support read flow based balancing Support setting the Store weight and weight based balancing  TiKV:  Coprocessor now supports more pushdown functions Support pushing down the sampling operation Support manually triggering data compact to collect space quickly Improve the performance and stability Add a Debug API for debugging TiSpark Beta Release: Support configuration framework Support ThriftSever/JDBC and Spark SQL  Acknowledgement Special thanks to the following enterprises and teams!  Archon Mobike Samsung Electronics SpeedyCloud Tencent Cloud UCloud  Thanks to the open source software and services from the following organizations and individuals:  Asta Xie CNCF CoreOS Databricks Docker Github Grafana gRPC Jepsen Kubernetes Namazu Prometheus RedHat RocksDB Team Rust Team  Thanks to the individual contributors:  8cbx Akihiro Suda aliyx alston111111 andelf Andy Librian Arthur Yang astaxie Bai, Yang bailaohe Bin Liu Blame cosmos Breezewish Carlos Ferreira Ce Gao Changjian Zhang Cheng Lian Cholerae Hu Chu Chao coldwater Cole R Lawrence cuiqiu cuiyuan Cwen Dagang David Chen David Ding dawxy dcadevil Deshi Xiao Di Tang disksing dongxu dreamquster Drogon Du Chuan Dylan Wen eBoyy Eric Romano Ewan Chou Fiisio follitude Fred Wang fud fudali gaoyangxiaozhu Gogs goroutine Gregory Ian Guanqun Lu Guilherme Hübner Franco Haibin Xie Han Fei hawkingrei Hiroaki Nakamura hiwjd Hongyuan Wang Hu Ming Hu Ziming Huachao Huang HuaiyuXu Huxley Hu iamxy Ian insion iroi44 Ivan.Yang Jack Yu jacky liu Jan Mercl Jason W Jay Jay Lee Jianfei Wang Jiaxing Liang Jie Zhou jinhelin Jonathan Boulle Karl Ostendorf knarfeh Kuiba leixuechun li Li Shihai Liao Qiang Light lijian Lilian Lee Liqueur Librazy Liu Cong Liu Shaohui liubo0127 liyanan lkk2003rty Louis louishust luckcolors Lynn Mae Huang maiyang maxwell mengshangqi Michael Belenchenko mo2zie morefreeze MQ mxlxm Neil Shen netroby ngaut Nicole Nie nolouch onlymellb overvenus PaladinTyrion paulg Priya Seth qgxiaozhan qhsong Qiannan qiukeren qiuyesuifeng queenypingcap qupeng Rain Li ranxiaolong Ray Rick Yu shady ShawnLi Shen Li Sheng Tang Shirly Shuai Li ShuNing ShuYu Wang siddontang silenceper Simon J Mudd Simon Xia skimmilk6877 sllt soup Sphinx Steffen sumBug sunhao2017 Tao Meng Tao Zhou tennix tiancaiamao TianGuangyu Tristan Su ueizhou UncP Unknwon v01dstar Van WangXiangUSTC wangyanjun wangyisong1996 weekface wegel Wei Fu Wenbin Xiao Wenting Li Wenxuan Shi winkyao woodpenker wuxuelian Xiang Li xiaojian cai Xuanjia Yang Xuanwo XuHuaiyu Yang Zhexuan Yann Autissier Yanzhe Chen Yiding Cui Yim youyouhu Yu Jun Yuwen Shen Zejun Li Zhang Yuning zhangjinpeng1987 ZHAO Yijun Zhe-xuan Yang ZhengQian ZhengQianFang zhengwanbo ZhiFeng Hu Zhiyuan Zheng Zhou Tao Zhoubirdblue zhouningnan Ziyi Yan zs634134578 zxylvlp zyguan zz-jason  "},
		{"url": "https://pingcap.com/docs/releases/101/",
		"title": "TiDB 1.0.1 Release Notes", 
		"content": " TiDB 1.0.1 Release Notes On November 1, 2017, TiDB 1.0.1 is released with the following updates:TiDB:  Support canceling DDL Job. Optimize the IN expression. Correct the result type of the Show statement. Support log slow query into a separate log file. Fix bugs.  TiKV:  Support flow control with write bytes. Reduce Raft allocation. Increase coprocessor stack size to 10MB. Remove the useless log from the coprocessor.  "},
		{"url": "https://pingcap.com/docs/releases/102/",
		"title": "TiDB 1.0.2 Release Notes", 
		"content": " TiDB 1.0.2 Release Notes On November 13, 2017, TiDB 1.0.2 is released with the following updates:TiDB:  Optimize the cost estimation of index point query Support the Alter Table Add Column (ColumnDef ColumnPosition) syntax Optimize the queries whose where conditions are contradictory Optimize the Add Index operation to rectify the progress and reduce repetitive operations Optimize the Index Look Join operator to accelerate the query speed for small data size Fix the issue with prefix index judgment  Placement Driver (PD):  Improve the stability of scheduling under exceptional situations  TiKV:  Support splitting table to ensure one region does not contain data from multiple tables Limit the length of a key to be no more than 4 KB More accurate read traffic statistics Implement deep protection on the coprocessor stack Fix the LIKE behavior and the do_div_mod bug  "},
		{"url": "https://pingcap.com/docs/releases/103/",
		"title": "TiDB 1.0.3 Release Notes", 
		"content": " TiDB 1.0.3 Release Notes On November 28, 2017, TiDB 1.0.3 is released with the following updates:TiDB  Optimize the performance in transaction conflicts scenario Add the TokenLimit option in the config file Output the default database in slow query logs Remove the DDL statement from query duration metrics Optimize the query cost estimation Fix the index prefix issue when creating tables Support pushing down the expressions for the Float type to TiKV Fix the issue that it is slow to add index for tables with discrete integer primary index Reduce the unnecessary statistics updates Fix a potential issue during the transaction retry  PD  Support adding more types of schedulers using API  TiKV  Fix the deadlock issue with the PD client Fix the issue that the wrong leader value is prompted for NotLeader Fix the issue that the chunk size is too large in the coprocessor  To upgrade from 1.0.2 to 1.0.3, follow the rolling upgrade order of PD -&amp;gt; TiKV -&amp;gt; TiDB."},
		{"url": "https://pingcap.com/docs/releases/104/",
		"title": "TiDB 1.0.4 Release Notes", 
		"content": " TiDB 1.0.4 Release Notes On December 11, 2017, TiDB 1.0.4 is released with the following updates:TiDB  Speed up the loading of the statistics when starting the tidb-server Improve the performance of the show variables statement Fix a potential issue when using the Add Index statement to handle the combined indexes Fix a potential issue when using the Rename Table statement to move a table to another database Accelerate the effectiveness for the Alter/Drop User statement  TiKV  Fix a possible performance issue when a snapshot is applied  Fix the performance issue for reverse scan after removing a lot of data Fix the wrong encoded result for the Decimal type under special circumstances  To upgrade from 1.0.3 to 1.0.4, follow the rolling upgrade order of PD -&amp;gt; TiKV -&amp;gt; TiDB."},
		{"url": "https://pingcap.com/docs/releases/105/",
		"title": "TiDB 1.0.5 Release Notes", 
		"content": " TiDB 1.0.5 Release Notes On December 26, 2017, TiDB 1.0.5 is released with the following updates:TiDB  Add the max value for the current Auto_Increment ID in the Show Create Table statement. Fix a potential goroutine leak. Support outputting slow queries into a separate file. Load the TimeZone variable from TiKV when creating a new session. Support the schema state check so that the Show Create Tableand Analyze statements process the public table/index only. The set transaction read only should affect the tx_read_only variable. Clean up incremental statistic data when rolling back. Fix the issue of missing index length in the Show Create Table statement.  PD  Fix the issue that the leaders stop balancing under some circumstances.  869 874  Fix potential panic during bootstrapping.  TiKV  Fix the issue that it is slow to get the CPU ID using the get_cpuid function. Support the dynamic-level-bytes parameter to improve the space collection situation.  To upgrade from 1.0.4 to 1.0.5, follow the rolling upgrade order of PD -&amp;gt; TiKV -&amp;gt; TiDB."},
		{"url": "https://pingcap.com/docs/releases/106/",
		"title": "TiDB 1.0.6 Release Notes", 
		"content": " TiDB 1.0.6 Release Notes On January 08, 2018, TiDB 1.0.6 is released with the following updates:TiDB:  Support the Alter Table Auto_Increment syntax Fix the bug in Cost Based computation and the Null Json issue in statistics Support the extension syntax to shard the implicit row ID to avoid write hot spot for a single table Fix a potential DDL issue Consider the timezone setting in the curtime, sysdate and curdate functions Support the SEPARATOR syntax in the GROUP_CONCAT function Fix the wrong return type issue of the GROUP_CONCAT function.  PD:  Fix store selection problem of hot-region scheduler  TiKV: None.To upgrade from 1.0.5 to 1.0.6, follow the rolling upgrade order of PD -&amp;gt; TiKV -&amp;gt; TiDB."},
		{"url": "https://pingcap.com/docs/releases/107/",
		"title": "TiDB 1.0.7 Release Notes", 
		"content": " TiDB 1.0.7 Release Notes On January 22, 2018, TiDB 1.0.7 is released with the following updates:TiDB:  Optimize the FIELD_LIST command Fix data race of the information schema Avoid adding read-only statements to history Add the session variable to control the log query Fix the resource leak issue in statistics Fix the goroutine leak issue Add schema info API for the http status server Fix an issue about IndexJoin Update the behavior when RunWorker is false in DDL Improve the stability of test results in statistics Support PACK_KEYS syntax for the CREATE TABLE statement Add row_id column for the null pushdown schema to optimize performance  PD:  Fix possible scheduling loss issue in abnormal conditions Fix the compatibility issue with proto3 Add the log  TiKV:  Support Table Scan Support the remote mode in tikv-ctl Fix the format compatibility issue of tikv-ctl proto Fix the loss of scheduling command from PD Add timeout in Push metric  To upgrade from 1.0.6 to 1.0.7, follow the rolling upgrade order of PD -&amp;gt; TiKV -&amp;gt; TiDB."},
		{"url": "https://pingcap.com/docs/releases/108/",
		"title": "TiDB 1.0.8 Release Notes", 
		"content": " TiDB 1.0.8 Release Notes On Feburary 11, 2018, TiDB 1.0.8 is released with the following updates:TiDB:  Fix issues in the Outer Join result in some scenarios Optimize the performance of the InsertIntoIgnore statement Fix the issue in the ShardRowID option Add limitation (Configurable, the default value is 5000) to the DML statements number within a transaction Fix an issue in the Table/Column aliases returned by the Prepare statement Fix an issue in updating statistics delta Fix a panic error in the Drop Column statement Fix an DML issue when running the Add Column After statement Improve the stability of the GC process by ignoring the regions with GC errors Run GC concurrently to accelerate the GC process Provide syntax support for the CREATE INDEX statement  PD:  Reduce the lock overheat of the region heartbeats Fix the issue that a hot region scheduler selects the wrong Leader  TiKV:  Use DeleteFilesInRanges to clear stale data and improve the TiKV starting speed Using Decimal in Coprocessor sum Sync the metadata of the received Snapshot compulsorily to ensure its safety  To upgrade from 1.0.7 to 1.0.8, follow the rolling upgrade order of PD -&amp;gt; TiKV -&amp;gt; TiDB."},
		{"url": "https://pingcap.com/docs-cn/releases/11alpha/",
		"title": "TiDB 1.1 Alpha Release Notes", 
		"content": " TiDB 1.1 Alpha Release Notes 2018 年 1 月 19 日，TiDB 发布 1.1 Alpha 版。该版本对 MySQL 兼容性、SQL 优化器、系统稳定性、性能做了大量的工作。TiDB：  SQL parser  兼容更多语法  SQL 查询优化器  统计信息减小内存占用 优化统计信息启动时载入的时间 更精确的代价估算 使用 Count-Min Sketch 更精确地估算点查的代价 支持更复杂的条件，更充分使用索引  SQL 执行器  使用 Chunk 结构重构所有执行器算子，提升分析型语句执行性能，减少内存占用 优化 INSERT INGORE 语句性能 下推更多的类型和函数 支持更多的 SQL_MODE 优化 Load Data 性能，速度提升 10 倍 优化 Use Database 性能 支持对物理算子内存使用进行统计  Server  支持 PROXY protocol   PD：  增加更多的 API 支持 TLS 给 Simulator 增加更多的 case 调度适应不同的 Region size Fix 了一些调度的 bug  TiKV：  支持 Raft learner 优化 Raft Snapshot，减少 IO 开销 支持 TLS 优化 RocksDB 配置，提升性能 优化 Coprocessor count (*) 和点查 unique index 的性能 增加更多的 Failpoint 以及稳定性测试 case 解决 PD 和 TiKV 之间重连的问题 增强数据恢复工具 TiKV-CTL 的功能 Region 支持按 table 进行分裂 支持 Delete Range 功能 支持设置 snapshot 导致的 IO 上限 完善流控机制  "},
		{"url": "https://pingcap.com/docs/releases/11alpha/",
		"title": "TiDB 1.1 Alpha Release Notes", 
		"content": " TiDB 1.1 Alpha Release Notes On January 19, 2018, TiDB 1.1 Alpha is released. This release has great improvement in MySQL compatibility, SQL optimization, stability, and performance.TiDB:  SQL parser  Support more syntax  SQL query optimizer  Use more compact structure to reduce statistics info memory usage Speed up loading statistics info when starting tidb-server Provide more accurate query cost evaluation Use Count-Min Sketch to evaluate the cost of queries using unique index more accurately Support more complex conditions to make full use of index  SQL executor  Refactor all executor operators using Chunk architecture, improve the execution performance of analytical statements and reduce memory usage Optimize performance of the INSERT INGORE statement Push down more types and functions to TiKV Support more SQL_MODE Optimize the Load Data performance to increase the speed by 10 times Optimize the Use Database performance Support statistics on the memory usage of physical operators  Server  Support the PROXY protocol   PD:  Add more APIs Support TLS Add more cases for scheduling Simulator Schedule to adapt to different Region sizes Fix some bugs about scheduling  TiKV:  Support Raft learner Optimize Raft Snapshot and reduce the IO overhead Support TLS Optimize the RocksDB configuration to improve performance Optimize count (*) and query performance of unique index in Coprocessor Add more failpoints and stability test cases Solve the reconnection issue between PD and TiKV Enhance the features of the data recovery tool TiKV-CTL Support splitting according to table in Region Support the Delete Range feature Support setting the IO limit caused by snapshot Improve the flow control mechanism  "},
		{"url": "https://pingcap.com/docs-cn/op-guide/ansible-deployment/",
		"title": "TiDB Ansible 部署方案", 
		"content": " TiDB Ansible 部署方案 概述 Ansible 是一款自动化运维工具，TiDB-Ansible 是 PingCAP 基于 Ansible playbook 功能编写的集群部署工具。使用 TiDB-Ansible 可以快速部署一个完整的 TiDB 集群（包括 PD、TiDB、TiKV 和集群监控模块)。本部署工具可以通过配置文件设置集群拓扑，一键完成以下各项运维工作： 初始化操作系统参数 部署组件 滚动升级，滚动升级时支持模块存活检测 数据清理 环境清理 配置监控模块  准备机器  部署目标机器若干 建议 4 台及以上，TiKV 至少 3 实例，且与 TiDB、PD 模块不位于同一主机，详见部署建议。 推荐安装 CentOS 7.3 及以上版本 Linux 操作系统，x86_64 架构(amd64)，数据盘请使用 ext4 文件系统，挂载 ext4 文件系统时请添加 nodelalloc 挂载参数，可参考数据盘 ext4 文件系统挂载参数。 机器之间内网互通，防火墙如 iptables 等请在部署时关闭。 机器的时间、时区设置一致，开启 NTP 服务且在正常同步时间，可参考如何检测 NTP 服务是否正常。 创建 tidb 普通用户作为程序运行用户，tidb 用户可以免密码 sudo 到 root 用户，可参考如何配置 ssh 互信及 sudo 免密码。   注：使用 Ansible 方式部署时，TiKV 及 PD 节点数据目录所在磁盘请使用 SSD 磁盘，否则无法通过检测。 如果仅验证功能，建议使用 Docker Compose 部署方案单机进行测试。 部署中控机一台: 中控机可以是部署目标机器中的某一台。 推荐安装 CentOS 7.3 及以上版本 Linux 操作系统(默认包含 Python 2.7)。 该机器需开放外网访问，用于下载 TiDB 及相关软件安装包。 配置 ssh authorized_key 互信，在中控机上可以使用 tidb 用户免密码 ssh 登录到部署目标机器，可参考如何配置 ssh 互信及 sudo 免密码。   在中控机器上安装 Ansible 及其依赖 请按以下方式在 CentOS 7 系统的中控机上安装 Ansible。 通过 epel 源安装， 会自动安装 Ansible 相关依赖(如 Jinja2==2.7.2 MarkupSafe==0.11)，安装完成后，可通过 ansible --version 查看版本，请务必确认是 Ansible 2.4 及以上版本，否则会有兼容问题。# yum install epel-release  # yum install ansible curl  # ansible --version  ansible 2.4.2.0  其他系统可参考 如何安装 Ansible。 在中控机器上下载 TiDB-Ansible 以 tidb 用户登录中控机并进入 /home/tidb 目录，使用以下命令从 Github TiDB-Ansible 项目 上下载 TiDB-Ansible 相应版本，默认的文件夹名称为 tidb-ansible。下载 GA 版本：cd /home/tidb git clone -b release-1.0 https://github.com/pingcap/tidb-ansible.git 或下载 master 版本：cd /home/tidb git clone https://github.com/pingcap/tidb-ansible.git  注： 生产环境请下载 GA 版本部署 TiDB。 分配机器资源，编辑 inventory.ini 文件 inventory.ini 文件路径为 tidb-ansible/inventory.ini。标准 TiDB 集群需要 6 台机器: 2 个 TiDB 节点 3 个 PD 节点 3 个 TiKV 节点，第一台 TiDB 机器同时用作监控机  单机单 TiKV 实例集群拓扑如下    Name Host IP Services     node1 172.16.10.1 PD1, TiDB1   node2 172.16.10.2 PD2, TiDB2   node3 172.16.10.3 PD3   node4 172.16.10.4 TiKV1   node5 172.16.10.5 TiKV2   node6 172.16.10.6 TiKV3    [tidb_servers] 172.16.10.1 172.16.10.2 [pd_servers] 172.16.10.1 172.16.10.2 172.16.10.3 [tikv_servers] 172.16.10.4 172.16.10.5 172.16.10.6 [monitoring_servers] 172.16.10.1 [grafana_servers] 172.16.10.1 [monitored_servers] 172.16.10.1 172.16.10.2 172.16.10.3 172.16.10.4 172.16.10.5 172.16.10.6 单机多 TiKV 实例集群拓扑如下(以两实例为例)    Name Host IP Services     node1 172.16.10.1 PD1, TiDB1   node2 172.16.10.2 PD2, TiDB2   node3 172.16.10.3 PD3   node4 172.16.10.4 TiKV1-1, TiKV1-2   node5 172.16.10.5 TiKV2-1, TiKV2-2   node6 172.16.10.6 TiKV3-1, TiKV3-2    [tidb_servers] 172.16.10.1 172.16.10.2 [pd_servers] 172.16.10.1 172.16.10.2 172.16.10.3 [tikv_servers] TiKV1-1 ansible_host=172.16.10.4 deploy_dir=/data1/deploy tikv_port=20171 labels=&amp;#34;host=tikv1&amp;#34; TiKV1-2 ansible_host=172.16.10.4 deploy_dir=/data2/deploy tikv_port=20172 labels=&amp;#34;host=tikv1&amp;#34; TiKV2-1 ansible_host=172.16.10.5 deploy_dir=/data1/deploy tikv_port=20171 labels=&amp;#34;host=tikv2&amp;#34; TiKV2-2 ansible_host=172.16.10.5 deploy_dir=/data2/deploy tikv_port=20172 labels=&amp;#34;host=tikv2&amp;#34; TiKV3-1 ansible_host=172.16.10.6 deploy_dir=/data1/deploy tikv_port=20171 labels=&amp;#34;host=tikv3&amp;#34; TiKV3-2 ansible_host=172.16.10.6 deploy_dir=/data2/deploy tikv_port=20172 labels=&amp;#34;host=tikv3&amp;#34; [monitoring_servers] 172.16.10.1 [grafana_servers] 172.16.10.1 [monitored_servers] 172.16.10.1 172.16.10.2 172.16.10.3 172.16.10.4 172.16.10.5 172.16.10.6 ...... [pd_servers:vars] location_labels = [&amp;#34;host&amp;#34;]  服务配置文件参数调整 多实例情况下，需要修改 tidb-ansible/conf/tikv.yml 中的 end-point-concurrency 以及 block-cache-size 参数:  end-point-concurrency: 总数低于 CPU Vcores 即可 rocksdb defaultcf block-cache-size(GB) = MEM * 80% / TiKV 实例数量 * 30% rocksdb writecf block-cache-size(GB) = MEM * 80% / TiKV 实例数量 * 45% rocksdb lockcf block-cache-size(GB) = MEM * 80% / TiKV 实例数量 * 2.5% (最小 128 MB) raftdb defaultcf block-cache-size(GB) = MEM * 80% / TiKV 实例数量 * 2.5% (最小 128 MB)  如果多个 TiKV 实例部署在同一块物理磁盘上，需要修改 conf/tikv.yml 中的 capacity 参数:  capacity = (DISK - 日志空间) / TiKV 实例数量，单位为 GB    inventory.ini 变量调整 部署目录调整 部署目录通过 deploy_dir 变量控制，默认全局变量已设置为 /home/tidb/deploy，对所有服务生效。如数据盘挂载目录为 /data1，可设置为 /data1/deploy，样例如下:## Global variables [all:vars] deploy_dir = /data1/deploy 如为某一服务单独设置部署目录，可在配置服务主机列表时配置主机变量，以 TiKV 节点为例，其他服务类推，请务必添加第一列别名，以免服务混布时混淆。TiKV1-1 ansible_host=172.16.10.4 deploy_dir=/data1/deploy 其他变量调整    变量 含义     cluster_name 集群名称，可调整   tidb_version TiDB 版本，TiDB-Ansible 各分支默认已配置   deployment_method 部署方式，默认为 binary，可选 docker   process_supervision 进程监管方式，默认为 systemd，可选 supervise   timezone 修改部署目标机器时区，默认为 Asia/Shanghai，可调整，与 set_timezone 变量结合使用   set_timezone 默认为 True，即修改部署目标机器时区，关闭可修改为 False   enable_firewalld 开启防火墙，默认不开启   enable_ntpd 检测部署目标机器 NTP 服务，默认为 True，请勿关闭   set_hostname 根据 IP 修改部署目标机器主机名，默认为 False   enable_binlog 是否部署 pump 并开启 binlog，默认为 False，依赖 Kafka 集群，参见 zookeeper_addrs 变量   zookeeper_addrs binlog Kafka 集群的 zookeeper 地址   enable_slow_query_log TiDB 慢查询日志记录到单独文件({{ deploy_dir }}/log/tidb_slow_query.log)，默认为 False，记录到 tidb 日志   deploy_without_tidb KV 模式，不部署 TiDB 服务，仅部署 PD、TiKV 及监控服务，请将 inventory.ini 文件中 tidb_servers 主机组 IP 设置为空。    部署任务  ansible-playbook 执行 Playbook 时默认并发为 5，部署目标机器较多时可添加 -f 参数指定并发，如 ansible-playbook deploy.yml -f 10  确认 tidb-ansible/inventory.ini 文件中 ansible_user = tidb，本例使用 tidb 用户作为服务运行用户，配置如下：## Connection # ssh via root: # ansible_user = root # ansible_become = true # ansible_become_user = tidb # ssh via normal user ansible_user = tidb 执行以下命令如果所有 server 返回 tidb 表示 ssh 互信配置成功。ansible -i inventory.ini all -m shell -a &amp;#39;whoami&amp;#39; 执行以下命令如果所有 server 返回 root 表示 tidb 用户 sudo 免密码配置成功。ansible -i inventory.ini all -m shell -a &amp;#39;whoami&amp;#39; -b 执行 local_prepare.yml playbook，联网下载 TiDB binary 到中控机：ansible-playbook local_prepare.yml 初始化系统环境，修改内核参数ansible-playbook bootstrap.yml 部署 TiDB 集群软件ansible-playbook deploy.yml 启动 TiDB 集群ansible-playbook start.yml   如希望使用 root 用户远程连接部署，请参考使用 root 用户远程连接 TiDB Ansible 部署方案，不推荐使用该方式部署。 测试集群  测试连接 TiDB 集群，推荐在 TiDB 前配置负载均衡来对外统一提供 SQL 接口。  使用 MySQL 客户端连接测试，TCP 4000 端口是 TiDB 服务默认端口。mysql -u root -h 172.16.10.1 -P 4000 通过浏览器访问监控平台。地址：http://172.16.10.1:3000 默认帐号密码是：admin/admin  滚动升级   滚动升级 TiDB 服务，滚动升级期间不影响业务运行(最小环境 ：pd*3 、tidb*2、tikv*3) 如果集群环境中有 pump / drainer 服务，请先停止 drainer 后滚动升级 (升级 TiDB 时会升级 pump)。   自动下载 binary  修改 inventory.ini 中的 tidb_version 参数值，指定需要升级的版本号，本例指定升级的版本号为 v1.0.2tidb_version = v1.0.2 删除原有的 downloads 目录 tidb-ansible/downloads/rm -rf downloads 使用 playbook 下载 TiDB 1.0 版本 binary，自动替换 binary 到 tidb-ansible/resource/bin/ansible-playbook local_prepare.yml  手动下载 binary  除 “下载 binary” 中描述的方法之外，也可以手动下载 binary，解压后手动替换 binary 到 tidb-ansible/resource/bin/，请注意替换链接中的版本号wget http://download.pingcap.org/tidb-v1.0.0-linux-amd64-unportable.tar.gz  使用 Ansible 滚动升级  滚动升级 TiKV 节点( 只升级 TiKV 服务 )ansible-playbook rolling_update.yml --tags=tikv 滚动升级 PD 节点( 只升级单独 PD 服务 )ansible-playbook rolling_update.yml --tags=pd 滚动升级 TiDB 节点( 只升级单独 TiDB 服务 )ansible-playbook rolling_update.yml --tags=tidb 滚动升级所有服务ansible-playbook rolling_update.yml  常见运维操作汇总    任务 Playbook     启动集群 ansible-playbook start.yml   停止集群 ansible-playbook stop.yml   销毁集群 ansible-playbook unsafe_cleanup.yml (若部署目录为挂载点，会报错，可忽略）   清除数据(测试用) ansible-playbook unsafe_cleanup_data.yml   滚动升级 ansible-playbook rolling_update.yml   滚动升级 TiKV ansible-playbook rolling_update.yml --tags=tikv   滚动升级除 pd 外模块 ansible-playbook rolling_update.yml --skip-tags=pd   滚动升级监控组件 ansible-playbook rolling_update_monitor.yml    常见部署问题 如何下载安装指定版本 TiDB 如需安装 TiDB 1.0.4 版本，需要先下载 TiDB-Ansible release-1.0 分支，确认 inventory.ini 文件中 tidb_version = v1.0.4，安装步骤同上。从 github 下载 TiDB-Ansible release-1.0 分支:git clone -b release-1.0 https://github.com/pingcap/tidb-ansible.git 如何自定义端口 修改 inventory.ini 文件，在相应服务 IP 后添加以下主机变量即可：   组件 端口变量 默认端口 说明     TiDB tidb_port 4000 应用及 DBA 工具访问通信端口   TiDB tidb_status_port 10080 TiDB 状态信息上报通信端口   TiKV tikv_port 20160 TiKV 通信端口   PD pd_client_port 2379 提供 TiDB 和 PD 通信端口   PD pd_peer_port 2380 PD 集群节点间通信端口   pump pump_port 8250 pump 通信端口   prometheus prometheus_port 9090 Prometheus 服务通信端口   pushgateway pushgateway_port 9091 TiDB, TiKV, PD 监控聚合和上报端口   node_exporter node_exporter_port 9100 TiDB 集群每 …"},
		{"url": "https://pingcap.com/docs-cn/op-guide/binary-deployment/",
		"title": "TiDB Binary 部署方案详解", 
		"content": " TiDB Binary 部署指导 概述 一个完整的 TiDB 集群包括 PD，TiKV 以及 TiDB。启动顺序依次是 PD，TiKV 以及 TiDB。在关闭数据库服务时，请按照启动的相反顺序进行逐一关闭服务。阅读本章前，请先确保阅读 TiDB 整体架构及部署建议。本文档描述了三种场景的二进制部署方式： 快速了解和试用 TiDB，推荐使用单节点方式快速部署。 功能性测试 TiDB，推荐使用功能性测试部署。 生产环境使用 TiDB，推荐使用多节点集群模式部署。  TiDB 组件及默认端口 1. TiDB 数据库组件（必装）    组件 默认端口 协议 说明     ssh 22 TCP sshd 服务   TiDB 4000 TCP 应用及 DBA 工具访问通信端口   TiDB 10080 TCP TiDB 状态信息上报通信端口   TiKV 20160 TCP TiKV 通信端口   PD 2379 TCP 提供 TiDB 和 PD 通信端口   PD 2380 TCP PD 集群节点间通信端口    2. TiDB 数据库组件（选装）    组件 默认端口 协议 说明     Prometheus 9090 TCP Prometheus 服务通信端口   Pushgateway 9091 TCP TiDB, TiKV, PD 监控聚合和上报端口   Node_exporter 9100 TCP TiDB 集群每个节点的系统信息上报通信端口   Grafana 3000 TCP Web 监控服务对外服务和客户端(浏览器)访问端口   alertmanager 9093 TCP 告警服务端口    TiDB 安装前系统配置与检查 操作系统检查    配置 描述     支持平台 请查看和了解系统部署建议   文件系统 TiDB 部署环境推荐使用 ext4 文件系统   Swap 空间 TiDB 部署推荐关闭 Swap 空间   Disk Block Size 设置系统磁盘 Block 大小为 4096    网络与防火墙    配置 描述     防火墙 / 端口 请查看 TiDB 所需端口在各个节点之间是否能正常访问    操作系统参数    配置 说明     Nice Limits 系统用户 tidb 的 nice 值设置为缺省值 0   min_free_kbytes 在 sysctl.conf 中关于 vm.min_free_kbytes 的设置需要足够高   User Open Files Limit 对数据库管理员 tidb 的 open 文件数设置为 1000000   System Open File Limits 对系统的 open 文件数设置为 1000000   User Process Limits 在 limits.conf 配置的 tidb 用户的 nproc 为 4096   Address Space Limits 在 limits.conf 配置的 tidb 用户空间为 unlimited   File Size Limits 在 limits.conf 配置的 tidb 用户 fsize 为 unlimited   Disk Readahead 设置数据磁盘 readahead 至少为 4096   NTP 服务 为各个节点配置 NTP 时间同步服务   SELinux 关闭各个节点的 SELinux 服务   CPU Frequency Scaling TiDB 推荐打开 CPU 超频   Transparent Hugepages 针对 Red Hat 7+ 和 CentOS 7+ 系统, Transparent Hugepages 必须被设置为 always   I/O Scheduler 设置数据磁盘 I/0 Schedule 设置为 deadline 模式   vm.swappiness 设置 vm.swappiness = 0     注意：请联系系统管理员进行操作系统参数调整。 数据库运行用户设置    配置 说明     LANG 环境设定 设置 LANG = en_US.UTF8   TZ 时区设定 确保所有节点的时区 TZ 设置为一样的值    创建系统数据库运行账户 在 Linux 环境下，在每台安装节点上创建 tidb 作为数据库系统运行用户并设置集群节点之间的 ssh 互信访问。以下是一个示例，具体创建用户与开通 ssh 互信访问请联系系统管理员进行。# useradd tidb # usermod -a -G tidb tidb # su - tidb Last login: Tue Aug 22 12:06:23 CST 2017 on pts/2 -bash-4.2$ ssh-keygen -t rsa Generating public/private rsa key pair. Enter file in which to save the key (/home/tidb/.ssh/id_rsa): Created directory &amp;#39;/home/tidb/.ssh&amp;#39;. Enter passphrase (empty for no passphrase): Enter same passphrase again: Your identification has been saved in /home/tidb/.ssh/id_rsa. Your public key has been saved in /home/tidb/.ssh/id_rsa.pub. The key fingerprint is: 5a:00:e6:df:9e:40:25:2c:2d:e2:6e:ee:74:c6:c3:c1 tidb@t001 The key&amp;#39;s randomart image is: +--[ RSA 2048]----+ | oo. . | | .oo.oo | | . ..oo | | .. o o | | . E o S | | oo . = . | | o. * . o | | ..o . | | .. | +-----------------+ -bash-4.2$ cd .ssh -bash-4.2$ cat id_rsa.pub &amp;gt;&amp;gt; authorized_keys -bash-4.2$ chmod 644 authorized_keys -bash-4.2$ ssh-copy-id -i ~/.ssh/id_rsa.pub 192.168.1.100 下载官方 Binary TiDB 官方提供了支持 Linux 版本的二进制安装包，官方推荐使用 Redhat 7+、CentOS 7+ 以上版本的操作系统，不推荐在 Redhat 6、CentOS 6 上部署 TiDB 集群。操作系统：Linux ( Redhat 7+，CentOS 7+ ) 执行步骤：# 下载压缩包 wget http://download.pingcap.org/tidb-latest-linux-amd64.tar.gz wget http://download.pingcap.org/tidb-latest-linux-amd64.sha256 # 检查文件完整性，返回 ok 则正确 sha256sum -c tidb-latest-linux-amd64.sha256 # 解开压缩包 tar -xzf tidb-latest-linux-amd64.tar.gz cd tidb-latest-linux-amd64 单节点方式快速部署 在获取 TiDB 二进制文件包后，我们可以在单机上面，运行和测试 TiDB 集群，请按如下步骤依次启动 PD，TiKV，TiDB。 注意：以下启动各个应用程序组件实例的时候，请选择后台启动，避免前台失效后程序自动退出。 步骤一. 启动 PD：./bin/pd-server --data-dir=pd  --log-file=pd.log 步骤二. 启动 TiKV：./bin/tikv-server --pd=&amp;#34;127.0.0.1:2379&amp;#34;  --data-dir=tikv  --log-file=tikv.log 步骤三. 启动 TiDB：./bin/tidb-server --store=tikv  --path=&amp;#34;127.0.0.1:2379&amp;#34;  --log-file=tidb.log 步骤四. 使用 MySQL 客户端连接 TiDB:mysql -h 127.0.0.1 -P 4000 -u root -D test 功能性测试部署 如果只是对 TiDB 进行测试，并且机器数量有限，我们可以只启动一台 PD 测试整个集群。这里我们使用四个节点，部署一个 PD，三个 TiKV，以及一个 TiDB，各个节点以及所运行服务信息如下：   Name Host IP Services     node1 192.168.199.113 PD1, TiDB   node2 192.168.199.114 TiKV1   node3 192.168.199.115 TiKV2   node4 192.168.199.116 TiKV3    请按如下步骤依次启动 PD 集群，TiKV 集群以及 TiDB： 注意：以下启动各个应用程序组件实例的时候，请选择后台启动，避免前台失效后程序自动退出。 步骤一. 在 node1 启动 PD：./bin/pd-server --name=pd1  --data-dir=pd1  --client-urls=&amp;#34;http://192.168.199.113:2379&amp;#34;  --peer-urls=&amp;#34;http://192.168.199.113:2380&amp;#34;  --initial-cluster=&amp;#34;pd1=http://192.168.199.113:2380&amp;#34;  --log-file=pd.log 步骤二. 在 node2，node3，node4 启动 TiKV：./bin/tikv-server --pd=&amp;#34;192.168.199.113:2379&amp;#34;  --addr=&amp;#34;192.168.199.114:20160&amp;#34;  --data-dir=tikv1  --log-file=tikv.log ./bin/tikv-server --pd=&amp;#34;192.168.199.113:2379&amp;#34;  --addr=&amp;#34;192.168.199.115:20160&amp;#34;  --data-dir=tikv2  --log-file=tikv.log ./bin/tikv-server --pd=&amp;#34;192.168.199.113:2379&amp;#34;  --addr=&amp;#34;192.168.199.116:20160&amp;#34;  --data-dir=tikv3  --log-file=tikv.log 步骤三. 在 node1 启动 TiDB：./bin/tidb-server --store=tikv  --path=&amp;#34;192.168.199.113:2379&amp;#34;  --log-file=tidb.log 步骤四. 使用 MySQL 客户端连接 TiDB：mysql -h 192.168.199.113 -P 4000 -u root -D test 多节点集群模式部署 在生产环境中，我们推荐多节点部署 TiDB 集群，首先请参考部署建议。这里我们使用六个节点，部署三个 PD，三个 TiKV，以及一个 TiDB，各个节点以及所运行服务信息如下：   Name Host IP Services     node1 192.168.199.113 PD1, TiDB   node2 192.168.199.114 PD2   node3 192.168.199.115 PD3   node4 192.168.199.116 TiKV1   node5 192.168.199.117 TiKV2   node6 192.168.199.118 TiKV3    请按如下步骤依次启动 PD 集群，TiKV 集群以及 TiDB：步骤一 . 在 node1，node2，node3 依次启动 PD：./bin/pd-server --name=pd1  --data-dir=pd1  --client-urls=&amp;#34;http://192.168.199.113:2379&amp;#34;  --peer-urls=&amp;#34;http://192.168.199.113:2380&amp;#34;  --initial-cluster=&amp;#34;pd1=http://192.168.199.113:2380,pd2=http://192.168.199.114:2380,pd3=http://192.168.199.115:2380&amp;#34;  -L &amp;#34;info&amp;#34;  --log-file=pd.log ./bin/pd-server --name=pd2  --data-dir=pd2  --client-urls=&amp;#34;http://192.168.199.114:2379&amp;#34;  --peer-urls=&amp;#34;http://192.168.199.114:2380&amp;#34;  --initial-cluster=&amp;#34;pd1=http://192.168.199.113:2380,pd2=http://192.168.199.114:2380,pd3=http://192.168.199.115:2380&amp;#34;  --join=&amp;#34;http://192.168.199.113:2379&amp;#34;  -L &amp;#34;info&amp;#34;  --log-file=pd.log ./bin/pd-server --name=pd3  --data-dir=pd3  --client-urls=&amp;#34;http://192.168.199.115:2379&amp;#34;  --peer-urls=&amp;#34;http://192.168.199.115:2380&amp;#34;  --initial-cluster=&amp;#34;pd1=http://192.168.199.113:2380,pd2=http://192.168.199.114:2380,pd3=http://192.168.199.115:2380&amp;#34;  --join=&amp;#34;http://192.168.199.113:2379&amp;#34;  -L &amp;#34;info&amp;#34;  --log-file=pd.log 步骤二. 在 node4，node5，node6 启动 TiKV：./bin/tikv-server --pd=&amp;#34;192.168.199.113:2379,192.168.199.114:2379,192.168.199.115:2379&amp;#34;  --addr=&amp;#34;192.168.199.116:20160&amp;#34;  --data-dir=tikv1  --log-file=tikv.log ./bin/tikv-server --pd=&amp;#34;192.168.199.113:2379,192.168.199.114:2379,192.168.199.115:2379&amp;#34;  --addr=&amp;#34;192.168.199.117:20160&amp;#34;  --data-dir=tikv2  --log-file=tikv.log ./bin/tikv-server --pd=&amp;#34;192.168.199.113:2379,192.168.199.114:2379,192.168.199.115:2379&amp;#34;  --addr=&amp;#34;192.168.199.118:20160&amp;#34;  --data-dir=tikv3  --log-file=tikv.log 步骤三. 在 node1 启动 TiDB：./bin/tidb-server --store=tikv  --path=&amp;#34;192.168.199.113:2379,192.168.199.114:2379,192.168.199.115:2379&amp;#34;  --log-file=tidb.log 步骤四. 使用 MySQL 客户端连接 TiDB：mysql -h 192.168.199.113 -P 4000 -u root -D test  注意：在生产环境中启动 TiKV 时，建议使用 --config 参数指定配置文件路径，如果不设置这个参数，TiKV 不会读取配置文件。同样，在生产环境中部署 PD 时，也建议使用 --config 参数指定配置文件路径。 TiKV 调优参见：TiKV 性能参数调优。 注意：如果使用 nohup 在生产环境中启动集群，需要将启动命令放到一个脚本文件里面执行，否则会出现因为 Shell 退出导致 nohup 启动的进程也收到异常信号退出的问题，具体参考进程异常退出。 TiDB 监控和告警环境安装 安装部署监控和告警环境的系统信息如下：   Name Host IP Services     node1 192.168.199.113 node_export, pushgateway, Prometheus, Grafana   node2 192.168.199.114 node_export   node3 192.168.199.115 node_export   node4 192.168.199.116 node_export    获取二进制包 # 下载压缩包 wget https://github.com/prometheus/prometheus/releases/download/v1.5.2/prometheus-1.5.2.linux-amd64.tar.gz wget https://github.com/prometheus/node_exporter/releases/download/v0.14.0-rc.2/node_exporter-0.14.0-rc.2.linux-amd64.tar.gz wget https://grafanarel.s3.amazonaws.com/builds/grafana-4.1.2-1486989747.linux-x64.tar.gz wget https://github.com/prometheus/pushgateway/releases/download/v0.3.1/pushgateway-0.3.1.linux-amd64.tar.gz # 解开压缩包 tar -xzf …"},
		{"url": "https://pingcap.com/docs/trouble-shooting/",
		"title": "TiDB Cluster Troubleshooting Guide", 
		"content": " TiDB Cluster Troubleshooting Guide You can use this guide to help you diagnose and solve basic problems while using TiDB. If your problem is not resolved, please collect the following information and create an issue: The exact error message and the operations while the error occurs The state of all the components The error / fatal / panic information in the log of the component that reports the error The configuration and deployment topology The TiDB component related issue in dmesg  For other information, see Frequently Asked Questions (FAQ).Cannot connect to the database  Make sure all the services are started, including tidb-server, pd-server, and tikv-server. Use the ps command to check if all the processes are running. If a certain process is not running, see the following corresponding sections to diagnose and solve the issue. If all the processes are running, check the tidb-server log to see if the following messages are displayed:  InfomationSchema is out of date: This message is displayed if the tikv-server cannot be connected. Check the state and log of pd-server and tikv-server. panic: This message is displayed if there is an issue with the program. Please provide the detailed panic log and create an issue.    If the data is cleared and the services are re-deployed, make sure that: All the data in tikv-server and pd-server are cleared. The specific data is stored in tikv-server and the metadata is stored in pd-server. If only one of the two servers is cleared, the data will be inconsistent. After the data in pd-server and tikv-server are cleared and the pd-server and tikv-server are restarted, the tidb-server must be restarted too. The cluster ID is randomly allocated when the pd-server is initialized. So when the cluster is re-deployed, the cluster ID changes and you need to restart the tidb-server to get the new cluster ID.   Cannot start tidb-server See the following for the situations when the tidb-server cannot be started: Error in the startup parameters. See the TiDB configuration and options. The port is occupied. Use the lsof -i:port command to show all the networking related to a given port and make sure the port to start the tidb-server is not occupied. Cannot connect to pd-server. Check if the network between TiDB and PD is running smoothly, including whether the network can be pinged or if there is any issue with the Firewall configuration. If there is no issue with the network, check the state and log of the pd-server process.   Cannot start tikv-server See the following for the situations when the tikv-server cannot be started: Error in the startup parameters: See the TiKV configuration and options. The port is occupied: Use the lsof -i:port command to show all the networking related to a given port and make sure the port to start the tikv-server is not occupied. Cannot connect to pd-server.  Check if the network between TiDB and PD is running smoothly, including whether the network can be pinged or if there is any issue with the Firewall configuration. If there is no issue with the network, check the state and log of the pd-server process.  The file is occupied. Do not open two TiKV files on one database file directory.  Cannot start pd-server See the following for the situations when the pd-server cannot be started: Error in the startup parameters. See the PD configuration and options. The port is occupied. Use the lsof -i:port command to show all the networking related to a given port and make sure the port to start the pd-server is not occupied.  The TiDB/TiKV/PD process aborts unexpectedly  Is the process started on the foreground? The process might exit because the client aborts. Is nohup+&amp;amp; run in the command line? This might cause the process to abort because it receives the hup signal. It is recommended to write and run the startup command in a script.  TiDB panic Please provide panic log and create an issue.The connection is rejected Make sure the network parameters of the operating system are correct, including but not limited to: The port in the connection string is consistent with the tidb-server starting port. The firewall is configured correctly.  Open too many files Before starting the process, make sure the result of ulimit -n is large enough. It is recommended to set the value to unlimited or larger than 1000000.Database access times out and the system load is too high Provide the following information: The deployment topology  How many tidb-server/pd-server/tikv-server instances are deployed? How are these instances distributed in the machines?  The hardware configuration of the machines where these instances are deployed:  The number of CPU cores The size of the memory The type of the disk (SSD or Hard Drive Disk) Are they physical machines or virtual machines?  Are there other services besides the TiDB cluster? Are the pd-servers and tikv-servers deployed separately? What is the current operation? Check the CPU thread name using the top -H command. Are there any exceptions in the network or IO monitoring data recently?  "},
		{"url": "https://pingcap.com/docs/tispark/tispark-user-guide/",
		"title": "TiDB Connector for Spark User Guide", 
		"content": " TiDB Connector for Spark User Guide The TiDB Connector for Spark is a thin layer built for running Apache Spark on top of TiDB/TiKV to answer the complex OLAP queries. It takes advantages of both the Spark platform and the distributed TiKV cluster and seamlessly glues to TiDB, the distributed OLTP database, to provide a Hybrid Transactional/Analytical Processing (HTAP) solution to serve as a one-stop solution for both online transactions and analysis.The TiDB Connector for Spark depends on the TiKV cluster and the PD cluster. You also need to set up a Spark cluster. This document provides a brief introduction to how to setup and use the TiDB Connector for Spark. It requires some basic knowledge of Apache Spark. For more information, see Spark website.Overview The TiDB Connector for Spark is an OLAP solution that runs Spark SQL directly on TiKV, the distributed storage engine. TiDB Connector for Spark integrates with Spark Catalyst Engine deeply. It provides precise control of the computing, which allows Spark read data from TiKV efficiently. It also supports index seek, which improves the performance of the point query execution significantly. It utilizes several strategies to push down the computing to reduce the size of dataset handling by Spark SQL, which accelerates the query execution. It also uses the TiDB built-in statistical information for the query plan optimization. From the data integration point of view, TiDB Connector for Spark and TiDB serve as a solution runs both transaction and analysis directly on the same platform without building and maintaining any ETLs. It simplifies the system architecture and reduces the cost of maintenance. also, you can deploy and utilize tools from the Spark ecosystem for further data processing and manipulation on TiDB. For example, using the TiDB Connector for Spark for data analysis and ETL; retrieving data from TiKV as a machine learning data source; generating reports from the scheduling system and so on.  Environment setup  The current version of the TiDB Connector for Spark supports Spark 2.1. For Spark 2.0 and Spark 2.2, it has not been fully tested yet. It does not support any versions earlier than 2.0. The TiDB Connector for Spark requires JDK 1.8+ and Scala 2.11 (Spark2.0 + default Scala version). The TiDB Connector for Spark runs in any Spark mode such as YARN, Mesos, and Standalone.  Recommended configuration Deployment of TiKV and the TiDB Connector for Spark clusters Configuration of the TiKV cluster For independent deployment of TiKV and the TiDB Connector for Spark, it is recommended to refer to the following recommendations Hardware configuration For general purposes, please refer to the TiDB and TiKV hardware configuration recommendations. If the usage is more focused on the analysis scenarios, you can increase the memory of the TiKV nodes to at least 64G.  TiKV parameters (default)[server] end-point-concurrency = 8 # For OLAP scenarios, consider increasing this parameter [raftstore] sync-log = false [rocksdb] max-background-compactions = 6 max-background-flushes = 2 [rocksdb.defaultcf] block-cache-size = &amp;#34;10GB&amp;#34; [rocksdb.writecf] block-cache-size = &amp;#34;4GB&amp;#34; [rocksdb.raftcf] block-cache-size = &amp;#34;1GB&amp;#34; [rocksdb.lockcf] block-cache-size = &amp;#34;1GB&amp;#34; [storage] scheduler-worker-pool-size = 4  Configuration of the independent deployment of the Spark cluster and the TiDB Connector for Spark cluster See the Spark official website for the detail hardware recommendations.The following is a short overview of the TiDB Connector for Spark configuration.It is recommended to allocate 32G memory for Spark. Please reserve at least 25% of the memory for the operating system and buffer cache.It is recommended to provision at least 8 to 16 cores on per machine for Spark. Initially, you can assign all the CPU cores to Spark.See the official configuration on the Spark website. The following is an example based on the spark-env.sh configuration:SPARK_EXECUTOR_MEMORY = 32g SPARK_WORKER_MEMORY = 32g SPARK_WORKER_CORES = 8 Hybrid deployment configuration for the TiDB Connector for Spark and TiKV cluster For the hybrid deployment of the TiDB Connector for Spark and TiKV, add the TiDB Connector for Spark required resources to the TiKV reserved resources, and allocate 25% of the memory for the system.Deploy the TiDB Connector for Spark Download the TiDB Connector for Spark&amp;rsquo;s jar package here.Deploy the TiDB Connector for Spark on the existing Spark cluster Running TiDB Connector for Spark on an existing Spark cluster does not require a reboot of the cluster. You can use Spark&amp;rsquo;s --jars parameter to introduce the TiDB Connector for Spark as a dependency:spark-shell --jars $PATH/tispark-0.1.0.jar If you want to deploy TiDB Connector for Spark as a default component, simply place the TiDB Connector for Spark jar package into the jars path for each node of the Spark cluster and restart the Spark cluster:${SPARK_INSTALL_PATH}/jars In this way, you can use either Spark-Submit or Spark-Shell to use the TiDB Connector for Spark directly.Deploy TiDB Connector for Spark without the Spark cluster If you do not have a Spark cluster, we recommend using the standalone mode. To use the Spark Standalone model, you can simply place a compiled version of Spark on each node of the cluster. If you encounter problems, see its official website. And you are welcome to file an issue on our GitHub.Download and install You can download Apache SparkFor the Standalone mode without Hadoop support, use Spark 2.1.x and any version of Pre-build with Apache Hadoop 2.x with Hadoop dependencies. If you need to use the Hadoop cluster, please choose the corresponding Hadoop version. You can also choose to build from the source code to match the previous version of the official Hadoop 2.6. Please note that the TiDB Connector for Spark currently only supports Spark 2.1.x version.Suppose you already have a Spark binaries, and the current PATH is SPARKPATH, please copy the TiDB Connector for Spark jar package to the ${SPARKPATH}/jars directory.Start a Master node Execute the following command on the selected Spark Master node:cd $SPARKPATH ./sbin/start-master.sh  After the above step is completed, a log file will be printed on the screen. Check the log file to confirm whether the Spark-Master is started successfully. You can open the http://spark-master-hostname:8080 to view the cluster information (if you does not change the Spark-Master default port number). When you start Spark-Slave, you can also use this panel to confirm whether the Slave is joined to the cluster.Start a Slave node Similarly, you can start a Spark-Slave node with the following command:./sbin/start-slave.sh spark://spark-master-hostname:7077 After the command returns, you can see if the Slave node is joined to the Spark cluster correctly from the panel as well. Repeat the above command at all Slave nodes. After all Slaves are connected to the master, you have a Standalone mode Spark cluster.Spark SQL shell and JDBC server If you want to use JDBC server and interactive SQL shell, please copy start-tithriftserver.sh stop-tithriftserver.sh to your Spark&amp;rsquo;s sbin folder and tispark-sql to the bin folder.To start interactive shell:./bin/tispark-sql To use Thrift Server, you can start it similar way as default Spark Thrift Server:./sbin/start-tithriftserver.sh And stop it like below:./sbin/stop-tithriftserver.sh Demo Assuming that you have successfully started the TiDB Connector for Spark cluster as described above, here&amp;rsquo;s a quick introduction to how to use Spark SQL for OLAP analysis. Here we use a table named lineitem in the tpch database as an example.Assuming that your PD node is located at 192.168.1.100, port 2379, add the following command to $SPARK_HOME/conf/spark-defaults.conf:spark.tispark.pd.addresses 192.168.1.100:2379 And then enter the following command in the Spark-Shell:import org.apache.spark.sql.TiContext val ti = new …"},
		{"url": "https://pingcap.com/docs/sql/dml/",
		"title": "TiDB Data Manipulation Language", 
		"content": " TiDB Data Manipulation Language Data manipulation language (DML) is a family of syntax elements used for selecting, inserting, deleting and updating data in a database.SELECT SELECT is used to retrieve rows selected from one or more tables.Syntax SELECT [ALL | DISTINCT | DISTINCTROW ] [HIGH_PRIORITY] [SQL_CACHE | SQL_NO_CACHE] [SQL_CALC_FOUND_ROWS] select_expr [, select_expr ...] [FROM table_references [WHERE where_condition] [GROUP BY {col_name | expr | position} [ASC | DESC], ...] [HAVING where_condition] [ORDER BY {col_name | expr | position} [ASC | DESC], ...] [LIMIT {[offset,] row_count | row_count OFFSET offset}] [FOR UPDATE | LOCK IN SHARE MODE]] Description of the syntax elements    Syntax Element Description     ALL, DISTINCT, DISTINCTROW The ALL, DISTINCT/DISTINCTROW modifiers specify whether duplicate rows should be returned. ALL (the default) specifies that all matching rows should be returned.   HIGH_PRIORITY HIGH_PRIORITY gives the current statement higher priority than other statements.   SQL_CACHE, SQL_NO_CACHE, SQL_CALC_FOUND_ROWS To guarantee compatibility with MySQL, TiDB parses these three modifiers, but will ignore them.   select_expr Each select_expr indicates a column to retrieve. including the column names and expressions. * represents all the columns.   |FROM table_references The FROM table_references clause indicates the table (such as (select * from t;)) , or tables(such as select * from t1 join t2;)&#39;) or even 0 tables (such asselect 1+1 from dual;(which is equivalent toselect 1+1;&amp;lsquo;)) from which to retrieve rows.   WHERE where_condition The WHERE clause, if given, indicates the condition or conditions that rows must satisfy to be selected. The result contains only the data that meets the condition(s).   GROUP BY The GROUP BY statement is used to group the result-set.   HAVING where_condition The HAVING clause and the WHERE clause are both used to filter the results. The HAVING clause filters the results of GROUP BY, while the WHERE clause filter the results before aggregationã   ORDER BY The ORDER BY clause is used to sort the data in ascending or descending order, based on columns, expressions or items in the select_expr list.   LIMIT The LIMIT clause can be used to constrain the number of rows. LIMIT takes one or two numeric arguments. With one argument, the argument specifies the maximum number of rows to return, the first row to return is the first row of the table by default; with two arguments, the first argument specifies the offset of the first row to return, and the second specifies the maximum number of rows to return.   FOR UPDATE All the data in the result sets are read-locked, in order to detect the concurrent updates. TiDB uses the Optimistic Transaction Model. The transaction conflicts are detected in the commit phase instead of statement execution phase. while executing the SELECT FOR UPDATE statement, if there are other transactions trying to update relavant data, the SELECT FOR UPDATE transaction will fail.   LOCK IN SHARE MODE To guarantee compatibility, TiDB parses these three modifiers, but will ignore them.    INSERT INSERT inserts new rows into an existing table. TiDB is compatible with all the INSERT syntaxes of MySQL.Syntax Insert Statement: INSERT [LOW_PRIORITY | DELAYED | HIGH_PRIORITY] [IGNORE] [INTO] tbl_name insert_values [ON DUPLICATE KEY UPDATE assignment_list] insert_values: [(col_name [, col_name] ...)] {VALUES | VALUE} (expr_list) [, (expr_list)] ... | SET assignment_list | [(col_name [, col_name] ...)] SELECT ... expr_list: expr [, expr] ... assignment: col_name = expr assignment_list: assignment [, assignment] ... Description of the syntax elements    Syntax Elements Description     LOW_PRIORITY LOW_PRIORITY gives the statement lower priority. TiDB lowers the priority of the current statement.   DELAYED To guarantee compatibility, TiDB parses this modifier, but will ignore it.   HIGH_PRIORITY HIGH_PRIORITY gives the current statement higher priority than other statements. TiDB raises the priority of the current statement.   IGNORE If IGNORE modifier is specified and there is a duplicate key error, the data cannot be inserted without an error.   tbl_name tbl_name is the table into which the rows should be inserted.   insert_values The insert_values is the value to be inserted. For more information, see insert_values.   ON DUPLICATE KEY UPDATE assignment_list If ON DUPLICATE KEY UPDATE is specified, and there is a conflict in a UNIQUE index or PRIMARY KEY, the data cannot be inserted, instead, the existing row will be updated using assignment_list.    insert_values You can use the following ways to specify the data set: Value List  Place the values to be inserted in a Value List.CREATE TABLE tbl_name ( a int, b int, c int ); INSERT INTO tbl_name VALUES(1,2,3),(4,5,6),(7,8,9); In the example above, (1,2,3),(4,5,6),(7,8,9) are the Value Lists enclosed within parentheses and separated by commas. Each Values List means a row of data, in this example, 3 rows are inserted. You can also specify the ColumnName List to insert rows to some of the columns. and contains exactly as many values as are to be inserted per row.INSERT INTO tbl_name (a,c) VALUES(1,2),(4,5),(7,8); In the example above, only the a and c columns are listed, the the b of each row will be set to Null. Assignment List  Insert the values by using Assignment Statements, for example:INSERT INTO tbl_name a=1, b=2, c=3; In this way, only one row of data can be inserted at a time, and the value of each column needs the assignment statement. Select Statement  The data set to be inserted is obtained using a SELECT statement. The column to be inserted into is obtained from the Schema in the SELECT statement.CREATE TABLE tbl_name1 ( a int, b int, c int ); INSERT INTO tbl_name SELECT * from tbl_name1; In the example above, the data is selected from tal_name1, and then inserted into tbl_name.DELETE DELETE is a DML statement that removes rows from a table. TiDB is compatible with all the DELETE syntaxes of MySQL except for PARTITION. There are two kinds of DELETE, Single-Table DELETE and Multiple-Table DELETE.Single-Table DELETE syntax The Single_Table DELETE syntax deletes rows from a single table.DELETE syntax DELETE [LOW_PRIORITY] [QUICK] [IGNORE] FROM tbl_name [WHERE where_condition] [ORDER BY ...] [LIMIT row_count] Multiple-Table DELETE syntax The Multiple_Table DELETE syntax deletes rows of multiple tables, and has the following two kinds of formats:DELETE [LOW_PRIORITY] [QUICK] [IGNORE] tbl_name[.*] [, tbl_name[.*]] ... FROM table_references [WHERE where_condition] DELETE [LOW_PRIORITY] [QUICK] [IGNORE] FROM tbl_name[.*] [, tbl_name[.*]] ... USING table_references [WHERE where_condition] Both of the two syntax formats can be used to delete multiple tables, or delete the selected results from multiple tables. There are still differences between the two formats. The first one will delete data of every table in the table list before FROM. The second one will delete the data of the tables in the table list which is after FROM and before USING.Description of the syntax elements    Syntax Elements Description     LOW_PRIORITY LOW_PRIORITY gives the statement lower priority. TiDB lowers the priority of the current statement.   QUICK To guarantee compatibility with MySQL, TiDB parses these three modifiers, but will ignore them.   IGNORE To guarantee compatibility with MySQL, TiDB parses these three modifiers, but will ignore them.   tbl_name the table names to be deleted   WHERE where_condition the Where expression, which deletes rows that meets the expression   ORDER BY To sort the data set which are to be deleted   LIMIT row_count the top number of rows to be deleted as specified inrow_count    Update UPDATE is used to update data of the tables.Syntax There are two kinds of UPDATE syntax, Single-table UPDATE and Multi-Table UPDATE.Single-table UPDATE UPDATE [LOW_PRIORITY] [IGNORE] table_reference SET assignment_list [WHERE …"},
		{"url": "https://pingcap.com/docs/sql/datatype/",
		"title": "TiDB Data Type", 
		"content": " TiDB Data Type Overview TiDB supports all the data types in MySQL except the Spatial type, including numeric type, string type, date &amp;amp; time type, and JSON type.The definition of the data type is: T(M[, D]). In this format: T indicates the specific data type. M indicates the maximum display width for integer types. For floating-point and fixed-point types, M is the total number of digits that can be stored (the precision). For string types, M is the maximum length. The maximum permissible value of M depends on the data type. D applies to floating-point and fixed-point types and indicates the number of digits following the decimal point (the scale). fsp applies to the TIME, DATETIME, and TIMESTAMP types and represents the fractional seconds precision. The fsp value, if given, must be in the range 0 to 6. A value of 0 signifies that there is no fractional part. If omitted, the default precision is 0.  Numeric types Overview TiDB supports all the MySQL numeric types, including: Integer Types (Exact Value) Floating-Point Types (Approximate Value) Fixed-Point Types (Exact Value)  Integer types (exact value) TiDB supports all the MySQL integer types, including INTEGER/INT, TINYINT, SMALLINT, MEDIUMINT, and BIGINT. For more information, see Numeric Type Overview in MySQL.Type definition Syntax:BIT[(M)] &amp;gt; The BIT data type. A type of BIT(M) enables storage of M-bit values. M can range from 1 to 64. TINYINT[(M)] [UNSIGNED] [ZEROFILL] &amp;gt; The TINYINT data type. The value range for signed: [-128, 127] and the range for unsigned is [0, 255]. BOOL, BOOLEAN &amp;gt; BOOLEAN and is equivalent to TINYINT(1). If the value is &amp;#34;0&amp;#34;, it is considered as False; otherwise, it is considered True. In TiDB, True is &amp;#34;1&amp;#34; and False is &amp;#34;0&amp;#34;. SMALLINT[(M)] [UNSIGNED] [ZEROFILL] &amp;gt; SMALLINT. The signed range is: [-32768, 32767], and the unsigned range is [0, 65535]. MEDIUMINT[(M)] [UNSIGNED] [ZEROFILL] &amp;gt; MEDIUMINT. The signed range is: [-8388608, 8388607], and the unsigned range is [0, 16777215]. INT[(M)] [UNSIGNED] [ZEROFILL] &amp;gt; INT. The signed range is: [-2147483648, 2147483647], and the unsigned range is [0, 4294967295]. INTEGER[(M)] [UNSIGNED] [ZEROFILL] &amp;gt; Same as INT. BIGINT[(M)] [UNSIGNED] [ZEROFILL] &amp;gt; BIGINT. The signed range is: [-9223372036854775808, 9223372036854775807], and the unsigned range is [0, 18446744073709551615]. The meaning of the fields:   Syntax Element Description     M the length of the type. Optional.   UNSIGNED UNSIGNED. If omitted, it is SIGNED.   ZEROFILL If you specify ZEROFILL for a numeric column, TiDB automatically adds the UNSIGNED attribute to the column.    Storage and range See the following for the requirements of the storage and minimum value/maximim value of each data type:   Type Storage Required (bytes) Minimum Value (Signed/Unsigned) Maximum Value (Signed/Unsigned)     TINYINT 1 -128 / 0 127 / 255   SMALLINT 2 -32768 / 0 32767 / 65535   MEDIUMINT 3 -8388608 / 0 8388607 / 16777215   INT 4 -2147483648 / 0 2147483647 / 4294967295   BIGINT 8 -9223372036854775808 / 0 9223372036854775807 / 18446744073709551615    Floating-point types (approximate value) TiDB supports all the MySQL floating-point types, including FLOAT, and DOUBLE. For more information, Floating-Point Types (Approximate Value) - FLOAT, DOUBLE in MySQL.Type definition Syntax:FLOAT[(M,D)] [UNSIGNED] [ZEROFILL] &amp;gt; A small (single-precision) floating-point number. Permissible values are -3.402823466E+38 to -1.175494351E-38, 0, and 1.175494351E-38 to 3.402823466E+38. These are the theoretical limits, based on the IEEE standard. The actual range might be slightly smaller depending on your hardware or operating system. DOUBLE[(M,D)] [UNSIGNED] [ZEROFILL] &amp;gt; A normal-size (double-precision) floating-point number. Permissible values are -1.7976931348623157E+308 to -2.2250738585072014E-308, 0, and 2.2250738585072014E-308 to 1.7976931348623157E+308. These are the theoretical limits, based on the IEEE standard. The actual range might be slightly smaller depending on your hardware or operating system. DOUBLE PRECISION [(M,D)] [UNSIGNED] [ZEROFILL], REAL[(M,D)] [UNSIGNED] [ZEROFILL] &amp;gt; Synonym for DOUBLE. FLOAT(p) [UNSIGNED] [ZEROFILL] &amp;gt; A floating-point number. p represents the precision in bits, but TiDB uses this value only to determine whether to use FLOAT or DOUBLE for the resulting data type. If p is from 0 to 24, the data type becomes FLOAT with no M or D values. If p is from 25 to 53, the data type becomes DOUBLE with no M or D values. The range of the resulting column is the same as for the single-precision FLOAT or double-precision DOUBLE data types described earlier in this section. The meaning of the fields:   Syntax Element Description     M the total number of digits   D the number of digits following the decimal point   UNSIGNED UNSIGNED. If omitted, it is SIGNED.   ZEROFILL If you specify ZEROFILL for a numeric column, TiDB automatically adds the UNSIGNED attribute to the column.    Storage See the following for the requirements of the storage:   Data Type Storage Required (bytes)     FLOAT 4   FLOAT(p) If 0 &amp;lt;= p &amp;lt;= 24, it is 4; if 25 &amp;lt;= p &amp;lt;= 53, it is 8   DOUBLE 8    Fixed-point types (exact value) TiDB supports all the MySQL floating-point types, including DECIMAL, and NUMERIC. For more information, Fixed-Point Types (Exact Value) - DECIMAL, NUMERIC in MySQL.Type definition SyntaxDECIMAL[(M[,D])] [UNSIGNED] [ZEROFILL] &amp;gt; A packed “exact” fixed-point number. M is the total number of digits (the precision), and D is the number of digits after the decimal point (the scale). The decimal point and (for negative numbers) the - sign are not counted in M. If D is 0, values have no decimal point or fractional part. The maximum number of digits (M) for DECIMAL is 65. The maximum number of supported decimals (D) is 30. If D is omitted, the default is 0. If M is omitted, the default is 10. NUMERIC[(M[,D])] [UNSIGNED] [ZEROFILL] &amp;gt; Synonym for DECIMAL. The meaning of the fields:   Syntax Element Description     M the total number of digits   D the number of digits after the decimal point   UNSIGNED UNSIGNED. If omitted, it is SIGNED.   ZEROFILL If you specify ZEROFILL for a numeric column, TiDB automatically adds the UNSIGNED attribute to the column.    Date and time types Overview TiDB supports all the MySQL floating-point types, including DATE, DATETIME, TIMESTAMP, TIME, and YEAR. For more information, Date and Time Types in MySQL.Type definition Syntax:DATE &amp;gt; A date. The supported range is &amp;#39;1000-01-01&amp;#39; to &amp;#39;9999-12-31&amp;#39;. TiDB displays DATE values in &amp;#39;YYYY-MM-DD&amp;#39; format. DATETIME[(fsp)] &amp;gt; A date and time combination. The supported range is &amp;#39;1000-01-01 00:00:00.000000&amp;#39; to &amp;#39;9999-12-31 23:59:59.999999&amp;#39;. TiDB displays DATETIME values in &amp;#39;YYYY-MM-DD HH:MM:SS[.fraction]&amp;#39; format, but permits assignment of values to DATETIME columns using either strings or numbers. An optional fsp value in the range from 0 to 6 may be given to specify fractional seconds precision. If omitted, the default precision is 0. TIMESTAMP[(fsp)] &amp;gt; A timestamp. The range is &amp;#39;1970-01-01 00:00:01.000000&amp;#39; to &amp;#39;2038-01-19 03:14:07.999999&amp;#39;. An optional fsp value in the range from 0 to 6 may be given to specify fractional seconds precision. If omitted, the default precision is 0. An optional fsp value in the range from 0 to 6 may be given to specify fractional seconds precision. If omitted, the default precision is 0. TIME[(fsp)] &amp;gt; A time. The range is &amp;#39;-838:59:59.000000&amp;#39; to &amp;#39;838:59:59.000000&amp;#39;. TiDB displays TIME values in &amp;#39;HH:MM:SS[.fraction]&amp;#39; format. An optional fsp value in the range from 0 to 6 may be given to specify fractional seconds precision. If omitted, the default precision is 0. YEAR[(2|4)] &amp;gt; A year in two-digit or four-digit format. The default is the four-digit format. In four-digit format, values display as 1901 to 2155, and 0000. In two-digit …"},
		{"url": "https://pingcap.com/docs/op-guide/docker-compose/",
		"title": "TiDB Docker Compose", 
		"content": " TiDB Docker Compose Use docker-compose Note: If you are using docker-compose, you don&amp;rsquo;t need to create a Docker network and start TiDB,TiKV and PD containers separately. The following docker-compose.yml file is enough.version: &amp;#39;2&amp;#39; services: pd1: image: pingcap/pd expose: - &amp;#34;2379&amp;#34; - &amp;#34;2380&amp;#34; volumes: - /etc/localtime:/etc/localtime:ro command: - --name=pd1 - --client-urls=http://0.0.0.0:2379 - --peer-urls=http://0.0.0.0:2380 - --advertise-client-urls=http://pd1:2379 - --advertise-peer-urls=http://pd1:2380 - --initial-cluster=pd1=http://pd1:2380,pd2=http://pd2:2380,pd3=http://pd3:2380 privileged: true pd2: image: pingcap/pd expose: - &amp;#34;2379&amp;#34; - &amp;#34;2380&amp;#34; volumes: - /etc/localtime:/etc/localtime:ro command: - --name=pd2 - --client-urls=http://0.0.0.0:2379 - --peer-urls=http://0.0.0.0:2380 - --advertise-client-urls=http://pd2:2379 - --advertise-peer-urls=http://pd2:2380 - --initial-cluster=pd1=http://pd1:2380,pd2=http://pd2:2380,pd3=http://pd3:2380 privileged: true pd3: image: pingcap/pd expose: - &amp;#34;2379&amp;#34; - &amp;#34;2380&amp;#34; volumes: - /etc/localtime:/etc/localtime:ro command: - --name=pd3 - --client-urls=http://0.0.0.0:2379 - --peer-urls=http://0.0.0.0:2380 - --advertise-client-urls=http://pd3:2379 - --advertise-peer-urls=http://pd3:2380 - --initial-cluster=pd1=http://pd1:2380,pd2=http://pd2:2380,pd3=http://pd3:2380 privileged: true tikv1: image: pingcap/tikv expose: - &amp;#34;20160&amp;#34; volumes: - /etc/localtime:/etc/localtime:ro command: - --addr=0.0.0.0:20160 - --advertise-addr=tikv1:20160 - --data-dir=/var/tikv - --pd=pd1:2379,pd2:2379,pd3:2379 depends_on: - &amp;#34;pd1&amp;#34; - &amp;#34;pd2&amp;#34; - &amp;#34;pd3&amp;#34; entrypoint: /tikv-server privileged: true tikv2: image: pingcap/tikv expose: - &amp;#34;20160&amp;#34; volumes: - /etc/localtime:/etc/localtime:ro command: - --addr=0.0.0.0:20160 - --advertise-addr=tikv2:20160 - --data-dir=/var/tikv - --pd=pd1:2379,pd2:2379,pd3:2379 depends_on: - &amp;#34;pd1&amp;#34; - &amp;#34;pd2&amp;#34; - &amp;#34;pd3&amp;#34; entrypoint: /tikv-server privileged: true tikv3: image: pingcap/tikv expose: - &amp;#34;20160&amp;#34; volumes: - /etc/localtime:/etc/localtime:ro command: - --addr=0.0.0.0:20160 - --advertise-addr=tikv3:20160 - --data-dir=/var/tikv - --pd=pd1:2379,pd2:2379,pd3:2379 depends_on: - &amp;#34;pd1&amp;#34; - &amp;#34;pd2&amp;#34; - &amp;#34;pd3&amp;#34; entrypoint: /tikv-server privileged: true tidb: image: pingcap/tidb ports: - &amp;#34;4000&amp;#34; - &amp;#34;10080&amp;#34; volumes: - /etc/localtime:/etc/localtime:ro command: - --store=tikv - --path=pd1:2379,pd2:2379,pd3:2379 - -L=warn depends_on: - &amp;#34;tikv1&amp;#34; - &amp;#34;tikv2&amp;#34; - &amp;#34;tikv3&amp;#34; privileged: true  Use docker-compose up -d to create and start the cluster. Use docker-compose port tidb 4000 to print the TiDB public port. For example, if the output is 0.0.0.0:32966, the TiDB public port is 32966. Use mysql -h 127.0.0.1 -P 32966 -u root -D test to connect to TiDB and enjoy it. Use docker-compose down to stop and remove the cluster.  "},
		{"url": "https://pingcap.com/docs/op-guide/docker-deployment/",
		"title": "TiDB Docker Deployment", 
		"content": " Docker Deployment This page shows you how to manually deploy a multi-node TiDB cluster on multiple machines using Docker.To learn more, see TiDB architecture and Software and Hardware Requirements.Preparation Before you start, make sure that you have: Installed the latest version of Docker Pulled the latest images of TiDB, TiKV and PD from Docker Hub. If not, pull the images using the following commands:docker pull pingcap/tidb:latest docker pull pingcap/tikv:latest docker pull pingcap/pd:latest  Multi nodes deployment Assume we have 6 machines with the following details:   Host Name IP Services Data Path     host1 192.168.1.101 PD1 &amp;amp; TiDB /data   host2 192.168.1.102 PD2 /data   host3 192.168.1.103 PD3 /data   host4 192.168.1.104 TiKV1 /data   host5 192.168.1.105 TiKV2 /data   host6 192.168.1.106 TiKV3 /data    1. Start PD Start PD1 on the host1docker run -d --name pd1   -p 2379:2379   -p 2380:2380   -v /etc/localtime:/etc/localtime:ro   -v /data:/data   pingcap/pd:latest   --name=&amp;#34;pd1&amp;#34;   --data-dir=&amp;#34;/data/pd1&amp;#34;   --client-urls=&amp;#34;http://0.0.0.0:2379&amp;#34;   --advertise-client-urls=&amp;#34;http://192.168.1.101:2379&amp;#34;   --peer-urls=&amp;#34;http://0.0.0.0:2380&amp;#34;   --advertise-peer-urls=&amp;#34;http://192.168.1.101:2380&amp;#34;   --initial-cluster=&amp;#34;pd1=http://192.168.1.101:2380,pd2=http://192.168.1.102:2380,pd3=http://192.168.1.103:2380&amp;#34; Start PD2 on the host2docker run -d --name pd2   -p 2379:2379   -p 2380:2380   -v /etc/localtime:/etc/localtime:ro   -v /data:/data   pingcap/pd:latest   --name=&amp;#34;pd2&amp;#34;   --data-dir=&amp;#34;/data/pd2&amp;#34;   --client-urls=&amp;#34;http://0.0.0.0:2379&amp;#34;   --advertise-client-urls=&amp;#34;http://192.168.1.102:2379&amp;#34;   --peer-urls=&amp;#34;http://0.0.0.0:2380&amp;#34;   --advertise-peer-urls=&amp;#34;http://192.168.1.102:2380&amp;#34;   --initial-cluster=&amp;#34;pd1=http://192.168.1.101:2380,pd2=http://192.168.1.102:2380,pd3=http://192.168.1.103:2380&amp;#34; Start PD3 on the host3docker run -d --name pd3   -p 2379:2379   -p 2380:2380   -v /etc/localtime:/etc/localtime:ro   -v /data:/data   pingcap/pd:latest   --name=&amp;#34;pd3&amp;#34;   --data-dir=&amp;#34;/data/pd3&amp;#34;   --client-urls=&amp;#34;http://0.0.0.0:2379&amp;#34;   --advertise-client-urls=&amp;#34;http://192.168.1.103:2379&amp;#34;   --peer-urls=&amp;#34;http://0.0.0.0:2380&amp;#34;   --advertise-peer-urls=&amp;#34;http://192.168.1.103:2380&amp;#34;   --initial-cluster=&amp;#34;pd1=http://192.168.1.101:2380,pd2=http://192.168.1.102:2380,pd3=http://192.168.1.103:2380&amp;#34; 2. Start TiKV Start TiKV1 on the host4docker run -d --name tikv1   -p 20160:20160   -v /etc/localtime:/etc/localtime:ro   -v /data:/data   pingcap/tikv:latest   --addr=&amp;#34;0.0.0.0:20160&amp;#34;   --advertise-addr=&amp;#34;192.168.1.104:20160&amp;#34;   --data-dir=&amp;#34;/data/tikv1&amp;#34;   --pd=&amp;#34;192.168.1.101:2379,192.168.1.102:2379,192.168.1.103:2379&amp;#34; Start TiKV2 on the host5docker run -d --name tikv2   -p 20160:20160   -v /etc/localtime:/etc/localtime:ro   -v /data:/data   pingcap/tikv:latest   --addr=&amp;#34;0.0.0.0:20160&amp;#34;   --advertise-addr=&amp;#34;192.168.1.105:20160&amp;#34;   --data-dir=&amp;#34;/data/tikv2&amp;#34;   --pd=&amp;#34;192.168.1.101:2379,192.168.1.102:2379,192.168.1.103:2379&amp;#34; Start TiKV3 on the host6docker run -d --name tikv3   -p 20160:20160   -v /etc/localtime:/etc/localtime:ro   -v /data:/data   pingcap/tikv:latest   --addr=&amp;#34;0.0.0.0:20160&amp;#34;   --advertise-addr=&amp;#34;192.168.1.106:20160&amp;#34;   --data-dir=&amp;#34;/data/tikv3&amp;#34;   --pd=&amp;#34;192.168.1.101:2379,192.168.1.102:2379,192.168.1.103:2379&amp;#34; 3. Start TiDB Start TiDB on the host1docker run -d --name tidb   -p 4000:4000   -p 10080:10080   -v /etc/localtime:/etc/localtime:ro   pingcap/tidb:latest   --store=tikv   --path=&amp;#34;192.168.1.101:2379,192.168.1.102:2379,192.168.1.103:2379&amp;#34; 4. Use the MySQL client to connect to TiDB Install the MySQL client on host1 and run:$ mysql -h 127.0.0.1 -P 4000 -u root -D test mysql&amp;gt; show databases; +--------------------+ | Database | +--------------------+ | INFORMATION_SCHEMA | | PERFORMANCE_SCHEMA | | mysql | | test | +--------------------+ 4 rows in set (0.00 sec) How to customize the configuration file The TiKV and PD can be started with a specified configuration file, which includes some advanced parameters, for the performance tuning.Assume that the path to configuration file of PD and TiKV on the host is /path/to/config/pd.toml and /path/to/config/tikv.tomlYou can start TiKV and PD as follows:docker run -d --name tikv1   -p 20160:20160   -v /etc/localtime:/etc/localtime:ro   -v /data:/data   -v /path/to/config/tikv.toml:/tikv.toml:ro   pingcap/tikv:latest   --addr=&amp;#34;0.0.0.0:20160&amp;#34;   --advertise-addr=&amp;#34;192.168.1.104:20160&amp;#34;   --data-dir=&amp;#34;/data/tikv1&amp;#34;   --pd=&amp;#34;192.168.1.101:2379,192.168.1.102:2379,192.168.1.103:2379&amp;#34;   --config=&amp;#34;/tikv.toml&amp;#34;docker run -d --name pd1   -p 2379:2379   -p 2380:2380   -v /etc/localtime:/etc/localtime:ro   -v /data:/data   -v /path/to/config/pd.toml:/pd.toml:ro   pingcap/pd:latest   --name=&amp;#34;pd1&amp;#34;   --data-dir=&amp;#34;/data/pd1&amp;#34;   --client-urls=&amp;#34;http://0.0.0.0:2379&amp;#34;   --advertise-client-urls=&amp;#34;http://192.168.1.101:2379&amp;#34;   --peer-urls=&amp;#34;http://0.0.0.0:2380&amp;#34;   --advertise-peer-urls=&amp;#34;http://192.168.1.101:2380&amp;#34;   --initial-cluster=&amp;#34;pd1=http://192.168.1.101:2380,pd2=http://192.168.1.102:2380,pd3=http://192.168.1.103:2380&amp;#34;   --config=&amp;#34;/pd.toml&amp;#34;"},
		{"url": "https://pingcap.com/docs-cn/op-guide/docker-deployment/",
		"title": "TiDB Docker 部署方案", 
		"content": " TiDB Docker 部署方案 本篇将展示如何在多台主机上使用 Docker 部署一个 TiDB 集群。阅读本章前，请先确保阅读 TiDB 整体架构 及 部署建议。环境准备 安装 Docker Docker 可以方便地在 Linux / Mac OS / Windows 平台安装，安装方法请参考 Docker 官方文档。拉取 TiDB 的 Docker 镜像 部署 TiDB 集群主要包括 3 个服务组件: TiDB TiKV PD  对应的最新 Docker 镜像可以通过 Docker 官方镜像仓库 获取：docker pull pingcap/tidb:latest docker pull pingcap/tikv:latest docker pull pingcap/pd:latest 部署一个多节点集群 假设我们打算在 6 台主机上部署一个 TiDB 集群:   主机名 IP 部署服务 数据盘挂载     host1 192.168.1.101 PD1 &amp;amp; TiDB /data   host2 192.168.1.102 PD2 /data   host3 192.168.1.103 PD3 /data   host4 192.168.1.104 TiKV1 /data   host5 192.168.1.105 TiKV2 /data   host6 192.168.1.106 TiKV3 /data    启动 PD 登录 host1 执行：docker run -d --name pd1   -p 2379:2379   -p 2380:2380   -v /etc/localtime:/etc/localtime:ro   -v /data:/data   pingcap/pd:latest   --name=&amp;#34;pd1&amp;#34;   --data-dir=&amp;#34;/data/pd1&amp;#34;   --client-urls=&amp;#34;http://0.0.0.0:2379&amp;#34;   --advertise-client-urls=&amp;#34;http://192.168.1.101:2379&amp;#34;   --peer-urls=&amp;#34;http://0.0.0.0:2380&amp;#34;   --advertise-peer-urls=&amp;#34;http://192.168.1.101:2380&amp;#34;   --initial-cluster=&amp;#34;pd1=http://192.168.1.101:2380,pd2=http://192.168.1.102:2380,pd3=http://192.168.1.103:2380&amp;#34; 登录 host2 执行：docker run -d --name pd2   -p 2379:2379   -p 2380:2380   -v /etc/localtime:/etc/localtime:ro   -v /data:/data   pingcap/pd:latest   --name=&amp;#34;pd2&amp;#34;   --data-dir=&amp;#34;/data/pd2&amp;#34;   --client-urls=&amp;#34;http://0.0.0.0:2379&amp;#34;   --advertise-client-urls=&amp;#34;http://192.168.1.102:2379&amp;#34;   --peer-urls=&amp;#34;http://0.0.0.0:2380&amp;#34;   --advertise-peer-urls=&amp;#34;http://192.168.1.102:2380&amp;#34;   --initial-cluster=&amp;#34;pd1=http://192.168.1.101:2380,pd2=http://192.168.1.102:2380,pd3=http://192.168.1.103:2380&amp;#34; 登录 host3 执行：docker run -d --name pd3   -p 2379:2379   -p 2380:2380   -v /etc/localtime:/etc/localtime:ro   -v /data:/data   pingcap/pd:latest   --name=&amp;#34;pd3&amp;#34;   --data-dir=&amp;#34;/data/pd3&amp;#34;   --client-urls=&amp;#34;http://0.0.0.0:2379&amp;#34;   --advertise-client-urls=&amp;#34;http://192.168.1.103:2379&amp;#34;   --peer-urls=&amp;#34;http://0.0.0.0:2380&amp;#34;   --advertise-peer-urls=&amp;#34;http://192.168.1.103:2380&amp;#34;   --initial-cluster=&amp;#34;pd1=http://192.168.1.101:2380,pd2=http://192.168.1.102:2380,pd3=http://192.168.1.103:2380&amp;#34; 启动 TiKV 登录 host4 执行：docker run -d --name tikv1   -p 20160:20160   --ulimit nofile=1000000:1000000   -v /etc/localtime:/etc/localtime:ro   -v /data:/data   pingcap/tikv:latest   --addr=&amp;#34;0.0.0.0:20160&amp;#34;   --advertise-addr=&amp;#34;192.168.1.104:20160&amp;#34;   --data-dir=&amp;#34;/data/tikv1&amp;#34;   --pd=&amp;#34;192.168.1.101:2379,192.168.1.102:2379,192.168.1.103:2379&amp;#34; 登录 host5 执行：docker run -d --name tikv2   -p 20160:20160   --ulimit nofile=1000000:1000000   -v /etc/localtime:/etc/localtime:ro   -v /data:/data   pingcap/tikv:latest   --addr=&amp;#34;0.0.0.0:20160&amp;#34;   --advertise-addr=&amp;#34;192.168.1.105:20160&amp;#34;   --data-dir=&amp;#34;/data/tikv2&amp;#34;   --pd=&amp;#34;192.168.1.101:2379,192.168.1.102:2379,192.168.1.103:2379&amp;#34; 登录 host6 执行：docker run -d --name tikv3   -p 20160:20160   --ulimit nofile=1000000:1000000   -v /etc/localtime:/etc/localtime:ro   -v /data:/data   pingcap/tikv:latest   --addr=&amp;#34;0.0.0.0:20160&amp;#34;   --advertise-addr=&amp;#34;192.168.1.106:20160&amp;#34;   --data-dir=&amp;#34;/data/tikv3&amp;#34;   --pd=&amp;#34;192.168.1.101:2379,192.168.1.102:2379,192.168.1.103:2379&amp;#34; 启动 TiDB 登录 host1 执行：docker run -d --name tidb   -p 4000:4000   -p 10080:10080   -v /etc/localtime:/etc/localtime:ro   pingcap/tidb:latest   --store=tikv   --path=&amp;#34;192.168.1.101:2379,192.168.1.102:2379,192.168.1.103:2379&amp;#34; 使用 MySQL 标准客户端连接 TiDB 测试 登录 host1 并确保已安装 MySQL 命令行客户端，执行：$ mysql -h 127.0.0.1 -P 4000 -u root -D test mysql&amp;gt; show databases; +--------------------+ | Database | +--------------------+ | INFORMATION_SCHEMA | | PERFORMANCE_SCHEMA | | mysql | | test | +--------------------+ 4 rows in set (0.00 sec) 如何自定义配置文件 TiKV 和 PD 可以通过指定配置文件的方式来加载更加丰富的启动参数，用于性能调优。假定配置文件在宿主机上的存放路径 /path/to/config/pd.toml 和 /path/to/config/tikv.toml。启动 Docker 时需要调整相应的启动参数，以 tikv1 和 pd1 为例：docker run -d --name tikv1   -p 20160:20160   -v /etc/localtime:/etc/localtime:ro   -v /data:/data   -v /path/to/config/tikv.toml:/tikv.toml:ro   pingcap/tikv:latest   --addr=&amp;#34;0.0.0.0:20160&amp;#34;   --advertise-addr=&amp;#34;192.168.1.104:20160&amp;#34;   --data-dir=&amp;#34;/data/tikv1&amp;#34;   --pd=&amp;#34;192.168.1.101:2379,192.168.1.102:2379,192.168.1.103:2379&amp;#34;   --config=&amp;#34;/tikv.toml&amp;#34;docker run -d --name pd1   -p 2379:2379   -p 2380:2380   -v /etc/localtime:/etc/localtime:ro   -v /data:/data   -v /path/to/config/pd.toml:/pd.toml:ro   pingcap/pd:latest   --name=&amp;#34;pd1&amp;#34;   --data-dir=&amp;#34;/data/pd1&amp;#34;   --client-urls=&amp;#34;http://0.0.0.0:2379&amp;#34;   --advertise-client-urls=&amp;#34;http://192.168.1.101:2379&amp;#34;   --peer-urls=&amp;#34;http://0.0.0.0:2380&amp;#34;   --advertise-peer-urls=&amp;#34;http://192.168.1.101:2380&amp;#34;   --initial-cluster=&amp;#34;pd1=http://192.168.1.101:2380,pd2=http://192.168.1.102:2380,pd3=http://192.168.1.103:2380&amp;#34;   --config=&amp;#34;/pd.toml&amp;#34;"},
		{"url": "https://pingcap.com/recruit-cn/engineer/tidb-engineer/",
		"title": "TiDB Engineer", 
		"content": " TiDB Engineer 岗位职责  负责分布式数据库查询优化器相关的设计，开发，文档撰写和新人指导 负责分布式数据库 SQL 层的设计，开发和性能优化 参与分布式数据库底层系统存储系统的设计  职位要求  三年以上相关领域开发经验，扎实的编程能力，熟悉 C/C++/Go/Java/Python 中的一种 对分布式系统的架构和原理有比较深入的了解 熟悉 MapReduce/Spark/Hive 等分布式计算框架中的一种或多种 熟悉 MySQL/PostgreSQL/Greenplum 等数据库系统实现原理 优秀的发现和解决问题能力，良好的沟通能力，具备团队合作精神  加分项  拥抱开源，对前沿技术有浓厚的热情和探索欲望，有开源项目经历 熟悉 Spark 内核，并阅读过其中的源码 熟悉 MySQL/PostgreSQL/Greenplum 的查询引擎，并阅读过其中的源码  待遇 20K - 40K + 期权, 13薪 + 奖金, 优秀者可面议工作地点 北京，上海，广州，杭州，特别优秀可 remote"},
		{"url": "https://pingcap.com/docs-cn/FAQ/",
		"title": "TiDB FAQ", 
		"content": " 一、 TiDB 介绍、架构、原理 1.1 TiDB 介绍及整体架构 1.1.1 TiDB 整体架构 https://pingcap.com/docs-cn/overview/1.1.2 TiDB 是什么？ TiDB 是一个分布式 NewSQL 数据库。它支持水平弹性扩展、ACID 事务、标准 SQL、MySQL 语法和 MySQL 协议，具有数据强一致的高可用特性，是一个不仅适合 OLTP 场景还适合 OLAP 场景的混合数据库。1.1.3 TiDB 是基于 MySQL 开发的吗？ 不是，虽然 TiDB 支持 MySQL 语法和协议，但是 TiDB 是由 PingCAP 团队完全自主开发的产品。1.1.4 TiDB、TiKV、Placement Driver (PD) 主要作用？  TiDB 是 Server 计算层，主要负责 SQL 的解析、制定查询计划、生成执行器。 TiKV 是分布式 Key-Value 存储引擎，用来存储真正的数据，简而言之，TiKV 是 TiDB 的存储引擎。 PD 是 TiDB 集群的管理组件，负责存储 TiKV 的元数据，同时也负责分配时间戳以及对 TiKV 做负载均衡调度。  1.1.5 TiDB 易用性如何？ TiDB 使用起来很简单，可以将 TiDB 集群当成 MySQL 来用，你可以将 TiDB 用在任何以 MySQL 作为后台存储服务的应用中，并且基本上不需要修改应用代码，同时你可以用大部分流行的 MySQL 管理工具来管理 TiDB。1.1.6 TiDB 和 MySQL 兼容性如何？ TiDB 目前还不支持触发器、存储过程、自定义函数、外键，除此之外，TiDB 支持绝大部分 MySQL 5.7 的语法。详情参见：与 MySQL 兼容性对比。1.1.7 TiDB 具备高可用的特性吗？ TiDB 天然具备高可用特性，TiDB、TiKV、PD 这三个组件都能容忍部分实例失效，不影响整个集群的可用性。具体见 TiDB 高可用性。1.1.8 TiDB 数据是强一致的吗？ TiDB 使用 Raft 在多个副本之间做数据同步，从而保证数据的强一致，单个副本失效时，不影响数据的可靠性。1.1.9 TiDB 支持分布式事务吗？ TiDB 支持 ACID 分布式事务，事务模型是以 Google 的 Percolator 模型为基础，并做了一些优化。这个模型需要一个时间戳分配器，分配唯一且递增的时间戳。在 TiDB 集群中，PD 承担时间戳分配器的角色。1.1.10 TiDB 支持哪些编程语言？ 只要支持 MySQL Client/Driver 的编程语言，都可以直接使用 TiDB。1.1.11 TiDB 是否支持其他存储引擎？ 是的，除了 TiKV 之外，TiDB 还支持一些流行的单机存储引擎，比如 GolevelDB、RocksDB、BoltDB 等。如果一个存储引擎是支持事务的 KV 引擎，并且能提供一个满足 TiDB 接口要求的 Client，即可接入 TiDB。1.1.12 官方有没有三中心跨机房多活部署的推荐方案？ 从 TiDB 架构来讲，支持真正意义上的跨中心异地多活，从操作层面讲，依赖数据中心之间的网络延迟和稳定性，一般建议延迟在 5ms 以下，目前我们已经有相似客户方案，具体请咨询官方: info@pingcap.com。1.1.13 除了官方文档，有没有其他 TiDB 知识获取途径？ 目前 官方文档是获取 TiDB 相关知识最主要、最及时的发布途径。除此之外，我们也有一些技术沟通群，如有需求可发邮件至 info@pingcap.com 获取。1.1.14 TiDB 对那些 MySQL variables 兼容？ 详细可参考：系统变量1.1.15 TiDB 是否支持 select for update 吗？ 支持，但语义上和 MySQL 有区别，TiDB 是分布式数据库，采用的乐观锁机制，也就说 select for update 不在事务开启就锁住数据，而是其他事务在提交的时候进行冲突检查，如有冲突，会进行回滚。1.1.16 TiDB 的 codec 能保证 UTF8 的字符串是 memcomparable 的吗？我们的 key 需要支持 UTF8，有什么编码建议吗？ TiDB 字符集默认就是 UTF8 而且目前只支持 UTF8，字符串就是 memcomparable 格式的。1.2 TiDB 原理 1.2.1 存储 TiKV 1.2.1.1 TiKV 详细解读 http://t.cn/RTKRRWv1.2.2 计算 TiDB 1.2.2.1 TiDB 详细解读 http://t.cn/RTKRkBh1.2.3 调度 PD 1.2.3.1 PD 详细解读 http://t.cn/RTKEZ0U二、安装部署升级 2.1 环境准备 2.1.1 操作系统版本要求    Linux 操作系统平台 版本     Red Hat Enterprise Linux 7.3 及以上   CentOS 7.3 及以上   Oracle Enterprise Linux 7.3 及以上    2.1.1.1 为什么要在 CentOS 7 上部署 TiDB 集群？ TiDB 作为一款开源分布式 NewSQL 数据库，可以很好的部署和运行在 Intel 架构服务器环境及主流虚拟化环境，并支持绝大多数的主流硬件网络，作为一款高性能数据库系统，TiDB 支持主流的 Linux 操作系统环境，具体可以参考 TiDB 的官方部署要求。 其中 TiDB 在 CentOS 7.3 的环境下进行大量的测试，同时也有很多这个操作系统的部署最佳实践，因此，我们推荐客户在部署 TiDB 的时候使用 CentOS 7.3+ 以上的Linux 操作系统。2.1.2 硬件要求 TiDB 支持部署和运行在 Intel x86-64 架构的 64 位通用硬件服务器平台。对于开发，测试，及生产环境的服务器硬件配置有以下要求和建议：2.1.2.1 开发及测试环境    组件 CPU 内存 本地存储 网络 实例数量(最低要求)     TiDB 8核+ 16 GB+ SAS, 200 GB+ 千兆网卡 1（可与 PD 同机器）   PD 8核+ 16 GB+ SAS, 200 GB+ 千兆网卡 1（可与 TiDB 同机器）   TiKV 8核+ 32 GB+ SSD, 200 GB+ 千兆网卡 3       服务器总计 4    2.1.2.2 线上环境    组件 CPU 内存 硬盘类型 网络 实例数量(最低要求)     TiDB 16核+ 48 GB+ SAS 万兆网卡（2块最佳） 2   PD 8核+ 16 GB+ SSD 万兆网卡（2块最佳） 3   TiKV 16核+ 48 GB+ SSD 万兆网卡（2块最佳） 3   监控 8核+ 16 GB+ SAS 千兆网卡 1       服务器总计 9    2.1.2.3 2 块网卡的目的是？万兆的目的是？ 作为一个分布式集群，TiDB 对时间的要求还是比较高的，尤其是 PD 需要分发唯一的时间戳，如果 PD 时间不统一，如果有 PD 切换，将会等待更长的时间。2 块网卡可以做 bond，保证数据传输的稳定，万兆可以保证数据传输的速度，千兆网卡容易出现瓶颈，我们强烈建议使用万兆网卡。2.1.2.4 SSD 不做 RAID 是否可行？ 资源可接受的话，我们建议做 RAID 10，如果资源有限，也可以不做 RAID。2.2 安装部署 2.2.1 Ansible 部署方式（强烈推荐） 详细可参考：TiDB Ansible 部署方案2.2.1.1 为什么修改了 TiKV/PD 的 toml 配置文件，却没有生效？ 这种情况一般是因为没有使用 --config 参数来指定配置文件（目前只会出现在 binary 部署的场景），TiKV/PD 会按默认值来设置。如果要使用配置文件，请设置 TiKV/PD 的 --config 参数。对于 TiKV 组件，修改配置后重启服务即可；对于 PD 组件，只会在第一次启动时读取配置文件，之后可以使用 pd-ctl 的方式来修改配置，详情可参考：https://pingcap.com/docs-cn/tools/pd-control/2.2.1.2 TiDB 监控框架 Prometheus + Grafana 监控机器建议单独还是多台部署？ 监控机建议单独部署。建议 CPU 8 core，内存 16 GB 以上，硬盘 500 GB 以上。2.2.1.3 有一部分监控信息显示不出来？ 查看访问监控的机器时间跟集群内机器的时间差，如果比较大，更正时间后即可显示正常。2.2.1.4 supervise/svc/svstat 服务具体起什么作用？  supervise 守护进程 svc 启停服务 svstat 查看进程状态  2.2.1.5 inventory.ini 变量参数解读    变量 含义     cluster_name 集群名称，可调整   tidb_version TiDB 版本，TiDB-Ansible 各分支默认已配置   deployment_method 部署方式，默认为 binary，可选 docker   process_supervision 进程监管方式，默认为 systemd，可选 supervise   timezone 修改部署目标机器时区，默认为 Asia/Shanghai, 可调整，与set_timezone 变量结合使用   set_timezone 默认为 True，即修改部署目标机器时区，关闭可修改为 False   enable_elk 目前不支持，请忽略   enable_firewalld 开启防火墙，默认不开启   enable_ntpd 检测部署目标机器 NTP 服务，默认为 True，请勿关闭   machine_benchmark 检测部署目标机器磁盘 IOPS，默认为 True，请勿关闭   set_hostname 根据 IP 修改部署目标机器主机名，默认为 False   enable_binlog 是否部署 pump 并开启 binlog，默认为 False，依赖 Kafka 集群，参见 zookeeper_addrs 变量   zookeeper_addrs binlog Kafka 集群的 zookeeper 地址   enable_slow_query_log TiDB 慢查询日志记录到单独文件({{ deploy_dir }}/log/tidb_slow_query.log)，默认为 False，记录到 tidb 日志   deploy_without_tidb KV 模式，不部署 TiDB 服务，仅部署 PD、TiKV 及监控服务，请将 inventory.ini 文件中 tidb_servers 主机组 IP 设置为空。    2.2.2 TiDB 离线 Ansible 部署方案 首先这不是我们建议的方式，如果中控机没有外网，也可以通过离线 Ansible 部署方式，详情可参考：https://pingcap.com/docs-cn/op-guide/offline-ansible-deployment/2.2.3 Docker Compose 快速构建集群（单机部署） 使用 docker-compose 在本地一键拉起一个集群，包括集群监控，还可以根据需求自定义各个组件的软件版本和实例个数，以及自定义配置文件，这种只限于开发环境，详细可参考：官方文档。2.3 升级 2.3.1 如何使用 Ansible 滚动升级？ 滚动升级 TiKV 节点( 只升级单独服务 )ansible-playbook rolling_update.yml --tags=tikv滚动升级所有服务ansible-playbook rolling_update.yml2.3.2 滚动升级有那些影响? 滚动升级 TiDB 服务，滚动升级期间不影响业务运行，需要配置最小集群拓扑（TiDB * 2、PD * 3、TiKV * 3），如果集群环境中有 Pump/Drainer 服务，建议先停止 Drainer 后滚动升级（升级 TiDB 时会升级 Pump）。2.3.3 Binary 如何升级？ Binary 不是我们建议的安装方式，对升级支持也不友好，建议换成 Ansible 部署。2.3.4 一般升级选择升级 TiKV 还是所有组件都升级？ 常规需要一起升，因为整个版本都是一起测试的，单独升级只限当发生一个紧急故障时，需要单独对一个有问题的角色做升级。2.3.5 启动集群或者升级集群过程中出现 “Timeout when waiting for search string 200 OK” 是什么原因？如何处理？ 可能有以下几种原因：进程没有正常启动；端口被占用；进程没有正常停掉；停掉集群的情况下使用 rolling_update.yml 来升级集群（操作错误）。处理方式：登录到相应节点查看进程或者端口的状态；纠正错误的操作步骤。三、集群管理 3.1 集群日常管理 3.1.1 Ansible 常见运维操作有那些？    任务 Playbook     启动集群 ansible-playbook start.yml   停止集群 ansible-playbook stop.yml   销毁集群 ansible-playbook unsafe_cleanup.yml (若部署目录为挂载点，会报错，可忽略）   清除数据(测试用) ansible-playbook cleanup_data.yml   滚动升级 ansible-playbook rolling_update.yml   滚动升级 TiKV ansible-playbook rolling_update.yml &amp;ndash;tags=tikv   滚动升级除 PD 外模块 ansible-playbook rolling_update.yml &amp;ndash;skip-tags=pd   滚动升级监控组件 ansible-playbook rolling_update_monitor.yml    3.1.2 TiDB 如何登陆？ 和 MySQL 登陆方式一样，可以按照下面例子进行登陆。mysql -h 127.0.0.1 -uroot -P40003.1.3 TiDB 如何修改数据库系统变量？ 和 MySQL 一样，TiDB 也分为静态参数和固态参数，静态参数可以直接通过set global xxx = n的方式进行修改，不过新参数值只限于该实例生命周期有效。3.1.4 TiDB (TiKV) 有哪些数据目录？ 默认在 ${data-dir}/data/ 目录下，其中包括 backup、db、raft、snap 四个目录，分别存储备份、数据、raft 数据及镜像数据。3.1.5 TiDB 有哪些系统表？ 和 MySQL 类似，TiDB 中也有系统表，用于存放数据库运行时所需信息，具体信息参考：TiDB 系统数据库文档。3.1.6 TiDB 各节点服务器下是否有日志文件，如何管理？ 默认情况下各节点服务器会在日志中输出标准错误，如果启动的时候通过 --log-file 参数指定了日志文件，那么日志会输出到指定的文件中，并且按天做 rotation。3.1.7 如何规范停止 TiDB？ 如果是用 Ansible 部署的，可以使用 ansible-playbook stop.yml 命令停止 TiDB 集群。如果不是 Ansible 部署的，可以直接 kill 掉所有服务。如果使用 kill 命令，TiDB 的组件会做 graceful 的 shutdown。3.1.8 TiDB 里面可以执行 kill 命令吗？ 可以 kill DML 语句，首先使用 show processlist，找到对应 session 的 id，然后执行 kill id。但是，目前不能 kill DDL 语句。DDL 语句一旦开始执行便不能停止，除非出错，出错以后会自动停止运行。3.1.9 TiDB 是否支持会话超时？ TiDB 暂不支持数据库层面的会话超时，目前想要实现超时，在没 LB（Load Balancing）的时候，需要应用侧记录发起的 Session 的 ID，通过应用自定义超时，超时以后需要到发起 Query 的节点上用 kill id 来杀掉 SQL。目前建议使用应用程序来实现会话超时，当达到超时时间，应用层就会抛出异常继续执行后续的程序段。3.1.10 TiDB 生产环境的版本管理策略是怎么样的？如何尽可能避免频繁升级？ TiDB 版本目前逐步标准化，每次 Release 都包含详细的 Change log，版本功能 变化详情，生产环境是否有必要升级取决于业务系统，建议升级之前详细了解前后版本的功能差异。版本号说明参考：Release Version: v1.0.3-1-ga80e796，v1.0.3 表示 GA 标准版 -1 表示该版本 commit 1 次，ga80e796 代表版本的 git-hash。3.1.11 分不清 TiDB master 版本之间的区别，经常用错 TiDB-Ansible 版本? TiDB 目前社区非常活跃，在 GA 版本发布后，还在不断的优化和修改 BUG，因此 TiDB 的版本更新周期比较快，会不定期有新版本发布，请关注我们的 新版本发布官方网站。此外 TiDB 安装推荐使用 TiDB-Ansible 进行安装，TiDB-Ansible 的版本也会随着 TiDB 的版本发布进行更新，因此建议用户在安装升级新版本的时候使用最新的 TiDB-Ansible 安装包版本进行安装。 此外，在 TiDB 版本 GA 后，对 TiDB 的版本号进行了统一管理，TiDB 的版本可以通过几种方式进行查看： 通过 select tidb_version() 进行查看； 通过执行 tidb-server -V 进行查看。  3.1.12 有没有图形化部署 TiDB 的工具？ 暂时没有。3.1.13 TiDB 如何进行水平扩展？ 当您的业务不断增长时，数据库可能会面临三方面瓶颈，第一是存储空间，第二是计算资源，第三是读写容量，这时可以对 TiDB 集群做水平扩展。 如果是存储资源不够，可以通过添加 TiKV Server 节点来解决，新节点启动后，PD 会自动将其他节点的部分数据迁移过去，无需人工介入。 如果是计算资源不够，可以查看 TiDB Server 和 TiKV Server 节点的 CPU 消耗情况，再考虑添加 TiDB Server 节点或者是 TiKV Server 节点来解决，如添加 TiDB Server 节点，将其添加到前端 Load Balancer 配置之中即可。 如果是容量跟不上，一般可以考虑同时增加 TiDB Server 和 TiKV Server 节点。  3.1.14 Percolator 用了分布式锁，crash 的客户端会保持锁，会造成锁没有 release？ 详细可参 …"},
		{"url": "https://pingcap.com/docs/FAQ/",
		"title": "TiDB FAQ", 
		"content": " TiDB FAQ This document lists the Most Frequently Asked Questions about TiDB.Product General What is TiDB? TiDB is a distributed SQL database that features in horizontal scalability, high availability and consistent distributed transactions. It also enables you to use MySQL’s SQL syntax and protocol to manage and retrieve data.Is TiDB based on MySQL? No. TiDB supports MySQL syntax and protocol, but it is a new open source database that is developed and maintained by PingCAP, Inc.What is the difference between TiDB and MySQL Group Replication? MySQL Group Replication (MGR) is a high available solution based on the standalone MySQL, but it does not solve the scalability problem. TiDB is more suitable for distributed scenarios in architecture, and the various decisions in the development process are also based on distributed scenarios.How do TiDB and TiKV work together? What is the relationship between the two? TiDB works as the SQL layer and TiKV works as the Key-Value layer. TiDB is mainly responsible for parsing SQL, specifying query plan, and generating executor while TiKV is to store the actual data and works as the storage engine.TiDB provides TiKV the SQL enablement and turns TiKV into a NewSQL database. TiDB and TiKV work together to be as scalable as a NoSQL database while maintains the ACID transactions of a relational database.What does Placement Driver (PD) do? Placement Driver (PD) works as the cluster manager of TiDB. It manages the TiKV metadata and makes decisions for data placement and load balancing. PD periodically checks replication constraints to balance load and data automatically.Is it easy to use TiDB? Yes, it is. When all the required services are started, you can use TiDB as easily as a MySQL server. You can replace MySQL with TiDB to power your applications without changing a single line of code in most cases. You can also manage TiDB using the popular MySQL management tools.When to use TiDB? TiDB is at your service if your applications require any of the following: Horizontal scalability High availability Strong consistency Support for distributed ACID transactions  When not to use TiDB? TiDB is not a good choice if the number of the rows in your database table is less than 100GB and there is no requirement for high availability, strong consistency and cross-datacenter replication.How is TiDB strongly-consistent? TiDB uses the Raft consensus algorithm to ensure consistency among multiple replicas. At the bottom layer, TiDB uses a model of replication log + State Machine to replicate data. For the write requests, the data is written to a Leader and the Leader then replicates the command to its Followers in the form of log. When the majority of nodes in the cluster receive this log, this log is committed and can be applied into the State Machine. TiDB has the latest data even if a minority of the replicas are lost.Does TiDB support distributed transactions? Yes. The transaction model in TiDB is inspired by Google’s Percolator, a paper published in 2006. It’s mainly a two-phase commit protocol with some practical optimizations. This model relies on a timestamp allocator to assign monotone increasing timestamp for each transaction, so the conflicts can be detected. PD works as the timestamp allocator in a TiDB cluster.Does the conflict of multiple transactions (such as updating the same row at the same time) cause commit failure of some transactions? Yes. The transaction that fails to commit in a transaction conflict retreats and retries at the appropriate time. The number of retries in TiDB is 10 times by default.What programming language can I use to work with TiDB? Any language that has MySQL client or driver.How does TiDB compare to traditional relational databases like Oracle and MySQL? TiDB scores in horizontal scalability while still maintains the traditional relation database features. You can easily increase the capacity or balance the load by adding more machines.How does TiDB compare to NoSQL databases like Cassandra, Hbase, or MongoDB? TiDB is as scalable as NoSQL databases but features in the usability and functionality of traditional SQL databases, such as SQL syntax and consistent distributed transactions.An error message is displayed when using go get to install TiDB. Manually clone TiDB to the GOPATH directory and run the make command. TiDB uses Makefile to manage the dependencies.If you are a developer and familiar with Go, you can run make parser; ln -s _vendor/src vendor in the root directory of TiDB and then run commands like go run, go test and go install. However, this is not recommended.How is TiDB highly available? TiDB is self-healing. All of the three components, TiDB, TiKV and PD, can tolerate failures of some of their instances. With its strong consistency guarantee, whether it’s data machine failures or even downtime of an entire data center, your data can be recovered automatically. For more information, see High availability.Does TiDB release space immediately after deleting data? DELETE, TRUNCATE and DROP do not release space immediately. For TRUNCATE and DROP operations, TiDB deletes the data and releases the space after reaching the GC (garbage collection) time (10 minutes by default). For the DELETE operation, TiDB deletes the data and does not release the space based on the GC mechanism, but reuses the space when subsequent data is committed to RocksDB and compacted.Can I execute DDL operations on the target table when loading data? No. None of the DDL operations can be executed on the target table when you load data, otherwise the data fails to load.Does TiDB support the replace into syntax? Yes. But the load data does not support the replace into syntax.How to export the data in TiDB? Currently, TiDB does not support select into outfile. You can use the following methods to export the data in TiDB: See MySQL uses mysqldump to export part of the table data in Chinese and export data using mysqldump and the WHERE condition. Use the MySQL client to export the results of select to a file.  Does TiDB support session timeout? Currently, TiDB does not support session timeout in the database level. If you want to implement session timeout, use the session id started by side records in the absence of LB (Load Balancing), and customize the session timeout on the application. After timeout, kill sql using kill tidb id on the node that starts the query. It is currently recommended to implement session timeout using applications. When the timeout time is reached, the application layer reports an exception and continues to execute subsequent program segments.What is the TiDB version management strategy for production environment? How to avoid frequent upgrade? Currently, TiDB has a standard management of various versions. Each release contains a detailed change log and release notes. Whether it is necessary to upgrade in the production environment depends on the application system. It is recommended to learn the details about the functional differences between the previous and later versions before upgrading.Take Release Version: v1.0.3-1-ga80e796 as an example of version number description: v1.0.3 indicates the standard GA version. -1 indicates the current version has one commit. ga80e796 indicates the version git-hash.  What&amp;rsquo;s the difference between various TiDB master versions? How to avoid using the wrong TiDB-Ansible version? The TiDB community is highly active. After the GA release, the engineers have been keeping optimizing and fixing bugs. Therefore, the TiDB version is updated quite fast. If you want to keep informed of the latest version, see TiDB Weekly update.It is recommended to deploy the TiDB cluster using the latest version of TiDB-Ansible, which will also be updated along with the TiDB version. Besides, TiDB has a unified management of the version number after GA release. You can view the version number using the following two methods: select tidb_version() tidb-server -V …"},
		{"url": "https://pingcap.com/docs-cn/releases/prega/",
		"title": "TiDB Pre-GA Release Notes", 
		"content": " TiDB Pre-GA Release Notes 8 月 30 日，TiDB 发布 Pre-GA 版。该版本对 MySQL 兼容性、SQL 优化器、系统稳定性、性能做了大量的工作。TiDB:  SQL 查询优化器  调整代价模型 优化索引选择，支持不同类型字段比较的索引选择 支持基于贪心算法的 Join Reorder  大量 MySQL 兼容性相关功能 支持 Natural Join 完成 JSON 类型支持 (Experimental)，包括对 JSON 中的字段查询、更新、建索引 裁剪无用数据，减小执行器内存消耗 支持在 SQL 语句中设置优先级，并根据查询类型自动设置部分语句的优先级 完成表达式重构，执行速度提升 30% 左右  PD:  支持手动切换 PD 集群 Leader  TiKV:  Raft Log 使用独立的 RocksDB 实例 使用 DeleteRange 加快删除副本速度 Coprocessor 支持更多运算符下推 提升性能，提升稳定性  TiSpark Beta Release:  支持谓词下推 支持聚合下推 支持范围裁剪 通过 TPC-H 测试 (除去一个需要 View 的 Query)  "},
		{"url": "https://pingcap.com/docs/QUICKSTART/",
		"title": "TiDB Quick Start Guide", 
		"content": " TiDB Quick Start Guide About TiDB TiDB (The pronunciation is: /’taɪdiːbi:/ tai-D-B, etymology: titanium) is a Hybrid Transactional/Analytical Processing (HTAP) database. Inspired by the design of Google F1 and Google Spanner, TiDB features infinite horizontal scalability, strong consistency, and high availability. The goal of TiDB is to serve as a one-stop solution for online transactions and analyses.About this guide This guide outlines how to perform a quick deployment of a TiDB cluster using TiDB-Ansible and walks you through the basic TiDB operations and administrations.Deploy the TiDB cluster This section describes how to deploy a TiDB cluster. A TiDB cluster consists of different components: TiDB servers, TiKV servers, and Placement Driver (PD) servers.The architecture is as follows:For details of deploying a TiDB cluster, see Ansible Deployment.Try TiDB This section describes some basic CRUD operations in TiDB.Create, show, and drop a database You can use the CREATE DATABASE statement to create a database.The Syntax is as follows:CREATE DATABASE db_name [options]; For example, the following statement creates a database with the name samp_db:CREATE DATABASE IF NOT EXISTS samp_db; You can use the SHOW DATABASES statement to show the databases:SHOW DATABASES; You can use the DROP DATABASE statement to delete a database, for example:DROP DATABASE samp_db; Create, show, and drop a table Use the CREATE TABLE statement to create a table. The Syntax is as follows:CREATE TABLE table_name column_name data_type constraint; For example:CREATE TABLE person ( number INT(11), name VARCHAR(255), birthday DATE ); Add IF NOT EXISTS to prevent an error if the table exists:CREATE TABLE IF NOT EXISTS person ( number INT(11), name VARCHAR(255), birthday DATE ); Use the SHOW CREATE statement to see the statement that creates the table. For example:SHOW CREATE table person; Use the SHOW FULL COLUMNS statement to display the information about all the columns in a table. For example:SHOW FULL COLUMNS FROM person; Use the DROP TABLE statement to delete a table. For example:DROP TABLE person; orDROP TABLE IF EXISTS person; Use the SHOW TABLES statement to show all the tables in a database. For example:SHOW TABLES FROM samp_db; Create, show, and drop an index For the columns whose value is not unique, you can use the CREATE INDEX or ALTER TABLE statements. For example:CREATE INDEX person_num ON person (number); orALTER TABLE person ADD INDEX person_num (number); You can also create unique indexes for the columns whose value is unique. For example:CREATE UNIQUE INDEX person_num ON person (number); orALTER TABLE person ADD UNIQUE person_num on (number); Use the SHOW INDEX to display all the indexes in a table:SHOW INDEX from person; Use the ALTER TABLE or DROP INDEX to delete an index. Like the CREATE INDEX statement, DROP INDEX can also be embedded in the ALTER TABLE statement. For example:DROP INDEX person_num ON person; ALTER TABLE person DROP INDEX person_num; Insert, select, update, and delete data Use the INSERT statement to insert data into a table. For example:INSERT INTO person VALUES(&amp;#34;1&amp;#34;,&amp;#34;tom&amp;#34;,&amp;#34;20170912&amp;#34;); Use the SELECT statement to see the data in a table. For example:SELECT * FROM person; +--------+------+------------+ | number | name | birthday | +--------+------+------------+ | 1 | tom | 2017-09-12 | +--------+------+------------+ Use the UPDATE statement to update the data in a table. For example:UPDATE person SET birthday=&amp;#39;20171010&amp;#39; WHERE name=&amp;#39;tom&amp;#39;; SELECT * FROM person; +--------+------+------------+ | number | name | birthday | +--------+------+------------+ | 1 | tom | 2017-10-10 | +--------+------+------------+ Use the DELETE statement to delete the data in a table. For example:DELETE FROM person WHERE number=1; SELECT * FROM person; Empty set (0.00 sec) Create, authorize, and delete a user Use the CREATE USER statement to create a user named tiuser with the password 123456:CREATE USER &amp;#39;tiuser&amp;#39;@&amp;#39;localhost&amp;#39; IDENTIFIED BY &amp;#39;123456&amp;#39;; Grant tiuser the privilege to retrieve the tables in the samp_db database:GRANT SELECT ON samp_db .* TO &amp;#39;tiuser&amp;#39;@&amp;#39;localhost&amp;#39;; Check the privileges of tiuser:SHOW GRANTS for tiuser@localhost; Delete tiuser:DROP USER &amp;#39;tiuser&amp;#39;@&amp;#39;localhost&amp;#39;; Monitor the TiDB cluster Open a browser to access the monitoring platform: http://172.16.10.3:3000.The default account and password are: admin/admin.About the key metrics    Service Panel Name Description Normal Range     PD Storage Capacity the total storage capacity of the TiDB cluster    PD Current Storage Size the occupied storage capacity of the TiDB cluster    PD Store Status &amp;ndash; up store the number of TiKV nodes that are up    PD Store Status &amp;ndash; down store the number of TiKV nodes that are down 0. If the number is bigger than 0, it means some node(s) are not down.   PD Store Status &amp;ndash; offline store the number of TiKV nodes that are manually offline    PD Store Status &amp;ndash; Tombstone store the number of TiKV nodes that are Tombstone    PD Current storage usage the storage occupancy rate of the TiKV cluster If it exceeds 80%, you need to consider adding more TiKV nodes.   PD 99% completed cmds duration seconds the 99th percentile duration to complete a pd-server request less than 5ms   PD average completed cmds duration seconds the average duration to complete a pd-server request less than 50ms   PD leader balance ratio the leader ratio difference of the nodes with the biggest leader ratio and the smallest leader ratio It is less than 5% for a balanced situation. It becomes bigger when a node is restarting.   PD region balance ratio the region ratio difference of the nodes with the biggest region ratio and the smallest region ratio It is less than 5% for a balanced situation. It becomes bigger when adding or removing a node.   TiDB handle requests duration seconds the response time to get TSO from PD less than 100ms   TiDB tidb server QPS the QPS of the cluster application specific   TiDB connection count the number of connections from application servers to the database Application specific. If the number of connections hops, you need to find out the reasons. If it drops to 0, you can check if the network is broken; if it surges, you need to check the application.   TiDB statement count the number of different types of statement within a given time application specific   TiDB Query Duration 99th percentile the 99th percentile query time    TiKV 99% &amp;amp; 99.99% scheduler command duration the 99th percentile and 99.99th percentile scheduler command duration For 99%, it is less than 50ms; for 99.99%, it is less than 100ms.   TiKV 95% &amp;amp; 99.99% storage async_request duration the 95th percentile and 99.99th percentile Raft command duration For 95%, it is less than 50ms; for 99.99%, it is less than 100ms.   TiKV server report failure message There might be an issue with the network or the message might not come from this cluster. If there are large amount of messages which contains unreachable, there might be an issue with the network. If the message contains store not match, the message does not come from this cluster.   TiKV Vote the frequency of the Raft vote Usually, the value only changes when there is a split. If the value of Vote remains high for a long time, the system might have a severe issue and some nodes are not working.   TiKV 95% and 99% coprocessor request duration the 95th percentile and the 99th percentile coprocessor request duration Application specific. Usually, the value does not remain high.   TiKV Pending task the number of pending tasks Except for PD worker, it is not normal if the value is too high.   TiKV stall RocksDB stall time If the value is bigger than 0, it means that RocksDB is too busy, and you need to pay attention to IO and CPU usage.   TiKV channel full The channel is full and the threads are too busy. If the value is bigger than 0, the …"},
		{"url": "https://pingcap.com/docs-cn/releases/rc1/",
		"title": "TiDB RC1 Release Notes", 
		"content": " TiDB RC1 Release Notes 2016 年 12 月 23 日，分布式关系型数据库 TiDB 正式发布 RC1。TiKV + 提升写入速度 + 降低磁盘空间占用 + 支持百 TB 级别数据 + 提升稳定性，集群规模支持 200 个节点 + 提供 Raw KV API，以及 Golang client PD + PD 调度策略框架优化，策略更加灵活合理 + 添加 label 支持，支持跨 DC 调度 + 提供 PD Controler，方便操作 PD 集群 TiDB + SQL 查询优化器 - 支持 eager aggregate - 更详细的 explain 信息 - union 算子并行化 - 子查询性能优化 - 条件下推优化 - 优化 CBO 框架 + 重构 time 相关类型的实现，提升和 MySQL 的兼容性 + 支持更多的 MySQL 内建函数 + Add Index 语句提速 + 支持用 change column 语句修改列名；支持使用 Alter table 的 modify column 和 change column 完成部分列类型转换 工具 + Loader：兼容 Percona 的 mydumper 数据格式，提供多线程导入、出错重试、断点续传等功能，并且针对 TiDB 有优化 + 开发完成一键部署工具"},
		{"url": "https://pingcap.com/docs/releases/rc1/",
		"title": "TiDB RC1 Release Notes", 
		"content": " TiDB RC1 Release Notes On December 23, 2016, TiDB RC1 is released. See the following updates in this release:TiKV:  The write speed has been improved. The disk space usage is reduced. Hundreds of TBs of data can be supported. The stability is improved and TiKV can support a cluster with 200 nodes. Supports the Raw KV API and the Golang client.  Placement Driver (PD): + The scheduling strategy framework is optimized and now the strategy is more flexible and reasonable. + The support for label is added to support Cross Data Center scheduling. + PD Controller is provided to operate the PD cluster more easily.TiDB:  The following features are added or improved in the SQL query optimizer:  Eager aggregation More detailed EXPLAIN information Parallelization of the UNION operator Optimization of the subquery performance Optimization of the conditional push-down Optimization of the Cost Based Optimizer (CBO) framework  The implementation of the time related data types are refactored to improve the compatibility with MySQL. More built-in functions in MySQL are supported. The speed of the add index statement is enhanced. The following statements are supported:  Use the CHANGE COLUMN statement to change the name of a column. Use MODIFY COLUMN and CHANGE COLUMN of the ALTER TABLE statement for some of the column type transfer.   New tools:  Loader is added to be compatible with the mydumper data format in Percona and provides the following functions:  Multi-thread import Retry if error occurs Breakpoint resume Targeted optimization for TiDB  The tool for one-click deployment is added.  "},
		{"url": "https://pingcap.com/docs-cn/releases/rc2/",
		"title": "TiDB RC2 Release Notes", 
		"content": " TiDB RC2 Release Notes 2017 年 3 月 1 日，TiDB 正式发布 RC2 版。该版本对 MySQL 兼容性、SQL 优化器、系统稳定性、性能做了大量的工作。对于 OLTP 场景，读取性能提升 60%，写入性能提升 30%。另外提供了权限管理功能，用户可以按照 MySQL 的权限管理方式控制数据访问权限。TiDB  SQL 查询优化器  统计信息收集和使用 关联子查询优化 优化 CBO 框架 通过 Unique Key 信息消除聚合 重构 Expression Distinct 转换为 GroupBy 支持 topn 操作下推  支持基本权限管理 新增大量 MySQL 内建函数 完善 Alter Table 语句，支持修改表名、默认值、注释 支持 Create Table Like 语句 支持 Show Warnings 语句 支持 Rename Table 语句 限制单个事务大小，避免大事务阻塞整个集群 Load Data 过程中对数据进行自动拆分 优化 AddIndex、Delete 语句性能 支持 &amp;ldquo;ANSI_QUOTES&amp;rdquo; sql_mode 完善监控 修复 Bug 修复内存泄漏问题  PD  支持 Label 对副本进行 Location 调度 基于 region 数量的快速调度 pd-ctl 支持更多功能  添加、删除 PD 通过 Key 获取 Region 信息 添加、删除 scheduler 和 operator 获取集群 label 信息   TiKV  支持 Async Apply 提升整体写入性能 使用 prefix seek 提升 Write CF 的读取性能 使用 memory hint prefix 提升 Raft CF 插入性能 优化单行读事务性能 支持更多下推功能 加入更多统计 修复 Bug  "},
		{"url": "https://pingcap.com/docs/releases/rc2/",
		"title": "TiDB RC2 Release Notes", 
		"content": " TiDB RC2 Release Notes On August 4, 2017, TiDB RC4 is released! This release is focused on the compatibility with MySQL, SQL query optimizer, system stability and performance in this version. What’s more, a new permission management mechanism is added and users can control data access in the same way as the MySQL privilege management system.TiDB:  Query optimizer  Collect column/index statistics and use them in the query optimizer Optimize the correlated subquery Optimize the Cost Based Optimizer (CBO) framework Eliminate aggregation using unique key information Refactor expression evaluation framework Convert Distinct to GroupBy Support the topn operation push-down  Support basic privilege management Add lots of MySQL built-in functions Improve the Alter Table statement and support the modification of table name, default value and comment Support the Create Table Like statement Support the Show Warnings statement Support the Rename Table statement Restrict the size of a single transaction to avoid the cluster blocking of large transactions Automatically split data in the process of Load Data Optimize the performance of the AddIndex and Delete statement Support &amp;ldquo;ANSI_QUOTES&amp;rdquo; sql_mode Improve the monitoring system Fix Bugs Solve the problem of memory leak  PD:  Support location aware replica scheduling Conduct fast scheduling based on the number of region pd-ctl support more features  Add or delete PD Obtain Region information with Key Add or delete scheduler and operator Obtain cluster label information   TiKV:  Support Async Apply to improve the entire write performance Use prefix seek to improve the read performance of Write CF Use memory hint prefix to improve the insert performance of Raft CF Optimize the single read transaction performance Support more push-down expressions Improve the monitoring system Fix Bugs  "},
		{"url": "https://pingcap.com/docs-cn/releases/rc3/",
		"title": "TiDB RC3 Release Notes", 
		"content": " TiDB RC3 Release Notes 2017 年 6 月 16 日，TiDB 正式发布 RC3 版。该版本对 MySQL 兼容性、SQL 优化器、系统稳定性、性能做了大量的工作。性能方面重点优化了负载均衡调度策略和流程。功能方面进一步完善权限管理功能，用户可以按照 MySQL 的权限管理方式控制数据访问权限。另外 DDL 的速度也得到显著的提升。 同时为了简化运维工作，开源了 TiDB-Ansible 项目，可以一键部署/升级/启停 TiDB 集群。TiDB  SQL 查询优化器  统计信息收集和使用 关联子查询优化 优化 CBO 框架 通过 Unique Key 信息消除聚合 重构 Expression Distinct 转换为 GroupBy 支持 topn 操作下推  支持基本权限管理 新增大量 MySQL 内建函数 完善 Alter Table 语句，支持修改表名、默认值、注释 支持 Create Table Like 语句 支持 Show Warnings 语句 支持 Rename Table 语句 限制单个事务大小，避免大事务阻塞整个集群 Load Data 过程中对数据进行自动拆分 优化 AddIndex、Delete 语句性能 支持 &amp;ldquo;ANSI_QUOTES&amp;rdquo; sql_mode 完善监控 修复 Bug 修复内存泄漏问题  PD  支持 Label 对副本进行 Location 调度 基于 region 数量的快速调度 pd-ctl 支持更多功能  添加、删除 PD 通过 Key 获取 Region 信息 添加、删除 scheduler 和 operator 获取集群 label 信息    TiKV  支持 Async Apply 提升整体写入性能 使用 prefix seek 提升 Write CF 的读取性能 使用 memory hint prefix 提升 Raft CF 插入性能 优化单行读事务性能 支持更多下推功能 加入更多统计 修复 Bug  "},
		{"url": "https://pingcap.com/docs/releases/rc3/",
		"title": "TiDB RC3 Release Notes", 
		"content": " TiDB RC3 Release Notes On June 20, 2017, TiDB RC4 is released!This release is focused on MySQL compatibility, SQL optimization, stability, and performance.Highlight:  The privilege management is refined to enable users to manage the data access privileges using the same way as in MySQL. DDL is accelerated. The load balancing policy and process are optimized for performance. TiDB-Ansible is open sourced. By using TiDB-Ansilbe, you can deploy, upgrade, start and shutdown a TiDB cluster with one click.  Detailed updates: TiDB:  The following features are added or improved in the SQL query optimizer:  Support incremental statistics Support the Merge Sort Join operator Support the Index Lookup Join operator Support the Optimizer Hint Syntax Optimize the memory consumption of the Scan, Join, Aggregation operators Optimize the Cost Based Optimizer (CBO) framework Refactor Expression  Support more complete privilege management DDL acceleration Support using HTTP API to get the data distribution information of tables Support using system variables to control the query concurrency Add more MySQL built-in functions Support using system variables to automatically split a big transaction into smaller ones to commit  Placement Driver (PD):  Support gRPC Provide the Disaster Recovery Toolkit Use Garbage Collection to clear stale data automatically Support more efficient data balance Support hot Region scheduling to enable load balancing and speed up the data importing Performance  Accelerate getting Client TSO Improve the efficiency of Region Heartbeat processing  Improve the pd-ctl function  Update the Replica configuration dynamically Get the Timestamp Oracle (TSO) Use ID to get the Region information   TiKV:  Support gRPC Support the Sorted String Table (SST) format snapshot to improve the load balancing speed of a cluster Support using the Heap Profile to uncover memory leaks Support Streaming SIMD Extensions (SSE) and speed up the CRC32 calculation Accelerate transferring leader for faster load balancing Use Batch Apply to reduce CPU usage and improve the write performance Support parallel Prewrite to improve the transaction write speed Optimize the scheduling of the coprocessor thread pool to reduce the impact of big queries on point get The new Loader supports data importing at the table level, as well as splitting a big table into smaller logical blocks to import concurrently to improve the data importing speed.  "},
		{"url": "https://pingcap.com/docs-cn/releases/rc4/",
		"title": "TiDB RC4 Release Notes", 
		"content": " TiDB RC4 Release Notes 8 月 4 日，TiDB 正式发布 RC4 版。该版本对 MySQL 兼容性、SQL 优化器、系统稳定性、性能做了大量的工作。性能方面重点优化了写入速度，计算任务调度支持优先级，避免分析型大事务影响在线事务。SQL 优化器全新改版，查询代价估算更加准确，且能够自动选择 Join 物理算子。功能方面进一步 MySQL 兼容性。 同时为了更好的支持 OLAP 业务，开源了 TiSpark 项目，可以通过 Spark 读取和分析 TiKV 中的数据。TiDB:  SQL 查询优化器重构  更好的支持 TopN 查询 支持 Join 算子根据代价自动选择 更完善的 Projection Elimination  Schema 版本检查区分 Table，避免 DDL 干扰其他正在执行的事务 支持 BatchIndexJoin 完善 Explain 语句 提升 Index Scan 性能 大量 MySQL 兼容性相关功能 支持 Json 类型及其操作 支持查询优先级、隔离级别的设置  PD:  支持通过 PD 设置 TiKV location labels 调度优化  支持 PD 主动向 TiKV 下发调度命令 加快 region heartbeat 响应速度 优化 balance 算法  优化数据加载，加快 failover 速度  TiKV:  支持查询优先级设置 支持 RC 隔离级别 完善 Jepsen，提升稳定性 支持 Document Store Coprocessor 支持更多下推函数 提升性能，提升稳定性  TiSpark Beta Release:  支持谓词下推 支持聚合下推 支持范围裁剪 通过 TPC-H 测试 (除去一个需要 View 的 Query)  "},
		{"url": "https://pingcap.com/docs/releases/rc4/",
		"title": "TiDB RC4 Release Notes", 
		"content": " TiDB RC4 Release Notes On August 4, 2017, TiDB RC4 is released! This release is focused on MySQL compatibility, SQL optimization, stability, and performance.Highlight:  For performance, the write performance is improved significantly, and the computing task scheduling supports prioritizing to avoid the impact of OLAP on OLTP. The optimizer is revised for a more accurate query cost estimating and for an automatic choice of the Join physical operator based on the cost. Many enhancements have been introduced to be more compatible with MySQL. TiSpark is now released to better support the OLAP business scenarios. You can now use Spark to access the data in TiKV.  Detailed updates: TiDB:  The SQL query optimizer refactoring:  Better support for TopN queries Support the automatic choice of the of the Join physical operator based on the cost Improved Projection Elimination  The version check of schema is based on Table to avoid the impact of DDL on the ongoing transactions Support BatchIndexJoin Improve the Explain statement Improve the Index Scan performance Many enhancements have been introduced to be more compatible with MySQL Support the JSON type and operations Support the configuration of query prioritizing and isolation level  Placement Driver (PD):  Support using PD to set the TiKV location labels Optimize the scheduler  PD is now supported to initialize the scheduling commands to TiKV. Accelerate the response speed of the region heartbeat. Optimize the balance algorithm  Optimize data loading to speed up failover  TiKV:  Support the configuration of query prioritizing Support the RC isolation level Improve Jepsen test results and the stability Support Document Store Coprocessor now supports more pushdown functions Improve the performance and stability  TiSpark Beta Release:  Implement the prediction pushdown Implement the aggregation pushdown Implement range pruning Capable of running full set of TPC-H except one query that needs view support  "},
		{"url": "https://pingcap.com/docs/ROADMAP/",
		"title": "TiDB Roadmap", 
		"content": " TiDB Roadmap This document defines the roadmap for TiDB development.TiDB： Optimizer Refactor Ranger Optimize the statistics info Optimize the cost model  Executor Parallel Operators Compact Row Format to reduce memory usage File Sort  Support View Support Window Function Common Table Expression Table Partition Hash time index to resolve the issue with hot regions Reverse Index Cluster Index Improve DDL Support utf8_general_ci collation  TiKV: Raft Region merge Local read thread Multi-thread raftstore None voter Pre-vote  RocksDB DeleteRange  Transaction Optimize transaction conflicts  Coprocessor Streaming  Tool Import distributed data Export distributed data Disaster Recovery  Flow control and degradation  PD:  [ ] Improve namespace [ ] Different replication policies for different namespaces and tables [ ] Decentralize scheduling table regions [ ] Scheduler supports prioritization to be more controllable  [ ] Use machine learning to optimize scheduling  TiSpark: Limit / Order push-down Access through the DAG interface and deprecate the Select interface Index Join and parallel merge join Data Federation  SRE &amp;amp; tools: Kubernetes based intergration for the on-premise version Dashboard UI for the on-premise version The cluster backup and recovery tool The data migration tool (Wormhole V2) Security and system diagnosis  "},
		{"url": "https://pingcap.com/docs/sql/transaction-isolation/",
		"title": "TiDB Transaction Isolation Levels", 
		"content": " TiDB Transaction Isolation Levels Transaction isolation is one of the foundations of database transaction processing. Isolation is the I in the acronym ACID (Atomicity, Consistency, Isolation, Durability), which represents the isolation property of database transactions.The SQL-92 standard defines four levels of transaction isolation: Read Uncommitted, Read Committed, Repeatable Read and Serializable. See the following table for details:   Isolation Level Dirty Read Nonrepeatable Read Phantom Read Serialization Anomaly     Read Uncommitted Possible Possible Possible Possible   Read Committed Not possible Possible Possible Possible   Repeatable Read Not possible Not possible Not possible in TiDB Possible   Serializable Not possible Not possible Not possible Not possible    TiDB offers two transaction isolation levels: Read Committed and Repeatable Read.TiDB uses the Percolator transaction model. A global read timestamp is obtained when the transaction is started, and a global commit timestamp is obtained when the transaction is committed. The execution order of transactions is confirmed based on the timestamps. To know more about the implementation of TiDB transaction model, see MVCC in TiKV.Use the following command to set the level of transaction isolation:SET SESSION TRANSACTION ISOLATION LEVEL [read committed|repeatable read] Repeatable Read Repeatable Read is the default transaction isolation level in TiDB. The Repeatable Read isolation level only sees data committed before the transaction begins, and it never sees either uncommitted data or changes committed during transaction execution by concurrent transactions. However, the transaction statement does see the effects of previous updates executed within its own transaction, even though they are not yet committed.For transactions running on different nodes, the start and commit order depends on the order that the timestamp is obtained from PD.Transactions of the Repeatable Read isolation level cannot concurrently update a same row. When committing, if the transaction finds that the row has been updated by another transaction after it starts, then the transaction rolls back and retries automatically. For example:create table t1(id int); insert into t1 values(0); start transaction; | start transaction; select * from t1; | select * from t1; update t1 set id=id+1; | update t1 set id=id+1; commit; | | commit; -- roll back and retry atutomatically Difference between TiDB and ANSI Repeatable Read The Repeatable Read isolation level in TiDB differs from ANSI Repeatable Read isolation level, though they sharing the same name. According to the standard described in the A Critique of ANSI SQL Isolation Levels paper, TiDB implements the snapshot isolation level, and it does not allow phantom reads but allows write skews. In contrast, the ANSI Repeatable Read isolation level allows phantom reads but does not allow write skews.Difference between TiDB and MySQL Repeatable Read The Repeatable Read isolation level in TiDB differs from that in MySQL. The MySQL Repeatable Read isolation level does not check whether the current version is visible when updating, which means it can continue to update even if the row has been updated after the transaction starts. In contrast, if the row has been updated after the transaction starts, the TiDB transaction is rolled back and retried. Transaction Retries in TiDB might fail, leading to a final failure of the transaction, while in MySQL the updating transaction can be successful.The MySQL Repeatable Read isolation level is not the snapshot isolation level. The consistency of MySQL Repeatable Read isolation level is weaker than both the snapshot isolation level and TiDB Repeatable Read isolation level.Read Committed The Read Committed isolation level differs from Repeatable Read isolation level. Read Committed only guarantees the uncommitted data cannot be read.Note: Because the transaction commit is a dynamic process, the Read Committed isolation level might read the data committed by part of the transaction. It is not recommended to use the Read Committed isolation level in a database that requires strict consistency.Transaction retry For the insert/delete/update operation, if the transaction fails and can be retried according to the system, the transaction is automatically retried within the system.You can control the number of retries by configuring the retry-limit parameter:[performance] ... # The maximum number of retries when commit a transaction. retry-limit = 10"},
		{"url": "https://pingcap.com/docs/sql/user-account-management/",
		"title": "TiDB User Account Management", 
		"content": " TiDB User Account Management User names and passwords TiDB stores the user accounts in the table of the mysql.user system database. Each account is identified by a user name and the client host. Each account may have a password.You can connect to the TiDB server using the MySQL client, and use the specified account and password to login:shell&amp;gt; mysql --port 4000 --user xxx --password Or use the abbreviation of command line parameters:shell&amp;gt; mysql -P 4000 -u xxx -p Add user accounts You can create TiDB accounts in two ways: By using the standard account-management SQL statements intended for creating accounts and establishing their privileges, such as CREATE USER and GRANT. By manipulating the grant tables directly with statements such as INSERT, UPDATE, or DELETE.  It is recommended to use the account-management statements, because manipulating the grant tables directly can lead to incomplete updates. You can also create accounts by using third party GUI tools.The following example uses the CREATE USER and GRANT statements to set up four accounts:mysql&amp;gt; CREATE USER &amp;#39;finley&amp;#39;@&amp;#39;localhost&amp;#39; IDENTIFIED BY &amp;#39;some_pass&amp;#39;; mysql&amp;gt; GRANT ALL PRIVILEGES ON *.* TO &amp;#39;finley&amp;#39;@&amp;#39;localhost&amp;#39; WITH GRANT OPTION; mysql&amp;gt; CREATE USER &amp;#39;finley&amp;#39;@&amp;#39;%&amp;#39; IDENTIFIED BY &amp;#39;some_pass&amp;#39;; mysql&amp;gt; GRANT ALL PRIVILEGES ON *.* TO &amp;#39;finley&amp;#39;@&amp;#39;%&amp;#39; WITH GRANT OPTION; mysql&amp;gt; CREATE USER &amp;#39;admin&amp;#39;@&amp;#39;localhost&amp;#39; IDENTIFIED BY &amp;#39;admin_pass&amp;#39;; mysql&amp;gt; GRANT RELOAD,PROCESS ON *.* TO &amp;#39;admin&amp;#39;@&amp;#39;localhost&amp;#39;; mysql&amp;gt; CREATE USER &amp;#39;dummy&amp;#39;@&amp;#39;localhost&amp;#39;; To see the privileges for an account, use SHOW GRANTS:mysql&amp;gt; SHOW GRANTS FOR &amp;#39;admin&amp;#39;@&amp;#39;localhost&amp;#39;; +-----------------------------------------------------+ | Grants for admin@localhost | +-----------------------------------------------------+ | GRANT RELOAD, PROCESS ON *.* TO &amp;#39;admin&amp;#39;@&amp;#39;localhost&amp;#39; | +-----------------------------------------------------+ Remove user accounts To remove a user account, use the DROP USER statement:mysql&amp;gt; DROP USER &amp;#39;jeffrey&amp;#39;@&amp;#39;localhost&amp;#39;; Reserved user accounts TiDB creates the &#39;root&#39;@&#39;%&#39; default account during the database initialization.Set account resource limits Currently, TiDB does not support setting account resource limits.Assign account passwords TiDB stores passwords in the mysql.user system database. Operations that assign or update passwords are permitted only to users with the CREATE USER privilege, or, alternatively, privileges for the mysql database (INSERT privilege to create new accounts, UPDATE privilege to update existing accounts).To assign a password when you create a new account, use CREATE USER and include an IDENTIFIED BY clause:CREATE USER &amp;#39;jeffrey&amp;#39;@&amp;#39;localhost&amp;#39; IDENTIFIED BY &amp;#39;mypass&amp;#39;; To assign or change a password for an existing account, use SET PASSWORD FOR or ALTER USER:SET PASSWORD FOR &amp;#39;root&amp;#39;@&amp;#39;%&amp;#39; = &amp;#39;xxx&amp;#39;; Or:ALTER USER &amp;#39;jeffrey&amp;#39;@&amp;#39;localhost&amp;#39; IDENTIFIED BY &amp;#39;mypass&amp;#39;;"},
		{"url": "https://pingcap.com/docs/sql/user-manual/",
		"title": "TiDB User Guide", 
		"content": " TiDB User Guide TiDB supports the SQL-92 standard and is compatible with MySQL. To help you easily get started with TiDB, TiDB user guide mainly inherits the MySQL document structure with some TiDB specific changes.TiDB server administration  The TiDB Server The TiDB Command Options The TiDB Data Directory The TiDB System Database The TiDB System Variables The Proprietary System Variables and Syntax in TiDB The TiDB Server Logs The TiDB Access Privilege System TiDB User Account Management Use Encrypted Connections  SQL optimization  Understand the Query Execution Plan Introduction to Statistics  Language structure  Literal Values Schema Object Names Keywords and Reserved Words User-Defined Variables Expression Syntax Comment Syntax  Globalization  Character Set Support Character Set Configuration Time Zone  Data types  Numeric Types Date and Time Types String Types JSON Types The ENUM data type The SET Type Data Type Default Values  Functions and operators  Function and Operator Reference Type Conversion in Expression Evaluation Operators Control Flow Functions String Functions Numeric Functions and Operators Date and Time Functions Bit Functions and Operators Cast Functions and Operators Encryption and Compression Functions Information Functions JSON Functions Functions Used with Global Transaction IDs [TBD] Aggregate (GROUP BY) Functions Miscellaneous Functions Precision Math  SQL statement syntax  Data Definition Statements Data Manipulation Statements Transactions Database Administration Statements Prepared SQL Statement Syntax Utility Statements TiDB SQL Syntax Diagram  JSON functions and generated column  JSON Functions and Generated Column  Connectors and APIs  Connectors and APIs  Compatibility with MySQL  Compatibility with MySQL  "},
		{"url": "https://pingcap.com/docs-cn/sql/tidb-specific/",
		"title": "TiDB 专用系统变量和语法", 
		"content": " TiDB 专用系统变量和语法 TiDB 在 MySQL 的基础上，定义了一些专用的系统变量和语法用来优化性能。System Variable 变量可以通过 SET 语句设置，例如set @@tidb_distsql_scan_concurrency = 10如果需要设值全局变量，执行set @@global.tidb_distsql_scan_concurrency = 10tidb_distsql_scan_concurrency 作用域: SESSION | GLOBAL默认值: 10这个变量用来设置 scan 操作的并发度，AP 类应用适合较大的值，TP 类应用适合较小的值。 对于 AP 类应用，最大值建议不要超过所有 TiKV 节点的 CPU 核数。tidb_index_lookup_size 作用域: SESSION | GLOBAL默认值: 20000这个变量用来设置 index lookup 操作的 batch 大小，AP 类应用适合较大的值，TP 类应用适合较小的值。tidb_index_lookup_concurrency 作用域: SESSION | GLOBAL默认值: 4这个变量用来设置 index lookup 操作的并发度，AP 类应用适合较大的值，TP 类应用适合较小的值。tidb_index_serial_scan_concurrency 作用域：SESSION | GLOBAL默认值：1这个变量用来设置顺序 scan 操作的并发度，AP 类应用适合较大的值，TP 类应用适合较小的值。tidb_index_join_batch_size 作用域：SESSION | GLOBAL默认值：25000这个变量用来设置 index lookup join 操作的 batch 大小，AP 类应用适合较大的值，TP 类应用适合较小的值。tidb_max_row_count_for_inlj 作用域：SESSION | GLOBAL默认值：128这个变量用来设置安全启用 Index Nested Loop Join 算法的外表大小。 如果外表行数大于这个值，只有使用 hint 语法才会启用 Index Nested Loop Join 算法。Optimizer Hint TiDB 在 MySQL 的 Optimizer Hint 语法上，增加了一些 TiDB 专有的 Hint 语法, 使用这些 Hint 的时候，TiDB 优化器会尽量使用指定的算法，在某些场景下会比默认算法更优。由于 hint 包含在类似 /*+ xxx */ 的 comment 里，MySQL 客户端在 5.7.7 之前，会默认把 comment 清除掉，如果需要在旧的客户端使用 hint，需要在启动客户端时加上 --comments 选项，例如 mysql -h 127.0.0.1 -P 4000 -uroot --commentsTIDB_SMJ(t1, t2) SELECT /*+ TIDB_SMJ(t1, t2) */ * from t1，t2 where t1.id = t2.id提示优化器使用 Sort Merge Join 算法，这个算法通常会占用更少的内存，但执行时间会更久。 当数据量太大，或系统内存不足时，建议尝试使用。TIDB_INLJ(t1, t2) SELECT /*+ TIDB_INLJ(t1, t2) */ * from t1，t2 where t1.id = t2.id提示优化器使用 Index Nested Loop Join 算法，这个算法可能会在某些场景更快，消耗更少系统资源，有的场景会更慢，消耗更多系统资源。对于外表经过 WHERE 条件过滤后结果集较小（小于 1 万行）的场景，可以尝试使用。TIDB_INLJ()中的参数是建立查询计划时，驱动表（外表）的候选表。即TIDB_INLJ(t1)只会考虑使用t1作为驱动表构建查询计划。"},
		{"url": "https://pingcap.com/docs-cn/sql/transaction-isolation/",
		"title": "TiDB 事务隔离级别", 
		"content": " TiDB 事务隔离级别 事务隔离级别是数据库事务处理的基础，ACID 中 I，即 Isolation，指的就是事务的隔离性。sql 92标准定义了4种隔离级别，读未提交、读已提交、可重复读、串行化，见下表。   Isolation Level Dirty Read Nonrepeatable Read Phantom Read Serialization Anomaly     Read uncommitted Possible Possible Possible Possible   Read committed Not possible Possible Possible Possible   Repeatable read Not possible Not possible Not possible in TiDB Possible   Serializable Not possible Not possible Not possible Not possible    TiDB 实现了其中的两种：读已提交和可重复读。TiDB 使用percolator事务模型，当事务启动时会获取全局读时间戳，事务提交时也会获取全局提交时间戳，并以此确定事务的执行顺序，如果想了解 TiDB 事务模型的实现可以详细阅读以下两篇文章：TiKV 的 MVCC（Multi-Version Concurrency Control）机制，Percolator 和 TiDB 事务算法。可以通过以下命令设置事务的隔离级别：SET SESSION TRANSACTION ISOLATION LEVEL [read committed|repeatable read]可重复读 可重复读是 TiDB 的默认隔离级别，当事务隔离级别为可重复读时，只能读到该事务启动时已经提交的其他事务修改的数据，未提交的数据或在事务启动后其他事务提交的数据是不可见的。对于本事务而言，事务语句可以看到之前的语句做出的修改。对于运行于不同节点的事务而言，不同事务启动和提交的顺序取决于从 PD 获取时间戳的顺序。处于可重复读隔离级别的事务不能并发的更新同一行，当时事务提交时发现该行在该事务启动后，已经被另一个已提交的事务更新过，那么该事务会回滚并启动自动重试。示例如下：create table t1(id int); insert into t1 values(0); start transaction; | start transaction; select * from t1; | select * from t1; update t1 set id=id+1; | update t1 set id=id+1; commit; | | commit; --回滚并自动重试 与 ANSI 可重复读隔离级别的区别 尽管名称是可重复读隔离级别，但是 TiDB 中可重复读隔离级别和 ANSI 可重复隔离级别是不同的，按照A Critique of ANSI SQL Isolation Levels论文中的标准，TiDB 实现的是论文中的 snapshot 隔离级别，该隔离级别不会出现幻读，但是会出现写偏斜，而 ANSI 可重复读隔离级别不会出现写偏斜，会出现幻读。与MySQL可重复读隔离级别的区别 MySQL 可重复读隔离级别在更新时并不检验当前版本是否可见，也就是说，即使该行在事务启动后被更新过，同样可以继续更新。这种情况在 TiDB 会导致事务回滚并后台重试，重试最终可能会失败，导致事务最终失败，而 MySQL 是可以更新成功的。 MySQL 的可重复读隔离级别并非 snapshot 隔离级别，MySQL 可重复读隔离级别的一致性要弱于 snapshot 隔离级别，也弱于 TiDB 的可重复读隔离级别。读已提交 读已提交隔离级别和可重复读隔离级别不同，它仅仅保证不能读到未提交事务的数据，需要注意的是，事务提交是一个动态的过程，因此读已提交隔离级别可能读到某个事务部分提交的数据。不推荐在有严格一致要求的数据库中使用读已提交隔离级别。事务重试 对于 insert/delete/update 操作，如果事务执行失败，并且系统判断该错误为可重试，会在系统内部自动重试事务。通过配置参数 retry-limit 可控制自动重试的次数：[performance] ... # The maximum number of retries when commit a transaction. retry-limit = 10"},
		{"url": "https://pingcap.com/docs-cn/op-guide/history-read/",
		"title": "TiDB 历史数据回溯", 
		"content": " TiDB 历史数据回溯 本文档用于描述 TiDB 如何读取历史版本数据，包括具体的操作流程以及历史数据的保存策略。功能说明 TiDB 实现了通过标准 SQL 接口读取历史数据功能，无需特殊的 client 或者 driver。当数据被更新、删除后，依然可以通过 SQL 接口将更新/删除前的数据读取出来。另外即使在更新数据之后，表结构发生了变化，TiDB 依旧能用旧的表结构将数据读取出来。操作流程 为支持读取历史版本数据， 引入了一个新的 system variable: tidb_snapshot ，这个变量是 Session 范围有效，可以通过标准的 Set 语句修改其值。其值为文本，记录了时间，格式为： “2016-10-08 16:45:26.999”，一般来说可以只写到秒，比如”2016-10-08 16:45:26”。 当这个变量被设置时，TiDB 会用这个时间戳建立 Snapshot（没有开销，只是创建数据结构），随后所有的 Select 操作都会在这个 Snapshot 上读取数据。 注意 TiDB 的事务是通过 PD 进行全局授时，所以存储的数据版本也是以 PD 所授时间戳作为版本号。在生成 Snapshot ·时，是以 tidb_snapshot 变量的值作为版本号，如果 TiDB Server 所在机器和 PD Server 所在机器的本地时间相差较大，需要以 PD 的时间为准。 当读取历史版本操作结束后，可以结束当前 Session 或者是通过 Set 语句将 tidb_snapshot 变量的值设为 ”“，即可读取最新版本的数据。历史数据保留策略 TiDB 使用 MVCC 管理版本，当更新/删除数据时，不会做真正的数据删除，只会添加一个新版本数据，所以可以保留历史数据。历史数据不会全部保留，超过一定时间的历史数据会被彻底删除，以减小空间占用以及避免历史版本过多引入的性能开销。我们使用周期性运行的 GC （Garbage Collection， 垃圾回收）来进行清理，GC 的触发方式为：每个 TiDB 启动后会在后台运行一个 gc_worker，每个集群中有一个 gc_worker 会被自动选为 leader，leader 负责维护 GC 的状态并向所有的 TiKV region leader 发送 GC 命令。GC 的运行状态记录记录在 mysql.tidb 系统表中，可通过 SQL 语句进行监测与配置。mysql&amp;gt; select variable_name, variable_value from mysql.tidb; +-----------------------+----------------------------+ | variable_name | variable_value | +-----------------------+----------------------------+ | bootstrapped | True | | tikv_gc_leader_uuid | 55daa0dfc9c0006 | | tikv_gc_leader_desc | host:pingcap-pc5 pid:10549 | | tikv_gc_leader_lease | 20160927-13:18:28 +0800 CST| | tikv_gc_run_interval | 10m0s | | tikv_gc_life_time | 10m0s | | tikv_gc_last_run_time | 20160927-13:13:28 +0800 CST| | tikv_gc_safe_point | 20160927-13:03:28 +0800 CST| +-----------------------+----------------------------+ 7 rows in set (0.00 sec) 其中需要重点关注的是 tikv_gc_life_time 和 tikv_gc_safe_point 这两行。tikv_gc_life_time 用于配置历史版本保留时间（默认值为 10m），用户可以使用 SQL 进行配置。比如我们需要一天内的所有历史版本都可读，那么可以使用 SQL update mysql.tidb set variable_value=&#39;24h&#39; where variable_name=&#39;tikv_gc_life_time&#39; 将此行设置为 24 小时。时长字符串的形式是数字后接时间单位的序列，如 24h、2h30m、2.5h。可以使用的时间单位包括 &amp;ldquo;h&amp;rdquo;、&amp;rdquo;m&amp;rdquo;、&amp;rdquo;s&amp;rdquo;。tikv_gc_safe_point 记录了当前的 safePoint，用户可以安全地使用大于 safePoint 的时间戳创建 snapshot 读取历史版本。safePoint 在每次 GC 开始运行时自动更新。需要注意的是，在数据更新频繁的场景下如果将 tikv_gc_life_time 设置得比较大（如数天甚至数月），可能会有一些潜在的问题： 随着版本的不断增多，数据占用的磁盘空间会随之增加。 大量的历史版本会在一定程度上导致查询变慢，主要影响范围查询（select count(*) from t）。 如果在运行中突然将 tikv_gc_life_time 配置调小，可能会导致大量历史数据被删除，造成 I/O 负担。  示例 初始化阶段，创建一个表，并插入几行数据：mysql&amp;gt; create table t (c int); Query OK, 0 rows affected (0.01 sec) mysql&amp;gt; insert into t values (1), (2), (3); Query OK, 3 rows affected (0.00 sec) 查看表中的数据：mysql&amp;gt; select * from t; +------+ | c | +------+ | 1 | | 2 | | 3 | +------+ 3 rows in set (0.00 sec) 查看当前时间：mysql&amp;gt; select now(); +---------------------+ | now() | +---------------------+ | 2016-10-08 16:45:26 | +---------------------+ 1 row in set (0.00 sec) 更新某一行数据：mysql&amp;gt; update t set c=22 where c=2; Query OK, 1 row affected (0.00 sec) 确认数据已经被更新：mysql&amp;gt; select * from t; +------+ | c | +------+ | 1 | | 22 | | 3 | +------+ 3 rows in set (0.00 sec) 设置一个特殊的环境变量，这个是一个 session scope 的变量，其意义为读取这个时间之前的最新的一个版本。注意这里的时间设置的是 update 语句之前的那个时间：mysql&amp;gt; set @@tidb_snapshot=&amp;#34;2016-10-08 16:45:26&amp;#34;; Query OK, 0 rows affected (0.00 sec) 这里读取到的内容即为 update 之前的内容，也就是历史版本：mysql&amp;gt; select * from t; +------+ | c | +------+ | 1 | | 2 | | 3 | +------+ 3 rows in set (0.00 sec) 清空这个变量后，即可读取最新版本数据：mysql&amp;gt; set @@tidb_snapshot=&amp;#34;&amp;#34;; Query OK, 0 rows affected (0.00 sec)mysql&amp;gt; select * from t; +------+ | c | +------+ | 1 | | 22 | | 3 | +------+ 3 rows in set (0.00 sec)"},
		{"url": "https://pingcap.com/docs-cn/sql/tidb-server/",
		"title": "TiDB 数据库管理", 
		"content": " TiDB 数据库管理 TiDB 服务 TiDB 是指 TiDB 数据库系统，本篇文档涉及到 TiDB 集群的基本管理功能。TiDB 集群启动配置 可以通过命令行参数或者配置文件设置服务参数，或者是两者一起使用。注意命令行参数的优先级高于配置文件，如果同一个参数两种方式都设置，会以命令行参数中的值为准。具体信息参考这篇文档。TiDB 数据库系统变量 TiDB 兼容 MySQL 的系统变量，同时定义了一些特有的系统变量用于调整数据库行为，具体信息参考 TiDB 专用系统变量和语法 文档。TiDB 系统表 和 MySQL 类似，TiDB 中也有系统表，用于存放数据库运行时所需信息。具体信息参考 TiDB 系统数据库文档。TiDB 数据目录 TiDB 数据存放在存储引擎中，数据目录取决于使用的存储引擎，存储引擎的选择参见 TiDB 启动参数文档。对于使用本地存储引擎的情况，数据存储在本机硬盘上，目录位置通过 path 参数控制。对于使用 TiKV 引擎的情况，数据存储在 TiKV 节点上，目录位置通过 data-dir 参数控制。TiDB 服务器日志文件 TiDB 集群的三个组件（tidb-server、tikv-server、pd-server）默认会将日志输出到标准错误中，并且三个组件都支持设置 --log-file 启动参数 （或者是配置文件中的配置项）将日志输出到文件中。通过配置文件可以调整日志的行为，具体信息请参见各个组件的配置文件说明。例如： tidb-server 日志配置项。"},
		{"url": "https://pingcap.com/docs-cn/sql/datatype/",
		"title": "TiDB 数据类型", 
		"content": " TiDB 数据类型 概述 TiDB 支持 MySQL 除空间类型之外的所有数据类型，包括数值型类型、字符串类型、时间&amp;amp;日期类型、Json 类型。数据类型定义一般为 T(M[, D])，其中: T 表示具体的类型 M 对于整数类型表示最大显示长度；对于浮点数或者定点数表示精度；对于字符类型表示最大长度。M 的最大值取决于具体的类型。 D 表示浮点数/定点数的小数位长度 对于时间&amp;amp;日期类型中的 TIME、DATETIME 以及 TIMESTAMP，定义中可以包含 Fsp 表示秒的精度，其取值范围是0到6，默认的精度为0  数值类型 概述 TiDB 支持 MySQL 所有的数值类型，按照精度可以分为: 整数类型（精确值) 浮点类型（近似值) 定点类型（精确值)  整数类型 TiDB 支持 MySQL 所有的整数类型，包括 INTEGER/INT、TINYINT、SMALLINT、MEDIUMINT 以及 BIGINT，完整信息参考这篇文档。类型定义 语法：BIT[(M)] &amp;gt; 比特值类型。M 表示比特位的长度，取值范围从1到64，其默认值是1。 TINYINT[(M)] [UNSIGNED] [ZEROFILL] &amp;gt; TINYINT 类型。有符号数的范围是[-128, 127]。无符号数的范围是[0, 255]。 BOOL, BOOLEAN &amp;gt; 布尔类型，和 TINYINT(1) 等价。零值被认为是 False，非零值认为是 True。在 TiDB 内部，True 存储为1， False 存储为0。 SMALLINT[(M)] [UNSIGNED] [ZEROFILL] &amp;gt; SMALLINT 类型。有符号数的范围是[-32768, 32767]。无符号数的范围是[0, 65535]。 MEDIUMINT[(M)] [UNSIGNED] [ZEROFILL] &amp;gt; MEDIUMINT 类型。有符号数的范围是[-8388608, 8388607]。无符号数的范围是[0, 16777215]。 INT[(M)] [UNSIGNED] [ZEROFILL] &amp;gt; INT 类型。 有符号数的范围是[-2147483648, 2147483647]。无符号数的范围是[0, 4294967295]。 INTEGER[(M)] [UNSIGNED] [ZEROFILL] &amp;gt; 和 INT 相同。 BIGINT[(M)] [UNSIGNED] [ZEROFILL] &amp;gt; BIGINT 类型。 有符号数的范围是[-9223372036854775808, 9223372036854775807]。无符号数的范围是[0, 18446744073709551615]。 字段意义:   语法元素 说明     M 类型长度，可选的   UNSIGNED 无符号数，如果不加这个标识，则为有符号数   ZEROFILL 补零标识，如果有这个标识，TiDB 会自动给类型增加 UNSIGNED 标识，但是没有做补零的操作    存储空间以及取值范围 每种类型对存储空间的需求以及最大/最小值如下表所示:   类型 存储空间 最小值(有符号/无符号) 最大值(有符号/无符号)     TINYINT 1 -128 / 0 127 / 255   SMALLINT 2 -32768 / 0 32767 / 65535   MEDIUMINT 3 -8388608 / 0 8388607 / 16777215   INT 4 -2147483648 / 0 2147483647 / 4294967295   BIGINT 8 -9223372036854775808 / 0 9223372036854775807 / 18446744073709551615    浮点类型 TiDB 支持 MySQL 所有的浮点类型，包括 FLOAT、DOUBLE，完整信息参考这篇文档。类型定义 语法：FLOAT[(M,D)] [UNSIGNED] [ZEROFILL] &amp;gt; 单精度浮点数。允许的值范围为 -2^128 ~ +2^128，也即 -3.402823466E+38 到 -1.175494351E-38、0 和 1.175494351E-38 到 3.402823466E+38。这些是理论限制，基于 IEEE 标准。实际的范围根据硬件或操作系统的不同可能稍微小些。 DOUBLE[(M,D)] [UNSIGNED] [ZEROFILL] &amp;gt; 双精度浮点数。允许的值范围为：-2^1024 ~ +2^1024，也即是 -1.7976931348623157E+308 到 -2.2250738585072014E-308、0 和 2.2250738585072014E-308 到 1.7976931348623157E+308。这些是理论限制，基于 IEEE 标准。实际的范围根据硬件或操作系统的不同可能稍微小些。 DOUBLE PRECISION [(M,D)] [UNSIGNED] [ZEROFILL], REAL[(M,D)] [UNSIGNED] [ZEROFILL] &amp;gt; 为 DOUBLE 的同义词。 FLOAT(p) [UNSIGNED] [ZEROFILL] &amp;gt; 浮点数。p 表示精度（以位数表示），只使用该值来确定是否结果列的数据类型为 FLOAT 或 DOUBLE。如果 p 为从 0 到 24，数据类型变为没有 M 或 D 值的 FLOAT。如果 p 为从 25 到 53，数据类型变为没有 M 或 D 值的 DOUBLE。结果列范围与本节前面描述的单精度 FLOAT 或双精度 DOUBLE 数据类型相同。 字段意义:   语法元素 说明     M 小数总位数   D 小数点后位数   UNSIGNED 无符号数，如果不加这个标识，则为有符号数   ZEROFILL 补零标识，如果有这个标识，TiDB 会自动给类型增加 UNSIGNED 标识    存储空间 每种类型对存储空间的需求如下表所示:   类型 存储空间     FLOAT 4   FLOAT(p) 如果 0 &amp;lt;= p &amp;lt;= 24 为 4 个字节, 如果 25 &amp;lt;= p &amp;lt;= 53 为 8 个字节   DOUBLE 8    定点类型 TiDB 支持 MySQL 所有的浮点类型，包括 DECIMAL、NUMERIC，完整信息参考这篇文档。类型定义 语法：DECIMAL[(M[,D])] [UNSIGNED] [ZEROFILL] &amp;gt; 定点数。M 是小数位数(精度)的总数，D 是小数点(标度)后面的位数。小数点和‘-’(负数)符号不包括在M中。如果 D 是 0，则值没有小数点或分数部分。如果 D 被省略， 默认是 0。如果 M 被省略， 默认是 10。 NUMERIC[(M[,D])] [UNSIGNED] [ZEROFILL] &amp;gt; DECIMAL的同义词。 字段意义:   语法元素 说明     M 小数总位数   D 小数点后位数   UNSIGNED 无符号数，如果不加这个标识，则为有符号数   ZEROFILL 补零标识，如果有这个标识，TiDB 会自动给类型增加 UNSIGNED 标识    日期时间类型 概述 TiDB 支持 MySQL 所有的日期时间类型，包括 DATE、DATETIME、TIMESTAMP、TIME 以及 YEAR，完整信息参考这篇文档。类型定义 语法：DATE &amp;gt; 日期。支持的范围为`1000-01-01`到`9999-12-31`。以`YYYY-MM-DD`格式显示 DATE 值。 DATETIME[(fsp)] &amp;gt; 日期和时间的组合。支持的范围是`1000-01-01 00:00:00.000000`到`9999-12-31 23:59:59.000000`。 以`YYYY-MM-DD HH:MM:SS[.fraction]`格式显示 DATETIME 值。fsp 参数是表示秒精度，取值范围为 0-6，默认值取 0。 TIMESTAMP[(fsp)] &amp;gt; 时间戳。支持的范围是`1970-01-01 00:00:01.000000`到`2038-01-19 03:14:07.999999`。 fsp 参数是表示秒精度，取值范围为 0-6，默认值取 0。 TIME[(fsp)] &amp;gt; 时间。范围是`-838:59:59.000000`到`838:59:59.000000`。以`HH:MM:SS[.fraction]`格式显示 TIME 值。 fsp 参数是表示秒精度，取值范围为：0-6。默认值取 0。 YEAR[(2|4)] &amp;gt; 两位或四位格式的年。默认是四位格式。在四位格式中，允许的值是 1901 到 2155 和 0000。在两位格式中，允许的值是 70 到 69，表示从 1970 年到 2069 年。 字符串类型 概述 TiDB 支持 MySQL 所有的字符串类型，包括 CHAR、VARCHAR、BINARY、VARBINARY、BLOB、TEXT、ENUM 以及 SET， 完整信息参考这篇文档。类型定义 语法：[NATIONAL] CHAR[(M)] [CHARACTER SET charset_name] [COLLATE collation_name] &amp;gt; 定长字符串。CHAR 列的长度固定为创建表时声明的长度。长度可以为从 0 到 255 的任何值。当保存 CHAR 值时，在它们的右边填充空格以达到指定的长度。 [NATIONAL] VARCHAR(M) [CHARACTER SET charset_name] [COLLATE collation_name] &amp;gt; 变长字符串。M 表示最大列长度，范围是 0 到 65535。VARCHAR 的最大实际长度由最长的行的大小和使用的字符集确定。 BINARY(M) &amp;gt; 类似于 CHAR， 区别在于 BINARY 存储的是二进制字符串。 VARBINARY(M) &amp;gt; 类似于 VARCHAR， 区别在于 VARBINARY 存储的是二进制字符串。 BLOB[(M)] &amp;gt; 二进制大文件。M 表示最大列长度，范围是 0 到 65535。 TINYBLOB &amp;gt; 类似于 BLOB, 区别在于最大列长度为 255。 MEDIUMBLOB &amp;gt; 类似于 BLOB, 区别在于最大列长度为 16777215。 LONGBLOB &amp;gt; 类似于 BLOB, 区别在于最大列长度为 4294967295。 TEXT[(M)] [CHARACTER SET charset_name] [COLLATE collation_name] &amp;gt; 文本串。M 表示最大列长度，范围是 0 到 65535。TEXT 的最大实际长度由最长的行的大小和使用的字符集确定。 TINYTEXT[(M)] [CHARACTER SET charset_name] [COLLATE collation_name] &amp;gt; 类似于 TEXT, 区别在于最大列长度为 255。 MEDIUMTEXT [CHARACTER SET charset_name] [COLLATE collation_name] &amp;gt; 类似于 TEXT, 区别在于最大列长度为 16777215。 LONGTEXT [CHARACTER SET charset_name] [COLLATE collation_name] &amp;gt; 类似于 TEXT, 区别在于最大列长度为 4294967295。 ENUM(&amp;#39;value1&amp;#39;,&amp;#39;value2&amp;#39;,...) [CHARACTER SET charset_name] [COLLATE collation_name] &amp;gt; 枚举。只能有一个值的字符串对象，其值通常选自允许值列表中，在某些情况下也可以是空串或者 NULL。 SET(&amp;#39;value1&amp;#39;,&amp;#39;value2&amp;#39;,...) [CHARACTER SET charset_name] [COLLATE collation_name] &amp;gt; 集合。可以有零或者多个值的字符串对象，每一个值必须选自允许值列表中。 Json 类型 Json 类型可以存储 Json 这种半结构化的数据，相比于直接将 Json 存储为字符串，它的好处在于： 使用 Binary 格式进行序列化，对 Json 的内部字段的查询、解析加快； 多了 Json 合法性验证的步骤，只有合法的 Json 文档才可以放入这个字段中；  Json 字段本身上，并不能创建索引。相反，可以对 Json 文档中的某个子字段创建索引。例如：CREATE TABLE city ( id INT PRIMARY KEY, detail JSON, population INT AS (JSON_EXTRACT(detail, &amp;#39;$.population&amp;#39;) ); INSERT INTO city VALUES (1, &amp;#39;{&amp;#34;name&amp;#34;: &amp;#34;Beijing&amp;#34;, &amp;#34;population&amp;#34;: 100}&amp;#39;); SELECT id FROM city WHERE population &amp;gt;= 100; 枚举类型 集合类型是一个字符串，其值必须是从一个固定集合中选取，这个固定集合在创建表的时候定义，语法是：ENUM(&amp;#39;value1&amp;#39;,&amp;#39;value2&amp;#39;,...) [CHARACTER SET charset_name] [COLLATE collation_name] # 例子 ENUM(&amp;#39;apple&amp;#39;, &amp;#39;orange&amp;#39;, &amp;#39;pear&amp;#39;) 枚举类型的值在 TiDB 内部使用数值来存储，每个值会按照定义的顺序转换为一个数字，比如上面的例子中，每个字符串值都会映射为一个数字：   值 数字     NULL NULL   &amp;ldquo; 0   &amp;lsquo;apple&amp;rsquo; 1   &amp;lsquo;orange&amp;rsquo; 2   &amp;lsquo;pear&amp;rsquo; 3    更多信息参考 MySQL 枚举文档。集合类型 集合类型是一个包含零个或多个值的字符串，其中每个值必须是从一个固定集合中选取，这个固定集合在创建表的时候定义，语法是：SET(&amp;#39;value1&amp;#39;,&amp;#39;value2&amp;#39;,...) [CHARACTER SET charset_name] [COLLATE collation_name] # 例子 SET(&amp;#39;1&amp;#39;, &amp;#39;2&amp;#39;) NOT NULL 上面的例子中，这列的有效值可以是：&amp;#39;&amp;#39; &amp;#39;1&amp;#39; &amp;#39;2&amp;#39; &amp;#39;1,2&amp;#39; 集合类型的值在 TiDB 内部会转换为一个 Int64 数值，每个元素是否存在用一个二进制位的 0/1 值来表示，比如这个例子 SET(&#39;a&#39;,&#39;b&#39;,&#39;c&#39;,&#39;d&#39;)，每一个元素都被映射为一个数字，且每个数字的二进制表示只会有一位是 1：   成员 十进制表示 二进制表示     &amp;lsquo;a&amp;rsquo; 1 0001   &amp;lsquo;b&amp;rsquo; 2 0010   &amp;lsquo;c&amp;rsquo; 4 0100   &amp;rsquo;d&amp;rsquo; 8 1000    这样对于值为 (&#39;a&#39;, &#39;c&#39;) 的元素，其二进制表示即为 0101。更多信息参考 MySQL 集合文档。数据类型的默认值 在一个数据类型描述中的 DEFAULT value 段描述了一个列的默认值。这个默认值必须是常量，不可以是一个函数或者是表达式。但是对于时间类型，可以例外的使用 NOW、CURRENT_TIMESTAMP、LOCALTIME、LOCALTIMESTAMP 等函数作为 DATETIME 或者 TIMESTAMP 的默认值。BLOB、TEXT 以及 JSON 不可以设置默认值。如果一个列的定义中没有 DEFAULT 的设置。TiDB 按照如下的规则决定: 如果该类型可以使用 NULL 作为值，那么这个列会在定义时添加隐式的默认值设置 DEFAULT NULL。 如果该类型无法使用 NULL 作为值，那么这个列在定义时不会添加隐式的默认值设置。  对于一个设置了 NOT NULL 但是没有显式设置 DEFAULT 的列，当 INSERT、REPLACE 没有涉及到该列的值时，TiDB 根据当时的 SQL_MODE 进行不同的行为： 如果此时是 strict sql mode，在事务中的语句会导致事务失败并回滚，非事务中的语句会直接报错。 如果此时不是 strict sql mode，TiDB 会为这列赋值为列数据类型的隐式默认值。  此时隐式默认值的设置按照如下规则： 对于数值类型，它们的默认值是 0。当有 AUTO_INCREMENT 参数时，默认值会按照增量情况赋予正确的值。 对于除了时间戳外的日期时间类型，默认值会是该类型的“零值”。时间戳类型的默认值会是当前的时间。 对于除枚举以外的字符串类型，默认值会是空字符串。对于枚举类型，默认值是枚举中的第一个值。  "},
		{"url": "https://pingcap.com/docs-cn/releases/README/",
		"title": "TiDB 版本发布历史", 
		"content": " TiDB 版本发布历史 TiDB 历史版本发布声明如下： 1.1 Alpha 1.0 Pre-GA RC4 RC3 RC2 RC1  "},
		{"url": "https://pingcap.com/docs-cn/sql/bit-functions-and-operators/",
		"title": "TiDB 用户文档", 
		"content": " 位函数和操作符 TiDB 中位函数和操作符的使用方法与 MySQL 基本一致，详情参见: Bit Functions and Operators。位函数和操作符表   函数和操作符名 功能描述     BIT_COUNT() 返回参数二进制表示中为 1 的个数   &amp;amp; 位与   ~ 按位取反   | 位或   0 位亦或   &amp;lt;&amp;lt; 左移   &amp;gt;&amp;gt; 右移    "},
		{"url": "https://pingcap.com/docs-cn/sql/cast-functions-and-operators/",
		"title": "TiDB 用户文档", 
		"content": " Cast 函数和操作符 Cast 函数和操作符用于将某种数据类型的值转换为另一种数据类型。TiDB 中该函数和操作符的使用方法与 MySQL基本一致，详情参见: Cast Functions and Operators.Cast 函数和操作符表   函数和操作符名 功能描述     BINARY 将一个字符串转换成一个二进制字符串   CAST() 将一个值转换成一个确定类型   CONVERT() 将一个值转换成一个确定类型    "},
		{"url": "https://pingcap.com/docs-cn/sql/date-and-time-functions/",
		"title": "TiDB 用户文档", 
		"content": " 日期和时间函数 TiDB 中日期和时间函数的使用方法与 MySQL 基本一致，详情参见: Date and Time Functions.日期时间函数表   函数名 功能描述     ADDDATE() 将时间间隔添加到日期上   ADDTIME() 时间数值相加   CONVERT_TZ() 转换时区   CURDATE() 返回当前日期   CURRENT_DATE(), CURRENT_DATE 与 CURDATE() 同义   CURRENT_TIME(), CURRENT_TIME 与 CURTIME() 同义   CURRENT_TIMESTAMP(), CURRENT_TIMESTAMP 与 NOW() 同义   CURTIME() 返回当前时间   DATE() 从日期或日期/时间表达式中提取日期部分   DATE_ADD() 将时间间隔添加到日期上   DATE_FORMAT() 返回满足指定格式的日期/时间   DATE_SUB() 从日期减去指定的时间间隔   DATEDIFF() 返回两个日期间隔的天数   DAY() 与 DAYOFMONTH() 同义   DAYNAME() 返回星期名称   DAYOFMONTH() 返回参数对应的天数部分(1-31)   DAYOFWEEK() 返回参数对应的星期下标   DAYOFYEAR() 返回参数代表一年的哪一天 (1-366)   EXTRACT() 提取日期/时间中的单独部分   FROM_DAYS() 将天数转化为日期   FROM_UNIXTIME() 将 Unix 时间戳格式化为日期   GET_FORMAT() 返回满足日期格式的字符串   HOUR() 提取日期/时间表达式中的小时部分   LAST_DAY 返回参数中月份的最后一天   LOCALTIME(), LOCALTIME 与 NOW() 同义   LOCALTIMESTAMP, LOCALTIMESTAMP() 与 NOW() 同义   MAKEDATE() 根据给定的年份和一年中的天数生成一个日期   MAKETIME() 根据给定的时、分、秒生成一个时间   MICROSECOND() 返回参数的微秒部分   MINUTE() 返回参数的分钟部分   MONTH() 返回参数的月份部分   MONTHNAME() 返回参数的月份名称   NOW() 返回当前日期和时间   PERIOD_ADD() 在年-月表达式上添加一段时间(数个月)   PERIOD_DIFF() 返回间隔的月数   QUARTER() 返回参数对应的季度(1-4)   SEC_TO_TIME() 将秒数转化为 &amp;lsquo;HH:MM:SS&amp;rsquo; 的格式   SECOND() 返回秒数(0-59)   STR_TO_DATE() 将字符串转化为日期   SUBDATE() 当传入三个参数时作为 DATE_SUB() 的同义   SUBTIME() 从一个时间中减去一段时间   SYSDATE() 返回该方法执行时的时间   TIME() 返回参数的时间表达式部分   TIME_FORMAT() 格式化时间   TIME_TO_SEC() 返回参数对应的秒数   TIMEDIFF() 返回时间间隔   TIMESTAMP() 传入一个参数时候,该方法返回日期或日期/时间表达式, 传入两个参数时候, 返回参数的和   TIMESTAMPADD() 在日期/时间表达式上增加一段时间间隔   TIMESTAMPDIFF() 从日期/时间表达式中减去一段时间间隔   TO_DAYS() 将参数转化对应的天数(从第 0 年开始)   TO_SECONDS() 将日期或日期/时间参数转化为秒数(从第 0 年开始)   UNIX_TIMESTAMP() 返回一个 Unix 时间戳   UTC_DATE() 返回当前的 UTC 日期   UTC_TIME() 返回当前的 UTC 时间   UTC_TIMESTAMP() 返回当前的 UTC 日期和时间   WEEK() 返回参数所在的一年中的星期数   WEEKDAY() 返回星期下标   WEEKOFYEAR() 返回参数在日历中对应的一年中的星期数   YEAR() 返回参数对应的年数   YEARWEEK() 返回年数和星期数    "},
		{"url": "https://pingcap.com/docs-cn/sql/functions-and-operators-reference/",
		"title": "TiDB 用户文档", 
		"content": " 函数和操作符概述 TiDB 中函数和操作符使用方法与 MySQL 基本一致, 详情参见: Functions and Operators在 SQL 语句中, 表达式可用于诸如 SELECT 语句的 ORDER BY 或 HAVING 子句, SELECT/ DELETE/ UPDATE 语句的 WHERE 子句, 或 SET 语句之类的地方.可使用字面值, 列名, NULL, 内置函数, 操作符等来书写表达式."},
		{"url": "https://pingcap.com/docs-cn/sql/information-functions/",
		"title": "TiDB 用户文档", 
		"content": " 信息函数 TiDB 中信息函数的使用方法与 MySQL 基本一致，详情参见: Information Functions.信息函数表   函数名 功能描述     CONNECTION_ID() 返回当前连接的连接 ID (线程 ID)   CURRENT_USER(), CURRENT_USER 返回当前用户的用户名和主机名   DATABASE() 返回默认(当前)的数据库名   FOUND_ROWS() 该函数返回对于一个包含 LIMIT 的 SELECT 查询语句，在不包含 LIMIT 的情况下回返回的记录数   LAST_INSERT_ID() 返回最后一条 INSERT 语句中自增列的值   SCHEMA() 与 DATABASE() 同义   SESSION_USER() 与 USER() 同义   SYSTEM_USER() 与 USER() 同义   USER() 返回客户端提供的用户名和主机名   VERSION() 返回当前 MySQL 服务器的版本信息   TIDB_VERSION 返回当前 TiDB 服务器的版本信息    "},
		{"url": "https://pingcap.com/docs-cn/sql/operators/",
		"title": "TiDB 用户文档", 
		"content": " 操作符    操作符名 功能描述     AND, &amp;amp;&amp;amp; 逻辑与   = 赋值 (可用于 SET 语句中, 或用于 UPDATE 语句的 SET 中 )   := 赋值   BETWEEN ... AND ... 判断值满足范围   BINARY 将一个字符串转换为一个二进制字符串   &amp;amp; 位与   ~ 位非   | 位或   ^ 按位异或   CASE case 操作符   DIV 整数除   / 除法   = 相等比较   &amp;lt;=&amp;gt; 空值安全型相等比较   &amp;gt; 大于   &amp;gt;= 大于或等于   IS 判断一个值是否等于一个布尔值   IS NOT 判断一个值是否不等于一个布尔值   IS NOT NULL 非空判断   IS NULL 空值判断   &amp;lt;&amp;lt; 左移   &amp;lt; 小于   &amp;lt;= 小于或等于   LIKE 简单模式匹配   - 减   %, MOD 求余   NOT, ! 取反   NOT BETWEEN ... AND ... 判断值是否不在范围内   !=, &amp;lt;&amp;gt; 不等于   NOT LIKE 不符合简单模式匹配   NOT REGEXP 不符合正则表达式模式匹配   ||, OR 逻辑或   + 加   REGEXP 使用正则表达式进行模式匹配   &amp;gt;&amp;gt; 右移   RLIKE REGEXP 同义词   * 乘   - 取反符号   XOR 逻辑亦或    操作符优先级 操作符优先级显示在以下列表中，从最高优先级到最低优先级。同一行显示的操作符具有相同的优先级。INTERVAL BINARY ! - (unary minus), ~ (unary bit inversion) ^ *, /, DIV, %, MOD -, + &amp;lt;&amp;lt;, &amp;gt;&amp;gt; &amp;amp; | = (comparison), &amp;lt;=&amp;gt;, &amp;gt;=, &amp;gt;, &amp;lt;=, &amp;lt;, &amp;lt;&amp;gt;, !=, IS, LIKE, REGEXP, IN BETWEEN, CASE, WHEN, THEN, ELSE NOT AND, &amp;amp;&amp;amp; XOR OR, || = (assignment), := 详情参见 这里.比较方法和操作符    操作符名 功能描述     BETWEEN ... AND ... 判断值是否在范围内   COALESCE() 返回第一个非空值   = 相等比较   &amp;lt;=&amp;gt; 空值安全型相等比较   &amp;gt; 大于   &amp;gt;= 大于或等于   GREATEST() 返回最大值   IN() 判断值是否在一个值的集合内   INTERVAL() 返回一个小于第一个参数的参数的下标   IS 判断是否等于一个布尔值   IS NOT 判断是否不等于一个布尔值   IS NOT NULL 非空判断   IS NULL 空值判断   ISNULL() 判断参数是否为空   LEAST() 返回最小值   &amp;lt; 小于   &amp;lt;= 小于或等于   LIKE 简单模式匹配   NOT BETWEEN ... AND ... 判断值是否不在范围内   !=, &amp;lt;&amp;gt; 不等于   NOT IN() 判断值是否不在一个值的集合内   NOT LIKE 不满足简单模式匹配   STRCMP() 比较两个字符串    详情参见 这里.逻辑操作符    操作符名 功能描述     AND, &amp;amp;&amp;amp; 逻辑与   NOT, ! 逻辑非   ||, OR 逻辑或   XOR 逻辑亦或    详情参见 这里.赋值操作符    操作符名 功能描述     = 赋值 (可用于 SET 语句中, 或用于 UPDATE 语句的 SET 中 )   := 赋值    详情参见 这里."},
		{"url": "https://pingcap.com/docs-cn/sql/precision-math/",
		"title": "TiDB 用户文档", 
		"content": " 精度数学 TiDB 中精度数学计算与 MySQL 中基本一致, 详情请参见: Precision Math. 数值类型 DECIMAL 数据类型的特性  数值类型 精确数值运算的范围包括精确值数据类型(整型和 DECIMAL 类型), 以及精确值数字字面量. 近似值数据类型和近似值数字字面量被作为浮点数来处理.精确值数字字面量包含整数部分或小数部分, 或二者都包含. 精确值数字字面量可以包含符号位. 例如: 1, .2, 3.4, -5, -6.78, +9.10.近似值数字字面量以一个包含尾数和指数的科学计数法表示(基数为 10). 其中尾数和指数可以分别或同时带有符号位. 例如: 1.2E3, 1.2E-3, -1.2E3, -1.2E-3.两个看起来相似的数字可能会被以不同的方式进行处理. 例如, 2.34 是精确值(定点数), 而 2.3E0 是近似值(浮点数).DECIMAL 数据类型是定点数类型, 其运算是精确计算. FLOAT 和 DOUBLE 数据类型是浮点类型, 其运算是近似计算.DECIMAL 数据类型的特性 本节讨论 DECIMAL 数据类型的特性, 主要涉及以下几点: 最大位数 存储格式 存储要求  DECIMAL 列的声明语法为 DECIMAL(M, D). 其中参数值意义及其范围如下: M 表示最大的数字位数 (精度). 1&amp;lt;= M &amp;lt;= 65. D 表示小数点右边数字的位数 (标度). 1 &amp;lt;= D &amp;lt;= 30 且 不大于 M.  M 的最大值 65 表示 DECIMAL 值的计算精确到 65 位数字. 该精度同样适用于其精确值字面量.DECIMAL 列的值采用二进制进行存储, 其将每 9 位十进制数字包装成 4 个字节. 其中整数和小数部分分别确定所需的存储空间. 如果数字位数为 9 的倍数, 则每 9 位十进制数字各采用 4 个字节进行存储, 对于剩余不足 9 位的数字, 所需的存储空间如下表所示.   剩余数字位数 存储所需字节数     0 0   1–2 1   3–4 2   5–6 3   7–9 4    例如, 定义类型为 DECIMAL(18, 9) 的列, 其小数点两侧均各包含 9 位十进制数字, 因此, 分别需要 4 个字节的存储空间. 定义类型为 DECIMAL(20, 6) 的列, 其小数部分包含 6 位十进制数字, 整数部分包含 14 位十进制数字. 整数部分中 9 位数字需要 4 个字节进行存储, 其余 5 位数字需要 3 个字节进行存储. 小数部分 6 位数字需要 3 个字节进行存储.DECIMAL 列不存储前导的字符 + 或字符 - 或数字 0. 如果将 +0003.1 插入到 DECIMAL(5, 1) 列中, 则将其存储为3.1. 对于负数, 不存储字符 - 的字面值.DECIMAL 列不允许插入大于列定义的隐含范围的值. 例如, DECIMAL(3, 0) 列范围为 -999 到 999. DECIMAL(M, D) 列小数点左边部分最多支持 M-D 位数字.有关 DECIMAL 值的内部格式完整说明, 请参阅 TiDB 源码文件 types/mydecimal.go.表达式计算 在涉及精度数学计算的表达式中，TiDB 会尽可能不做任何修改的使用每个输入的数值。比如：在计算比较函数时，参与运算的数字将不做任何改变。在严格 SQL 模式下，向一个数据列插入一个值时，如果该值处于这一列的值域范围内，这个值将直接不做任何修改的直接插入进去，提取这个值的时候，取得的值和插入的值将会是同一个值。当处于非严格 SQL 模式时，TiDB 会允许数据插入过程中发生的数据截断。处理数值类型表达式取决于这个表达式参数的具体值： 当表达式参数中包含近似值时，这个表达式的结果也是近似值，TiDB 会使用浮点数对应的计算逻辑返回一个浮点数的结果 当表达式参数中不包含任何近似值时（也就是说表达式的参数全部是精确值），如果某个精确值包含小数部分，TIDB 会对这个表达式使用 DECIMAL 对应的计算逻辑，返回一个 DECIMAL 的结果，精确到 65 位数字 其他情况下，表达式只会包含整数参数，这个表达式的结果也是精确的，TiDB 会使用整数对应的计算逻辑返回一个整数结果，精度和 BIGINT 保持一致（64位）  如果数值类型表达式中包含字符串参数，这些字符串参数将被转换成双精度浮点数，这个表达式的计算结果将是个近似值。向一个数值类型列插入数据的具体行为会受到 SQL 模式的影响。接下来的讨论将围绕严格模式以及 ERROR_FOR_DIVISION_BY_ZERO 模式展开，如果要打开所有的限制，可以简单的使用 TRADITIONAL 模式，这个模式将同时使用严格模式以及 ERROR_FOR_DIVISION_BY_ZERO 模式：SET sql_mode = &amp;#39;TRADITIONAL`; 向一个具有精确值类型（DECIMAL 或者整数类型）的列插入数据时，如果插入的数据位于该列的值域范围内将使用该数据的精确值。如果该数据的小数部分太长，将会发生数值修约，这时会有 warning 产生，具体内容可以看&amp;rdquo;数值修约&amp;rdquo;。如果该数据整数部分太长： 如果没有开启严格模式，这个值会被截断并产生一个 warning 如果开启了严格模式，将会产生一个数据溢出的 error  如果向一个数值类型列插入字符串，如果该字符串中包含非数值部分，TiDB 将这样做类型转换： 在严格模式下，没有以数字开头的字符串（即使是一个空字符串）不能被被用作数字值并会返回一个 error 或者是 warning； 以数字开头的字符串可以被转换，不过末尾的非数字部分会被截断。如果被截断的部分包含的不全是空格，在严格模式下这回产生一个 error 或者 warning  默认情况下，如果计算的过程中发生了除数是 0 的现象将会得到一个 NULL 结果，并且不会有 warning 产生。通过设置适当的 SQL 模式，除以 0 的操作可以被限制：当设置 ERROR_FOR_DIVISION_BY_ZERO SQL 模式时，TiDB 的行为是： 如果设置了严格 SQL 模式，INSERT 和 UPDATE 的过程中如果发生了除以 0 的操作，正在进行的 INSERT 或者 UPDATE 操作会被禁止，并且会返回一个 error 如果没有设置严格 SQL 模式，除以 0 的操作仅会返回一个 warning  假设我们有如下的 SQL 语句：INSERT INTO t SET i = 1/0; 不同的 SQL 模式将会导致不同的结果如下：   sql_mode 的值 结果     &amp;ldquo; 没有 warning，没有 error，i 被设为 NULL   strict 没有 warning，没有 error，i 被设为 NULL   ERROR_FOR_DIVISION_BY_ZERO 有 warning，没有 error，i 被设为 NULL   strict, ERROR_FOR_DIVISION_BY_ZERO 有 error，插入失败    数值修约 round() 函数的结果取决于他的参数是否是精确值： 如果参数是精确值，round() 函数将使用四舍五入的规则 如果参数是一个近似值，round() 表达式的结果可能和 MySQL 不太一样  TiDB &amp;gt; SELECT ROUND(2.5), ROUND(25E-1); +------------+--------------+ | ROUND(2.5) | ROUND(25E-1) | +------------+--------------+ | 3 | 3 | +------------+--------------+ 1 row in set (0.00 sec) 向一个 DECIMAL 或者整数类型列插入数据时，round 的规则将采用 round half away from zero 的方式：TiDB &amp;gt; CREATE TABLE t (d DECIMAL(10,0)); Query OK, 0 rows affected (0.01 sec) TiDB &amp;gt; INSERT INTO t VALUES(2.5),(2.5E0); Query OK, 2 rows affected, 2 warnings (0.00 sec) TiDB &amp;gt; SELECT d FROM t; +------+ | d | +------+ | 3 | | 3 | +------+ 2 rows in set (0.00 sec)"},
		{"url": "https://pingcap.com/docs-cn/sql/user-manual/",
		"title": "TiDB 用户文档", 
		"content": " TiDB 用户文档 TiDB 支持 SQL92 标准并兼容 MySQL 语法，为了帮您更好地使用 TiDB, 该文档沿用了 MySQL 大部分的文档结构， 同时针对 TiDB 特有的功能作了详细的描述。TiDB 数据库管理  TiDB 服务 TiDB 进程启动参数 TiDB 数据目录 TiDB 系统数据库 TiDB 系统变量 TiDB 专用系统变量和语法 TiDB 服务器日志文件 TiDB 访问权限管理 TiDB 用户账户管理 使用加密连接  SQL 优化  理解 TiDB 执行计划 统计信息  语言结构  字面值  字符串字面值 数字字面值 NULL 值 十六进制字面值 date 和 time 字面值 布尔值 bit-val 字面值  数据库、表、索引、列和别名 关键字和保留字 用户变量 表达式语法 注释语法  字符集和时区  字符集支持 字符集配置 时区  数据类型  数值类型 日期和时间类型 字符串类型 JSON 数据类型 数据类型默认值  函数和操作符  函数和操作符概述 表达式求值的类型转换 操作符 控制流程函数 字符串函数 数值函数与操作符 日期和时间函数 位函数和操作符 Cast 函数和操作符 加密和压缩函数 信息函数 JSON 函数 信息函数 全局事务 ID 函数 [TBD] GROUP BY 聚合函数 其他函数 精度数学  SQL 语句语法  数据定义语句(DDL) 数据操作语句(DML) 事务语句 数据库管理语句 Prepared SQL 语句语法 实用工具语句 TiDB SQL 语法图  JSON 支持  JSON 支持  Connectors 和 API  Connectors 和 API  错误码与故障诊断  错误码与故障诊断  与 MySQL 兼容性对比  与 MySQL 兼容性对比  "},
		{"url": "https://pingcap.com/docs-cn/sql/user-account-management/",
		"title": "TiDB 用户账户管理", 
		"content": " TiDB 用户账户管理 用户名和密码 TiDB 将用户账户存储在 mysql.user 系统表里面。每个账户由用户名和 host 作为标识。每个账户可以设置一个密码。通过 MySQL 客户端连接到 TiDB 服务器，通过指定的账户和密码登陆：shell&amp;gt; mysql --port 4000 --user xxx --password 使用缩写的命令行参数则是：shell&amp;gt; mysql -P 4000 -u xxx -p 添加用户 添加用户有两种方式： 通过标准的用户管理的 SQL 语句创建用户以及授予权限，比如 CREATE USER 和 GRANT 。 直接通过 INSERT ， UPDATE 和 DELETE 操作授权表。  推荐的方式是使用第一种。第二种方式修改容易导致一些不完整的修改，因此不推荐。还有另一种可选方式是使用第三方工具的图形化界面工具。下面的例子用 CREATE USER 和 GRANT 语句创建了四个账户：mysql&amp;gt; CREATE USER &amp;#39;finley&amp;#39;@&amp;#39;localhost&amp;#39; IDENTIFIED BY &amp;#39;some_pass&amp;#39;; mysql&amp;gt; GRANT ALL PRIVILEGES ON *.* TO &amp;#39;finley&amp;#39;@&amp;#39;localhost&amp;#39; WITH GRANT OPTION; mysql&amp;gt; CREATE USER &amp;#39;finley&amp;#39;@&amp;#39;%&amp;#39; IDENTIFIED BY &amp;#39;some_pass&amp;#39;; mysql&amp;gt; GRANT ALL PRIVILEGES ON *.* TO &amp;#39;finley&amp;#39;@&amp;#39;%&amp;#39; WITH GRANT OPTION; mysql&amp;gt; CREATE USER &amp;#39;admin&amp;#39;@&amp;#39;localhost&amp;#39; IDENTIFIED BY &amp;#39;admin_pass&amp;#39;; mysql&amp;gt; GRANT RELOAD,PROCESS ON *.* TO &amp;#39;admin&amp;#39;@&amp;#39;localhost&amp;#39;; mysql&amp;gt; CREATE USER &amp;#39;dummy&amp;#39;@&amp;#39;localhost&amp;#39;; 使用 SHOW GRANTS 可以看到为一个用户授予的权限：mysql&amp;gt; SHOW GRANTS FOR &amp;#39;admin&amp;#39;@&amp;#39;localhost&amp;#39;; +-----------------------------------------------------+ | Grants for admin@localhost | +-----------------------------------------------------+ | GRANT RELOAD, PROCESS ON *.* TO &amp;#39;admin&amp;#39;@&amp;#39;localhost&amp;#39; | +-----------------------------------------------------+ 删除用户 使用 DROP USER 语句可以删除用户，例如：mysql&amp;gt; DROP USER &amp;#39;jeffrey&amp;#39;@&amp;#39;localhost&amp;#39;; 保留用户账户 TiDB 在数据库初始化时会生成一个 &#39;root&#39;@&#39;%&#39; 的默认账户。设置资源限制 暂不支持。设置密码 TiDB 将密码存在 mysql.user 系统数据库里面。只有拥有 CREATE USER 权限，或者拥有 mysql 数据库权限（ INSERT 权限用于创建， UPDATE 权限用于更新）的用户才能够设置或修改密码。在 CREATE USER 创建用户时可以通过 IDENTIFIED BY 指定密码：CREATE USER &amp;#39;jeffrey&amp;#39;@&amp;#39;localhost&amp;#39; IDENTIFIED BY &amp;#39;mypass&amp;#39;; 为一个已存在的账户修改密码，可以通过 SET PASSWORD FOR 或者 ALTER USER 语句完成：SET PASSWORD FOR &amp;#39;root&amp;#39;@&amp;#39;%&amp;#39; = &amp;#39;xxx&amp;#39;; 或者ALTER USER &amp;#39;jeffrey&amp;#39;@&amp;#39;localhost&amp;#39; IDENTIFIED BY &amp;#39;mypass&amp;#39;;"},
		{"url": "https://pingcap.com/docs-cn/op-guide/monitor-overview/",
		"title": "TiDB 监控框架概述", 
		"content": " TiDB 监控框架概述 TiDB 使用开源时序数据库 Prometheus 作为监控和性能指标信息存储方案，使用 Grafana 作为可视化组件进行展示。Prometheus 是一个拥有多维度数据模型，灵活的查询语句的时序数据库。Prometheus 作为热门的开源项目，拥有活跃的社区及众多的成功案例。Prometheus 提供了多个组件供用户使用。目前，我们使用 Prometheus Server，来收集和存储时间序列数据。Client 代码库，在程序中定制需要的 Metric 。Push GateWay 来接收 Client Push 上来的数据，统一供 Prometheus 主服务器抓取。以及 AlertManager 来实现报警机制。其结构如下图：Grafana 是一个开源的 metric 分析及可视化系统。我们使用 Grafana 来展示 TiDB 的各项性能指标 。如下图所示:"},
		{"url": "https://pingcap.com/docs-cn/overview/",
		"title": "TiDB 简介与整体架构", 
		"content": " TiDB 简介与整体架构 TiDB 简介 TiDB 是 PingCAP 公司受 Google Spanner / F1 论文启发而设计的开源分布式 NewSQL 数据库。TiDB 具备如下 NewSQL 核心特性： SQL支持（TiDB 是 MySQL 兼容的） 水平弹性扩展（吞吐可线性扩展） 分布式事务 跨数据中心数据强一致性保证 故障自恢复的高可用 海量数据高并发实时写入与实时查询（HTAP 混合负载）  TiDB 的设计目标是 100% 的 OLTP 场景和 80% 的 OLAP 场景，更复杂的 OLAP 分析可以通过 TiSpark 项目来完成。TiDB 对业务没有任何侵入性，能优雅的替换传统的数据库中间件、数据库分库分表等 Sharding 方案。同时它也让开发运维人员不用关注数据库 Scale 的细节问题，专注于业务开发，极大的提升研发的生产力。三篇文章了解 TiDB 技术内幕： 说存储 说计算 谈调度  TiDB 整体架构 要深入了解 TiDB 的水平扩展和高可用特点，首先需要了解 TiDB 的整体架构。TiDB 集群主要分为三个组件：TiDB Server TiDB Server 负责接收 SQL 请求，处理 SQL 相关的逻辑，并通过 PD 找到存储计算所需数据的 TiKV 地址，与 TiKV 交互获取数据，最终返回结果。 TiDB Server 是无状态的，其本身并不存储数据，只负责计算，可以无限水平扩展，可以通过负载均衡组件（如LVS、HAProxy 或 F5）对外提供统一的接入地址。PD Server Placement Driver (简称 PD) 是整个集群的管理模块，其主要工作有三个： 一是存储集群的元信息（某个 Key 存储在哪个 TiKV 节点）；二是对 TiKV 集群进行调度和负载均衡（如数据的迁移、Raft group leader 的迁移等）；三是分配全局唯一且递增的事务 ID。PD 是一个集群，需要部署奇数个节点，一般线上推荐至少部署 3 个节点。TiKV Server TiKV Server 负责存储数据，从外部看 TiKV 是一个分布式的提供事务的 Key-Value 存储引擎。存储数据的基本单位是 Region，每个 Region 负责存储一个 Key Range （从 StartKey 到 EndKey 的左闭右开区间）的数据，每个 TiKV 节点会负责多个 Region 。TiKV 使用 Raft 协议做复制，保持数据的一致性和容灾。副本以 Region 为单位进行管理，不同节点上的多个 Region 构成一个 Raft Group，互为副本。数据在多个 TiKV 之间的负载均衡由 PD 调度，这里也是以 Region 为单位进行调度。核心特性 水平扩展 无限水平扩展是 TiDB 的一大特点，这里说的水平扩展包括两方面：计算能力和存储能力。TiDB Server 负责处理 SQL 请求，随着业务的增长，可以简单的添加 TiDB Server 节点，提高整体的处理能力，提供更高的吞吐。TiKV 负责存储数据，随着数据量的增长，可以部署更多的 TiKV Server 节点解决数据 Scale 的问题。PD 会在 TiKV 节点之间以 Region 为单位做调度，将部分数据迁移到新加的节点上。所以在业务的早期，可以只部署少量的服务实例（推荐至少部署 3 个 TiKV， 3 个 PD，2 个 TiDB），随着业务量的增长，按照需求添加 TiKV 或者 TiDB 实例。高可用 高可用是 TiDB 的另一大特点，TiDB/TiKV/PD 这三个组件都能容忍部分实例失效，不影响整个集群的可用性。下面分别说明这三个组件的可用性、单个实例失效后的后果以及如何恢复。 TiDBTiDB 是无状态的，推荐至少部署两个实例，前端通过负载均衡组件对外提供服务。当单个实例失效时，会影响正在这个实例上进行的 Session，从应用的角度看，会出现单次请求失败的情况，重新连接后即可继续获得服务。单个实例失效后，可以重启这个实例或者部署一个新的实例。 PDPD 是一个集群，通过 Raft 协议保持数据的一致性，单个实例失效时，如果这个实例不是 Raft 的 leader，那么服务完全不受影响；如果这个实例是 Raft 的 leader，会重新选出新的 Raft leader，自动恢复服务。PD 在选举的过程中无法对外提供服务，这个时间大约是3秒钟。推荐至少部署三个 PD 实例，单个实例失效后，重启这个实例或者添加新的实例。 TiKVTiKV 是一个集群，通过 Raft 协议保持数据的一致性（副本数量可配置，默认保存三副本），并通过 PD 做负载均衡调度。单个节点失效时，会影响这个节点上存储的所有 Region。对于 Region 中的 Leader 结点，会中断服务，等待重新选举；对于 Region 中的 Follower 节点，不会影响服务。当某个 TiKV 节点失效，并且在一段时间内（默认 10 分钟）无法恢复，PD 会将其上的数据迁移到其他的 TiKV 节点上。  "},
		{"url": "https://pingcap.com/docs-cn/sql/system-database/",
		"title": "TiDB 系统数据库", 
		"content": " TiDB 系统数据库 TiDB 的系统数据库跟 MySQL 类似，里面包含一些服务器运行时需要的信息。权限系统表 这些系统表里面包含了用户账户以及相应的授权信息： user 用户账户，全局权限，以及其它一些非权限的列 db 数据库级别的权限 tables_priv 表级的权限 columns_priv 列级的权限  服务端帮助信息系统表  help_topic 目前为空  统计信息相关系统表  stats_buckets 统计信息的桶 stats_histograms 统计信息的直方图 stats_meta 表的元信息，比如总行数和修改数  GC Worker 相关系统表  gc_delete_range  其它系统表  GLOBAL_VARIABLES 全局系统变量表 tidb 用于 TiDB 在 bootstrap 的时候记录相关版本信息  INFORMATION_SCHEMA 里面的表 INFORMATION_SCHEMA 库里面的表主要是为了兼容 MySQL 而存在，有些第三方软件会查询里面的信息。在目前 TiDB 的实现中，里面大部分只是一些空表。CHARACTER_SETS Table 提供字符集相关的信息，其实数据是假的。TiDB 默认支持并且只支持 utf8mb4 。mysql&amp;gt; select * from CHARACTER_SETS; +--------------------|----------------------|-----------------------|--------+ | CHARACTER_SET_NAME | DEFAULT_COLLATE_NAME | DESCRIPTION | MAXLEN | +--------------------|----------------------|-----------------------|--------+ | ascii | ascii_general_ci | US ASCII | 1 | | binary | binary | Binary pseudo charset | 1 | | latin1 | latin1_swedish_ci | cp1252 West European | 1 | | utf8 | utf8_general_ci | UTF-8 Unicode | 3 | | utf8mb4 | utf8mb4_general_ci | UTF-8 Unicode | 4 | +--------------------|----------------------|-----------------------|--------+ 5 rows in set (0.00 sec) COLLATIONS Table 同上。COLLATION_CHARACTER_SET_APPLICABILITY Table 空表。COLUMNS Table COLUMNS 表提供了关于所有表的列的信息。这张表里面的信息不准确，推荐使用 SHOW 语句查询：SHOW COLUMNS FROM table_name [FROM db_name] [LIKE &amp;#39;wild&amp;#39;] COLUMNS_PRIVILEGE Table 空表。ENGINES Table ENGINES 表提供了关于存储引擎的信息。目前这张表里面的数据是假的。生产环境中，TiDB 只推荐使用 TiKV 引擎。EVENTS Table 空表。FILES Table 空表。GLOBAL_STATUS Table 空表。GLOBAL_VARIABLES Table 空表。KEY_COLUMN_USAGE Table KEY_COLUMN_USAGE 这张表描述了关于列的 key 的约束，比如是否是主键列。OPTIMIZER_TRACE Table 空表。PARAMETERS Table 空表。PARTITIONS Table 空表。PLUGINS Table 空表。PROFILING Table 空表。REFERENTIAL_CONSTRAINTS Table 空表。ROUTINES Table 空表。SCHEMATA Table SCHEMATA 表提供了关于数据库的信息。表中的内容和 SHOW DATABASES 基本等价。mysql&amp;gt; select * from SCHEMATA; +--------------|--------------------|----------------------------|------------------------|----------+ | CATALOG_NAME | SCHEMA_NAME | DEFAULT_CHARACTER_SET_NAME | DEFAULT_COLLATION_NAME | SQL_PATH | +--------------|--------------------|----------------------------|------------------------|----------+ | def | INFORMATION_SCHEMA | utf8 | utf8_bin | NULL | | def | mysql | utf8 | utf8_bin | NULL | | def | PERFORMANCE_SCHEMA | utf8 | utf8_bin | NULL | | def | test | utf8 | utf8_bin | NULL | +--------------|--------------------|----------------------------|------------------------|----------+ 4 rows in set (0.00 sec) SCHEMA_PRIVILEGES Table 空表。SESSION_STATUS Table 空表。SESSION_VARIABLES Table SESSION_VARIABLES 表提供了关于 session 变量的信息。表中的数据跟 SHOW SESSION VARIABLES 类似。STATISTICS Table 统计信息的表。mysql&amp;gt; desc statistics; +---------------|---------------------|------|------|---------|-------+ | Field | Type | Null | Key | Default | Extra | +---------------|---------------------|------|------|---------|-------+ | TABLE_CATALOG | varchar(512) | YES | | NULL | | | TABLE_SCHEMA | varchar(64) | YES | | NULL | | | TABLE_NAME | varchar(64) | YES | | NULL | | | NON_UNIQUE | varchar(1) | YES | | NULL | | | INDEX_SCHEMA | varchar(64) | YES | | NULL | | | INDEX_NAME | varchar(64) | YES | | NULL | | | SEQ_IN_INDEX | bigint(2) UNSIGNED | YES | | NULL | | | COLUMN_NAME | varchar(21) | YES | | NULL | | | COLLATION | varchar(1) | YES | | NULL | | | CARDINALITY | bigint(21) UNSIGNED | YES | | NULL | | | SUB_PART | bigint(3) UNSIGNED | YES | | NULL | | | PACKED | varchar(10) | YES | | NULL | | | NULLABLE | varchar(3) | YES | | NULL | | | INDEX_TYPE | varchar(16) | YES | | NULL | | | COMMENT | varchar(16) | YES | | NULL | | | INDEX_COMMENT | varchar(1024) | YES | | NULL | | +---------------|---------------------|------|------|---------|-------+ 下列操作是等价的。SELECT * FROM INFORMATION_SCHEMA.STATISTICS WHERE table_name = &amp;#39;tbl_name&amp;#39; AND table_schema = &amp;#39;db_name&amp;#39; SHOW INDEX FROM tbl_name FROM db_name TABLES Table TABLES 表提供了数据库里面关于表的信息。以下操作是等价的：SELECT table_name FROM INFORMATION_SCHEMA.TABLES WHERE table_schema = &amp;#39;db_name&amp;#39; [AND table_name LIKE &amp;#39;wild&amp;#39;] SHOW TABLES FROM db_name [LIKE &amp;#39;wild&amp;#39;] TABLESPACES Table 空表。TABLE_CONSTRAINTS Table TABLE_CONSTRAINTS 记录了表的约束信息。其中： CONSTRAINT_TYPE 的取值可以是 UNIQUE, PRIMARY KEY, 或者 FOREIGN KEY。 UNIQUE 和 PRIMARY KEY 信息跟 SHOW INDEX 看到的是一样的。  TABLE_PRIVILEGES Table 空表。TRIGGERS Table 空表。USER_PRIVILEGES Table USER_PRIVILEGES 表提供了关于全局权限的信息。这张表的内容是根据 mysql.user 表生成的。mysql&amp;gt; desc USER_PRIVILEGES; +----------------|--------------|------|------|---------|-------+ | Field | Type | Null | Key | Default | Extra | +----------------|--------------|------|------|---------|-------+ | GRANTEE | varchar(81) | YES | | NULL | | | TABLE_CATALOG | varchar(512) | YES | | NULL | | | PRIVILEGE_TYPE | varchar(64) | YES | | NULL | | | IS_GRANTABLE | varchar(3) | YES | | NULL | | +----------------|--------------|------|------|---------|-------+ 4 rows in set (0.00 sec) VIEWS Table 空表。TiDB 暂不支持视图。"},
		{"url": "https://pingcap.com/docs-cn/op-guide/op-guide/",
		"title": "TiDB 运维文档", 
		"content": " TiDB 运维文档 软硬件环境需求  软硬件环境需求  部署集群  Ansible 部署方案 (强烈推荐) 离线 Ansible 部署方案 (强烈推荐) Docker 部署方案 跨机房部署方案  配置集群  配置参数  监控集群  整体监控框架概述 重要监控指标详解 组件状态 API &amp;amp; 监控  扩容缩容  使用 Ansible 扩容缩容 集群扩容缩容方案  升级  使用 Ansible 升级  性能调优  TiKV 性能参数调优  备份与迁移  备份与恢复 数据迁移  全量导入 增量导入   Binary 部署方案  Binary 部署方案  "},
		{"url": "https://pingcap.com/docs-cn/sql/server-command-option/",
		"title": "TiDB 进程启动参数和配置", 
		"content": " TiDB 进程启动参数 启动 TiDB 进程时，可以指定一些程序启动参数。TiDB 接受许多的启动参数，执行这个命令可以得到一个简要的说明：./tidb-server --help 获取版本信息可以使用下面命令：./tidb-server -V 以下是启动参数的完整描述。-L  Log 级别 默认: &amp;ldquo;info&amp;rdquo; 可选值包括 debug, info, warn, error 或者 fatal  -P  TiDB 服务监听端口 默认: &amp;ldquo;4000&amp;rdquo; TiDB 服务将会使用这个端口接受 MySQL 客户端发过来的请求  --binlog-socket  TiDB 服务使用 unix socket file 方式接受内部连接，如 PUMP 服务 默认: &amp;ldquo;&amp;rdquo; 譬如使用 &amp;ldquo;/tmp/pump.sock&amp;rdquo; 来接受 PUMP unix socket file 通信  --config  TiDB 配置文件 默认: &amp;ldquo;&amp;rdquo; 配置文件的路径  --lease  Schema 的租约时间，单位：秒 默认: &amp;ldquo;10&amp;rdquo; Schema 的 lease 主要用在 online schema changes 上面。这个值会影响到实际的 DDL 语句的执行时间。大多数情况下，用户不需要修改这个值，除非您清晰的了解 TiDB DDL 的内部实现机制  --host  TiDB 服务监听 host 默认: &amp;ldquo;0.0.0.0&amp;rdquo; TiDB 服务会监听这个 host 0.0.0.0 默认会监听所有的网卡 address。如果有多块网卡，可以指定对外提供服务的网卡，譬如 192.168.100.113  --log-file  Log 文件 默认: &amp;ldquo;&amp;rdquo; 如果没设置这个参数，log 会默认输出到 &amp;ldquo;stderr&amp;rdquo;，如果设置了， log 就会输出到对应的文件里面，在每天凌晨，log 会自动轮转使用一个新的文件，并且将以前的文件改名备份  --metrics-addr  Prometheus Push Gateway 地址 默认: &amp;ldquo;&amp;rdquo; 如果为空，TiDB 不会将统计信息推送给 Push Gateway ，参数格式 如 --metrics-addr=192.168.100.115:9091  --metrics-intervel  推送统计信息到 Prometheus Push Gateway 的时间间隔 默认: 15s 设置为 0 表明不推送统计信息给 Push Gateway ，如: --metrics-interval=2 是每两秒推送到 Push Gataway  --path  对于本地存储引擎 &amp;ldquo;goleveldb&amp;rdquo;, &amp;ldquo;BoltDB&amp;rdquo; 来说，path 指定的是实际的数据存放路径 对于 &amp;ldquo;memory&amp;rdquo; 存储引擎来说，path 不用设置 对于 &amp;ldquo;TiKV&amp;rdquo; 存储引擎来说，path 指定的是实际的 PD 地址。例如 PD 部署在 192.168.100.113:2379, 192.168.100.114:2379 和 192.168.100.115:2379 上面，那么 path 为 &amp;ldquo;192.168.100.113:2379, 192.168.100.114:2379, 192.168.100.115:2379&amp;rdquo; 默认: &amp;ldquo;/tmp/tidb&amp;rdquo;  --report-status  打开 (true) 或者关闭 (false) 服务状态监听端口 默认: true 值可以为 (true) 或者 (false). (true) 表明开启状态监听端口。 (false) 表明关闭。状态监听端口用于通过 HTTP 方式对外报告一些服务内部信息  --run-ddl  tidb-server 是否运行 DDL 语句，集群内大于两台以上 tidb-server 时设置 默认: true 值可以为 (true) 或者 (false). (true) 表明自身会运行 DDL. (false) 表明自身不会运行 DDL  --socket string  TiDB 服务使用 unix socket file 方式接受外部连接 默认: &amp;ldquo;&amp;rdquo; 譬如可以使用 &amp;ldquo;/tmp/tidb.sock&amp;rdquo; 来打开 unix socket file  --status  TiDB 服务状态监听端口 默认: &amp;ldquo;10080&amp;rdquo; 这个端口是为了展示 TiDB 内部数据用的。包括 prometheus 统计 以及 pprof Prometheus 统计可以通过 &amp;ldquo;http://host:status_port/metrics&amp;quot; 访问 Pprof 数据可以通过 &amp;ldquo;http://host:status_port/debug/pprof&amp;quot; 访问  --store  用来指定 TiDB 底层使用的存储引擎 默认: &amp;ldquo;mocktikv&amp;rdquo; 可选值包括 &amp;ldquo;memory&amp;rdquo;, &amp;ldquo;goleveldb&amp;rdquo;, &amp;ldquo;boltdb&amp;rdquo;, &amp;ldquo;mocktikv&amp;rdquo; 或者 &amp;ldquo;tikv&amp;rdquo;。（前面都是本地存储引擎，而 TiKV 是一个分布式存储引擎） 例如，通过 tidb-server --store=memory 来启动一个纯内存引擎的 TiDB  TiDB 服务器配置文件 启动 TiDB 服务器时，通过 --config path 可以指定服务器的配置文件。对于配置中重叠的选项，命令行启动参数的优先级高于配置文件。一份配置文件的示例参见 https://github.com/pingcap/tidb/blob/master/config/config.toml.example以下是启动参数的完整描述。host 同启动参数 hostport 同启动参数 Ppath 同启动参数 pathsocket 同启动参数 socketbinlog-socket 同启动参数 binlog-socketrun-ddl 同启动参数 run-ddlcross-join  默认: true 在做 join 的时候，两边表没有任何条件（where 字段），默认可以执行这样的语句。但是设置为 false，则如有这样的 join 语句出现，server 会拒绝执行  join-concurrency  join-concurrency 并发执行 join 的 goroutine 数量 默认: 5 看数据量和数据分布情况，一般情况下是越多越好，数值越大对 CPU 开销越大  query-log-max-len  日志中记录最大 sql 语句长度 默认: 2048 过长的请求输出到 log 时会被截断  slow-threshold int  大于这个值得 sql 语句将被记录 默认: 300 值只能是一个整数 (int) ，单位是毫秒  slow-query-file  慢查询日志文件 默认: &amp;ldquo;&amp;rdquo; 值是文件名，若指定了一个非空字符串，则慢查询日志会被重定向到相应的文件  retry-limit  事务遇见冲突时，提交事物最大重试次数 默认: 10 设置较大的重试次数会影响 TiDB 集群性能  skip-grant-table  允许任何人不带密码连接，并且所有的操作不检查权限 默认: false 值可以是(true) or (false)。启用此选项需要本机的 root 权限，一般用于忘记密码时重置  stats-lease  增量扫描全表并分析表的数据量 索引等一些信息 默认: &amp;ldquo;3s&amp;rdquo; 使用此参数需要先手动执行 analyze table name; 自动更新统计信息,持久化存储到 TiKV，会耗费一些内存开销,  tcp-keep-alive  TiDB 在 tcp 层开启 keepalive 默认: false  ssl-cert  PEM 格式的 SSL 证书文件路径 默认: &amp;ldquo;&amp;rdquo; 当同时设置了该选项和 --ssl-key 选项时，TiDB 将接受（但不强制）客户端使用 TLS 安全地连接到 TiDB。 若指定的证书或私钥无效，则 TiDB 会照常启动，但无法接受安全连接。  ssl-key  PEM 格式的 SSL 证书密钥文件路径，即 --ssl-cert 所指定的证书的私钥 默认: &amp;ldquo;&amp;rdquo; 目前 TiDB 不支持加载由密码保护的私钥。  ssl-ca  PEM 格式的受信任 CA 的证书文件路径 默认: &amp;ldquo;&amp;rdquo; 当同时设置了该选项和 --ssl-cert、--ssl-key 选项时，TiDB 将在客户端出示证书的情况下根据该选项指定的受信任的 CA 列表验证客户端证书。若验证失败，则连接会被终止。 即使设置了该选项，若客户端没有出示证书，则安全连接仍然继续，不会进行客户端证书验证。  "},
		{"url": "https://pingcap.com/docs-cn/op-guide/horizontal-scale/",
		"title": "TiDB 集群扩容缩容方案", 
		"content": " TiDB 集群扩容缩容方案 概述 TiDB 集群可以在不影响线上服务的情况下动态进行扩容和缩容。下面分别介绍如果增加或者删除 PD，TiKV 以及 TiDB 的节点。下面用到的 pd-ctl 文档可以参考 pd-control。PD 假设现在我们有三个 PD 服务，详细信息如下：   Name ClientUrls PeerUrls     pd1 http://host1:2379 http://host1:2380   pd2 http://host2:2379 http://host2:2380   pd3 http://host3:2379 http://host3:2380    我们可以通过 pd-ctl 来查看当前所有 PD 节点的信息：./pd-ctl -u http://host1:2379 &amp;gt;&amp;gt; member 动态添加节点 我们可以使用 join 参数，将一个新的 PD 服务加入到现有的 PD 集群里面。 如果我们需要添加 pd4，只需要在 --join 参数里面填入当前 PD 集群任意一个 PD 服务的 client url，比如：./bin/pd-server --name=pd4   --client-urls=&amp;#34;http://host4:2379&amp;#34;   --peer-urls=&amp;#34;http://host4:2380&amp;#34;   --join=&amp;#34;http://host1:2379&amp;#34; 动态删除节点 如果我们需要删除 pd4，可以通过 pd-ctl 来完成：./pd-ctl -u http://host1:2379 &amp;gt;&amp;gt; member delete pd4 动态迁移节点 如果想把现有的 PD 节点迁移到新的机器上，我们可以先在新的机器上添加节点，然后把旧的机器上的节点删除掉。 迁移过程中应该一个节点一个节点逐个迁移，每完成一个步骤可以先查看一下当前的所有节点信息来进行确认。TiKV 我们可以通过 pd-ctl 来查看当前所有 TiKV 节点的信息：./pd-ctl -u http://host1:2379 &amp;gt;&amp;gt; store 动态添加节点 动态添加一个新的 TiKV 服务非常容易，只需要在新的机器上启动一个 TiKV 服务，不需要其他特殊操作。 新启动的 TiKV 服务会自动注册到现有集群的 PD 中，PD 会自动做负载均衡，逐步地把一部分数据迁移到新的TiKV 服务中，从而降低现有 TiKV 服务的压力。动态删除节点 安全地删除（下线）一个 TiKV 服务需要先告诉 PD，这样 PD 可以先把这个 TiKV 服务上面的数据迁移到其他 TiKV 服务上，保证数据有足够的副本数。假设我们需要删除 store id 为 1 的 TiKV 服务，可以通过 pd-ctl 来完成：./pd-ctl -u http://host1:2379 &amp;gt;&amp;gt; store delete 1 然后可以查看这个 TiKV 服务的状态：./pd-ctl -u http://host1:2379 &amp;gt;&amp;gt; store 1 { &amp;#34;store&amp;#34;: { &amp;#34;id&amp;#34;: 1, &amp;#34;address&amp;#34;: &amp;#34;127.0.0.1:21060&amp;#34;, &amp;#34;state&amp;#34;: 1, &amp;#34;state_name&amp;#34;: &amp;#34;Offline&amp;#34; }, &amp;#34;status&amp;#34;: { ... } } 我们可以通过这个 store 的 state_name 来确定这个 store 的状态： state_name=Up: 这个 store 正常服务 state_name=Disconnected: 当前没有检测到这个 store 的心跳，可能是故障或网络连接中断 state_name=Down: 超过一小时（可通过 max-down-time 配置）没有收到 store 心跳，此时 PD 会为这个 store 上的数据添加副本 state_name=Offline: 这个 store 正在下线，此时 store 仍在服务中 state_name=Tombstone: 这个 store 已经完成下线，此时 store 上已经没有数据，可以关闭实例  动态迁移节点 迁移 TiKV 服务也是通过先在新的机器上添加节点，然后把旧的机器上的节点下线来完成。 迁移过程中可以先把新集群的机器全部添加到已有的集群中，然后再把旧的节点一个一个地下线。 可以通过查看正在下线的节点的状态信息来确定这个节点是否已经完成下线，确认完成以后再下线下一个节点。TiDB TiDB 是一个无状态的服务，这也就意味着我们能直接添加和删除 TiDB。 需要注意的是，如果我们在 TiDB 的服务的前面搭建了一个 proxy（譬如 HAProxy），则需要更新 proxy 的配置并重新载入。"},
		{"url": "https://pingcap.com/docs-cn/trouble-shooting/",
		"title": "TiDB 集群故障诊断", 
		"content": " TiDB 集群故障诊断 当试用 TiDB 遇到问题时，请先参考本篇文档。如果问题未解决，请按文档要求收集必要的信息通过 Github 提供给 TiDB 开发者。如何给 TiDB 开发者报告错误 当使用 TiDB 遇到问题并且通过后面所列信息无法解决时，请收集以下信息并创建新 Issue: 具体的出错信息以及正在执行的操作 当前所有组件的状态 出问题组件 log 中的 error/fatal/panic 信息 机器配置以及部署拓扑 dmesg 中 TiDB 组件相关的问题  数据库连接不上 首先请确认集群的各项服务是否已经启动，包括 tidb-server、pd-server、tikv-server。请用 ps 命令查看所有进程是否在。如果某个组件的进程已经不在了，请参考对应的章节排查错误。如果所有的进程都在，请查看 tidb-server 的日志，看是否有报错？常见的错误包括： InfomationSchema is out of date无法连接 tikv-server，请检查 pd-server 以及 tikv-server 的状态和日志。 panic程序有错误，请将具体的 panic log 提供给 TiDB 开发者。  如果是清空数据并重新部署服务，请确认以下信息： pd-server、tikv-server 数据都已清空tikv-server 存储具体的数据，pd-server 存储 tikv-server 中数据的的元信息。如果只清空 pd-server 或只清空 tikv-server 的数据，会导致两边数据不匹配。 清空 pd-server 和 tikv-server 的数据并重启后，也需要重启 tidb-server集群 ID 是由 pd-server 在集群初始化时随机分配，所以重新部署集群后，集群 ID 会发生变化。tidb-server 业务需要重启以获取新的集群 ID。  tidb-server 启动报错 tidb-server 无法启动的常见情况包括： 启动参数错误请参考TiDB 命令行参数文档。 端口被占用：lsof -i:port请确保 tidb-server 启动所需要的端口未被占用。 无法连接 pd-server首先检查 pd-server 的进程状态和日志，确保 pd-server 成功启动，对应端口已打开：lsof -i:port。若 pd-server 正常，则需要检查 tidb-server 机器和 pd-server 对应端口之间的连通性， 确保网段连通且对应服务端口已添加到防火墙白名单中，可通过 nc 或 curl 工具检查。例如，假设 tidb 服务位于 192.168.1.100，无法连接的 pd 位于 192.168.1.101，且 2379 为其 client port， 则可以在 tidb 机器上执行 nc -v -z 192.168.1.101 2379，测试是否可以访问端口。 或使用 curl -v 192.168.1.101:2379/pd/api/v1/leader 直接检查 pd 是否正常服务。  tikv-server 启动报错  启动参数错误请参考TiKV 启动参数文档。 端口被占用：lsof -i:port请确保 tikv-server 启动所需要的端口未被占用： lsof -i:port。 无法连接 pd-server首先检查 pd-server 的进程状态和日志。确保 pd-server 成功启动，对应端口已打开：lsof -i:port。若 pd-server 正常，则需要检查 tikv-server 机器和 pd-server 对应端口之间的连通性， 确保网段连通且对应服务端口已添加到防火墙白名单中，可通过 nc 或 curl 工具检查。具体命令参考上一节。 文件被占用不要在一个数据库文件目录上打开两个 tikv。  pd-server 启动报错  启动参数错误请参考PD 命令行参数文档。 端口被占用：lsof -i:port请确保 pd-server 启动所需要的端口未被占用： lsof -i:port。  TiDB/TiKV/PD 进程异常退出  进程是否是启动在前台当前终端退出给其所有子进程发送 HUP 信号，从而导致进程退出。 是否是在命令行用过 nohup+&amp;amp; 方式直接运行这样依然可能导致进程因终端连接突然中断，作为终端 SHELL 的子进程被杀掉。 推荐将启动命令写在脚本中，通过脚本运行（相当于二次 fork 启动）。  TiKV 进程异常重启  检查 dmesg 或者 syslog 里面是否有 OOM 信息如果有 OOM 信息并且杀掉的进程为 TiKV，请减少 TiKV 的 RocksDB 的各个 CF 的 block-cache-size 值。 检查 TiKV 日志是否有 panic 的 log提交 Issue 并附上 panic 的 log。  TiDB panic 请提供 panic 的 log连接被拒绝  请确保操作系统的网络参数正确，包括但不限于  连接字符串中的端口和 tidb-server 启动的端口需要一致 请保证防火墙的配置正确   Too many open files 在启动进程之前，请确保 ulimit -n 的结果足够大，推荐设为 unlimited 或者是大于 1000000。数据库访问超时，系统负载高 首先请提供如下信息： 部署的拓扑结构  tidb-server/pd-server/tikv-server 部署了几个实例 这些实例在机器上是如何分布的  机器的硬件配置  CPU 核数 内存大小 硬盘类型（SSD 还是机械硬盘） 是实体机还是虚拟机  机器上除了 TiDB 集群之外是否还有其他服务 pd-server 和 tikv-server 是否分开部署 目前正在进行什么操作 用 top -H 命令查看当前占用 CPU 的线程名 最近一段时间的网络/IO 监控数据是否有异常  "},
		{"url": "https://pingcap.com/docs-cn/op-guide/monitor/",
		"title": "TiDB 集群监控", 
		"content": " TiDB 集群监控 TiDB 集群状态监控目前有两种接口，第一种是通过 HTTP 接口对外汇报组件的信息，我们称之为组件的状态接口；第二种是使用 prometheus 记录组件中各种操作的详细信息，我们称之为 metrics 接口。组件状态接口 这类接口可以获取组件的一些基本信息，并且可以作为 keepalive 监测接口。另外 PD 的接口可以看到整个 TiKV 集群的详细信息。TiDB Server TiDB 对外暴露的 HTTP 接口是 http://host:port/status ，默认的端口号是 10080 （可以通过 &amp;ndash;status 参数设置），可以通过访问这个接口获取当前 TiDB Server 的状态，以及判断是否存活。返回结果是 Json 格式：curl http://127.0.0.1:10080/status { connections: 0, version: &amp;#34;5.5.31-TiDB-1.0&amp;#34;, git_hash: &amp;#34;b99521846ff6f71f06e2d49a3f98fa1c1d93d91b&amp;#34; }  connection: 当前 TiDB Server 上的客户端连接数 version: TiDB 版本号 git_hash: TiDB 当前代码的 Git Hash  PD Server PD API 地址： http://${host}:${port}/pd/api/v1/${api_name}。其中 port 默认为 2379，各类 api_name 详细信息参见 PD API Doc。通过这个接口可以获取当前所有 TiKV 的状态以及负载均衡信息。其中最重要也是最常用的接口获取 TiKV 集群所有节点状态的接口，下面以一个单个 TiKV 构成的集群为例，说明一些用户需要了解的信息：curl http://127.0.0.1:2379/pd/api/v1/stores { &amp;#34;count&amp;#34;: 1, TiKV 节点数量 &amp;#34;stores&amp;#34;: [ // TiKV 节点的列表 // 下面列出的是这个集群中单个 TiKV 节点的信息 { &amp;#34;store&amp;#34;: { &amp;#34;id&amp;#34;: 1, &amp;#34;address&amp;#34;: &amp;#34;127.0.0.1:22161&amp;#34;, &amp;#34;state&amp;#34;: 0 }, &amp;#34;status&amp;#34;: { &amp;#34;store_id&amp;#34;: 1, // 节点的 ID &amp;#34;capacity&amp;#34;: 1968874332160, // 存储总容量 &amp;#34;available&amp;#34;: 1264847716352, // 存储剩余容量 &amp;#34;region_count&amp;#34;: 1, // 该节点上存放的 Region 数量 &amp;#34;sending_snap_count&amp;#34;: 0, &amp;#34;receiving_snap_count&amp;#34;: 0, &amp;#34;start_ts&amp;#34;: &amp;#34;2016-10-24T19:54:00.110728339+08:00&amp;#34;, // 启动时间 &amp;#34;last_heartbeat_ts&amp;#34;: &amp;#34;2016-10-25T10:52:54.973669928+08:00&amp;#34;, // 最后一次心跳时间 &amp;#34;total_region_count&amp;#34;: 1, // 总 Region 数量 &amp;#34;leader_region_count&amp;#34;: 1, // Leader Region 数量 &amp;#34;uptime&amp;#34;: &amp;#34;14h58m54.862941589s&amp;#34; }, &amp;#34;scores&amp;#34;: [ 100, 35 ] } ] } Metrics 监控 这部分主要对整个集群的状态、性能做监控，通过 Prometheus+Grafana 展现 metrics 数据，在下面一节会介绍如何搭建监控系统。TiDB Server  query 处理时间，可以看到延迟和吞吐 ddl 过程监控 TiKV client 相关的监控 PD client 相关的监控  PD Server  命令执行的总次数 某个命令执行失败的总次数 某个命令执行成功的耗时统计 某个命令执行失败的耗时统计 某个命令执行完成并返回结果的耗时统计  TiKV Server  GC 监控 执行 KV 命令的总次数 Scheduler 执行命令的耗时统计 Raft propose 命令的总次数 Raft 执行命令的耗时统计 Raft 执行命令失败的总次数 Raft 处理 ready 状态的总次数  使用 Prometheus+Grafana 部署架构 整个架构如下图所示，在 TiDB/PD/TiKV 三个组件的启动参数中添加 Prometheus Pushgateway 地址:搭建监控系统 Prometheus Push Gateway 参考： https://github.com/prometheus/pushgatewayPrometheus Server 参考： https://github.com/prometheus/prometheus#installGrafana 参考： http://docs.grafana.org配置 TiDB/PD/TiKV 配置  TiDB设置 --metrics-addr 和 --metrics-interval 两个参数，其中 metrics-addr 设为 Push Gateway 的地址，metrics-interval 为 push 的频率，单位为秒，默认值为 15 PD修改 toml 配置文件，填写 Push Gateway 的地址和推送频率[metric] # prometheus client push interval, set &amp;#34;0s&amp;#34; to disable prometheus. interval = &amp;#34;15s&amp;#34; # prometheus pushgateway address, leaves it empty will disable prometheus. address = &amp;#34;host:port&amp;#34; TiKV修改 toml 配置文件，填写 Push Gateway 的地址和推送频率，job 字段一般设为“tikv”。[metric] # the Prometheus client push interval. Setting the value to 0s stops Prometheus client from pushing. interval = &amp;#34;15s&amp;#34; # the Prometheus pushgateway address. Leaving it empty stops Prometheus client from pushing. address = &amp;#34;host:port&amp;#34; # the Prometheus client push job name. Note: A node id will automatically append, e.g., &amp;#34;tikv_1&amp;#34;. job = &amp;#34;tikv&amp;#34;  PushServer 配置 一般无需特殊配置，使用默认端口 9091 即可Prometheus 配置：在 yaml 配置文件中添加 Push Gateway 地址：scrape_configs: # The job name is added as a label `job=&amp;lt;job_name&amp;gt;` to any timeseries scraped from this config. - job_name: &amp;#39;TiDB&amp;#39; # Override the global default and scrape targets from this job every 5 seconds. scrape_interval: 5s honor_labels: true static_configs: - targets: [&amp;#39;host:port&amp;#39;] # use the Push Gateway address labels: group: &amp;#39;production&amp;#39; Grafana 配置  进入 Grafana Web 界面（默认地址: http://localhost:3000 ，默认账号: admin 密码: admin）点击 Grafana Logo -&amp;gt; 点击 Data Sources -&amp;gt; 点击 Add data source -&amp;gt; 填写 data source 信息 ( 注: Type 选 Prometheus，Url 为 Prometheus 地址，其他根据实际情况填写 ） 导入 dashboard 配置文件点击 Grafana Logo -&amp;gt; 点击 Dashboards -&amp;gt; 点击 Import -&amp;gt; 选择需要的 Dashboard 配置文件上传 -&amp;gt; 选择对应的 data source  "},
		{"url": "https://pingcap.com/docs/tools/tidb-binlog-kafka/",
		"title": "TiDB-Binlog user guide", 
		"content": " TiDB-Binlog User Guide This document describes how to deploy the Kafka version of TiDB-Binlog. If you need to deploy the local version of TiDB-Binlog, see the TiDB-Binlog user guide for the local version.About TiDB-Binlog TiDB-Binlog is a tool for enterprise users to collect binlog files for TiDB and provide real-time backup and synchronization.TiDB-Binlog supports the following scenarios: Data synchronization: to synchronize TiDB cluster data to other databases Real-time backup and recovery: to back up TiDB cluster data, and recover in case of cluster outages   TiDB-Binlog architecture The TiDB-Binlog architecture is as follows:The TiDB-Binlog cluster mainly consists of three components:Pump Pump is a daemon that runs on the background of each TiDB host. Its main function is to record the binlog files generated by TiDB in real time and write to the file in the disk sequentially.Drainer Drainer collects binlog files from each Pump node, converts them into specified database-compatible SQL statements in the commit order of the transactions in TiDB, and synchronizes to the target database or writes to the file sequentially.Kafka &amp;amp; Zookeeper The Kafka cluster stores the binlog data written by Pump and provides the binlog data to Drainer for reading. Note: In the local version of TiDB-Binlog, the binlog is stored in files, while in the latest version, the binlog is stored using Kafka. Install TiDB-Binlog Download Binary for the CentOS 7.3+ platform # Download the tool package. wget http://download.pingcap.org/tidb-binlog-latest-linux-amd64.tar.gz wget http://download.pingcap.org/tidb-binlog-latest-linux-amd64.sha256 # Check the file integrity. If the result is OK, the file is correct. sha256sum -c tidb-binlog-latest-linux-amd64.sha256 # Extract the package. tar -xzf tidb-binlog-latest-linux-amd64.tar.gz cd tidb-binlog-latest-linux-amd64 Deploy TiDB-Binlog Note  You need to deploy a Pump for each TiDB server in the TiDB cluster. Currently, the TiDB server only supports the binlog in UNIX socket. When you deploy a Pump manually, to start the service, follow the order of Pump -&amp;gt; TiDB; to stop the service, follow the order of TiDB -&amp;gt; Pump.We set the startup parameter binlog-socket as the specified unix socket file path of the corresponding parameter socket in Pump. The final deployment architecture is as follows: Drainer does not support renaming DDL on the table of the ignored schemas (schemas in the filter list). To start Drainer in the existing TiDB cluster, usually you need to do a full backup, get the savepoint, import the full backup, and start Drainer and synchronize from the savepoint.To guarantee the integrity of data, perform the following operations 10 minutes after Pump is started: Use the generate_binlog_position tool to generate the Drainer savepoint file. The tool is involved in the tidb-tools project. See the README description for usage. Do a full backup. For example, back up TiDB using mydumper. Import the full backup to the target system. Set the file path of the savepoint and start Drainer:bin/drainer --config=conf/drainer.toml --data-dir=${drainer_savepoint_dir}  The drainer outputs pb and you need to set the following parameters in the configuration file:[syncer] db-type = &amp;#34;pb&amp;#34; disable-dispatch = true [syncer.to] dir = &amp;#34;/path/pb-dir&amp;#34; Before you deploy the TiDB-Binlog, install the Kafka and Zookeeper cluster and pay attention to the following items: Make sure that Kafka is 0.9 version or later. It is required to set the parameter auto.create.topics.enable=true. It is recommended to deploy Kafka and Zookeeper on 3 to 5 servers. The size of the disk space depends on the business data volume.   Deploy Pump using TiDB-Ansible  If you have not deployed the Kafka cluster, use the Kafka-Ansible to deploy. When you deploy the TiDB cluster using TiDB-Ansible, edit the tidb-ansible/inventory.ini file, set enable_binlog = True, and configure the zookeeper_addrs variable as the Zookeeper address of the Kafka cluster. In this way, Pump is deployed while you deploy the TiDB cluster.  Configuration example:# binlog trigger enable_binlog = True # zookeeper address of kafka cluster, example: # zookeeper_addrs = &amp;#34;192.168.0.11:2181,192.168.0.12:2181,192.168.0.13:2181&amp;#34; zookeeper_addrs = &amp;#34;192.168.0.11:2181,192.168.0.12:2181,192.168.0.13:2181&amp;#34; Deploy Pump using Binary  Description of Pump command line argumentsUsage of Pump: -L string log level: debug, info, warn, error, fatal (default &amp;#34;info&amp;#34;) -V to print Pump version info -addr string the RPC address that Pump provides service (default &amp;#34;127.0.0.1:8250&amp;#34;) -advertise-addr string the RPC address that Pump provides external service -config string to configure the file path of Pump; if you specifies the configuration file, Pump reads the configuration first; if the corresponding configuration also exists in the command line argument, Pump uses the command line configuration to cover that in the configuration file -data-dir string the path of storing Pump data -kafka-addrs string the connected Kafka address (default &amp;#34;127.0.0.1:9092&amp;#34;) -zookeeper-addrs string the Zookeeper address; if you set the option, the Kafka address is got from Zookeeper; if you do not set the option, the value of kafka-addrs is used -gc int the maximum days that the binlog is retained (default 7), and 0 means retaining the binlog permanently -heartbeat-interval int the interval between heartbeats that Pump sends to PD (unit: second) -log-file string the path of the log file -log-rotate string the log file rotating frequency (hour/day) -metrics-addr string the Prometheus pushgataway address; leaving it empty disables Prometheus push -metrics-interval int the frequency of reporting monitoring information (default 15, unit: second) -pd-urls string the node address of the PD cluster (default &amp;#34;http://127.0.0.1:2379&amp;#34;) -socket string the monitoring address of the unix socket service (default &amp;#34;unix:///tmp/pump.sock&amp;#34;) Pump configuration file# Pump configuration. # the RPC address that Pump provides service addr = &amp;#34;127.0.0.1:8250&amp;#34; # the RPC address that Pump provides external service advertise-addr = &amp;#34;&amp;#34; # an integer value to control expiry date of the binlog data, indicates how long (in days) the binlog data is stored. # (default value is 0, means binlog data would never be removed) gc = 7 # the path of storing Pump data data-dir = &amp;#34;data.pump&amp;#34; # the connected Kafka address (default &amp;#34;127.0.0.1:9092&amp;#34;) kafka-addrs = &amp;#34;127.0.0.1:9092&amp;#34; # the Zookeeper address; if you set the option, the Kafka address is got from Zookeeper; if you do not set the option, the value of kafka-addrs is used zookeeper-addrs = &amp;#34;127.0.0.1:2181&amp;#34; # the interval between heartbeats that Pump sends to PD (unit: second) heartbeat-interval = 3 # the node address of the PD cluster (default &amp;#34;http://127.0.0.1:2379&amp;#34;) pd-urls = &amp;#34;http://127.0.0.1:2379&amp;#34; # the monitoring address of the unix socket service (default &amp;#34;unix:///tmp/pump.sock&amp;#34;) socket = &amp;#34;unix:///tmp/pump.sock&amp;#34; Startup example./bin/pump -config pump.toml  Deploy Drainer using Binary  Description of Drainer command line argumentsUsage of Drainer: -L string log level: debug, info, warn, error, fatal (default &amp;#34;info&amp;#34;) -V to print Pump version info -addr string the address that Drainer provides service (default &amp;#34;127.0.0.1:8249&amp;#34;) -c int to synchronize the downstream concurrency number, and a bigger value means better throughput performance (default 1) -config string to configure the file path of Drainer; if you specifies the configuration file, Drainer reads the configuration first; if the corresponding configuration also exists in the command line argument, Pump uses the command line configuration to cover that in the configuration file -data-dir string the path of storing Drainer data (default &amp;#34;data.drainer&amp;#34;) -kafka-addrs string the connected Kafka address …"},
		{"url": "https://pingcap.com/docs/tools/tidb-binlog/",
		"title": "TiDB-Binlog user guide", 
		"content": " TiDB-Binlog User Guide About TiDB-Binlog TiDB-Binlog is a tool for enterprise users to collect binlog files for TiDB and provide real-time backup and synchronization.TiDB-Binlog supports the following scenarios: Data synchronization: to synchronize TiDB cluster data to other databases Real-time backup and recovery: to back up TiDB cluster data, and recover in case of cluster outages   TiDB-Binlog architecture The TiDB-Binlog architecture is as follows:The TiDB-Binlog cluster mainly consists of two components:Pump Pump is a daemon that runs on the background of each TiDB host. Its main function is to record the binlog files generated by TiDB in real time and write to the file in the disk sequentially.Drainer Drainer collects binlog files from each Pump node, converts them into specified database-compatible SQL statements in the commit order of the transactions in TiDB, and synchronizes to the target database or writes to the file sequentially.Install TiDB-Binlog Download Binary for the CentOS 7.3+ platform # Download the tool package. wget http://download.pingcap.org/tidb-binlog-latest-linux-amd64.tar.gz wget http://download.pingcap.org/tidb-binlog-latest-linux-amd64.sha256 # Check the file integrity. If the result is OK, the file is correct. sha256sum -c tidb-binlog-latest-linux-amd64.sha256 # Extract the package. tar -xzf tidb-binlog-latest-linux-amd64.tar.gz cd tidb-binlog-latest-linux-amd64 Deploy TiDB-Binlog  It is recommended to deploy Pump using Ansible. Build a new TiDB cluster with a startup order of pd-server -&amp;gt; tikv-server -&amp;gt; pump -&amp;gt; tidb-server -&amp;gt; drainer. Edit the tidb-ansible inventory.ini file:enable_binlog = True Run ansible-playbook deploy.yml Run ansible-playbook start.yml  Deploy Binlog for an existing TiDB cluster. Edit the tidb-ansible inventory.ini file:enable_binlog = True Run ansible-playbook rolling_update.yml   Note  You need to deploy a Pump for each TiDB server in a TiDB cluster. Currently, the TiDB server only supports the binlog in UNIX socket.  We set the startup parameter binlog-socket as the specified unix socket file path of the corresponding parameter socket in Pump. The final deployment architecture is as follows: Currently, you need to deploy Drainer manually. Drainer does not support renaming DDL on the table of the ignored schemas (schemas in the filter list). To start Drainer in the existing TiDB cluster, usually you need to do a full backup, get the savepoint, import the full backup, and start Drainer and synchronize from the savepoint. To guarantee the integrity of data, perform the following operations 10 minutes after Pump is started: Run Drainer at the gen-savepoint model and generate the Drainer savepoint file:bin/drainer -gen-savepoint --data-dir= ${drainer_savepoint_dir} --pd-urls=${pd_urls} Do a full backup. For example, back up TiDB using mydumper. Import the full backup to the target system. Set the file path of the savepoint and start Drainer:bin/drainer --config=conf/drainer.toml --data-dir=${drainer_savepoint_dir}  The drainer outputs pb and you need to set the following parameters in the configuration file.[syncer] db-type = &amp;#34;pb&amp;#34; disable-dispatch = true [syncer.to] dir = &amp;#34;/path/pb-dir&amp;#34;  Examples and parameters explanation Pump Example./bin/pump -config pump.toml Parameters ExplanationUsage of Pump: -L string log level: debug, info, warn, error, fatal (default &amp;#34;info&amp;#34;) -V print Pump version info -addr string addr(i.e. &amp;#39;host:port&amp;#39;) to listen on for client traffic (default &amp;#34;127.0.0.1:8250&amp;#34;). -advertise-addr string addr(i.e. &amp;#39;host:port&amp;#39;) to advertise to the public -config string path to the Pump configuration file -data-dir string path to store binlog data -gc int recycle binlog files older than gc days, zero means never recycle (default 7) -heartbeat-interval int number of seconds between heartbeat ticks (default 2) -log-file string log file path -log-rotate string log file rotate type, hour/day -metrics-addr string Prometheus pushgataway address; leaving it empty will disable Prometheus push -metrics-interval int Prometheus client push interval in second, set &amp;#34;0&amp;#34; to disable Prometheus push (default 15) -pd-urls string a comma separated list of the PD endpoints (default &amp;#34;http://127.0.0.1:2379&amp;#34;) -socket string unix socket addr to listen on for client traffic Configuration file# Pump Configuration. # addr(i.e. &amp;#39;host:port&amp;#39;) to listen on for client traffic addr = &amp;#34;127.0.0.1:8250&amp;#34; # addr(i.e. &amp;#39;host:port&amp;#39;) to advertise to the public advertise-addr = &amp;#34;&amp;#34; # a integer value to control expiry date of the binlog data, indicates for how long (in days) the binlog data would be stored. # (default value is 0, means binlog data would never be removed) gc = 7 # path to the data directory of Pump&amp;#39;s data data-dir = &amp;#34;data.pump&amp;#34; # number of seconds between heartbeat ticks (in 2 seconds) heartbeat-interval = 2 # a comma separated list of PD endpoints pd-urls = &amp;#34;http://127.0.0.1:2379&amp;#34; # unix socket addr to listen on for client traffic socket = &amp;#34;unix:///tmp/pump.sock&amp;#34; Drainer Example./bin/drainer -config drainer.toml Parameters ExplanationUsage of Drainer: -L string log level: debug, info, warn, error, fatal (default &amp;#34;info&amp;#34;) -V print version info -addr string addr (i.e. &amp;#39;host:port&amp;#39;) to listen on for Drainer connections (default &amp;#34;127.0.0.1:8249&amp;#34;) -c int parallel worker count (default 1) -config string path to the configuration file -data-dir string Drainer data directory path (default data.drainer) (default &amp;#34;data.drainer&amp;#34;) -dest-db-type string target db type: mysql or pb; see syncer section in conf/drainer.toml (default &amp;#34;mysql&amp;#34;) -detect-interval int the interval time (in seconds) of detecting Pumps&amp;#39; status (default 10) -disable-dispatch disable dispatching sqls that in one same binlog; if set true, work-count and txn-batch would be useless -gen-savepoint generate the savepoint from cluster -ignore-schemas string disable synchronizing those schemas (default &amp;#34;INFORMATION_SCHEMA,PERFORMANCE_SCHEMA,mysql&amp;#34;) -log-file string log file path -log-rotate string log file rotate type, hour/day -metrics-addr string Prometheus pushgateway address; leaving it empty will disable Prometheus push -metrics-interval int Prometheus client push interval in second, set &amp;#34;0&amp;#34; to disable Prometheus push (default 15) -pd-urls string a comma separated list of PD endpoints (default &amp;#34;http://127.0.0.1:2379&amp;#34;) -txn-batch int number of binlog events in a transaction batch (default 1)  Configuration file# Drainer Configuration # addr (i.e. &amp;#39;host:port&amp;#39;) to listen on for Drainer connections addr = &amp;#34;127.0.0.1:8249&amp;#34; # the interval time (in seconds) of detect Pumps&amp;#39; status detect-interval = 10 # Drainer meta data directory path data-dir = &amp;#34;data.drainer&amp;#34; # a comma separated list of PD endpoints pd-urls = &amp;#34;http://127.0.0.1:2379&amp;#34; # The file path of log log-file = &amp;#34;drainer.log&amp;#34; # syncer Configuration [syncer] # disable synchronizing these schemas ignore-schemas = &amp;#34;INFORMATION_SCHEMA,PERFORMANCE_SCHEMA,mysql&amp;#34; # number of binlog events in a transaction batc txn-batch = 1 # worker count to execute binlogs worker-count = 1 disable-dispatch = false # downstream storage, equal to --dest-db-type # valid values are &amp;#34;mysql&amp;#34;, &amp;#34;pb&amp;#34; db-type = &amp;#34;mysql&amp;#34; # The replicate-do-db prioritizes over replicate-do-table if having the same db name. # Regular expressions are supported, and starting with &amp;#39;~&amp;#39; declares the use of regular expressions. # replicate-do-db = [&amp;#34;~^b.*&amp;#34;,&amp;#34;s1&amp;#34;] # [[syncer.replicate-do-table]] # db-name =&amp;#34;test&amp;#34; # tbl-name = &amp;#34;log&amp;#34; # [[syncer.replicate-do-table]] # db-name =&amp;#34;test&amp;#34; # tbl-name = &amp;#34;~^a.*&amp;#34; # the downstream mysql protocol database [syncer.to] host = &amp;#34;127.0.0.1&amp;#34; user = &amp;#34;root&amp;#34; password = &amp;#34;&amp;#34; port = 3306 # uncomment …"},
		{"url": "https://pingcap.com/docs-cn/tools/tidb-binlog-kafka/",
		"title": "TiDB-Binlog 部署方案", 
		"content": " TiDB-Binlog 部署方案 本文档介绍如何部署 Kafka 版本的 TiDB-Binlog。如需部署 local 版本的 TiDB-Binlog，可参考 local 版本的 TiDB-Binlog 部署文档。TiDB-Binlog 简介 TiDB-Binlog 用于收集 TiDB 的 Binlog，并提供实时备份和同步功能的商业工具。TiDB-Binlog 支持以下功能场景: 数据同步: 同步 TiDB 集群数据到其他数据库 实时备份和恢复: 备份 TiDB 集群数据，同时可以用于 TiDB 集群故障时恢复  TiDB-Binlog 架构 首先介绍 TiDB-Binlog 的整体架构。TiDB-Binlog 集群主要分为三个组件：Pump Pump 是一个守护进程，在每个 TiDB 的主机上后台运行。他的主要功能是实时记录 TiDB 产生的 Binlog 并顺序写入kafka中Drainer Drainer 从 kafka 中收集 Binlog，并按照在 TiDB 中事务的提交顺序转化为指定数据库兼容的 SQL 语句，最后同步到目的数据库或者写到顺序文件Kafka &amp;amp; Zookeeper Kafka 集群用来存储由 Pump 写入的 binlog 数据，并提供给 Drainer 进行读取。（local版本将 binlog 存储在文件中，最新版本都使用 Kafka 存储）TiDB-Binlog 安装 下载官方 Binary  CentOS 7+# 下载压缩包 wget http://download.pingcap.org/tidb-binlog-latest-linux-amd64.tar.gz wget http://download.pingcap.org/tidb-binlog-latest-linux-amd64.sha256 # 检查文件完整性，返回 ok 则正确 sha256sum -c tidb-binlog-latest-linux-amd64.sha256 # 解开压缩包 tar -xzf tidb-binlog-latest-linux-amd64.tar.gz cd tidb-binlog-latest-linux-amd64  TiDB-Binlog 部署 注意  需要为一个 TiDB 集群中的每台 TiDB server 部署一个 pump，目前 TiDB server 只支持以 unix socket 方式的输出 binlog。 手动部署时， 启动优先级为： PUMP &amp;gt; TiDB ； 停止优先级为 TiDB &amp;gt; PUMP我们设置 TiDB 启动参数 binlog-socket 为对应的 pump 的参数 socket 所指定的 unix socket 文件路径，最终部署结构如下图所示： drainer 不支持对 ignore schemas（在过滤列表中的 schemas） 的 table 进行 rename DDL 操作 在已有的 TiDB 集群中启动 drainer，一般需要全量备份 并且获取 savepoint，然后导入全量备份，最后启动 drainer 从 savepoint 开始同步；为了保证数据的完整性，在 Pump 运行 10 分钟左右后按顺序进行下面的操作 使用 tidb-tools 项目中的 generate_binlog_position 工具生成 drainer 启动需要的 savepoint 文件中，make generate_binlog_position 编译该工具，具体的使用参考工具的 README 说明。也可以直接下载获取该工具：generate_binlog_position， 并使用sha256sum验证该文件 sha256。 全量备份，例如 mydumper 备份 tidb 全量导入备份到目标系统 kafka 版本 drainer 启动的 savepoint 默认保存在下游 database tidb_binlog 下的 checkpoint 表中，如果 checkpoint 表中没有效的数据，可以通过设置 initial-commit-ts 启动 drainer 从指定位置开始消费 - bin/drainer --config=conf/drainer.toml --initial-commit-ts=${commitTS}  drainer 输出的 pb, 需要在配置文件设置下面的参数[syncer] db-type = &amp;#34;pb&amp;#34; disable-dispatch = true [syncer.to] dir = &amp;#34;/path/pb-dir&amp;#34;  Kafka 和 Zookeeper 集群需要在部署 TiDB-Binlog 之前部署好。Kafka 需要 0.9 及以上版本。 kafka 集群配置推荐    名字 数量 内存 CPU 硬盘     Kafka 3+ 16G 8+ 2+ 1TB   Zookeeper 3+ 8G 4+ 2+ 300G    kafka 配置参数推荐  auto.create.topics.enable=true 如果还没有创建 topic，Kafka 会在 broker 上自动创建 topic broker.id 必备参数用来标识 Kafka 集群，不能重复，如 broker.id = 1 fs.file-max = 1000000 Kafka 会使用大量文件和网络 socket， 建议修改成 1000000， 修改方法（vi /etc/sysctl.conf）    使用 tidb-ansible 部署 PUMP  如无 Kafka 集群，可使用 kafka-ansible 部署 Kafka 集群。 使用 TiDB-Ansible 部署 TiDB 集群时，修改 tidb-ansible/inventory.ini 文件，设置 enable_binlog = True，并配置 zookeeper_addrs 变量为 Kafka 集群的 zookeeper 地址，这样部署 TiDB 集群时会部署 PUMP。  配置样例:# binlog trigger enable_binlog = True # zookeeper address of kafka cluster, example: # zookeeper_addrs = &amp;#34;192.168.0.11:2181,192.168.0.12:2181,192.168.0.13:2181&amp;#34; zookeeper_addrs = &amp;#34;192.168.0.11:2181,192.168.0.12:2181,192.168.0.13:2181&amp;#34; 使用 Binary 部署 PUMP 使用样例： 假设我们有三个 PD，三个 Zookeeper，一个 TiDB，各个节点信息如下 ``` TiDB=&amp;#34;192.168.0.10&amp;#34; PD1=&amp;#34;192.168.0.16&amp;#34; PD2=&amp;#34;192.168.0.15&amp;#34; PD3=&amp;#34;192.168.0.14&amp;#34; ZK1=&amp;#34;192.168.0.13&amp;#34; ZK2=&amp;#34;192.168.0.12&amp;#34; ZK3=&amp;#34;192.168.0.11&amp;#34; ``` 在 ip=&amp;#34;192.168.0.10&amp;#34; 的机器上面部署 Drainer Pump 对应的 PD 集群的 ip=&amp;#34;192.168.0.16,192.168.0.15,192.168.0.14&amp;#34; 对应的 Kafka 集群的 Zookeeper 的 ip=&amp;#34;192.168.0.13,192.168.0.12,192.168.0.11&amp;#34; 以此为例，说明 Pump Drainer 的使用  PUMP 命令行参数说明  Usage of pump: -L string 日志输出信息等级设置: debug, info, warn, error, fatal (默认 &amp;#34;info&amp;#34;) -V 打印版本信息 -addr string pump 提供服务的 rpc 地址(-addr=&amp;#34;192.168.0.10:8250&amp;#34;) -advertise-addr string pump 对外提供服务的 rpc 地址(-advertise-addr=&amp;#34;192.168.0.10:8250&amp;#34;) -config string 配置文件路径,如果你指定了配置文件，pump 会首先读取配置文件的配置 如果对应的配置在命令行参数里面也存在，pump 就会使用命令行参数的配置来覆盖配置文件里面的 -data-dir string pump 数据存储位置路径 -zookeeper-addrs string (-zookeeper_addrs = &amp;#34;192.168.0.11:2181,192.168.0.12:2181,192.168.0.13:2181&amp;#34;) zookeeper 地址，该选项从 zookeeper 中获取 kafka 地址 -gc int 日志最大保留天数 (默认 7)， 设置为 0 可永久保存 -heartbeat-interval uint pump 向 pd 发送心跳间隔 (单位 秒) -log-file string log 文件路径 -log-rotate string log 文件切换频率, hour/day -metrics-addr string prometheus pushgataway 地址，不设置则禁止上报监控信息 -metrics-interval int 监控信息上报频率 (默认 15，单位 秒) -pd-urls string pd 集群节点的地址 (-pd-urls=&amp;#34;http://192.168.0.16:2379,http://192.168.0.15:2379,http://192.168.0.14:2379&amp;#34;) -socket string unix socket 模式服务监听地址 (默认 unix:///tmp/pump.sock)  PUMP 配置文件  # pump Configuration. # pump 提供服务的 rpc 地址(&amp;#34;192.168.0.10:8250&amp;#34;) addr = &amp;#34;192.168.0.10:8250&amp;#34; # pump 对外提供服务的 rpc 地址(&amp;#34;192.168.0.10:8250&amp;#34;) advertise-addr = &amp;#34;&amp;#34; # binlog 最大保留天数 (默认 7)， 设置为 0 可永久保存 gc = 7 # pump 数据存储位置路径 data-dir = &amp;#34;data.pump&amp;#34; # zookeeper 地址，设置该选项从 zookeeper 中获取 kafka 地址 # zookeeper-addrs = &amp;#34;192.168.0.11:2181,192.168.0.12:2181,192.168.0.13:2181&amp;#34; # pump 向 pd 发送心跳间隔 (单位 秒)  heartbeat-interval = 3 # pd 集群节点的地址 pd-urls = &amp;#34;http://192.168.0.16:2379,http://192.168.0.15:2379,http://192.168.0.14:2379&amp;#34; # unix socket 模式服务监听地址 (默认 unix:///tmp/pump.sock) socket = &amp;#34;unix:///tmp/pump.sock&amp;#34;  启动示例  ./bin/pump -config pump.toml 使用 Binary 部署 Drainer  Drainer 命令行参数说明  Usage of drainer: -L string 日志输出信息等级设置: debug, info, warn, error, fatal (默认 &amp;#34;info&amp;#34;) -V 打印版本信息 -addr string drainer 提供服务的地址(-addr=&amp;#34;192.168.0.10:8249&amp;#34;) -c int 同步下游的并发数，该值设置越高同步的吞吐性能越好 (default 1) -config string 配置文件路径, drainer 会首先读取配置文件的配置 如果对应的配置在命令行参数里面也存在，drainer 就会使用命令行参数的配置来覆盖配置文件里面的 -data-dir string drainer 数据存储位置路径 (默认 &amp;#34;data.drainer&amp;#34;) -zookeeper-addrs string (-zookeeper-addrs=&amp;#34;192.168.0.11:2181,192.168.0.12:2181,192.168.0.13:2181&amp;#34;) zookeeper 地址，该选项从 zookeeper 中获取 kafka 地址 -dest-db-type string drainer 下游服务类型 (默认为 mysql) -detect-interval int 向 pd 查询在线 pump 的时间间隔 (默认 10，单位 秒) -disable-dispatch 是否禁用拆分单个 binlog 的 sqls 的功能，如果设置为 true，则按照每个 binlog 顺序依次还原成单个事务进行同步( 下游服务类型为 mysql, 该项设置为 False ) -ignore-schemas string db 过滤列表 (默认 &amp;#34;INFORMATION_SCHEMA,PERFORMANCE_SCHEMA,mysql,test&amp;#34;), 不支持对 ignore schemas 的 table 进行 rename DDL 操作 -initial-commit-ts (默认为 0) 如果 drainer 没有相关的断点信息，可以通过该项来设置相关的断点信息 -log-file string log 文件路径 -log-rotate string log 文件切换频率, hour/day -metrics-addr string prometheus pushgataway 地址，不设置则禁止上报监控信息 -metrics-interval int 监控信息上报频率 (默认 15，单位 秒) -pd-urls string pd 集群节点的地址 (-pd-urls=&amp;#34;http://192.168.0.16:2379,http://192.168.0.15:2379,http://192.168.0.14:2379&amp;#34;) -txn-batch int 输出到下游数据库一个事务的 sql 数量 (default 1)  Drainer 配置文件  # drainer Configuration. # drainer 提供服务的地址(&amp;#34;192.168.0.10:8249&amp;#34;) addr = &amp;#34;192.168.0.10:8249&amp;#34; # 向 pd 查询在线 pump 的时间间隔 (默认 10，单位 秒) detect-interval = 10 # drainer 数据存储位置路径 (默认 &amp;#34;data.drainer&amp;#34;) data-dir = &amp;#34;data.drainer&amp;#34; # zookeeper 地址，该选项从 zookeeper 中获取 kafka 地址 # zookeeper-addrs = &amp;#34;192.168.0.11:2181,192.168.0.12:2181,192.168.0.13:2181&amp;#34; # pd 集群节点的地址 pd-urls = &amp;#34;http://192.168.0.16:2379,http://192.168.0.15:2379,http://192.168.0.14:2379&amp;#34; # log 文件路径 log-file = &amp;#34;drainer.log&amp;#34; # syncer Configuration. [syncer] ## db 过滤列表 (默认 &amp;#34;INFORMATION_SCHEMA,PERFORMANCE_SCHEMA,mysql,test&amp;#34;), ## 不支持对 ignore schemas 的 table 进行 rename DDL 操作 ignore-schemas = &amp;#34;INFORMATION_SCHEMA,PERFORMANCE_SCHEMA,mysql&amp;#34; # 输出到下游数据库一个事务的 sql 数量 (default 1) txn-batch = 1 # 同步下游的并发数，该值设置越高同步的吞吐性能越好 (default 1) worker-count = 1 # 是否禁用拆分单个 binlog 的 sqls 的功能，如果设置为 true，则按照每个 binlog # 顺序依次还原成单个事务进行同步( 下游服务类型为 mysql, 该项设置为 False ) disable-dispatch = false # drainer 下游服务类型 (默认为 mysql) # 参数有效值为 &amp;#34;mysql&amp;#34;, &amp;#34;pb&amp;#34; db-type = &amp;#34;mysql&amp;#34; # replicate-do-db priority over replicate-do-table if have same db name # and we support regex expression , # 以 &amp;#39;~&amp;#39; 开始声明使用正则表达式 #replicate-do-db = [&amp;#34;~^b.*&amp;#34;,&amp;#34;s1&amp;#34;] #[[syncer.replicate-do-table]] #db-name =&amp;#34;test&amp;#34; #tbl-name = &amp;#34;log&amp;#34; #[[syncer.replicate-do-table]] #db-name =&amp;#34;test&amp;#34; #tbl-name = &amp;#34;~^a.*&amp;#34; # db-type 设置为 mysql 时，下游数据库服务器参数 [syncer.to] host = &amp;#34;192.168.0.10&amp;#34; user = &amp;#34;root&amp;#34; password = &amp;#34;&amp;#34; port = 3306 # db-type 设置为 pb 时,存放 binlog 文件的目录 # [syncer.to] # dir = &amp;#34;data.drainer&amp;#34;  启动示例  ./bin/drainer …"},
		{"url": "https://pingcap.com/docs-cn/tools/tidb-binlog/",
		"title": "TiDB-Binlog 部署方案", 
		"content": " TiDB-Binlog 部署方案 TiDB-Binlog 简介 TiDB-Binlog 用于收集 TiDB 的 Binlog，并提供实时备份和同步功能的商业工具。TiDB-Binlog 支持以下功能场景: 数据同步: 同步 TiDB 集群数据到其他数据库 实时备份和恢复: 备份 TiDB 集群数据，同时可以用于 TiDB 集群故障时恢复  TiDB-Binlog 架构 首先介绍 TiDB-Binlog 的整体架构。TiDB-Binlog 集群主要分为两个组件：Pump Pump 是一个守护进程，在每个 TiDB 的主机上后台运行。他的主要功能是实时记录 TiDB 产生的 Binlog 并顺序写入磁盘文件Drainer Drainer 从各个 Pump 节点收集 Binlog，并按照在 TiDB 中事务的提交顺序转化为指定数据库兼容的 SQL 语句，最后同步到目的数据库或者写到顺序文件TiDB-Binlog 安装 下载官方 Binary  CentOS 7+# 下载压缩包 wget http://download.pingcap.org/tidb-binlog-local-linux-amd64.tar.gz wget http://download.pingcap.org/tidb-binlog-local-linux-amd64.sha256 # 检查文件完整性，返回 ok 则正确 sha256sum -c tidb-binlog-local-linux-amd64.sha256 # 解开压缩包 tar -xzf tidb-binlog-local-linux-amd64.tar.gz cd tidb-binlog-local-linux-amd64  TiDB-Binlog 部署 注意  需要为一个 TiDB 集群中的每台 TiDB server 部署一个 pump，目前 TiDB server 只支持以 unix socket 方式的输出 binlog。 手动部署时， 启动优先级为： PUMP &amp;gt; TiDB ； 停止优先级为 TiDB &amp;gt; PUMP我们设置 TiDB 启动参数 binlog-socket 为对应的 pump 的参数 socket 所指定的 unix socket 文件路径，最终部署结构如下图所示： drainer 不支持对 ignore schemas（在过滤列表中的 schemas） 的 table 进行 rename DDL 操作 在已有的 TiDB 集群中启动 drainer，一般需要全量备份 并且获取 savepoint，然后导入全量备份，最后启动 drainer 从 savepoint 开始同步；为了保证数据的完整性，在 pump 运行 10 分钟左右后按顺序进行下面的操作 以 gen-savepoint model 运行 drainer 生成 drainer savepint 文件，bin/drainer -gen-savepoint --data-dir= ${drainer_savepoint_dir} --pd-urls=${pd_urls} 全量备份，例如 mydumper 备份 tidb 全量导入备份到目标系统 设置 savepoint 文件路径，然后启动 drainer， bin/drainer --config=conf/drainer.toml --data-dir=${drainer_savepoint_dir}  drainer 输出的 pb, 需要在配置文件设置下面的参数[syncer] db-type = &amp;#34;pb&amp;#34; disable-dispatch = true [syncer.to] dir = &amp;#34;/path/pb-dir&amp;#34;  使用 tidb-ansible 部署 PUMP (推荐)  搭建全新的 TiDB Cluster，启动顺序 pd-server -&amp;gt; tikv-server -&amp;gt; pump -&amp;gt; tidb-server -&amp;gt; drainer 修改 tidb-ansible inventory.ini 文件  enable_binlog = True  执行 ansible-playbook deploy.yml 执行 ansible-playbook start.yml  drainer 目前需要手动部署   对已有的 TiDB Cluster 部署 binlog 修改 tidb-ansible inventory.ini 文件  enable_binlog = True  执行 ansible-playbook rolling_update.yml &amp;ndash;tags=tidb  drainer 目前需要手动部署    使用 Binary 部署 PUMP  PUMP 命令行参数说明Usage of pump: -L string 日志输出信息等级设置: debug, info, warn, error, fatal (默认 &amp;#34;info&amp;#34;) -V 打印版本信息 -addr string pump 提供服务的 rpc 地址(默认 &amp;#34;127.0.0.1:8250&amp;#34;) -advertise-addr string pump 对外提供服务的 rpc 地址(默认 &amp;#34;127.0.0.1:8250&amp;#34;) -config string 配置文件路径,如果你指定了配置文件，pump 会首先读取配置文件的配置 如果对应的配置在命令行参数里面也存在，pump 就会使用命令行参数的配置来覆盖配置文件里面的 -data-dir string pump 数据存储位置路径 -gc int 日志最大保留天数 (默认 7)， 设置为 0 可永久保存 -heartbeat-interval uint pump 向 pd 发送心跳间隔 (单位 秒) -log-file string log 文件路径 -log-rotate string log 文件切换频率, hour/day -metrics-addr string prometheus pushgataway 地址，不设置则禁止上报监控信息 -metrics-interval int 监控信息上报频率 (默认 15，单位 秒) -pd-urls string pd 集群节点的地址 (默认 &amp;#34;http://127.0.0.1:2379&amp;#34;) -socket string unix socket 模式服务监听地址 (默认 unix:///tmp/pump.sock)  PUMP 配置文件  # pump Configuration. # pump 提供服务的 rpc 地址(默认 &amp;#34;127.0.0.1:8250&amp;#34;) addr = &amp;#34;127.0.0.1:8250&amp;#34; # pump 对外提供服务的 rpc 地址(默认 &amp;#34;127.0.0.1:8250&amp;#34;) advertise-addr = &amp;#34;&amp;#34; # binlog 最大保留天数 (默认 7)， 设置为 0 可永久保存 gc = 7 # pump 数据存储位置路径 data-dir = &amp;#34;data.pump&amp;#34; # pump 向 pd 发送心跳间隔 (单位 秒) heartbeat-interval = 3 # pd 集群节点的地址 (默认 &amp;#34;http://127.0.0.1:2379&amp;#34;) pd-urls = &amp;#34;http://127.0.0.1:2379&amp;#34; # unix socket 模式服务监听地址 (默认 unix:///tmp/pump.sock) socket = &amp;#34;unix:///tmp/pump.sock&amp;#34; 启动示例./bin/pump -config pump.toml  使用 Binary 部署 Drainer  Drainer 命令行参数说明Usage of drainer: -L string 日志输出信息等级设置: debug, info, warn, error, fatal (默认 &amp;#34;info&amp;#34;) -V 打印版本信息 -addr string drainer 提供服务的地址(默认 &amp;#34;127.0.0.1:8249&amp;#34;) -c int 同步下游的并发数，该值设置越高同步的吞吐性能越好 (default 1) -config string 配置文件路径, drainer 会首先读取配置文件的配置 如果对应的配置在命令行参数里面也存在，drainer 就会使用命令行参数的配置来覆盖配置文件里面的 -data-dir string drainer 数据存储位置路径 (默认 &amp;#34;data.drainer&amp;#34;) -dest-db-type string drainer 下游服务类型 (默认为 mysql) -detect-interval int 向 pd 查询在线 pump 的时间间隔 (默认 10，单位 秒) -disable-dispatch 是否禁用拆分单个 binlog 的 sqls 的功能，如果设置为 true，则按照每个 binlog 顺序依次还原成单个事务进行同步( 下游服务类型为 mysql, 该项设置为 False ) -gen-savepoint 如果设置为 true, 则只生成 drainer 的 savepoint meta 文件, 可以配合 mydumper 使用 -ignore-schemas string db 过滤列表 (默认 &amp;#34;INFORMATION_SCHEMA,PERFORMANCE_SCHEMA,mysql,test&amp;#34;), 不支持对 ignore schemas 的 table 进行 rename DDL 操作 -log-file string log 文件路径 -log-rotate string log 文件切换频率, hour/day -metrics-addr string prometheus pushgataway 地址，不设置则禁止上报监控信息 -metrics-interval int 监控信息上报频率 (默认 15，单位 秒) -pd-urls string pd 集群节点的地址 (默认 &amp;#34;http://127.0.0.1:2379&amp;#34;) -txn-batch int 输出到下游数据库一个事务的 sql 数量 (default 1) Drainer 配置文件# drainer Configuration. # drainer 提供服务的地址(默认 &amp;#34;127.0.0.1:8249&amp;#34;) addr = &amp;#34;127.0.0.1:8249&amp;#34; # 向 pd 查询在线 pump 的时间间隔 (默认 10，单位 秒) detect-interval = 10 # drainer 数据存储位置路径 (默认 &amp;#34;data.drainer&amp;#34;) data-dir = &amp;#34;data.drainer&amp;#34; # pd 集群节点的地址 (默认 &amp;#34;http://127.0.0.1:2379&amp;#34;) pd-urls = &amp;#34;http://127.0.0.1:2379&amp;#34; # log 文件路径 log-file = &amp;#34;drainer.log&amp;#34; # syncer Configuration. [syncer] ## db 过滤列表 (默认 &amp;#34;INFORMATION_SCHEMA,PERFORMANCE_SCHEMA,mysql,test&amp;#34;), ## 不支持对 ignore schemas 的 table 进行 rename DDL 操作 ignore-schemas = &amp;#34;INFORMATION_SCHEMA,PERFORMANCE_SCHEMA,mysql&amp;#34; # 输出到下游数据库一个事务的 sql 数量 (default 1) txn-batch = 1 # 同步下游的并发数，该值设置越高同步的吞吐性能越好 (default 1) worker-count = 1 # 是否禁用拆分单个 binlog 的 sqls 的功能，如果设置为 true，则按照每个 binlog # 顺序依次还原成单个事务进行同步( 下游服务类型为 mysql, 该项设置为 False ) disable-dispatch = false # drainer 下游服务类型 (默认为 mysql) # 参数有效值为 &amp;#34;mysql&amp;#34;, &amp;#34;pb&amp;#34; db-type = &amp;#34;mysql&amp;#34; # replicate-do-db priority over replicate-do-table if have same db name # and we support regex expression , # 以 &amp;#39;~&amp;#39; 开始声明使用正则表达式 #replicate-do-db = [&amp;#34;~^b.*&amp;#34;,&amp;#34;s1&amp;#34;] #[[syncer.replicate-do-table]] #db-name =&amp;#34;test&amp;#34; #tbl-name = &amp;#34;log&amp;#34; #[[syncer.replicate-do-table]] #db-name =&amp;#34;test&amp;#34; #tbl-name = &amp;#34;~^a.*&amp;#34; # db-type 设置为 mysql 时，下游数据库服务器参数 [syncer.to] host = &amp;#34;127.0.0.1&amp;#34; user = &amp;#34;root&amp;#34; password = &amp;#34;&amp;#34; port = 3306 # db-type 设置为 pb 时,存放 binlog 文件的目录 # [syncer.to] # dir = &amp;#34;data.drainer&amp;#34; 启动示例./bin/drainer -config drainer.toml  TiDB-Binlog 监控 这部分主要对 TiDB-Binlog 的状态、性能做监控，通过 Prometheus + Grafana 展现 metrics 数据，pump/drainer 配置 使用 ansible 部署的 pump 服务，已经在启动参数设置 metrics 。drainer 启动时可以设置 --metrics-addr 和 --metrics-interval 两个参数，其中 metrics-addr 设为 Push Gateway 的地址，metrics-interval 为 push 的频率，单位为秒，默认值为15Grafana 配置  进入 Grafana Web 界面（默认地址: http://localhost:3000，默认账号: admin 密码: admin）点击 Grafana Logo -&amp;gt; 点击 Data Sources -&amp;gt; 点击 Add data source -&amp;gt; 填写 data source 信息 ( 注: Type 选 Prometheus，Url 为 Prometheus 地址，根据实际情况 添加/填写 ） 导入 dashboard 配置文件点击 Grafana Logo -&amp;gt; 点击 Dashboards -&amp;gt; 点击 Import -&amp;gt; 选择需要的 dashboard 配置文件上传 -&amp;gt; 选择对应的 data source  "},
		{"url": "https://pingcap.com/recruit-cn/engineer/tikv-engineer/",
		"title": "TiKV Engineer", 
		"content": " TiKV Engineer 岗位职责  负责分布式数据库 TiKV 相关的设计，开发 负责构建分布式压力测试框架，稳定性测试框架  职位要求  三年以上相关领域开发经验，扎实的编程能力，精通 C/C++/Go/Rust 中的一种 对分布式系统的架构和原理有比较深入的了解 优秀的发现和解决问题能力，良好的沟通能力，具备团队合作精神  加分项  拥抱开源，对前沿技术有浓厚的热情和探索欲望，有开源项目经历 熟悉 Paxos/Raft 等分布式一致性算法 熟悉分布式事务模型 熟悉操作系统底层知识，有 TCP/IP， IO 等系统调优经验  待遇 20K - 40K + 期权, 13薪 + 奖金, 优秀者可面议工作地点 北京，上海，广州，杭州，特别优秀可 remote"},
		{"url": "https://pingcap.com/docs-cn/op-guide/tune-tikv/",
		"title": "TiKV 性能参数调优", 
		"content": " TiKV 性能参数调优 本文档用于描述如何根据机器配置情况来调整 TiKV 的参数，使 TiKV 的性能达到最优。TiKV 最底层使用的是 RocksDB 做为持久化存储，所以 TiKV 的很多性能相关的参数都是与 RocksDB 相关的。TiKV 使用了两个 RocksDB 实例，默认 RocksDB 实例存储 KV 数据，Raft RocksDB 实例（简称 RaftDB）存储 Raft 数据。TiKV 使用了 RocksDB 的 Column Falimies 特性。默认 RocksDB 实例将 KV 数据存储在内部的 default、write 和 lock 3 个 CF 内。 default CF 存储的是真正的数据，与其对应的参数位于 [rocksdb.defaultcf] 项中； write CF 存储的是数据的版本信息（MVCC）以及索引相关的数据，相关的参数位于 [rocksdb.writecf] 项中； lock CF 存储的是锁信息，系统使用默认参数。  Raft RocksDB 实例存储 Raft log。 default CF 主要存储的是 raft log，与其对应的参数位于 [raftdb.defaultcf] 项中。  每个 CF 都有单独的 block-cache，用于缓存数据块，加速 RocksDB 的读取速度，block-cache 的大小通过参数 block-cache-size 控制，block-cache-size 越大，能够缓存的热点数据越多，对读取操作越有利，同时占用的系统内存也会越多。每个 CF 有各自的 write-buffer，大小通过 write-buffer-size 控制。参数说明 # 日志级别，可选值为：trace，debug，info，warn，error，off log-level = &amp;#34;info&amp;#34; [server] # 监听地址 # addr = &amp;#34;127.0.0.1:20160&amp;#34; # 建议使用默认值 # notify-capacity = 40960 # messages-per-tick = 4096 # gRPC 线程池大小 # grpc-concurrency = 4 # TiKV 每个实例之间的 gRPC 连接数 # grpc-raft-conn-num = 10 # TiDB 过来的大部分读请求都会发送到 TiKV 的 coprocessor 进行处理，该参数用于设置 # coprocessor 线程的个数，如果业务是读请求比较多，增加 coprocessor 的线程数，但应比系统的 # CPU 核数小。例如：TiKV 所在的机器有 32 core，在重读的场景下甚至可以将该参数设置为 30。在没有 # 设置该参数的情况下，TiKV 会自动将该值设置为 CPU 总核数乘以 0.8。 # end-point-concurrency = 8 # 可以给 TiKV 实例打标签，用于副本的调度 # labels = {zone = &amp;#34;cn-east-1&amp;#34;, host = &amp;#34;118&amp;#34;, disk = &amp;#34;ssd&amp;#34;} [storage] # 数据目录 # data-dir = &amp;#34;/tmp/tikv/store&amp;#34; # 通常情况下使用默认值就可以了。在导数据的情况下建议将该参数设置为 1024000。 # scheduler-concurrency = 102400 # 该参数控制写入线程的个数，当写入操作比较频繁的时候，需要把该参数调大。使用 top -H -p tikv-pid # 发现名称为 sched-worker-pool 的线程都特别忙，这个时候就需要将 scheduler-worker-pool-size # 参数调大，增加写线程的个数。 # scheduler-worker-pool-size = 4 [pd] # pd 的地址 # endpoints = [&amp;#34;127.0.0.1:2379&amp;#34;,&amp;#34;127.0.0.2:2379&amp;#34;,&amp;#34;127.0.0.3:2379&amp;#34;] [metric] # 将 metrics 推送给 Prometheus pushgateway 的时间间隔 interval = &amp;#34;15s&amp;#34; # Prometheus pushgateway 的地址 address = &amp;#34;&amp;#34; job = &amp;#34;tikv&amp;#34; [raftstore] # 默认为 true，表示强制将数据刷到磁盘上。如果是非金融安全级别的业务场景，建议设置成 false， # 以便获得更高的性能。 sync-log = true # Raft RocksDB 目录。默认值是 [storage.data-dir] 的 raft 子目录。 # 如果机器上有多块磁盘，可以将 Raft RocksDB 的数据放在不同的盘上，提高 TiKV 的性能。 # raftdb-dir = &amp;#34;/tmp/tikv/store/raft&amp;#34; region-max-size = &amp;#34;384MB&amp;#34; # region 分裂阈值 region-split-size = &amp;#34;256MB&amp;#34; # 当 region 写入的数据量超过该阈值的时候，TiKV 会检查该 region 是否需要分裂。为了减少检查过程 # 中扫描数据的成本，数据过程中可以将该值设置为32MB，正常运行状态下使用默认值即可。 region-split-check-diff = &amp;#34;32MB&amp;#34; [rocksdb] # RocksDB 进行后台任务的最大线程数，后台任务包括 compaction 和 flush。具体 RocksDB 为什么需要进行 compaction， # 请参考 RocksDB 的相关资料。在写流量比较大的时候（例如导数据），建议开启更多的线程， # 但应小于 CPU 的核数。例如在导数据的时候，32 核 CPU 的机器，可以设置成 28。 # max-background-jobs = 8 # RocksDB 能够打开的最大文件句柄数。 # max-open-files = 40960 # RocksDB MANIFEST 文件的大小限制. # 更详细的信息请参考：https://github.com/facebook/rocksdb/wiki/MANIFEST max-manifest-file-size = &amp;#34;20MB&amp;#34; # RocksDB write-ahead logs 目录。如果机器上有两块盘，可以将 RocksDB 的数据和 WAL 日志放在 # 不同的盘上，提高 TiKV 的性能。 # wal-dir = &amp;#34;/tmp/tikv/store&amp;#34; # 下面两个参数用于怎样处理 RocksDB 归档 WAL。 # 更多详细信息请参考：https://github.com/facebook/rocksdb/wiki/How-to-persist-in-memory-RocksDB-database%3F # wal-ttl-seconds = 0 # wal-size-limit = 0 # RocksDB WAL 日志的最大总大小，通常情况下使用默认值就可以了。 # max-total-wal-size = &amp;#34;4GB&amp;#34; # 可以通过该参数打开或者关闭 RocksDB 的统计信息。 # enable-statistics = true # 开启 RocksDB compaction 过程中的预读功能，如果使用的是机械磁盘，建议该值至少为2MB。 # compaction-readahead-size = &amp;#34;2MB&amp;#34; [rocksdb.defaultcf] # 数据块大小。RocksDB 是按照 block 为单元对数据进行压缩的，同时 block 也是缓存在 block-cache # 中的最小单元（类似其他数据库的 page 概念）。 block-size = &amp;#34;64KB&amp;#34; # RocksDB 每一层数据的压缩方式，可选的值为：no,snappy,zlib,bzip2,lz4,lz4hc,zstd。 # no:no:lz4:lz4:lz4:zstd:zstd 表示 level0 和 level1 不压缩，level2 到 level4 采用 lz4 压缩算法, # level5 和 level6 采用 zstd 压缩算法,。 # no 表示没有压缩，lz4 是速度和压缩比较为中庸的压缩算法，zlib 的压缩比很高，对存储空间比较友 # 好，但是压缩速度比较慢，压缩的时候需要占用较多的 CPU 资源。不同的机器需要根据 CPU 以及 I/O 资 # 源情况来配置怎样的压缩方式。例如：如果采用的压缩方式为&amp;#34;no:no:lz4:lz4:lz4:zstd:zstd&amp;#34;，在大量 # 写入数据的情况下（导数据），发现系统的 I/O 压力很大（使用 iostat 发现 %util 持续 100% 或者使 # 用 top 命令发现 iowait 特别多），而 CPU 的资源还比较充裕，这个时候可以考虑将 level0 和 # level1 开启压缩，用 CPU 资源换取 I/O 资源。如果采用的压缩方式 # 为&amp;#34;no:no:lz4:lz4:lz4:zstd:zstd&amp;#34;，在大量写入数据的情况下，发现系统的 I/O 压力不大，但是 CPU # 资源已经吃光了，top -H 发现有大量的 bg 开头的线程（RocksDB 的 compaction 线程）在运行，这 # 个时候可以考虑用 I/O 资源换取 CPU 资源，将压缩方式改成&amp;#34;no:no:no:lz4:lz4:zstd:zstd&amp;#34;。总之，目 # 的是为了最大限度地利用系统的现有资源，使 TiKV 的性能在现有的资源情况下充分发挥。 compression-per-level = [&amp;#34;no&amp;#34;, &amp;#34;no&amp;#34;, &amp;#34;lz4&amp;#34;, &amp;#34;lz4&amp;#34;, &amp;#34;lz4&amp;#34;, &amp;#34;zstd&amp;#34;, &amp;#34;zstd&amp;#34;] # RocksDB memtable 的大小。 write-buffer-size = &amp;#34;128MB&amp;#34; # 最多允许几个 memtable 存在。写入到 RocksDB 的数据首先会记录到 WAL 日志里面，然后会插入到 # memtable 里面，当 memtable 的大小到达了 write-buffer-size 限定的大小的时候，当前的 # memtable 会变成只读的，然后生成一个新的 memtable 接收新的写入。只读的 memtable 会被 # RocksDB 的 flush 线程（max-background-flushes 参数能够控制 flush 线程的最大个数） # flush 到磁盘，成为 level0 的一个 sst 文件。当 flush 线程忙不过来，导致等待 flush 到磁盘的 # memtable 的数量到达 max-write-buffer-number 限定的个数的时候，RocksDB 会将新的写入 # stall 住，stall 是 RocksDB 的一种流控机制。在导数据的时候可以将 max-write-buffer-number # 的值设置的更大一点，例如 10。 max-write-buffer-number = 5 # 当 level0 的 sst 文件个数到达 level0-slowdown-writes-trigger 指定的限度的时候， # RocksDB 会尝试减慢写入的速度。因为 level0 的 sst 太多会导致 RocksDB 的读放大上升。 # level0-slowdown-writes-trigger 和 level0-stop-writes-trigger 是 RocksDB 进行流控的 # 另一个表现。当 level0 的 sst 的文件个数到达 4（默认值），level0 的 sst 文件会和 level1 中 # 有 overlap 的 sst 文件进行 compaction，缓解读放大的问题。 level0-slowdown-writes-trigger = 20 # 当 level0 的 sst 文件个数到达 level0-stop-writes-trigger 指定的限度的时候，RocksDB 会 # stall 住新的写入。 level0-stop-writes-trigger = 36 # 当 level1 的数据量大小达到 max-bytes-for-level-base 限定的值的时候，会触发 level1 的 # sst 和 level2 种有 overlap 的 sst 进行 compaction。 # 黄金定律：max-bytes-for-level-base 的设置的第一参考原则就是保证和 level0 的数据量大致相 # 等，这样能够减少不必要的 compaction。例如压缩方式为&amp;#34;no:no:lz4:lz4:lz4:lz4:lz4&amp;#34;，那么 # max-bytes-for-level-base 的值应该是 write-buffer-size 的大小乘以 4，因为 level0 和 # level1 都没有压缩，而且 level0 触发 compaction 的条件是 sst 的个数到达 4（默认值）。在 # level0 和 level1 都采取了压缩的情况下，就需要分析下 RocksDB 的日志，看一个 memtable 的压 # 缩成一个 sst 文件的大小大概是多少，例如 32MB，那么 max-bytes-for-level-base 的建议值就应 # 该是 32MB * 4 = 128MB。 max-bytes-for-level-base = &amp;#34;512MB&amp;#34; # sst 文件的大小。level0 的 sst 文件的大小受 write-buffer-size 和 level0 采用的压缩算法的 # 影响，target-file-size-base 参数用于控制 level1-level6 单个 sst 文件的大小。 target-file-size-base = &amp;#34;32MB&amp;#34; # 在不配置该参数的情况下，TiKV 会将该值设置为系统总内存量的 40%。如果需要在单个物理机上部署多个 # TiKV 节点，需要显式配置该参数，否则 TiKV 容易出现 OOM 的问题。 # block-cache-size = &amp;#34;1GB&amp;#34; [rocksdb.writecf] # 保持和 rocksdb.defaultcf.compression-per-level 一致。 compression-per-level = [&amp;#34;no&amp;#34;, &amp;#34;no&amp;#34;, &amp;#34;lz4&amp;#34;, &amp;#34;lz4&amp;#34;, &amp;#34;lz4&amp;#34;, &amp;#34;zstd&amp;#34;, &amp;#34;zstd&amp;#34;] # 保持和 rocksdb.defaultcf.write-buffer-size 一致。 write-buffer-size = &amp;#34;128MB&amp;#34; max-write-buffer-number = 5 min-write-buffer-number-to-merge = 1 # 保持和 rocksdb.defaultcf.max-bytes-for-level-base 一致。 max-bytes-for-level-base = &amp;#34;512MB&amp;#34; target-file-size-base = &amp;#34;32MB&amp;#34; # 在不配置该参数的情况下，TiKV 会将该值设置为系统总内存量的 15%。如果需要在单个物理机上部署多个 # TiKV 节点，需要显式配置该参数。版本信息（MVCC）相关的数据以及索引相关的数据都记录在 write 这 # 个 cf 里面，如果业务的场景下单表索引较多，可以将该参数设置的更大一点。 # block-cache-size = &amp;#34;256MB&amp;#34; [raftdb] # RaftDB 能够打开的最大文件句柄数。 # max-open-files = 40960 # 可以通过该参数打开或者关闭 RaftDB 的统计信息。 # enable-statistics = true # 开启 RaftDB compaction 过程中的预读功能，如果使用的是机械磁盘，建议该值至少为2MB。 # compaction-readahead-size = &amp;#34;2MB&amp;#34; [raftdb.defaultcf] # 保持和 rocksdb.defaultcf.compression-per-level 一致。 compression-per-level = [&amp;#34;no&amp;#34;, &amp;#34;no&amp;#34;, &amp;#34;lz4&amp;#34;, &amp;#34;lz4&amp;#34;, &amp;#34;lz4&amp;#34;, &amp;#34;zstd&amp;#34;, &amp;#34;zstd&amp;#34;] # 保持和 rocksdb.defaultcf.write-buffer-size 一致。 write-buffer-size = &amp;#34;128MB&amp;#34; max-write-buffer-number = 5 min-write-buffer-number-to-merge = 1 # 保持和 rocksdb.defaultcf.max-bytes-for-level-base 一致。 max-bytes-for-level-base = &amp;#34;512MB&amp;#34; target-file-size-base = &amp;#34;32MB&amp;#34; # 通常配置在 256MB 到 2GB 之间，通常情况下使用默认值就可以了，但如果系统资源比较充足可以适当调大点。 block-cache-size = &amp;#34;256MB&amp;#34; TiKV 内存使用情况 除了以上列出的 block-cache 以及 write-buffer 会占用系统内存外： 需预留一些内存作为系统的 page cache TiKV 在处理大的查询的时候（例如 select * from ...）会读取数据然后在内存中生成对应的数据结构返回给 TiDB，这个过程中 TiKV 会占用一部分内存  TiKV 机器配置推荐  生产环境中，不建议将 TiKV 部署在 CPU 核数小于 8 或内存低于 32GB 的机器上 如果对写入吞吐要求比较高，建议使用吞吐能力比较好的磁盘 如果对读写的延迟要求非常高，建议使 …"},
		{"url": "https://pingcap.com/docs/tispark/tispark-quick-start-guide/",
		"title": "TiSpark Quick Start Guide", 
		"content": " Quick Start Guide for the TiDB Connector for Spark To make it easy to try the TiDB Connector for Spark, TiDB cluster integrates Spark, TiSpark jar package and TiSpark sample data by default, in both the Pre-GA and master versions installed using TiDB-Ansible.Deployment information  Spark is deployed by default in the spark folder in the TiDB instance deployment directory. The TiSpark jar package is deployed by default in the jars folder in the Spark deployment directory.spark/jars/tispark-0.1.0-beta-SNAPSHOT-jar-with-dependencies.jar TiSpark sample data and import scripts are deployed by default in the TiDB-Ansible directory.tidb-ansible/resources/bin/tispark-sample-data  Prepare the environment Install JDK on the TiDB instance Download the latest version of JDK 1.8 from Oracle JDK official download page. The version used in the following example is jdk-8u141-linux-x64.tar.gz.Extract the package and set the environment variables based on your JDK deployment directory.Edit the ~/.bashrc file. For example:export JAVA_HOME=/home/pingcap/jdk1.8.0_144 export PATH=$JAVA_HOME/bin:$PATH Verify the validity of JDK:$ java -version java version &amp;#34;1.8.0_144&amp;#34; Java(TM) SE Runtime Environment (build 1.8.0_144-b01) Java HotSpot(TM) 64-Bit Server VM (build 25.144-b01, mixed mode) Import the sample data Assume that the TiDB cluster is started. The service IP of one TiDB instance is 192.168.0.2, the port is 4000, the user name is root, and the password is null.cd tidb-ansible/resources/bin/tispark-sample-data Edit the TiDB login information in sample_data.sh. For example:mysql -h 192.168.0.2 -P 4000 -u root &amp;lt; dss.ddl Run the script:./sample_data.sh  Note: You need to install the MySQL client on the machine that runs the script. If you are a CentOS user, you can install it through the command yum -y install mysql. Log into TiDB and verify that the TPCH_001 database and the following tables are included.$ mysql -uroot -P4000 -h192.168.0.2 MySQL [(none)]&amp;gt; show databases; +--------------------+ | Database | +--------------------+ | INFORMATION_SCHEMA | | PERFORMANCE_SCHEMA | | TPCH_001 | | mysql | | test | +--------------------+ 5 rows in set (0.00 sec) MySQL [(none)]&amp;gt; use TPCH_001 Reading table information for completion of table and column names You can turn off this feature to get a quicker startup with -A Database changed MySQL [TPCH_001]&amp;gt; show tables; +--------------------+ | Tables_in_TPCH_001 | +--------------------+ | CUSTOMER | | LINEITEM | | NATION | | ORDERS | | PART | | PARTSUPP | | REGION | | SUPPLIER | +--------------------+ 8 rows in set (0.00 sec) Use example Assume that the IP of your PD node is 192.168.0.2, and the port is 2379.First start the spark-shell in the spark deployment directory:$ cd spark $ bin/spark-shellimport org.apache.spark.sql.TiContext val ti = new TiContext(spark) // Mapping all TiDB tables from `TPCH_001` database as Spark SQL tables ti.tidbMapDatabase(&amp;#34;TPCH_001&amp;#34;) Then you can call Spark SQL directly:scala&amp;gt; spark.sql(&amp;#34;select count(*) from lineitem&amp;#34;).show The result is:+--------+ |count(1)| +--------+ | 60175| +--------+ Now run a more complex Spark SQL:scala&amp;gt; spark.sql( &amp;#34;&amp;#34;&amp;#34;select | l_returnflag, | l_linestatus, | sum(l_quantity) as sum_qty, | sum(l_extendedprice) as sum_base_price, | sum(l_extendedprice * (1 - l_discount)) as sum_disc_price, | sum(l_extendedprice * (1 - l_discount) * (1 + l_tax)) as sum_charge, | avg(l_quantity) as avg_qty, | avg(l_extendedprice) as avg_price, | avg(l_discount) as avg_disc, | count(*) as count_order |from | lineitem |where | l_shipdate &amp;lt;= date &amp;#39;1998-12-01&amp;#39; - interval &amp;#39;90&amp;#39; day |group by | l_returnflag, | l_linestatus |order by | l_returnflag, | l_linestatus &amp;#34;&amp;#34;&amp;#34;.stripMargin).show The result is:+------------+------------+---------+--------------+--------------+ |l_returnflag|l_linestatus| sum_qty|sum_base_price|sum_disc_price| +------------+------------+---------+--------------+--------------+ | A| F|380456.00| 532348211.65|505822441.4861| | N| F| 8971.00| 12384801.37| 11798257.2080| | N| O|742802.00| 1041502841.45|989737518.6346| | R| F|381449.00| 534594445.35|507996454.4067| +------------+------------+---------+--------------+--------------+ (Continued) -----------------+---------+------------+--------+-----------+ sum_charge| avg_qty| avg_price|avg_disc|count_order| -----------------+---------+------------+--------+-----------+ 526165934.000839|25.575155|35785.709307|0.050081| 14876| 12282485.056933|25.778736|35588.509684|0.047759| 348| 1029418531.523350|25.454988|35691.129209|0.049931| 29181| 528524219.358903|25.597168|35874.006533|0.049828| 14902| -----------------+---------+------------+--------+-----------+ See more examples."},
		{"url": "https://pingcap.com/docs-cn/tispark/tispark-quick-start-guide/",
		"title": "TiSpark 快速入门指南", 
		"content": " TiSpark 快速入门指南 为了让大家快速体验 TiSpark, 通过 TiDB-Ansible 安装的 Pre-GA 或 master 版本 TiDB 集群中默认已集成 Spark、TiSpark jar 包及 TiSpark sample data。部署信息  Spark 默认部署在 TiDB 实例部署目录下 spark 目录中 TiSpark jar 包默认部署在 Spark 部署目录 jars 文件夹下：spark/jars/tispark-0.1.0-beta-SNAPSHOT-jar-with-dependencies.jar TiSpark sample data 及导入脚本默认部署在 TiDB-Ansible 目录下：tidb-ansible/resources/bin/tispark-sample-data  环境准备 在 TiDB 实例上安装 JDK 在 Oracle JDK 官方下载页面  下载 JDK 1.8 当前最新版，本示例中下载的版本为 jdk-8u141-linux-x64.tar.gz。解压并根据您的 JDK 部署目录设置环境变量， 编辑 ~/.bashrc 文件，比如：export JAVA_HOME=/home/pingcap/jdk1.8.0_144 export PATH=$JAVA_HOME/bin:$PATH 验证 JDK 有效性：$ java -version java version &amp;#34;1.8.0_144&amp;#34; Java(TM) SE Runtime Environment (build 1.8.0_144-b01) Java HotSpot(TM) 64-Bit Server VM (build 25.144-b01, mixed mode) 导入样例数据 假设 TiDB 集群已启动，其中一台 TiDB 实例服务 IP 为 192.168.0.2，端口为 4000，用户名为 root, 密码为空。cd tidb-ansible/resources/bin/tispark-sample-data 修改 sample_data.sh 中 TiDB 登录信息，比如：mysql -h 192.168.0.2 -P 4000 -u root &amp;lt; dss.ddl 执行脚本./sample_data.sh  执行脚本的机器上需要安装 MySQL client，CentOS 用户可通过 yum -y install mysql来安装。 登录 TiDB 并验证数据包含 TPCH_001 库及以下表：$ mysql -uroot -P4000 -h192.168.0.2 MySQL [(none)]&amp;gt; show databases; +--------------------+ | Database | +--------------------+ | INFORMATION_SCHEMA | | PERFORMANCE_SCHEMA | | TPCH_001 | | mysql | | test | +--------------------+ 5 rows in set (0.00 sec) MySQL [(none)]&amp;gt; use TPCH_001 Reading table information for completion of table and column names You can turn off this feature to get a quicker startup with -A Database changed MySQL [TPCH_001]&amp;gt; show tables; +--------------------+ | Tables_in_TPCH_001 | +--------------------+ | CUSTOMER | | LINEITEM | | NATION | | ORDERS | | PART | | PARTSUPP | | REGION | | SUPPLIER | +--------------------+ 8 rows in set (0.00 sec) 使用范例 假设您的 PD 节点 IP 为 192.168.0.2，端口 2379, 先进入 spark 部署目录启动 spark-shell:$ cd spark $ bin/spark-shellscala&amp;gt; import org.apache.spark.sql.TiContext scala&amp;gt; val ti = new TiContext(spark) // Mapping all TiDB tables from `TPCH_001` database as Spark SQL tables scala&amp;gt; ti.tidbMapDatabase(&amp;#34;TPCH_001&amp;#34;) 之后您可以直接调用 Spark SQL:scala&amp;gt; spark.sql(&amp;#34;select count(*) from lineitem&amp;#34;).show 结果为+--------+ |count(1)| +--------+ | 60175| +--------+ 下面执行另一个复杂一点的 Spark SQL:scala&amp;gt; spark.sql( &amp;#34;&amp;#34;&amp;#34;select | l_returnflag, | l_linestatus, | sum(l_quantity) as sum_qty, | sum(l_extendedprice) as sum_base_price, | sum(l_extendedprice * (1 - l_discount)) as sum_disc_price, | sum(l_extendedprice * (1 - l_discount) * (1 + l_tax)) as sum_charge, | avg(l_quantity) as avg_qty, | avg(l_extendedprice) as avg_price, | avg(l_discount) as avg_disc, | count(*) as count_order |from | lineitem |where | l_shipdate &amp;lt;= date &amp;#39;1998-12-01&amp;#39; - interval &amp;#39;90&amp;#39; day |group by | l_returnflag, | l_linestatus |order by | l_returnflag, | l_linestatus &amp;#34;&amp;#34;&amp;#34;.stripMargin).show 结果为：+------------+------------+---------+--------------+--------------+ |l_returnflag|l_linestatus| sum_qty|sum_base_price|sum_disc_price| +------------+------------+---------+--------------+--------------+ | A| F|380456.00| 532348211.65|505822441.4861| | N| F| 8971.00| 12384801.37| 11798257.2080| | N| O|742802.00| 1041502841.45|989737518.6346| | R| F|381449.00| 534594445.35|507996454.4067| +------------+------------+---------+--------------+--------------+ (续) -----------------+---------+------------+--------+-----------+ sum_charge| avg_qty| avg_price|avg_disc|count_order| -----------------+---------+------------+--------+-----------+ 526165934.000839|25.575155|35785.709307|0.050081| 14876| 12282485.056933|25.778736|35588.509684|0.047759| 348| 1029418531.523350|25.454988|35691.129209|0.049931| 29181| 528524219.358903|25.597168|35874.006533|0.049828| 14902| -----------------+---------+------------+--------+-----------+ 更多样例请参考 https://github.com/ilovesoup/tpch/tree/master/sparksql 。"},
		{"url": "https://pingcap.com/docs-cn/tispark/tispark-user-guide/",
		"title": "TiSpark 用户指南", 
		"content": " TiSpark 用户指南 TiSpark 是 PingCAP 为解决用户复杂 OLAP 需求而推出的产品。借助 Spark 平台，同时融合 TiKV 分布式集群的优势，和 TiDB 一起为用户一站式解决 HTAP （Hybrid Transactional/Analytical Processing）需求。 TiSpark 依赖于 TiKV 集群和 Placement Driver(PD)。当然，TiSpark 也需要您搭建一个 Spark 集群。本文简单介绍如何部署和使用 TiSpark。本文假设你对 Spark 有基本认知。你可以参阅 Apache Spark 官网 了解 Spark 相关信息。概述 TiSpark 是将 Spark SQL 直接运行在分布式存储引擎 TiKV 上的 OLAP 解决方案。其架构图如下： TiSpark 深度整合了 Spark Catalyst 引擎, 可以对计算提供精确的控制，使 Spark 能够高效的读取 TiKV 中的数据，提供索引支持以实现高速的点查。 通过多种计算下推减少 Spark SQL 需要处理的数据大小，以加速查询；利用 TiDB 的内建的统计信息选择更优的查询计划。 从数据集群的角度看，TiSpark + TiDB 可以让用户无需进行脆弱和难以维护的 ETL，直接在同一个平台进行事务和分析两种工作，简化了系统架构和运维。 除此之外，用户借助 TiSpark 项目可以在 TiDB 上使用 Spark 生态圈提供的多种工具进行数据处理。例如使用 TiSpark 进行数据分析和 ETL；使用 TiKV 作为机器学习的数据源；借助调度系统产生定时报表等等。  环境准备 现有 TiSpark 版本支持 Spark 2.1。对于 Spark 2.0 及 Spark 2.2 还没有经过良好的测试验证，对于更低版本暂时无法支持。TiSpark 需要 JDK 1.8+ 以及 Scala 2.11（Spark2.0+ 默认 Scala 版本）。TiSpark 可以在 YARN，Mesos，Standalone 等任意 Spark 模式下运行。推荐配置 部署 TiKV 和 TiSpark 集群 TiKV 集群部署配置 对于 TiKV 和 TiSpark 分开部署的场景，可以参考如下建议： 硬件配置建议普通场景可以参考 TiDB 和 TiKV 硬件配置建议，但是如果是偏重分析的场景，可以将 TiKV 节点增加到至少 64G 内存。 TiKV 参数建议[server] end-point-concurrency = 8 # 如果使用场景偏向分析，则可以考虑扩大这个参数 [raftstore] sync-log = false [rocksdb] max-background-compactions = 6 max-background-flushes = 2 [rocksdb.defaultcf] block-cache-size = &amp;#34;10GB&amp;#34; [rocksdb.writecf] block-cache-size = &amp;#34;4GB&amp;#34; [rocksdb.raftcf] block-cache-size = &amp;#34;1GB&amp;#34; [rocksdb.lockcf] block-cache-size = &amp;#34;1GB&amp;#34; [storage] scheduler-worker-pool-size = 4  Spark / TiSpark 集群独立部署配置 关于 Spark 的详细硬件推荐配置请参考官网，如下是 TiSpark 所需环境的简单描述：Spark 推荐 32G 内存以上配额。请在配置中预留 25% 的内存给操作系统。Spark 推荐每台计算节点配备 CPU 累计 8 到 16 核以上。你可以初始设定分配所有 CPU 核给 Spark。Spark 的具体配置方式也请参考官方说明。下面给出的是根据 spark-env.sh 配置的范例：SPARK_EXECUTOR_MEMORY=32g SPARK_WORKER_MEMORY=32g SPARK_WORKER_CORES=8 TiSpark 与 TiKV 集群混合部署配置 对于 TiKV、TiSpark 混合部署场景，请在原有 TiKV 预留资源之外累加 Spark 所需部分并分配 25% 的内存作为系统本身占用。部署 TiSpark TiSpark 的 jar 包可以在这里下载。已有 Spark 集群的部署方式 如果在已有 Spark 集群上运行 TiSpark，您无需重启集群。您可以使用 Spark 的 &amp;ndash;jars 参数将 TiSpark 作为依赖引入:spark-shell --jars $PATH/tispark-0.1.0.jar 如果想将 TiSpark 作为默认组件部署，只需要将 TiSpark 的 jar 包放进 Spark 集群每个节点的 jars 路径并重启 Spark 集群：${SPARK_INSTALL_PATH}/jars 这样无论您是使用 Spark-Submit 还是 Spark-Shell 都可以直接使用 TiSpark。没有 Spark 集群的部署方式 如果您没有使用中的 Spark 集群，我们推荐 Saprk Standalone 方式部署。我们在这里简单介绍下 Standalone 部署方式。如果遇到问题，可以去官网寻找帮助；也欢迎在我们的 GitHub 上提 issue。下载安装包并安装 你可以在这里下载 Apache Spark。对于 Standalone 模式且无需 Hadoop 支持，请选择 Spark 2.1.x 且带有 Hadoop 依赖的 Pre-build with Apache Hadoop 2.x 任意版本。如您有需要配合使用的 Hadoop 集群，请选择对应的 Hadoop 版本号。您也可以选择从源代码自行构建以配合官方 Hadoop 2.6 之前的版本。请注意目前 TiSpark 仅支持 Spark 2.1.x 版本。假设您已经有了 Spark 二进制文件，并且当前 PATH 为 SPARKPATH。请将 TiSpark jar 包拷贝到 ${SPARKPATH}/jars 目录下。启动 Master 在选中的 Spark Master 节点执行如下命令：cd $SPARKPATH ./sbin/start-master.sh 在这步完成以后，屏幕上会打印出一个 log 文件。检查 log 文件确认 Spark-Master 是否启动成功。您可以打开 http://spark-master-hostname:8080 查看集群信息（如果你没有改动 Spark-Master 默认 Port Numebr）。 在启动 Spark-Slave 的时候，您也可以通过这个面板来确认 Slave 是否已经加入集群。启动 Slave 类似地，可以用如下命令启动 Spark-Slave节点：./sbin/start-slave.sh spark://spark-master-hostname:7077 命令返回以后，您就可以通过刚才的面板查看这个 Slave 是否已经正确的加入了 Spark 集群。 在所有 Slave 节点重复刚才的命令。在确认所有的 Slave 都可以正确连接 Master，这样您就拥有了一个 Standalone 模式的 Spark 集群。一个使用范例 假设您已经按照上述步骤成功启动了 TiSpark 集群， 下面简单介绍如何使用 Spark SQL 来做 OLAP 分析。这里我们用名为 tpch 数据库中的 lineitem 表作为范例。假设你的 PD 节点位于 192.168.1.100，端口为 2379，在$SPARK_HOME/conf/spark-defaults.conf加入spark.tispark.pd.addresses 192.168.1.100:2379 然后在 Spark-Shell 里输入下面的命令：import org.apache.spark.sql.TiContext val ti = new TiContext(spark) ti.tidbMapDatabase(&amp;#34;tpch&amp;#34;) 之后您可以直接调用 Spark SQL:spark.sql(&amp;#34;select count(*) from lineitem&amp;#34;).show 结果为：+-------------+ | Count (1) | +-------------+ | 600000000 | +-------------+ TiSpark FAQ  Q. 是独立部署还是和现有 Spark／Hadoop 集群共用资源？A. 您可以利用现有 Spark 集群无需单独部署，但是如果现有集群繁忙，TiSpark 将无法达到理想速度。 Q. 是否可以和 TiKV 混合部署？A. 如果 TiDB 以及 TiKV 负载较高且运行关键的线上任务，请考虑单独部署 TiSpark；并且考虑使用不同的网卡保证 OLTP 的网络资源不被侵占而影响线上业务。如果线上业务要求不高或者机器负载不大，可以考虑与 TiKV 混合部署。  "},
		{"url": "https://pingcap.com/docs/sql/time-zone/",
		"title": "Time Zone", 
		"content": " Time Zone The time zone in TiDB is decided by the global time_zone system variable and the session time_zone system variable. The initial value for time_zone is &amp;lsquo;SYSTEM&amp;rsquo;, which indicates that the server time zone is the same as the system time zone.You can use the following statement to set the global server time_zone value at runtime:mysql&amp;gt; SET GLOBAL time_zone = timezone; Each client has its own time zone setting, given by the session time_zone variable. Initially, the session variable takes its value from the global time_zone variable, but the client can change its own time zone with this statement:mysql&amp;gt; SET time_zone = timezone; You can use the following statment to view the current values of the global and client-specific time zones:mysql&amp;gt; SELECT @@global.time_zone, @@session.time_zone; To set the format of the value of the time_zone: The value &amp;lsquo;SYSTEM&amp;rsquo; indicates that the time zone should be the same as the system time zone. The value can be given as a string indicating an offset from UTC, such as &amp;lsquo;+10:00&amp;rsquo; or &amp;lsquo;-6:00&amp;rsquo;. The value can be given as a named time zone, such as &amp;lsquo;Europe/Helsinki&amp;rsquo;, &amp;lsquo;US/Eastern&amp;rsquo;, or &amp;lsquo;MET&amp;rsquo;.  The current session time zone setting affects the display and storage of time values that are zone-sensitive. This includes the values displayed by functions such as NOW() or CURTIME(), Note: Only the values of the Timestamp data type is affected by time zone. This is because the Timestamp data type uses the literal value + time zone information. Other data types, such as Datetime/Date/Time, do not have time zone information, thus their values are not affected by the changes of time zone. mysql&amp;gt; create table t (ts timestamp, dt datetime); Query OK, 0 rows affected (0.02 sec) mysql&amp;gt; set @@time_zone = &amp;#39;UTC&amp;#39;; Query OK, 0 rows affected (0.00 sec) mysql&amp;gt; insert into t values (&amp;#39;2017-09-30 11:11:11&amp;#39;, &amp;#39;2017-09-30 11:11:11&amp;#39;); Query OK, 1 row affected (0.00 sec) mysql&amp;gt; set @@time_zone = &amp;#39;+8:00&amp;#39;; Query OK, 0 rows affected (0.00 sec) mysql&amp;gt; select * from t; +---------------------|---------------------+ | ts | dt | +---------------------|---------------------+ | 2017-09-30 19:11:11 | 2017-09-30 11:11:11 | +---------------------|---------------------+ 1 row in set (0.00 sec) In this example, no matter how you adjust the value of the time zone, the value of the Datetime data type is not affected. But the displayed value of the Timestamp data type changes if the time zone information changes. In fact, the value that is stored in the storage does not change, it&amp;rsquo;s just displayed differently according to different time zone setting. Note: Time zone is involved during the conversion of the value of Timestamp and Datetime, which is handled based on the current time_zone of the session. For data migration, you need to pay special attention to the time zone setting of the master database and the slave database.   "},
		{"url": "https://pingcap.com/docs/sql/transaction/",
		"title": "Transactions", 
		"content": " Transactions TiDB supports distributed transactions. The statements that relate to transactions include the Autocommit variable, START TRANSACTION/BEGIN, COMMIT and ROLLBACK.Autocommit Syntax:SET autocommit = {0 | 1} If you set the value of autocommit to 1, the status of the current Session is autocommit. If you set the value of autocommit to 0, the status of the current Session is non-autocommit. The value of autocommit is 1 by default.In the autocommit status, the updates are automatically committed to the database after you run each statement. Otherwise, the updates are only committed when you run the COMMIT or BEGIN statement.Besides, autocommit is also a System Variable. You can update the current Session or the Global value using the following variable assignment statement:SET @@SESSION.autocommit = {0 | 1}; SET @@GLOBAL.autocommit = {0 | 1}; START TRANSACTION, BEGIN Syntax:BEGIN; START TRANSACTION; START TRANSACTION WITH CONSISTENT SNAPSHOT; The three statements above are all statements that transactions start with, through which you can explicitly start a new transaction. If at this time, the current Session is in the process of a transaction, a new transaction is started after the current transaction is committed.COMMIT Syntax:COMMIT; This statement is used to commit the current transaction, including all the updates between BEGIN and COMMIT.ROLLBACK Syntax:ROLLBACK; This statement is used to roll back the current transaction and cancels all the updates between BEGIN and COMMIT.Explicit and implicit transaction TiDB supports explicit transactions (BEGIN/COMMIT) and implicit transactions (SET autocommit = 1).If you set the value of autocommit to 1 and start a new transaction through BEGIN, the autocommit is disabled before COMMIT/ROLLBACK which makes the transaction becomes explicit.For DDL statements, the transaction is committed automatically and does not support rollback. If you run the DDL statement while the current Session is in the process of a transaction, the DDL statement is run after the current transaction is committed.Transaction isolation level TiDB uses SNAPSHOT ISOLATION by default. You can set the isolation level of the current Session to READ COMMITTED using the following statement:SET SESSION TRANSACTION ISOLATION LEVEL READ COMMITTED;"},
		{"url": "https://pingcap.com/docs-cn/op-guide/try-tidb/",
		"title": "Try TiDB", 
		"content": " Try TiDB TiDB 支持 SQL92 标准并兼容 MySQL 语法，目前已经实现了大多数常用的 MySQL 语法。用户可以直接使用现有的 MySQL 客户端连接。如果现有的业务已经基于 MySQL 开发，大多数情况不需要修改代码即可直接替换单机的 MySQL。创建数据库 使用 CREATE DATABASE 语句可完成对数据库的创建, 创建命令的格式如下:CREATE DATABASE 数据库名 [其他选项]; 例如我们需要创建一个名为 samp_db 的数据库, 在命令行下执行以下命令:CREATE DATABASE IF NOT EXISTS samp_db; 查看 TiDB 中的所有数据库：SHOW DATABASES; 删除数据库：DROP DATABASE samp_db; 创建表 使用 CREATE TABLE + 表名 + 列名 + 数据类型 + 约束。具体例子如下：CREATE TABLE person ( number INT(11), name VARCHAR(255), birthday DATE ); 如果表已存在，则使用关键词 IF NOT EXISTS 可以防止发生错误。CREATE TABLE IF NOT EXISTS person ( number INT(11), name VARCHAR(255), birthday DATE ); 查看建表语句SHOW CREATE table person; 查看表所有的列：SHOW FULL COLUMNS FROM person; 删除表DROP TABLE person; 或者DROP TABLE IF EXISTS person; 查看 samp_db 中的所有表：SHOW TABLES FROM samp_db; 创建索引 对于值不唯一的列，可以使用 CREATE INDEX 和 ALTER TABLE：CREATE INDEX person_num ON person (number ); ALTER TABLE person ADD INDEX person_num (number )； 对于值唯一的列可以创建唯一索引：CREATE UNIQUE INDEX person_num ON person (number);  ALTER TABLE person ADD UNIQUE person_num ON (number);可利用 ALTER TABLE 或 DROP INDEX 语句来删除索引。类似于 CREATE INDEX 语句，DROP INDEX 可以在 ALTER TABLE 内部作为一条语句处理，语法如下。DROP INDEX person_num ON person; ALTER TABLE person DROP INDEX person_num ; 查看表内所有索引： SHOW INDEX FROM person ;增删改查数据 利用 INSERT 插入数据INSERT INTO person VALUES(&amp;#34;1&amp;#34;,&amp;#34;tom&amp;#34;,&amp;#34;20170912&amp;#34;); 利用 SELECT 检索数据SELECT * FROM person; +--------+------+------------+ | number | name | birthday | +--------+------+------------+ | 1 | tom | 2017-09-12 | +--------+------+------------+ 利用 UPDATE 修改表内数据：UPDATE person SET birthday=&amp;#39;20171010&amp;#39; WHERE name=&amp;#39;tom&amp;#39;; SELECT * FROM person; +--------+------+------------+ | number | name | birthday | +--------+------+------------+ | 1 | tom | 2017-10-10 | +--------+------+------------+ 利用 DELETE 删除表内数据：DELETE FROM person WHERE number=1; SELECT * FROM person; Empty set (0.00 sec) 创建用户 使用 CREATE USER 语句创建一个只在本地登录的用户 tiuser，密码为 123456：CREATE USER &amp;#39;tiuser&amp;#39;@&amp;#39;localhost&amp;#39; IDENTIFIED BY &amp;#39;123456&amp;#39;; 授权用户可查询 samp_db 库下的表：GRANT SELECT ON samp_db .* TO &amp;#39;tiuser&amp;#39;@&amp;#39;localhost&amp;#39;; 查询 tiuser 用户的授权： SHOW GRANTS FOR tiuser@localhost;删除用户DROP USER &amp;#39;tiuser&amp;#39;@&amp;#39;localhost&amp;#39;;"},
		{"url": "https://pingcap.com/docs/op-guide/tune-tikv/",
		"title": "Tune TiKV Performance", 
		"content": " Tune TiKV Performance This document describes how to tune the TiKV parameters for optimal performance.TiKV uses RocksDB for persistent storage at the bottom level of the TiKV architecture. Therefore, many of the performance parameters are related to RocksDB. TiKV uses two RocksDB instances: the default RocksDB instance stores KV data, the Raft RocksDB instance (RaftDB) stores Raft logs.TiKV implements Column Families (CF) from RocksDB.The default RocksDB instance stores KV data in the default, write and lock CFs. + The default CF stores the actual data. The corresponding parameters are in [rocksdb.defaultcf]. + The write CF stores the version information in Multi-Version Concurrency Control (MVCC) and index-related data. The corresponding parameters are in [rocksdb.writecf]. + The lock CF stores the lock information. The system uses the default parameters.The Raft RocksDB (RaftDB) instance stores Raft logs. + The default CF stores the Raft log. The corresponding parameters are in [raftdb.defaultcf].Each CF has a separate block cache to cache data blocks to accelerate the data reading speed in RocksDB. You can configure the size of the block cache by setting the block-cache-size parameter. The bigger the block-cache-size, the more hot data can be cached, and the easier to read data, in the meantime, the more system memory will be occupied.Each CF also has a separate write buffer. You can configure the size by setting the write-buffer-size parameter.Parameter specification # Log level: trace, debug, info, warn, error, off. log-level = &amp;#34;info&amp;#34; [server] # Set listening address # addr = &amp;#34;127.0.0.1:20160&amp;#34; # It is recommended to use the default value. # notify-capacity = 40960 # messages-per-tick = 4096 # Size of thread pool for gRPC # grpc-concurrency = 4 # The number of gRPC connections between each TiKV instance # grpc-raft-conn-num = 10 # Most read requests from TiDB are sent to the coprocessor of TiKV. This parameter is used to set the number of threads # of the coprocessor. If many read requests exist, add the number of threads and keep the number within that of the # system CPU cores. For example, for a 32-core machine deployed with TiKV, you can even set this parameter to 30 in # repeatable read scenarios. If this parameter is not set, TiKV automatically sets it to CPU cores * 0.8. # end-point-concurrency = 8 # Tag the TiKV instances to schedule replicas. # labels = {zone = &amp;#34;cn-east-1&amp;#34;, host = &amp;#34;118&amp;#34;, disk = &amp;#34;ssd&amp;#34;} [storage] # The data directory # data-dir = &amp;#34;/tmp/tikv/store&amp;#34; # In most cases, you can use the default value. When importing data, it is recommended to set the parameter to 1024000. # scheduler-concurrency = 102400 # This parameter controls the number of write threads. When write operations occur frequently, set this parameter value # higher. Run `top -H -p tikv-pid` and if the threads named `sched-worker-pool` are busy, set the value of parameter # `scheduler-worker-pool-size` higher and increase the number of write threads. # scheduler-worker-pool-size = 4 [pd] # PD address # endpoints = [&amp;#34;127.0.0.1:2379&amp;#34;,&amp;#34;127.0.0.2:2379&amp;#34;,&amp;#34;127.0.0.3:2379&amp;#34;] [metric] # The interval of pushing metrics to Prometheus pushgateway interval = &amp;#34;15s&amp;#34; # Prometheus pushgateway adress address = &amp;#34;&amp;#34; job = &amp;#34;tikv&amp;#34; [raftstore] # The default value is true，which means writing the data on the disk compulsorily. If it is not in a business scenario # of the financial security level, it is recommended to set the value to false to achieve better performance. sync-log = true # Raft RocksDB directory. The default value is Raft subdirectory of [storage.data-dir]. # If there are multiple disks on the machine, store the data of Raft RocksDB on different disks to improve TiKV performance. # raftdb-dir = &amp;#34;/tmp/tikv/store/raft&amp;#34; region-max-size = &amp;#34;384MB&amp;#34; # The threshold value of Region split region-split-size = &amp;#34;256MB&amp;#34; # When the data size in a Region is larger than the threshold value, TiKV checks whether this Region needs split. # To reduce the costs of scanning data in the checking process，set the value to 32MB during checking and set it to # the default value in normal operation. region-split-check-diff = &amp;#34;32MB&amp;#34; [rocksdb] # The maximum number of threads of RocksDB background tasks. The background tasks include compaction and flush. # For detailed information why RocksDB needs to implement compaction, see RocksDB-related materials. When write # traffic (like the importing data size) is big，it is recommended to enable more threads. But set the number of the enabled # threads smaller than that of CPU cores. For example, when importing data, for a machine with a 32-core CPU, # set the value to 28. # max-background-jobs = 8 # The maximum number of file handles RocksDB can open # max-open-files = 40960 # The file size limit of RocksDB MANIFEST. For more details, see https://github.com/facebook/rocksdb/wiki/MANIFEST max-manifest-file-size = &amp;#34;20MB&amp;#34; # The directory of RocksDB write-ahead logs. If there are two disks on the machine, store the RocksDB data and WAL logs # on different disks to improve TiKV performance. # wal-dir = &amp;#34;/tmp/tikv/store&amp;#34; # Use the following two parameters to deal with RocksDB archiving WAL. # For more details, see https://github.com/facebook/rocksdb/wiki/How-to-persist-in-memory-RocksDB-database%3F # wal-ttl-seconds = 0 # wal-size-limit = 0 # In most cases, set the maximum total size of RocksDB WAL logs to the default value. # max-total-wal-size = &amp;#34;4GB&amp;#34; # Use this parameter to enable or disable the statistics of RocksDB. # enable-statistics = true # Use this parameter to enable the readahead feature during RocksDB compaction. If you are using mechanical disks, it is recommended to set the value to 2MB at least. # compaction-readahead-size = &amp;#34;2MB&amp;#34; [rocksdb.defaultcf] # The data block size. RocksDB compresses data based on the unit of block. # Similar to page in other databases, block is the smallest unit cached in block-cache. block-size = &amp;#34;64KB&amp;#34; # The compaction mode of each layer of RocksDB data. The optional values include no, snappy, zlib, # bzip2, lz4, lz4hc, and zstd. # &amp;#34;no:no:lz4:lz4:lz4:zstd:zstd&amp;#34; indicates there is no compaction of level0 and level1; lz4 compaction algorithm is used # from level2 to level4; zstd compaction algorithm is used from level5 to level6. # &amp;#34;no&amp;#34; means no compaction. &amp;#34;lz4&amp;#34; is a compaction algorithm with moderate speed and compaction ratio. The # compaction ratio of zlib is high. It is friendly to the storage space, but its compaction speed is slow. This # compaction occupies many CPU resources. Different machines deploy compaction modes according to CPU and I/O resources. # For example, if you use the compaction mode of &amp;#34;no:no:lz4:lz4:lz4:zstd:zstd&amp;#34; and find much I/O pressure of the # system (run the iostat command to find %util lasts 100%, or run the top command to find many iowaits) when writing # (importing) a lot of data while the CPU resources are adequate, you can compress level0 and level1 and exchange CPU # resources for I/O resources. If you use the compaction mode of &amp;#34;no:no:lz4:lz4:lz4:zstd:zstd&amp;#34; and you find the I/O # pressure of the system is not big when writing a lot of data, but CPU resources are inadequate. Then run the top # command and choose the -H option. If you find a lot of bg threads (namely the compaction thread of RocksDB) are # running, you can exchange I/O resources for CPU resources and change the compaction mode to &amp;#34;no:no:no:lz4:lz4:zstd:zstd&amp;#34;. # In a word, it aims at making full use of the existing resources of the system and improving TiKV performance # in terms of the current resources. compression-per-level = [&amp;#34;no&amp;#34;, &amp;#34;no&amp;#34;, &amp;#34;lz4&amp;#34;, &amp;#34;lz4&amp;#34;, &amp;#34;lz4&amp;#34;, &amp;#34;zstd&amp;#34;, &amp;#34;zstd&amp;#34;] # The RocksDB memtable size write-buffer-size = &amp;#34;128MB&amp;#34; # The maximum …"},
		{"url": "https://pingcap.com/docs/sql/type-conversion-in-expression-evaluation/",
		"title": "Type Conversion in Expression Evaluation", 
		"content": " Type Conversion in Expression Evaluation TiDB behaves the same as MySQL: https://dev.mysql.com/doc/refman/5.7/en/type-conversion.html"},
		{"url": "https://pingcap.com/docs/sql/understanding-the-query-execution-plan/",
		"title": "Understand the Query Execution Plan", 
		"content": " Understand the Query Execution Plan Based on the details of your tables, the TiDB optimizer chooses the most efficient query execution plan, which consists of a series of operators. This document details the execution plan information returned by the EXPLAIN statement in TiDB.Optimize SQL statements using EXPLAIN The result of the EXPLAIN statement provides information about how TiDB executes SQL queries: EXPLAIN works together with SELECT, DELETE, INSERT, REPLACE, and UPDATE. When you run the EXPLAIN statement, TiDB returns the final physical execution plan which is optimized by the SQL statment of EXPLAIN. In other words, EXPLAIN displays the complete information about how TiDB executes the SQL statement, such as in which order, how tables are joined, and what the expression tree looks like. For more information, see EXPLAIN output format. TiDB dose not support EXPLAIN [options] FOR CONNECTION connection_id currently. We&amp;rsquo;ll do it in the future. For more information, see #4351.  The results of EXPLAIN shed light on how to index the data tables so that the execution plan can use the index to speed up the execution of SQL statements. You can also use EXPLAIN to check if the optimizer chooses the optimal order to join tables.EXPLAIN output format Currently, the EXPLAIN statement returns the following six columns: id, parent, children, task, operator info, and count. Each operator in the execution plan is described by the six properties. In the results returned by EXPLAIN, each row describes an operator. See the following table for details:   Property Name Description     id The id of an operator, to identify the uniqueness of an operator in the entire execution plan.   parent The parent of an operator. The current execution plan is like a tree structure composed of operators. The data flows from a child to its parent, and each operator has one and only one parent.   children the children and the data source of an operator   task the task that the current operator belongs to. The current execution plan contains two types of tasks: 1) the root task that runs on the TiDB server; 2) the cop task that runs concurrently on the TiKV server. The topological relations of the current execution plan in the task level is that a root task can be followed by many cop tasks. The root task uses the output of cop task as the input. The cop task executes the tasks that TiDB pushes to TiKV. Each cop task scatters in the TiKV cluster and is executed by multiple processes.   operator info The details about each operator. The information of each operator differs from others, see Operator Info.   count to predict the number of data items that the current operator outputs, based on the statistics and the execution logic of the operator    Overview Introduction to task Currently, the calculation task of TiDB contains two different tasks: cop task and root task. The cop task refers to the computing task that is pushed to the KV side and executed distributedly. The root task refers to the computing task that is executed at a single point in TiDB. One of the goals of SQL optimization is to push the calculation down to the KV side as much as possible.Table data and index data The table data in TiDB refers to the raw data of a table, which is stored in TiKV. For each row of the table data, its key is a 64-bit integer called Handle ID. If a table has int type primary key, the value of the primary key is taken as the Handle ID of the table data, otherwise the system automatically generates the Handle ID. The value of the table data is encoded by all the data in this row. When the table data is read, return the results in the order in which the Handle ID is incremented.Similar to the table data, the index data in TiDB is also stored in TiKV. The key of index data is ordered bytes encoded by index columns. The value is the Handle ID of each row of index data. You can use the Handle ID to read the non-index columns in this row. When the index data is read, return the results in the order in which the index columns are incremented. If the case of multiple index columns, make sure that the first column is incremented and that the i + 1 column is incremented when the i column is equal.Range query In the WHERE/HAVING/ON condition, analyze the results returned by primary key or index key queries. For example, number and date types of comparison symbols, greater than, less than, equal to, greater than or equal to, less than or equal to, and character type LIKE symbols.TiDB only supports the comparison symbols of which one side is a column and the other side is a constant or can be calculated as a constant. Query conditions like year(birth_day) &amp;lt; 1992 cannot use the index. Besides, try to use the same type to compare, to avoid that the index cannot be used because of additional cast operations. For example, in user_id = 123456, if the user_id is a string, you need to write 123456 as a string constant.Using AND and OR combination on the range query conditions of the same column is equivalent to getting the intersection or union set. For multidimensional combined indexes, you can write the conditions for multiple columns. For example, in the (a, b, c) combined index, when a is an equivalent query, you can continue to calculate the query range of b; when b is also an equivalent query, you can continue to calculate the query range of c; otherwise, if a is a non-equivalent query, you can only calculate the query range of a.Operator info TableReader and TableScan TableScan refers to scanning the table data at the KV side. TableReader refers to reading the table data from TiKV at the TiDB side. TableReader and TableScan are the two operators of one function. The table represents the table name in SQL statements. If the table is renamed, it displays the new name. The range represents the range of scanned data. If the WHERE/HAVING/ON condition is not specified in the query, full table scan is executed. If the range query condition is specified on the int type primary keys, range query is executed. The keep order indicates whether the table scan is returned in order.IndexReader and IndexLookUp The index data in TiDB is read in two ways: 1) IndexReader represents reading the index columns directly from the index, which is used when only index related columns or primary keys are quoted in SQL statements; 2) IndexLookUp represents filtering part of the data from the index, returning only the Handle ID, and retrieving the table data again using Handle ID. In the second way, data is retrieved twice from TiKV. The way of reading index data is automatically selected by the optimizer.Similar to TableScan, IndexScan is the operator to read index data in the KV side. The table represents the table name in SQL statements. If the table is renamed, it displays the new name. The index represents the index name. The range represents the range of scanned data. The out of order indicates whether the index scan is returned in order. In TiDB, the primary key composed of multiple columns or non-int columns is treated as the unique index.Selection Selection represents the selection conditions in SQL statements, usually used in WHERE/HAVING/ON clause.Projection Projection corresponds to the SELECT list in SQL statements, used to map the input data into new output data.Aggregation Aggregation corresponds to Group By in SQL statements, or the aggregate functions if the Group By statement does not exist, such as the COUNT or SUM function. TiDB supports two aggregation algorithms: Hash Aggregation and Stream Aggregation. Hash Aggregation is a hash-based aggregation algorithm. If Hash Aggregation is close to the read operator of Table or Index, the aggregation operator pre-aggregates in TiKV to improve the concurrency and reduce the network load.Join TiDB supports Inner Join and Left/Right Outer Join, and automatically converts the external connection that can be simplified to Inner Join.TiDB supports three Join …"},
		{"url": "https://pingcap.com/docs/sql/encrypted-connections/",
		"title": "Use Encrypted Connections", 
		"content": " Use Encrypted Connections It is recommended to use the encrypted connection to ensure data security because non-encrypted connection might lead to information leak.The TiDB server supports the encrypted connection based on the TLS (Transport Layer Security). The protocol is consistent with MySQL encrypted connections and is directly supported by existing MySQL clients such as MySQL operation tools and MySQL drivers. TLS is sometimes referred to as SSL (Secure Sockets Layer). Because the SSL protocol has known security vulnerabilities, TiDB does not support it. TiDB supports the following versions: TLS 1.0, TLS 1.1, and TLS 1.2.After using an encrypted connection, the connection has the following security properties: Confidentiality: the traffic plaintext cannot be eavesdropped Integrity: the traffic plaintext cannot be tampered Authentication: (optional) the client and the server can verify the identity of both parties to avoid man-in-the-middle attacks  The encrypted connections in TiDB are disabled by default. To use encrypted connections in the client, you must first configure the TiDB server and enable encrypted connections. In addition, similar to MySQL, the encrypted connections in TiDB consist of single optional connection. For a TiDB server with encrypted connections enabled, you can choose to securely connect to the TiDB server through an encrypted connection, or to use a generally unencrypted connection. Most MySQL clients do not use encrypted connections by default, so generally the client is explicitly required to use an encrypted connection.In short, to use encrypted connections, both of the following conditions must be met: Enable encrypted connections in the TiDB server. The client specifies to use an encrypted connection.  Configure TiDB to use encrypted connections See the following desrciptions about the related parameters to enable encrypted connections: ssl-cert: specifies the file path of the SSL certificate ssl-key: specifies the private key that matches the certificate ssl-ca: (optional) specifies the file path of the trusted CA certificate  To enable encrypted connections in the TiDB server, you must specify both of the ssl-cert and ssl-key parameters in the configuration file when you start the TiDB server. You can also specify the ssl-ca parameter for client authentication (see Enable authentication).All the files specified by the parameters are in PEM (Privacy Enhanced Mail) format. Currently, TiDB does not support the import of a password-protected private key, so it is required to provide a private key file without a password. If the certificate or private key is invalid, the TiDB server starts as usual, but the client cannot connect to the TiDB server through an encrypted connection.The certificate or key is signed and generated using OpenSSL, or quickly generated using the mysql_ssl_rsa_setup tool in MySQL:mysql_ssl_rsa_setup --datadir=./certs This command generates the following files in the certs directory:certs ├── ca-key.pem ├── ca.pem ├── client-cert.pem ├── client-key.pem ├── private_key.pem ├── public_key.pem ├── server-cert.pem └── server-key.pem The corresponding TiDB configuration file parameters are:[security] ssl-cert = &amp;#34;certs/server-cert.pem&amp;#34; ssl-key = &amp;#34;certs/server-key.pem&amp;#34; If the certificate parameters are correct, TiDB outputs Secure connection is enabled when started, otherwise it outputs Secure connection is NOT ENABLED.Configure the MySQL client to use encrypted connections The client of MySQL 5.7 or later versions attempts to establish an encrypted connection by default. If the server does not support encrypted connections, it automatically returns to unencrypted connections. The client of MySQL earlier than version 5.7 uses the unencrypted connection by default.You can change the connection behavior of the client using the following --ssl-mode parameters: --ssl-mode=REQUIRED: The client requires an encrypted connection. The connection cannot be established if the server side does not support encrypted connections. In the absence of the --ssl-mode parameter: The client attempts to use an encrypted connection, but the encrypted connection cannot be established if the server side does not support encrypted connections. Then the client uses an unencrypted connection. --ssl-mode=DISABLED: The client uses an unencrypted connection.  For more information, see Client-Side Configuration for Encrypted Connections in MySQL.Enable authentication If the ssl-ca parameter is not specified in the TiDB server or MySQL client, the client or the server does not perform authentication by default and cannot prevent man-in-the-middle attack. For example, the client might &amp;ldquo;securely&amp;rdquo; connect to a disguised client. You can configure the ssl-ca parameter for authentication in the server and client. Generally, you only need to authenticate the server, but you can also authenticate the client to further enhance the security. To authenticate the TiDB server from the MySQL client: Specify the ssl-cert andssl-key parameters in the TiDB server. Specify the --ssl-ca parameter in the MySQL client. Specify the --ssl-mode to VERIFY_IDENTITY in the MySQL client. Make sure that the certificate (ssl-cert) configured by the TiDB server is signed by the CA specified by the client --ssl-ca parameter, otherwise the authentication fails.   To authenticate the MySQL client from the TiDB server: Specify the ssl-cert, ssl-key, and ssl-ca parameters in the TiDB server. Specify the --ssl-cert and --ssl-key parameters in the client. Make sure the server-configured certificate and the client-configured certificate are both signed by the ssl-ca specified by the server.   To perform mutual authentication, meet both of the above requirements.   Note: Currently, it is optional that TiDB server authenticates the client. If the client does not present its identity certificate in the TLS handshake, the TLS connection can also be successfully established. Check whether the current connection uses encryption Use the SHOW STATUS LIKE &amp;quot;%Ssl%&amp;quot;; statement to get the details of the current connection, including whether encryption is used, the encryption protocol used by encrypted connections, the TLS version number and so on.See the following example of the result in an encrypted connection. The results change according to different TLS versions or encryption protocols supported by the client.mysql&amp;gt; SHOW STATUS LIKE &amp;#34;%Ssl%&amp;#34;; ...... | Ssl_verify_mode | 5 | | Ssl_version | TLSv1.2 | | Ssl_cipher | ECDHE-RSA-AES128-GCM-SHA256 | ...... Besides, for the official MySQL client, you can also use the STATUS or s statement to view the connection status:mysql&amp;gt; s ... SSL: Cipher in use is ECDHE-RSA-AES128-GCM-SHA256 ... Supported TLS versions, key exchange protocols, and encryption algorithms The TLS versions, key exchange protocols and encryption algorithms supported by TiDB are determined by the official Golang libraries.Supported TLS versions  TLS 1.0 TLS 1.1 TLS 1.2  Supported key exchange protocols and encryption algorithms  TLS_RSA_WITH_RC4_128_SHA TLS_RSA_WITH_3DES_EDE_CBC_SHA TLS_RSA_WITH_AES_128_CBC_SHA TLS_RSA_WITH_AES_256_CBC_SHA TLS_RSA_WITH_AES_128_CBC_SHA256 TLS_RSA_WITH_AES_128_GCM_SHA256 TLS_RSA_WITH_AES_256_GCM_SHA384 TLS_ECDHE_ECDSA_WITH_RC4_128_SHA TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA TLS_ECDHE_ECDSA_WITH_AES_256_CBC_SHA TLS_ECDHE_RSA_WITH_RC4_128_SHA TLS_ECDHE_RSA_WITH_3DES_EDE_CBC_SHA TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256 TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256 TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256 TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384 TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384 TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305 TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305  "},
		{"url": "https://pingcap.com/docs-cn/sql/user-defined-variables/",
		"title": "User-Defined Variables", 
		"content": " 用户自定义变量 用户自定义变量格式为 @var_name。var_name 目前只支持字母，数字，_$组成。用户自定义变量是大小写不敏感的。用户自定义变量是跟 session 绑定的，也就是说只有当前连接可以看见设置的用户变量，其他客户端连接无法查看到。用 SET 语句可以设置用户自定义变量：SET @var_name = expr [, @var_name = expr] ... 或 SET @var_name := expr 对于 SET 语句，赋值操作符可以是 = 也可以是 :=例：mysql&amp;gt; SET @a1=1, @a2=2, @a3:=4; mysql&amp;gt; SELECT @a1, @a2, @t3, @a4 := @a1+@a2+@a3; +------+------+------+--------------------+ | @a1 | @a2 | @a3 | @a4 := @a1+@a2+@a3 | +------+------+------+--------------------+ | 1 | 2 | 4 | 7 | +------+------+------+--------------------+ 如果设置用户变量用了 HEX 或者 BIT 值，TiDB会把它当成二进制字符串。如果你要将其设置成数字，那么需要手动加上 CAST转换: CAST(.. AS UNSIGNED)：mysql&amp;gt; SELECT @v1, @v2, @v3; +------+------+------+ | @v1 | @v2 | @v3 | +------+------+------+ | A | 65 | 65 | +------+------+------+ 1 row in set (0.00 sec) mysql&amp;gt; SET @v1 = b&amp;#39;1000001&amp;#39;; Query OK, 0 rows affected (0.00 sec) mysql&amp;gt; SET @v2 = b&amp;#39;1000001&amp;#39;+0; Query OK, 0 rows affected (0.00 sec) mysql&amp;gt; SET @v3 = CAST(b&amp;#39;1000001&amp;#39; AS UNSIGNED); Query OK, 0 rows affected (0.00 sec) mysql&amp;gt; SELECT @v1, @v2, @v3; +------+------+------+ | @v1 | @v2 | @v3 | +------+------+------+ | A | 65 | 65 | +------+------+------+ 1 row in set (0.00 sec) 如果获取一个没有设置过的变量，会返回一个 NULL：mysql&amp;gt; select @not_exist; +------------+ | @not_exist | +------------+ | NULL | +------------+ 1 row in set (0.00 sec) 用户自定义变量不能直接在 SQL 语句中被当成 identifier，例：mysql&amp;gt; select * from t; +------+ | a | +------+ | 1 | +------+ 1 row in set (0.00 sec) mysql&amp;gt; SET @col = &amp;#34;a&amp;#34;; Query OK, 0 rows affected (0.00 sec) mysql&amp;gt; SELECT @col FROM t; +------+ | @col | +------+ | a | +------+ 1 row in set (0.00 sec) mysql&amp;gt; SELECT `@col` FROM t; ERROR 1054 (42S22): Unknown column &amp;#39;@col&amp;#39; in &amp;#39;field list&amp;#39; mysql&amp;gt; SET @col = &amp;#34;`a`&amp;#34;; Query OK, 0 rows affected (0.00 sec) mysql&amp;gt; SELECT @col FROM t; +------+ | @col | +------+ | `a` | +------+ 1 row in set (0.01 sec) 但是有一个例外是如果你在 PREPARE 语句中使用它，是可以的：mysql&amp;gt; PREPARE stmt FROM &amp;#34;SELECT @c FROM t&amp;#34;; Query OK, 0 rows affected (0.00 sec) mysql&amp;gt; EXECUTE stmt; +------+ | @c | +------+ | a | +------+ 1 row in set (0.01 sec) mysql&amp;gt; DEALLOCATE PREPARE stmt; Query OK, 0 rows affected (0.00 sec) 更多细节"},
		{"url": "https://pingcap.com/docs/sql/user-defined-variables/",
		"title": "User-Defined Variables", 
		"content": " User-Defined Variables The format of the user-defined variables is @var_name. @var_name consists of alphanumeric characters, _, and $. The user-defined variables are case-insensitive.The user-defined variables are session specific, which means a user variable defined by one client cannot be seen or used by other clients. You can use the SET statement to set a user variable:SET @var_name = expr [, @var_name = expr] ... orSET @var_name := expr For SET, you can use = or := as the assignment operator.For example:mysql&amp;gt; SET @a1=1, @a2=2, @a3:=4; mysql&amp;gt; SELECT @a1, @a2, @t3, @a4 := @a1+@a2+@a3; +------+------+------+--------------------+ | @a1 | @a2 | @a3 | @a4 := @a1+@a2+@a3 | +------+------+------+--------------------+ | 1 | 2 | 4 | 7 | +------+------+------+--------------------+ Hexadecimal or bit values assigned to user variables are treated as binary strings in TiDB. To assign a hexadecimal or bit value as a number, use it in numeric context. For example, add 0 or use CAST(... AS UNSIGNED):mysql&amp;gt; SELECT @v1, @v2, @v3; +------+------+------+ | @v1 | @v2 | @v3 | +------+------+------+ | A | 65 | 65 | +------+------+------+ 1 row in set (0.00 sec) mysql&amp;gt; SET @v1 = b&amp;#39;1000001&amp;#39;; Query OK, 0 rows affected (0.00 sec) mysql&amp;gt; SET @v2 = b&amp;#39;1000001&amp;#39;+0; Query OK, 0 rows affected (0.00 sec) mysql&amp;gt; SET @v3 = CAST(b&amp;#39;1000001&amp;#39; AS UNSIGNED); Query OK, 0 rows affected (0.00 sec) mysql&amp;gt; SELECT @v1, @v2, @v3; +------+------+------+ | @v1 | @v2 | @v3 | +------+------+------+ | A | 65 | 65 | +------+------+------+ 1 row in set (0.00 sec) If you refer to a user-defined variable that has not been initialized, it has a value of NULL and a type of string.mysql&amp;gt; select @not_exist; +------------+ | @not_exist | +------------+ | NULL | +------------+ 1 row in set (0.00 sec) The user-defined variables cannot be used as an identifier in the SQL statement. For example:mysql&amp;gt; select * from t; +------+ | a | +------+ | 1 | +------+ 1 row in set (0.00 sec) mysql&amp;gt; SET @col = &amp;#34;a&amp;#34;; Query OK, 0 rows affected (0.00 sec) mysql&amp;gt; SELECT @col FROM t; +------+ | @col | +------+ | a | +------+ 1 row in set (0.00 sec) mysql&amp;gt; SELECT `@col` FROM t; ERROR 1054 (42S22): Unknown column &amp;#39;@col&amp;#39; in &amp;#39;field list&amp;#39; mysql&amp;gt; SET @col = &amp;#34;`a`&amp;#34;; Query OK, 0 rows affected (0.00 sec) mysql&amp;gt; SELECT @col FROM t; +------+ | @col | +------+ | `a` | +------+ 1 row in set (0.01 sec) An exception is that when you are constructing a string for use as a prepared statement to execute later:mysql&amp;gt; PREPARE stmt FROM &amp;#34;SELECT @c FROM t&amp;#34;; Query OK, 0 rows affected (0.00 sec) mysql&amp;gt; EXECUTE stmt; +------+ | @c | +------+ | a | +------+ 1 row in set (0.01 sec) mysql&amp;gt; DEALLOCATE PREPARE stmt; Query OK, 0 rows affected (0.00 sec) For more information, see User-Defined Variables in MySQL."},
		{"url": "https://pingcap.com/docs/sql/util/",
		"title": "Utility Statements", 
		"content": " Utility Statements DESCRIBE statement The DESCRIBE and EXPLAIN statements are synonyms, which can also be abbreviated as DESC. See the usage of the EXPLAIN statement.EXPLAIN statement {EXPLAIN | DESCRIBE | DESC} tbl_name [col_name] {EXPLAIN | DESCRIBE | DESC} [explain_type] explainable_stmt explain_type: FORMAT = format_name format_name: &amp;#34;DOT&amp;#34; explainable_stmt: { SELECT statement | DELETE statement | INSERT statement | REPLACE statement | UPDATE statement } For more information about the EXPLAIN statement, see Understand the Query Execution Plan.In addition to the MySQL standard result format, TiDB also supports DotGraph and you need to specify FORMAT = &amp;quot;dot&amp;quot; as in the following example:create table t(a bigint, b bigint); desc format = &amp;#34;dot&amp;#34; select A.a, B.b from t A join t B on A.a &amp;gt; B.b where A.a &amp;lt; 10; TiDB &amp;gt; desc format = &amp;#34;dot&amp;#34; select A.a, B.b from t A join t B on A.a &amp;gt; B.b where A.a &amp;lt; 10;desc format = &amp;#34;dot&amp;#34; select A.a, B.b from t A join t B on A.a &amp;gt; B.b where A.a &amp;lt; 10; +--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | dot contents | +--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | digraph HashRightJoin_7 { subgraph cluster7{ node [style=filled, color=lightgrey] color=black label = &amp;#34;root&amp;#34; &amp;#34;HashRightJoin_7&amp;#34; -&amp;gt; &amp;#34;TableReader_10&amp;#34; &amp;#34;HashRightJoin_7&amp;#34; -&amp;gt; &amp;#34;TableReader_12&amp;#34; } subgraph cluster9{ node [style=filled, color=lightgrey] color=black label = &amp;#34;cop&amp;#34; &amp;#34;Selection_9&amp;#34; -&amp;gt; &amp;#34;TableScan_8&amp;#34; } subgraph cluster11{ node [style=filled, color=lightgrey] color=black label = &amp;#34;cop&amp;#34; &amp;#34;TableScan_11&amp;#34; } &amp;#34;TableReader_10&amp;#34; -&amp;gt; &amp;#34;Selection_9&amp;#34; &amp;#34;TableReader_12&amp;#34; -&amp;gt; &amp;#34;TableScan_11&amp;#34; } | +--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ 1 row in set (0.00 sec) If the dot program (in the graphviz package) is installed on your computer, you can generate a PNG file using the following method:dot xx.dot -T png -O The xx.dot is the result returned by the above statement. If the dot program is not installed on your computer, copy the result to this website to get a tree diagram:USE statement USE db_name The USE statement is used to switch the default database. If the table in SQL statements does not display the specified database, then use the default database."},
		{"url": "https://pingcap.com/weekly/",
		"title": "Weeklies", 
		"content": ""},
		{"url": "https://pingcap.com/docs-cn/sql/mysql-compatibility/",
		"title": "与 MySQL 兼容性对比", 
		"content": " 与 MySQL 兼容性对比 TiDB 支持包括跨行事务，JOIN 及子查询在内的绝大多数 MySQL 的语法，用户可以直接使用现有的 MySQL 客户端连接。如果现有的业务已经基于 MySQL 开发，大多数情况不需要修改代码即可直接替换单机的 MySQL。包括现有的大多数 MySQL 运维工具（如 PHPMyAdmin, Navicat, MySQL Workbench 等），以及备份恢复工具（如 mysqldump, mydumper/myloader）等都可以直接使用。不过一些特性由于在分布式环境下没法很好的实现，目前暂时不支持或者是表现与 MySQL 有差异。一些 MySQL 语法在 TiDB 中可以解析通过，但是不会做任何后续的处理，例如 Create Table 语句中 Engine 以及 Partition 选项，都是解析并忽略。更多兼容性差异请参考具体的文档。不支持的特性  存储过程 视图 触发器 自定义函数 外键约束 全文索引 空间索引 非 UTF8 字符集  与 MySQL 有差异的特性 自增 ID TiDB 的自增 ID (Auto Increment ID) 只保证自增且唯一，并不保证连续分配。TiDB 目前采用批量分配的方式，所以如果在多台 TiDB 上同时插入数据，分配的自增 ID 会不连续。 注意：在有多台 TiDB 使用自增 ID 时，建议不要混用缺省值和自定义值。因为目前在如下情况下会报错：在有两个 TiDB（TiDB A 缓存 [1,5000] 的自增 ID，TiDB B 缓存 [5001,10000] 的自增 ID）的集群，使用如下 SQL 语句创建一个带有自增 ID 的表：create table t(id int unique key auto_increment, c int); 该语句执行如下： 客户端向 TiDB B 插入一条将 id 设置为 1 的语句，并执行成功。 客户端向 TiDB A 发送插入一条记录，且记录中 id 使用缺省值即 1，最终返回 Duplicated Error。  该问题近期会解决。 内建函数 TiDB 支持常用的 MySQL 内建函数，但是不是所有的函数都已经支持，具体请参考语法文档。DDL TiDB 实现了 F1 的异步 Schema 变更算法，DDL 执行过程中不会阻塞线上的 DML 操作。目前已经支持的 DDL 包括： Create Database Drop Database Create Table Drop Table Add Index Drop Index Add Column Drop Column Alter Column Change Column Modify Column Truncate Table Rename Table Create Table Like  以上语句还有一些支持不完善的地方，具体包括如下： Add/Drop primary key 操作目前不支持。 Add Index/Column 操作不支持同时创建多个索引或列。 Drop Column 操作不支持删除的列为主键列或索引列。 Add Column 操作不支持同时将新添加的列设为主键或唯一索引，也不支持将此列设成 auto_increment 属性。 Change/Modify Column 操作目前支持部分语法，细节如下：  在修改类型方面，只支持整数类型之间修改，字符串类型之间修改和 Blob 类型之间的修改，且只能使原类型长度变长。此外，不能改变列的 unsigned/charset/collate 属性。这里的类型分类如下：  具体支持的整型类型有：TinyInt，SmallInt，MediumInt，Int，BigInt。 具体支持的字符串类型有：Char，Varchar，Text，TinyText，MediumText，LongText。 具体支持的 Blob 类型有：Blob，TinyBlob，MediumBlob，LongBlob。  在修改类型定义方面，支持的包括 default value，comment，null，not null 和 OnUpdate，但是不支持从 null 到 not null 的修改。 支持 LOCK [=] {DEFAULT|NONE|SHARED|EXCLUSIVE} 语法，但是不做任何事情（pass through）。 不支持对enum类型的列进行修改   事务 TiDB 使用乐观事务模型，在执行 Update、Insert、Delete 等语句时，只有在提交过程中才会检查写写冲突，而不是像 MySQL 一样使用行锁来避免写写冲突。所以业务端在执行 SQL 语句后，需要注意检查 commit 的返回值，即使执行时没有出错，commit的时候也可能会出错。Load data  语法：LOAD DATA LOCAL INFILE &amp;#39;file_name&amp;#39; INTO TABLE table_name {FIELDS | COLUMNS} TERMINATED BY &amp;#39;string&amp;#39; ENCLOSED BY &amp;#39;char&amp;#39; ESCAPED BY &amp;#39;char&amp;#39; LINES STARTING BY &amp;#39;string&amp;#39; TERMINATED BY &amp;#39;string&amp;#39; (col_name ...); 其中 ESCAPED BY 目前只支持 &amp;lsquo;//&amp;lsquo;。 事务的处理：TiDB 在执行 load data 时，默认每 2 万行记录作为一个事务进行持久化存储。如果一次 load data 操作插入的数据超过 2 万行，那么会分为多个事务进行提交。如果某个事务出错，这个事务会提交失败，但它前面的事务仍然会提交成功，在这种情况下一次 load data 操作会有部分数据插入成功，部分数据插入失败。而 MySQL 中会将一次 load data 操作视为一个事务，如果其中发生失败情况，将会导致整个 load data 操作失败。  "},
		{"url": "https://pingcap.com/docs-cn/sql/transaction/",
		"title": "事务语句", 
		"content": " TiDB 事务语句 TiDB 支持分布式事务。涉及到事务的语句包括 Autocommit 变量、 START TRANSACTION/BEGIN、 COMMIT 以及 ROLLBACK。自动提交 语法：SET autocommit = {0 | 1} 通过设置 autocommit 的值为 1，可以将当前 Session 设置为自动提交状态，0 则表示当前 Session 为非自动提交状态。默认情况下， autocommit 的值为 1。在自动提交状态，每条语句运行后，会将其修改自动提交到数据库中。否则，会等到运行 COMMIT 语句或者是 BEGIN 语句的时候，才会将之前的修改提交到数据库。另外 autocommit 也是一个 System Variable，所以可以通过变量赋值语句修改当前 Session 或者是 Global 的值。SET @@SESSION.autocommit = {0 | 1}; SET @@GLOBAL.autocommit = {0 | 1}; START TRANSACTION, Begin 语法:BEGIN; START TRANSACTION; START TRANSACTION WITH CONSISTENT SNAPSHOT; 上述三条语句都是事务开始语句，效果相同。通过事务开始语句可以显式地开始一个新的事务，如果这个时候当前 Session 正在一个事务中间过程中，会将当前事务提交后，开启一个新的事务。COMMIT 语法：COMMIT; 提交当前事务，包括从 BEGIN 到 COMMIT 之间的所有修改。ROLLBACK 语法：ROLLBACK; 回滚当前事务，撤销从 BEGIN 到 ROLLBACK 之间的所有修改。显式事务和隐式事务 TiDB 可以显式地使用事务（BEGIN/COMMIT） 或者隐式的使用事务 （SET autocommit = 1）。如果在 autocmmit = 1 的状态下，通过 BEGIN 语句开启一个新的事务，那么在 COMMIT/ROLLBACK 之前，会禁用 autocommit，也就是变成显式事务。对于 DDL 语句，会自动提交并且不能回滚。如果运行 DDL 的时候，正在一个事务的中间过程中，会先将当前的事务提交，再运行 DDL。事务隔离级别 TiDB 默认使用 SNAPSHOT ISOLATION，可以通过下面的语句将当前 Session 的隔离级别设置为 READ COMMITTED。SET SESSION TRANSACTION ISOLATION LEVEL READ COMMITTED;"},
		{"url": "https://pingcap.com/docs-cn/op-guide/docker-compose/",
		"title": "使用 Docker Compose 构建集群", 
		"content": " 使用 Docker Compose 快速构建集群 Docker Compose 可以通过一个 YAML 文件定义多个容器的应用服务，然后一键启动或停止。可以用来在单机上一键部署一套 TiDB 测试集群，使用 Docker Compose 部署 TiDB 集群要求 Docker 是 17.06.0 及以上版本。快速开始  下载 tidb-docker-composegit clone https://github.com/pingcap/tidb-docker-compose.git 创建并启动集群cd tidb-docker-compose &amp;amp;&amp;amp; docker-compose up -d 访问集群mysql -h 127.0.0.1 -P 4000 -u root 访问集群 Grafana 监控页面：http://localhost:3000 默认用户名和密码都是 admin。集群数据可视化：http://localhost:8010  自定义集群 快速开始里面默认部署 3 个 PD，3 个 TiKV，1 个 TiDB 和监控组件 Prometheus，Pushgateway，Grafana 以及 tidb-vision。如果想自定义集群，可以直接修改 docker-compose.yml，但是手动修改比较繁琐而且容易出错，强烈建议使用 Helm 模板引擎生成 docker-compose.yml 文件。 安装 HelmHelm 可以用作模板渲染引擎，只需要下载其 binary 文件即可以使用。curl https://raw.githubusercontent.com/kubernetes/helm/master/scripts/get | bash 如果是 Mac 系统，也可以通过 Homebrew 安装：brew install kubernetes-helm 下载 tidb-docker-composegit clone https://github.com/pingcap/tidb-docker-compose.git 自定义集群cd tidb-docker-compose cp compose/values.yaml values.yaml vim values.yaml 修改 values.yaml 里面的配置，例如集群规模，TiDB 镜像版本等。tidb-vision 是 TiDB 集群可视化页面，可以可视化地显示 PD 对 TiKV 数据的调度。如果不想部署该组件，可以将 tidbVision 项留空。PD，TiKV，TiDB 和 tidb-vision 支持从 GitHub 源码或本地文件构建 Docker 镜像，供开发测试使用。 如果希望从 GitHub 源码构建某个组件的镜像，需要将其 image 字段留空，然后设置其 buildFrom 为 remote。 如果希望从本地已编译好的 binary 文件构建 PD，TiKV 或 TiDB 镜像，需要将其 image 字段留空，然后设置其 buildFrom 为 local，并将已编译好的 binary 拷贝到对应的 pd/bin/pd-server，tikv/bin/tikv-server，tidb/bin/tidb-server。 如果希望从本地构建 tidb-vision 镜像，需要将其 image 字段留空，然后设置其 buildFrom 为 local，并将 tidb-vision 项目拷贝到 tidb-vision/tidb-vision。  生成 docker-compose.yml 文件helm template -f values.yaml compose &amp;gt; generated-docker-compose.yml 使用生成的 docker-compose.yml 创建并启动集群docker-compose -f generated-docker-compose.yml up -d 访问集群mysql -h 127.0.0.1 -P 4000 -u root 访问集群 Grafana 监控页面：http://localhost:3000 默认用户名和密码都是 admin。如果启用了 tidb-vision，可以通过 http://localhost:8010 查看。  "},
		{"url": "https://pingcap.com/docs-cn/op-guide/root-ansible-deployment/",
		"title": "使用 root 用户远程连接 TiDB Ansible 部署方案", 
		"content": " 使用 root 用户远程连接 TiDB Ansible 部署方案  Ansible 远程连接用户(即 incentory.ini 文件中的 ansible_user)，从中控机使用 root 用户 SSH 到部署目标机器部署，不推荐采用该方式安装。  修改 inventory.ini, 本例使用 tidb 帐户作为服务运行用户：取消 ansible_user = root 、ansible_become = true 及 ansible_become_user 注释，给 ansible_user = tidb 添加注释：## Connection # ssh via root: ansible_user = root ansible_become = true ansible_become_user = tidb # ssh via normal user # ansible_user = tidb 使用 local_prepare.yml playbook, 联网下载 TiDB binary 到中控机：ansible-playbook local_prepare.yml 初始化系统环境，修改内核参数 如服务运行用户尚未建立，此初始化操作会自动创建该用户。 ansible-playbook bootstrap.yml 如果 ansible 使用 root 用户远程连接需要密码, 使用 -k 参数，执行其他 playbook 同理：ansible-playbook bootstrap.yml -k 部署 TiDB 集群软件ansible-playbook deploy.yml -k 启动 TiDB 集群ansible-playbook start.yml -k  "},
		{"url": "https://pingcap.com/docs-cn/sql/encrypted-connections/",
		"title": "使用加密连接", 
		"content": " 使用加密连接 TiDB 服务端默认采用非加密连接，因而具备监视信道流量能力的第三方可以知悉 TiDB 服务端与客户端之间发送和接受的数据，包括但不限于查询语句内容、查询结果等。若信道是不可信的，例如客户端是通过公网连接到 TiDB 服务端的，则非加密连接容易造成信息泄露，建议使用加密连接确保安全性。TiDB 服务端支持启用基于 TLS（传输层安全）协议的加密连接，协议与 MySQL 加密连接一致，现有 MySQL 客户端如 MySQL 运维工具和 MySQL 驱动等能直接支持。TLS 的前身是 SSL，因而 TLS 有时也被称为 SSL，但由于 SSL 协议有已知安全漏洞，TiDB 实际上并未支持。TiDB 支持的 TLS/SSL 协议版本为 TLS 1.0、TLS 1.1、TLS 1.2。使用加密连接后，连接将具有以下安全性质： 保密性：流量明文无法被窃听； 完整性：流量明文无法被篡改； 身份验证（可选）：客户端和服务端能验证双方身份，避免中间人攻击。  TiDB 的加密连接支持默认是关闭的，必须在 TiDB 服务端通过配置开启加密连接的支持后，才能在客户端中使用加密连接。另外，与 MySQL 一致，TiDB 加密连接是以单个连接为单位的，并且是可选的，因而对于开启了加密连接支持的 TiDB 服务端，客户端既可以选择通过加密连接安全地连接到该 TiDB 服务端，也可以选择使用普通的非加密连接。大部分 MySQL 客户端默认不采用加密连接，因此一般还要显式地要求客户端使用加密连接。简单来说，要使用加密连接必须同时满足以下两个条件： TiDB 服务端配置开启加密连接的支持 客户端指定使用加密连接  配置 TiDB 启用加密连接支持 在启动 TiDB 时，至少需要在配置文件中同时指定 ssl-cert 和 ssl-key 参数，才能使 TiDB 服务端接受加密连接。还可以指定 ssl-ca 参数进行客户端身份验证（请参见配置启用身份验证章节）。 ssl-cert：指定 SSL 证书文件路径 ssl-key：指定证书文件对应的私钥 ssl-ca：可选，指定受信任的 CA 证书文件路径  参数指定的文件都为 PEM 格式。另外目前 TiDB 尚不支持加载有密码保护的私钥，因此必须提供一个没有密码的私钥文件。若提供的证书或私钥无效，则 TiDB 服务端将照常启动，但并不支持客户端加密连接到 TiDB 服务端。上述证书及密钥可以使用 OpenSSL 签发和生成，也可以使用 MySQL 自带的工具 mysql_ssl_rsa_setup 快捷生成：mysql_ssl_rsa_setup --datadir=./certs 以上命令将在 certs 目录下生成以下文件：certs ├── ca-key.pem ├── ca.pem ├── client-cert.pem ├── client-key.pem ├── private_key.pem ├── public_key.pem ├── server-cert.pem └── server-key.pem 对应的 TiDB 配置文件参数为：[security] ssl-cert = &amp;#34;certs/server-cert.pem&amp;#34; ssl-key = &amp;#34;certs/server-key.pem&amp;#34; 若证书参数无误，则 TiDB 在启动时将会输出 Secure connection is enabled，否则 TiDB 会输出 Secure connection is NOT ENABLED。配置 MySQL 客户端使用加密连接 MySQL 5.7 及以上版本自带的客户端默认尝试使用安全连接，若服务端不支持安全连接则自动退回到使用非安全连接；MySQL 5.7 以下版本自带的客户端默认采用非安全连接。可以通过命令行参数修改客户端的连接行为，包括： 强制使用安全连接，若服务端不支持安全连接则连接失败 (--ssl-mode=REQUIRED) 尝试使用安全连接，若服务端不支持安全连接则退回使用不安全连接 使用不安全连接 (--ssl-mode=DISABLED)  详细信息请参阅 MySQL 文档中关于客户端配置安全连接的部分。配置启用身份验证 若在 TiDB 服务端或 MySQL 客户端中未指定 ssl-ca 参数，则默认不会进行客户端或服务端身份验证，无法抵御中间人攻击，例如客户端可能会“安全地”连接到了一个伪装的服务端。可以在服务端和客户端中配置 ssl-ca 参数进行身份验证。一般情况下只需验证服务端身份，但也可以验证客户端身份进一步增强安全性。 若要使 MySQL 客户端验证 TiDB 服务端身份，TiDB 服务端需至少配置 ssl-cert 和 ssl-key 参数，客户端需至少指定 --ssl-ca 参数，且 --ssl-mode 至少为 VERIFY_IDENTITY。必须确保 TiDB 服务端配置的证书（ssl-cert）是由客户端 --ssl-ca 参数所指定的 CA 所签发的，否则身份验证失败。 若要使 TiDB 服务端验证 MySQL 客户端身份，TiDB 服务端需配置 ssl-cert、ssl-key、ssl-ca 参数，客户端需至少指定 --ssl-cert、--ssl-key 参数。必须确保服务端配置的证书和客户端配置的证书都是由服务端配置指定的 ssl-ca 签发的。 若要进行双向身份验证，请同时满足上述要求。  注：目前 TiDB 尚不支持强制验证客户端身份，即服务端对客户端的身份验证是可选的。若客户端在 TLS 握手时未出示自己的身份证书，也能正常建立 TLS 连接。检查当前连接是否是加密连接 可以通过 SHOW STATUS LIKE &amp;quot;%Ssl%&amp;quot;; 了解当前连接的详细情况，包括是否使用了安全连接、安全连接采用的加密协议、TLS 版本号等。以下是一个安全连接中执行该语句的结果。由于客户端支持的 TLS 版本号和加密协议会有所不同，执行结果相应地也会有所变化。mysql&amp;gt; SHOW STATUS LIKE &amp;#34;%Ssl%&amp;#34;; ...... | Ssl_verify_mode | 5 | | Ssl_version | TLSv1.2 | | Ssl_cipher | ECDHE-RSA-AES128-GCM-SHA256 | ...... 除此以外，对于 MySQL 自带客户端，还可以使用 STATUS 或 s 语句查看连接情况：mysql&amp;gt; s ... SSL: Cipher in use is ECDHE-RSA-AES128-GCM-SHA256 ... 支持的 TLS 版本及密钥交换协议和加密算法 TiDB 支持的 TLS 版本及密钥交换协议和加密算法由 Golang 官方库决定。支持的 TLS 版本  TLS 1.0 TLS 1.1 TLS 1.2  支持的密钥交换协议及加密算法  TLS_RSA_WITH_RC4_128_SHA TLS_RSA_WITH_3DES_EDE_CBC_SHA TLS_RSA_WITH_AES_128_CBC_SHA TLS_RSA_WITH_AES_256_CBC_SHA TLS_RSA_WITH_AES_128_CBC_SHA256 TLS_RSA_WITH_AES_128_GCM_SHA256 TLS_RSA_WITH_AES_256_GCM_SHA384 TLS_ECDHE_ECDSA_WITH_RC4_128_SHA TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA TLS_ECDHE_ECDSA_WITH_AES_256_CBC_SHA TLS_ECDHE_RSA_WITH_RC4_128_SHA TLS_ECDHE_RSA_WITH_3DES_EDE_CBC_SHA TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256 TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256 TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256 TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384 TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384 TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305 TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305  "},
		{"url": "https://pingcap.com/docs-cn/sql/miscellaneous-functions/",
		"title": "其他函数", 
		"content": " 其他函数    函数名 功能描述     ANY_VALUE() 在 ONLY_FULL_GROUP_BY 模式下，防止带有 GROUP BY 的语句报错   SLEEP() 休眠指定秒数   UUID() 返回通用唯一识别码 (UUID)   VALUES() 定义 INSERT 过程中要用到的值   INET_ATON() 将 IP 地址转换为数值   INET_NTOA() 将数值转换为 IP 地址   INET6_ATON() 将 IPv6 地址转换为数值    INET6_NTOA() 将数值转换为 IPv6 地址   IS_IPV4() 判断参数是否为 IPv4 地址   IS_IPV4_COMPAT() 判断参数是否为兼容 IPv4 的地址   IS_IPV4_MAPPED() 判断参数是否为 IPv4 映射的地址   IS_IPV6() 判断参数是否为 IPv6 地址   GET_LOCK()  获取命名锁，TiDB 出于兼容性支持这个函数，实际上不会做任何操作，这点和 MySQL 有区别   RELEASE_LOCK() 释放命名锁    "},
		{"url": "https://pingcap.com/docs-cn/sql/encryption-and-compression-functions/",
		"title": "加密和压缩函数", 
		"content": " 加密和压缩函数    函数名 功能描述     MD5()  计算字符串的 MD5 校验和    PASSWORD()（在 MySQL 5.7.6 中已弃用） 计算并返回密码字符串   RANDOM_BYTES() 返回随机字节向量   SHA1(), SHA()  计算 SHA-1 160 位校验和    SHA2()  计算 SHA-2 校验和    AES_DECRYPT() 使用 AES 解密   AES_ENCRYPT() 使用 AES 加密   COMPRESS() 返回经过压缩的二进制字符串   UNCOMPRESS() 解压缩字符串   UNCOMPRESSED_LENGTH()  返回字符串解压后的长度   CREATE_ASYMMETRIC_PRIV_KEY() 创建私钥   CREATE_ASYMMETRIC_PUB_KEY() 创建公钥   CREATE_DH_PARAMETERS() 创建 DH 共享密钥   CREATE_DIGEST() 从字符串创建摘要   ASYMMETRIC_DECRYPT() 使用公钥或私钥解密密文   ASYMMETRIC_DERIVE() 从非对称密钥导出对称密钥   ASYMMETRIC_ENCRYPT() 使用公钥或私钥加密明文   ASYMMETRIC_SIGN() 从摘要创建签名   ASYMMETRIC_VERIFY() 验证签名字符串是否匹配摘要字符串    "},
		{"url": "https://pingcap.com/docs-cn/sql/literal-value-hex-decimal/",
		"title": "十六进制的字面值", 
		"content": " Hexadecimal Literals 十六进制字面值是有 X 和 0x 前缀的字符串，后接表示十六进制的数字。注意 0x 是大小写敏感的，不能表示为 0X。例:X&amp;#39;ac12&amp;#39; X&amp;#39;12AC&amp;#39; x&amp;#39;ac12&amp;#39; x&amp;#39;12AC&amp;#39; 0xac12 0x12AC 以下是不合法的十六进制字面值：X&amp;#39;1z&amp;#39; (z 不是合法的十六进制值) 0X12AC (0X 必须用小写的 0x) 对于使用 X&#39;val&#39; 格式的十六进制字面值，val 必须要有一个数字，可以在前面补一个 0 来避免语法错误。mysql&amp;gt; select X&amp;#39;aff&amp;#39;; ERROR 1105 (HY000): line 0 column 13 near &amp;#34;&amp;#34;hex literal: invalid hexadecimal format, must even numbers, but 3 (total length 13) mysql&amp;gt; select X&amp;#39;0aff&amp;#39;; +---------+ | X&amp;#39;0aff&amp;#39; | +---------+ | | +---------+ 1 row in set (0.00 sec) 默认情况，十六进制字面值是一个二进制字符串。如果需要将一个字符串或者数字转换为十六进制字面值，可以使用内建函数 HEX()：mysql&amp;gt; SELECT HEX(&amp;#39;TiDB&amp;#39;); +-------------+ | HEX(&amp;#39;TiDB&amp;#39;) | +-------------+ | 54694442 | +-------------+ 1 row in set (0.01 sec) mysql&amp;gt; SELECT X&amp;#39;54694442&amp;#39;; +-------------+ | X&amp;#39;54694442&amp;#39; | +-------------+ | TiDB | +-------------+ 1 row in set (0.00 sec)"},
		{"url": "https://pingcap.com/docs-cn/op-guide/configuration/",
		"title": "参数解释", 
		"content": " 参数解释 TiDB --binlog-socket  TiDB 服务使用 unix socket file 方式接受内部连接，如 PUMP 服务 默认: &amp;ldquo;&amp;rdquo; 譬如我们可以使用 &amp;ldquo;/tmp/pump.sock&amp;rdquo; 来接受 PUMP unix socket file 通信  --cross-join  默认: true 在做 join 的时候，两边表没有任何条件（where 字段），默认可以执行这样的语句。但是设置为 false，则如有这样的 join 语句出现，server 会拒绝执行  --host  TiDB 服务监听 host 默认: &amp;ldquo;0.0.0.0&amp;rdquo; TiDB 服务会监听这个 host 0.0.0.0 默认会监听所有的网卡 address。如果有多块网卡，可以指定对外提供服务的网卡，譬如192.168.100.113  --join-concurrency int  join-concurrency 并发执行 join 的 goroutine 数量 默认: 5 看数据量和数据分布情况，一般情况下是越多越好，数值越大对 CPU 开销越大  -L  Log 级别 默认: &amp;ldquo;info&amp;rdquo; 我们能选择 debug, info, warn, error 或者 fatal  --lease  Schema 的租约时间，单位：秒 默认: &amp;ldquo;10&amp;rdquo; Schema 的 lease 主要用在 online schema changes 上面。这个值会影响到实际的 DDL 语句的执行时间。千万不要随便改动这个值，除非你能知道相关的内部机制  --log-file  Log 文件 默认: &amp;ldquo;&amp;rdquo; 如果没设置这个参数，log 会默认输出到 &amp;ldquo;stderr&amp;rdquo;，如果设置了，log 就会输出到对应的文件里面，在每天凌晨，log 会自动轮转使用一个新的文件，并且将以前的文件改名备份  --metrics-addr  Prometheus Push Gateway 地址 默认: &amp;ldquo;&amp;rdquo; 如果为空，TiDB 不会将统计信息推送给 Push Gateway,参数格式 如 --metrics-addr=192.168.100.115:9091  --metrics-intervel  推送统计信息到 Prometheus Push Gateway 的时间间隔 默认: 15s 设置为 0 表明不推送统计信息给 Push Gateway,如: --metrics-interval=2 是每两秒推送到 Push Gataway  -P  TiDB 服务监听端口 默认: &amp;ldquo;4000&amp;rdquo; TiDB 服务将会使用这个端口接受 MySQL 客户端发过来的请求  --path  对于本地存储引擎 &amp;ldquo;goleveldb&amp;rdquo;, &amp;ldquo;BoltDB&amp;rdquo; 来说，path 指定的是实际的数据存放路径 对于 &amp;ldquo;memory&amp;rdquo; 存储引擎来说，path 不用设置 对于 &amp;ldquo;TiKV&amp;rdquo; 存储引擎来说，path 指定的是实际的 PD 地址。假设我们在 192.168.100.113:2379, 192.168.100.114:2379 和 192.168.100.115:2379 上面部署了 PD，那么 path 为 &amp;ldquo;192.168.100.113:2379, 192.168.100.114:2379, 192.168.100.115:2379&amp;rdquo; 默认: &amp;ldquo;/tmp/tidb&amp;rdquo;  --perfschema  使用 true/false 来打开或者关闭性能 schema 默认: false 值可以是 (true) or (false)。性能 Schema 可以帮助我们在运行时检测内部的执行情况。可以通过 performance schema 获取更多信息。但需要注意，开启性能 Schema，会影响 TiDB 的性能  --privilege  使用 true/false 来打开或者关闭权限功能(用于开发调试) 默认: true 值可以是(true) or (false)。当前版本的权限控制还在完善中，将来会去掉此选项  --query-log-max-len int  日志中记录最大 sql 语句长度 默认: 2048 过长的请求输出到 log 时会被截断  --report-status  打开 (true) 或者关闭 (false) 服务状态监听端口 默认: true 值可以为 (true) 或者 (false). (true) 表明我们开启状态监听端口。 (false) 表明关闭  --retry-limit int  事务遇见冲突时，提交事物最大重试次数 默认: 10 设置较大的重试次数会影响 TiDB 集群性能  --run-ddl  tidb-server 是否运行 DDL 语句，集群内大于两台以上 tidb-server 时设置 默认: true 值可以为 (true) 或者 (false). (true) 表明自身会运行 DDL. (false) 表明自身不会运行 DDL  --skip-grant-table  允许任何人不带密码连接，并且所有的操作不检查权限 默认: false 值可以是(true) or (false)。启用此选项需要有本机的root权限，一般用于忘记密码时重置  --slow-threshold int  大于这个值得 sql 语句将被记录 默认: 300 值只能是一个整数 (int) ，单位是毫秒  --socket string  TiDB 服务使用 unix socket file 方式接受外部连接 默认: &amp;ldquo;&amp;rdquo; 譬如我们可以使用 &amp;ldquo;/tmp/tidb.sock&amp;rdquo; 来打开 unix socket file  --ssl-ca  PEM 格式的受信任 CA 的证书文件路径 默认: &amp;ldquo;&amp;rdquo; 当同时设置了该选项和 --ssl-cert、--ssl-key 选项时，TiDB 将在客户端出示证书的情况下根据该选项指定的受信任的 CA 列表验证客户端证书。若验证失败，则连接会被终止。 即使设置了该选项，若客户端没有出示证书，则安全连接仍然继续，不会进行客户端证书验证。  --ssl-cert  PEM 格式的 SSL 证书文件路径 默认: &amp;ldquo;&amp;rdquo; 当同时设置了该选项和 --ssl-key 选项时，TiDB 将接受（但不强制）客户端使用 TLS 安全地连接到 TiDB。 若指定的证书或私钥无效，则 TiDB 会照常启动，但无法接受安全连接。  --ssl-key  PEM 格式的 SSL 证书密钥文件路径，即 --ssl-cert 所指定的证书的私钥 默认: &amp;ldquo;&amp;rdquo; 目前 TiDB 不支持加载由密码保护的私钥。  --status  TiDB 服务状态监听端口 默认: &amp;ldquo;10080&amp;rdquo; 这个端口是为了展示 TiDB 内部数据用的。包括 prometheus 统计 以及 pprof Prometheus 统计可以通过 &amp;ldquo;http://host:status_port/metrics&amp;quot; 访问 Pprof 数据可以通过 &amp;ldquo;http://host:status_port/debug/pprof&amp;quot; 访问  --statsLease string  增量扫描全表并分析表的数据量 索引等一些信息 默认: 3s 使用此参数需要先手动执行 analyze table name; 自动更新统计信息,持久化存储到tikv，会耗费一些内存开销,  --store  用来指定 TiDB 底层使用的存储引擎 默认: &amp;ldquo;goleveldb&amp;rdquo; 你可以选择 &amp;ldquo;memory&amp;rdquo;, &amp;ldquo;goleveldb&amp;rdquo;, &amp;ldquo;BoltDB&amp;rdquo; 或者 &amp;ldquo;TiKV&amp;rdquo;。（前面三个是本地存储引擎，而 TiKV 是一个分布式存储引擎） 例如，如果我们可以通过 tidb-server --store=memory 来启动一个纯内存引擎的 TiDB  --tcp-keep-alive  TiDB 在 tcp 层开启 keepalive 默认: false  Placement Driver (PD) --advertise-client-urls  对外客户端访问 URL 列表 默认: ${client-urls} 在某些情况下，譬如 docker，或者 NAT 网络环境，客户端并不能通过 PD 自己监听的 client URLs 来访问到 PD，这时候，你就可以设置 advertise urls 来让客户端访问 例如，docker 内部 IP 地址为 172.17.0.1，而宿主机的 IP 地址为 192.168.100.113 并且设置了端口映射 -p 2379:2379，那么可以设置为 --advertise-client-urls=&amp;ldquo;http://192.168.100.113:2379&amp;quot;，客户端可以通过 http://192.168.100.113:2379 来找到这个服务  --advertise-peer-urls  对外其他 PD 节点访问 URL 列表。 默认: ${peer-urls} 在某些情况下，譬如 docker，或者 NAT 网络环境，其他节点并不能通过 PD 自己监听的 peer URLs 来访问到 PD，这时候，你就可以设置 advertise urls 来让其他节点访问 例如，docker 内部 IP 地址为 172.17.0.1，而宿主机的 IP 地址为 192.168.100.113 并且设置了端口映射 -p 2380:2380，那么可以设置为 --advertise-peer-urls=&amp;ldquo;http://192.168.100.113:2380&amp;quot;，其他 PD 节点可以通过 http://192.168.100.113:2380 来找到这个服务  --client-urls  处理客户端请求监听 URL 列表 默认: &amp;ldquo;http://127.0.0.1:2379&amp;quot; 如果部署一个集群，--client-urls 必须指定当前主机的 IP 地址，例如 &amp;ldquo;http://192.168.100.113:2379&amp;quot;，如果是运行在 docker 则需要指定为 &amp;ldquo;http://0.0.0.0:2379&amp;quot;  --config  配置文件 默认: &amp;ldquo;&amp;rdquo; 如果你指定了配置文件，PD 会首先读取配置文件的配置。然后如果对应的配置在命令行参数里面也存在，PD 就会使用命令行参数的配置来覆盖配置文件里面的  --data-dir  PD 存储数据路径 默认: &amp;ldquo;default.${name}&amp;rdquo;  --initial-cluster  初始化 PD 集群配置。 默认: &amp;ldquo;{name}=http://{advertise-peer-url}&amp;rdquo; 例如，如果 name 是 &amp;ldquo;pd&amp;rdquo;, 并且 advertise-peer-urls 是 &amp;ldquo;http://192.168.100.113:2380&amp;quot;, 那么 initial-cluster 就是 pd=http://192.168.100.113:2380 如果你需要启动三台 PD，那么 initial-cluster 可能就是 pd1=http://192.168.100.113:2380, pd2=http://192.168.100.114:2380, pd3=192.168.100.115:2380  --join  动态加入 PD 集群 默认: &amp;ldquo;&amp;rdquo; 如果你想动态将一台 PD 加入集群，你可以使用 --join=&amp;quot;${advertise-client-urls}&amp;quot;， advertise-client-url 是当前集群里面任意 PD 的 advertise-client-url，你也可以使用多个 PD 的，需要用逗号分隔  -L  Log 级别 默认: &amp;ldquo;info&amp;rdquo; 我们能选择 debug, info, warn, error 或者 fatal  --log-file  Log 文件 默认: &amp;ldquo;&amp;rdquo; 如果没设置这个参数，log 会默认输出到 &amp;ldquo;stderr&amp;rdquo;，如果设置了，log 就会输出到对应的文件里面，在每天凌晨，log 会自动轮转使用一个新的文件，并且将以前的文件改名备份  --log-rotate  是否开启日志切割 默认：true 当值为 true 时,按照 PD 配置文件中 [log.file] 信息执行。  --name  当前 PD 的名字 默认: &amp;ldquo;pd&amp;rdquo; 如果你需要启动多个 PD，一定要给 PD 使用不同的名字  --peer-urls  处理其他 PD 节点请求监听 URL 列表。 default: &amp;ldquo;http://127.0.0.1:2380&amp;quot; 如果部署一个集群，--peer-urls 必须指定当前主机的 IP 地址，例如 &amp;ldquo;http://192.168.100.113:2380&amp;quot;，如果是运行在 docker 则需要指定为 &amp;ldquo;http://0.0.0.0:2380&amp;quot;  TiKV TiKV 在命令行参数上面支持一些可读性好的单位转换。 文件大小（以 bytes 为单位）: KB, MB, GB, TB, PB（也可以全小写） 时间（以毫秒为单位）: ms, s, m, h  -A, --addr  TiKV 监听地址 默认: &amp;ldquo;127.0.0.1:20160&amp;rdquo; 如果部署一个集群，--addr 必须指定当前主机的 IP 地址，例如 &amp;ldquo;192.168.100.113:20160&amp;rdquo;，如果是运行在 docker 则需要指定为 &amp;ldquo;0.0.0.0:20160&amp;rdquo;  --advertise-addr  TiKV 对外访问地址。 默认: ${addr} 在某些情况下，譬如 docker，或者 NAT 网络环境，客户端并不能通过 TiKV 自己监听的地址来访问到 TiKV，这时候，你就可以设置 advertise addr 来让 客户端访问 例如，docker 内部 IP 地址为 172.17.0.1，而宿主机的 IP 地址为 192.168.100.113 并且设置了端口映射 -p 20160:20160，那么可以设置为 --advertise-addr=&amp;ldquo;192.168.100.113:20160&amp;rdquo;，客户端可以通过 192.168.100.113:20160 来找到这个服务  -C, --config  配置文件 默认: &amp;ldquo;&amp;rdquo; 如果你指定了配置文件，TiKV 会首先读取配置文件的配置。然后如果对应的配置在命令行参数里面也存在，TiKV 就会使用命令行参数的配置来覆盖配置文件里面的  --capacity  TiKV 存储数据的容量 默认: 0 (无限) PD 需要使用这个值来对整个集群做 balance 操作。（提示：你可以使用 10GB 来替代 10737418240，从而简化参数的传递）  --data-dir  TiKV 数据存储路径 默认: &amp;ldquo;/tmp/tikv/store&amp;rdquo;  -L, --log  Log 级别 默认: &amp;ldquo;info&amp;rdquo; 我们能选择 trace, debug, info, warn, error, 或者 off  --log-file  Log 文件 默认: &amp;ldquo;&amp;rdquo; 如果没设置这个参数，log 会默认输出到 &amp;ldquo;stderr&amp;rdquo;，如果设置了，log 就会输出到对应的文件里面，在每天凌晨，log 会自动轮转使用一个新的文件，并且将以前的文件改名备份  --pd  PD 地址列表。 默认: &amp;ldquo;&amp;rdquo; TiKV 必须使用这个值连接 PD，才能正常工作。使用逗号来分隔多个 PD 地址，例如： 192.168.100.113:2379, 192.168.100.114:2379, 192.168.100.115:2379  "},
		{"url": "https://pingcap.com/recruit-cn/sales/presales-director/",
		"title": "售前技术总监", 
		"content": " 售前技术总监 岗位职责  带领售前团队 负责组织制定公司数据库产品、数据库解决方案的技术方案编写、标书的准备、讲解及用户答疑等工作 负责用户的技术交流、技术支持、POC 等工作 负责合作伙伴厂商的技术交流 和产品、社区、市场部门密切配合，负责相关的沟通、技术支持、技术文档撰写等工作  职位要求  8年以上 IT 领域售前工作经验 熟悉传统商业数据库（如 Oracle）及开源数据库，对云计算和大数据有深入的认识和实践 丰富的方案设计、标书应答、用户交流经验 良好的写作和口才、良好的沟通能力 工作条理性强，具有很强的责任心和团队合作精神 熟悉金融行业尤佳  待遇 25K - 35K , 13薪 + 业绩奖金，优秀者可面议工作地点 北京"},
		{"url": "https://pingcap.com/docs-cn/op-guide/backup-restore/",
		"title": "备份与恢复", 
		"content": " 备份与恢复 概述 该文档详细介绍了如何对 TiDB 进行备份恢复。本文档暂时只考虑全量备份与恢复。这里我们假定 TiDB 服务信息如下：   Name Address Port User Password     TiDB 127.0.0.1 4000 root *    在这个备份恢复过程中，我们会用到下面的工具: mydumper 从 TiDB 导出数据 loader 导入数据到 TiDB  下载 TiDB 工具集 (Linux) # 下载 tool 压缩包 wget http://download.pingcap.org/tidb-enterprise-tools-latest-linux-amd64.tar.gz wget http://download.pingcap.org/tidb-enterprise-tools-latest-linux-amd64.sha256 # 检查文件完整性，返回 ok 则正确 sha256sum -c tidb-enterprise-tools-latest-linux-amd64.sha256 # 解开压缩包 tar -xzf tidb-enterprise-tools-latest-linux-amd64.tar.gz cd tidb-enterprise-tools-latest-linux-amd64 使用 mydumper/loader 全量备份恢复数据 mydumper 是一个强大的数据备份工具，具体可以参考 https://github.com/maxbube/mydumper。我们使用 mydumper 从 TiDB 导出数据进行备份，然后用 loader 将其导入到 TiDB 里面进行恢复。 注意：虽然 TiDB 也支持使用 MySQL 官方的 mysqldump 工具来进行数据的备份恢复工作，但相比于 mydumper / loader，性能会慢很多，大量数据的备份恢复会花费很多时间，这里我们并不推荐。 mydumper/loader 全量备份恢复最佳实践 为了快速的备份恢复数据 (特别是数据量巨大的库), 可以参考下面建议 使用 mydumper 导出来的数据文件尽可能的小, 最好不要超过 64M, 可以设置参数 -F 64 loader的 -t 参数可以根据 tikv 的实例个数以及负载进行评估调整，例如 3个 tikv 的场景， 此值可以设为 3 *(1 ～ n)；当 tikv 负载过高，loader 以及 tidb 日志中出现大量 backoffer.maxSleep 15000ms is exceeded 可以适当调小该值，当 tikv 负载不是太高的时候，可以适当调大该值。  某次数据恢复示例，以及相关的配置  mydumper 导出后总数据量 214G，单表 8 列，20 亿行数据 集群拓扑  TIKV * 12 TIDB * 4 PD * 3  mydumper -F 设置为 16, loader -t 参数 64  结果：导入时间 11 小时左右，19.4 G/小时从 TiDB 备份数据 我们使用 mydumper 从 TiDB 备份数据，如下:./bin/mydumper -h 127.0.0.1 -P 4000 -u root -t 16 -F 64 -B test -T t1,t2 --skip-tz-utc -o ./var/test 上面，我们使用 -B test 表明是对 test 这个 database 操作，然后用 -T t1,t2 表明只导出 t1，t2 两张表。-t 16 表明使用 16 个线程去导出数据。-F 64 是将实际的 table 切分成多大的 chunk，这里就是 64MB 一个 chunk。--skip-tz-utc 添加这个参数忽略掉 TiDB 与导数据的机器之间时区设置不一致的情况，禁止自动转换。向 TiDB 恢复数据 我们使用 loader 将之前导出的数据导入到 TiDB，完成恢复操作。Loader 的下载和具体的使用方法见 Loader 使用文档./bin/loader -h 127.0.0.1 -u root -P 4000 -t 32 -d ./var/test 导入成功之后，我们可以用 MySQL 官方客户端进入 TiDB，查看:mysql -h127.0.0.1 -P4000 -uroot mysql&amp;gt; show tables; +----------------+ | Tables_in_test | +----------------+ | t1 | | t2 | +----------------+  mysql&amp;gt; select * from t1; +----+------+ | id | age | +----+------+ | 1 | 1 | | 2 | 2 | | 3 | 3 | +----+------+  mysql&amp;gt; select * from t2; +----+------+ | id | name | +----+------+ | 1 | a | | 2 | b | | 3 | c | +----+------+"},
		{"url": "https://pingcap.com/docs-cn/sql/string-functions/",
		"title": "字符串函数", 
		"content": " 字符串函数    函数名 功能描述     ASCII() 返回最左字符的数值   CHAR()  返回由整数的代码值所给出的字符组成的字符串    BIN() 返回一个数的二进制值的字符串表示   HEX() 返回十进制值或字符串值的十六进制表示   OCT() 返回一个数的八进制值的字符串表示   UNHEX() 返回 HEX 表示的数字所代表的字符串   TO_BASE64() 返回转换为 BASE64 的字符串参数   FROM_BASE64() 解码为 BASE64 的字符串并返回结果   LOWER() 返回小写字母的字符   LCASE() 与 LOWER() 功能相同   UPPER() 返回大写字母的字符   UCASE() 与 UPPER() 功能相同   LPAD()  返回左边由指定字符串填充的字符串参数    RPAD()  返回右边由指定字符串填充的字符串参数    TRIM() 删除字符串的前缀和后缀   LTRIM()  删除前面的空格字符    RTRIM() 删除结尾的空格字符   BIT_LENGTH()  返回字符串的位长度    CHAR_LENGTH()  返回字符串的字符长度    CHARACTER_LENGTH() 与 CHAR_LENGTH() 功能相同   LENGTH()  返回字符串的字节长度    OCTET_LENGTH() 与 LENGTH() 功能相同   INSERT() 在指定位置插入一个子字符串，直到指定的字符数   REPLACE() 替换指定的字符串   SUBSTR() 返回指定的子字符串   SUBSTRING() 返回指定的子字符串   SUBSTRING_INDEX() 返回最终定界符左边或右边的子字符串   MID() 返回从指定位置开始的子字符串   LEFT() 返回指定的最左字符   RIGHT() 返回指定的最右字符   INSTR() 返回子字符串的第一个出现位置   LOCATE() 返回子字符串的第一个出现位置，与 INSTR() 的参数位置相反   POSITION() 与 LOCATE() 功能相同   REPEAT() 返回重复指定次数的字符串   CONCAT() 返回连接的字符串   CONCAT_WS() 返回由分隔符连接的字符串   REVERSE() 返回和字符顺序相反的字符串   SPACE() 返回指定数目的空格组成的字符串   FIELD() 返回参数在后续参数中出现的第一个位置   ELT() 返回指定位置的字符串   EXPORT_SET()  返回一个字符串，其中值位中设置的每个位，可以得到一个 on 字符串，而每个未设置的位，可以得到一个 off 字符串    MAKE_SET()  返回一组逗号分隔的字符串，由位集合中具有相应位的字符串组成    FIND_IN_SET() 返回第一个参数在第二个参数中出现的位置   FORMAT() 返回指定小数位数格式的数字   ORD() 返回参数中最左字符的字符代码   QUOTE()  引用一个字符串，返回一个在 SQL 语句中可用作正确转义的数据值的结果    SOUNDEX() 返回一个 soundex 字符串   SOUNDS LIKE  按发音比较字符串     字符串比较函数    函数名 功能描述     LIKE 进行简单模式匹配   NOT LIKE 否定简单模式匹配   STRCMP() 比较两个字符串   MATCH 执行全文搜索    正则表达式    表达式名 功能描述     REGEXP 使用正则表达式进行模式匹配   RLIKE 与 REGEXP 功能相同   NOT REGEXP 否定 REGEXP    "},
		{"url": "https://pingcap.com/docs-cn/sql/literal-value-string-literals/",
		"title": "字符串字面值", 
		"content": " String Literals String Literals 是一个 bytes 或者 characters 的序列，两端被单引号 &#39; 或者双引号 &amp;quot; 包围，例如：&amp;#39;example string&amp;#39; &amp;#34;example string&amp;#34; 如果字符串是连续的，会被合并为一个独立的 string。以下表示是一样的：&amp;#39;a string&amp;#39; &amp;#39;a&amp;#39; &amp;#39; &amp;#39; &amp;#39;string&amp;#39; &amp;#34;a&amp;#34; &amp;#39; &amp;#39; &amp;#34;string&amp;#34; 如果 ANSI_QUOTES SQL MODE 开启了，那么只有单引号内的会被认为是 String Literals，对于双引号内的字符串，会被认为是一个 identifier。binary string 是一串 bytes 组成的字符串，每一个 binary string 有一个叫做 binary 的 character set 和 collation。一个非二进制的字符串是一个由字符组成的字符串，它有除 binary 外的 character set和与之兼容的 collation。对于两种字符串类型，比较都是基于每个字符的数值。对于 binary string 而言，比较单元就是字节，对于非二进制的字符串，那么单元就是字符，而有的字符集支持多字节字符。一个 String Literal 可以拥有一个可选的 character set introducer 和 COLLATE clause，可以用来指派特定的字符集跟 collation（TiDB 对此只是做了语法上的兼容，并不实质做处理)。[_charset_name]&amp;#39;string&amp;#39; [COLLATE collation_name] 例如：SELECT _latin1&amp;#39;string&amp;#39;; SELECT _binary&amp;#39;string&amp;#39;; SELECT _utf8&amp;#39;string&amp;#39; COLLATE utf8_bin; 你可以使用 N&amp;rsquo;literal&amp;rsquo; 或者 n&amp;rsquo;literal&amp;rsquo; 来创建使用 national character set 的字符串，下列语句是一样的：SELECT N&amp;#39;some text&amp;#39;; SELECT n&amp;#39;some text&amp;#39;; SELECT _utf8&amp;#39;some text&amp;#39;; 转义字符：| 转义序列 | 意义 | | :&amp;mdash;&amp;mdash;-: | :&amp;mdash;&amp;mdash;-:| | 0 | ASCII NUL (X&amp;rsquo;00&amp;rsquo;) 字符 | | &amp;rsquo; | 单引号 | | &amp;rdquo; | 双引号 | | b | 退格符号 | | n | 换行符 | | r | 回车符 | | t | tab 符（制表符）| | z | ASCII 26 (Ctrl + Z) | |  | 反斜杠  | | % | % | | _ | _ |如果要在 string literal 中使用 &#39; 或者 &amp;quot;，有以下几种办法： 在 &#39; 引用的字符串中，可以用 &#39;&#39; 来表示单引号。 在 &amp;quot; 引用的字符串中，可以用 &amp;quot;&amp;quot; 来表示双引号。 前面接转义符。 在 &#39; 中表示 &amp;quot; 或者在 &amp;quot; 中表示 &#39; 都不需要特别的处理。  更多细节。"},
		{"url": "https://pingcap.com/docs-cn/sql/character-set-support/",
		"title": "字符集支持", 
		"content": " 字符集支持 名词解释，下面的阐述中会交错使用中文或者英文，请互相对照： * Character Set：字符集 * Collation：排序规则目前 TiDB 支持以下字符集：mysql&amp;gt; SHOW CHARACTER SET; +---------|---------------|-------------------|--------+ | Charset | Description | Default collation | Maxlen | +---------|---------------|-------------------|--------+ | utf8 | UTF-8 Unicode | utf8_bin | 3 | | utf8mb4 | UTF-8 Unicode | utf8mb4_bin | 4 | | ascii | US ASCII | ascii_bin | 1 | | latin1 | Latin1 | latin1_bin | 1 | | binary | binary | binary | 1 | +---------|---------------|-------------------|--------+ 5 rows in set (0.00 sec) 注意：在 TiDB 中实际上 utf8 被当做成了 utf8mb4 来处理。对于字符集来说，至少会有一个 Collation（排序规则）与之对应。而大部分字符集实际上会有多个 Collation。利用以下的语句可以查看：mysql&amp;gt; SHOW COLLATION WHERE Charset = &amp;#39;latin1&amp;#39;; +-------------------|---------|------|---------|----------|---------+ | Collation | Charset | Id | Default | Compiled | Sortlen | +-------------------|---------|------|---------|----------|---------+ | latin1_german1_ci | latin1 | 5 | | Yes | 1 | | latin1_swedish_ci | latin1 | 8 | Yes | Yes | 1 | | latin1_danish_ci | latin1 | 15 | | Yes | 1 | | latin1_german2_ci | latin1 | 31 | | Yes | 1 | | latin1_bin | latin1 | 47 | | Yes | 1 | | latin1_general_ci | latin1 | 48 | | Yes | 1 | | latin1_general_cs | latin1 | 49 | | Yes | 1 | | latin1_spanish_ci | latin1 | 94 | | Yes | 1 | +-------------------|---------|------|---------|----------|---------+ 8 rows in set (0.00 sec) latin1 Collation（排序规则）分别有以下含义：   Collation 含义     latin1_bin latin1 编码的二进制表示   latin1_danish_ci 丹麦语/挪威语，不区分大小写   latin1_general_ci 多种语言的 (西欧)，不区分大小写   latin1_general_cs 多种语言的 (ISO 西欧)，区分大小写   latin1_german1_ci 德国 DIN-1 (字典序)，不区分大小写   latin1_german2_ci 德国 DIN-2，不区分大小写   latin1_spanish_ci 现代西班牙语，不区分大小写   latin1_swedish_ci 瑞典语/芬兰语，不区分大小写    每一个字符集，都有一个默认的 Collation，例如 utf8 的默认 Collation 就为 utf8_bin。注意 TiDB 目前的 Collation 都是区分大小写的。Collation 命名规则 TiDB 的 Collation 遵循着如下的命名规则： Collation 的前缀是它相应的字符集，通常之后会跟着一个或者更多的后缀来表名其他的排序规则， 例如：utf8_general_ci 和 lation1_swedish_ci 是 utf8 和 latin1 字符集的 Collation。但是 binary 字符集只有一个 Collation，就是 binary。 一个语言对应的 Collation 会包含语言的名字，例如 utf8_turkish_ci 和 utf8_hungarian_ci 是依据 Turkish(土耳其语) 和 Hungarian(匈牙利语) 的排序规则来排序。 Collation 的后缀表示了 Collation 是否区分大小写和是否区分口音。下面的表展示了这些特性：     后缀 含义     _ai 口音不敏感（Accent insensitive）   _as 口音敏感 （Accent insensitive）   _ci 大小写不敏感   _cs 大小写敏感     注意：目前为止 TiDB 只支持部分以上提到的 Collation。 数据库 Character Set 和 Collation 每个数据库都有相应的 Character Set 和 Collation，CREATE DATABASE 可以指定数据库的字符集和排序规则：CREATE DATABASE db_name [[DEFAULT] CHARACTER SET charset_name] [[DEFAULT] COLLATE collation_name] 在这里 SCHEMA 可以跟 DATABASE 互换使用。不同的数据库之间可以使用不一样的字符集和排序规则。通过系统变量 character_set_database 和 collation_database 可以查看到当前数据库的字符集以及排序规则：mysql&amp;gt; create schema test1 character set utf8 COLLATE uft8_general_ci; Query OK, 0 rows affected (0.09 sec) mysql&amp;gt; use test1; Database changed mysql&amp;gt; SELECT @@character_set_database, @@collation_database; +--------------------------|----------------------+ | @@character_set_database | @@collation_database | +--------------------------|----------------------+ | utf8 | uft8_general_ci | +--------------------------|----------------------+ 1 row in set (0.00 sec) mysql&amp;gt; create schema test2 character set latin1 COLLATE latin1_general_ci; Query OK, 0 rows affected (0.09 sec) mysql&amp;gt; use test2; Database changed mysql&amp;gt; SELECT @@character_set_database, @@collation_database; +--------------------------|----------------------+ | @@character_set_database | @@collation_database | +--------------------------|----------------------+ | latin1 | latin1_general_ci | +--------------------------|----------------------+ 1 row in set (0.00 sec) 在 INFORMATION_SCHEMA 中也可以查看到这两个值：SELECT DEFAULT_CHARACTER_SET_NAME, DEFAULT_COLLATION_NAME FROM INFORMATION_SCHEMA.SCHEMATA WHERE SCHEMA_NAME = &amp;#39;db_name&amp;#39;; 表的 Character Set 和 Collation 表的 Character Set 和 Collation 可以通过以下语句来设置：CREATE TABLE tbl_name (column_list) [[DEFAULT] CHARACTER SET charset_name] [COLLATE collation_name]] ALTER TABLE tbl_name [[DEFAULT] CHARACTER SET charset_name] [COLLATE collation_name] 例如：mysql&amp;gt; CREATE TABLE t1(a int) CHARACTER SET utf8 COLLATE utf8_general_ci; Query OK, 0 rows affected (0.08 sec) 如果 Column 的字符集和排序规则没有设置，那么表的字符集和排序规则就作为其默认值。列的 Character Set 和 Collation 列的 Character Set 和 Collation 的语法如下：col_name {CHAR | VARCHAR | TEXT} (col_length) [CHARACTER SET charset_name] [COLLATE collation_name] col_name {ENUM | SET} (val_list) [CHARACTER SET charset_name] [COLLATE collation_name] 客户端连接的 Character Sets 和 Collations  服务器的字符集和排序规则可以通过系统变量 character_set_server 和 collation_server 获取。 数据库的字符集和排序规则可以通过环境变量 character_set_database 和 collation_database 获取。  对于每一个客户端的连接，也有相应的变量表示字符集和排序规则：character_set_connection 和 collation_connection。character_set_client 代表客户端的字符集。在返回结果前，服务端会把结果根据 character_set_results 转换成对应的字符集。包括结果的元信息等。可以用以下的语句来影响这些跟客户端相关的字符集变量： SET NAMES &#39;charset_name&#39; [COLLATE &#39;collation_name&#39;]  SET NAMES 用来设定客户端会在之后的请求中使用的字符集。SET NAMES utf8 表示客户端会在接下来的请求中，都使用 utf8 字符集。服务端也会在之后返回结果的时候使用 utf8 字符集。 SET NAMES &#39;charset_name&#39; 语句其实等于下面语句的组合：SET character_set_client = charset_name; SET character_set_results = charset_name; SET character_set_connection = charset_name; COLLATE 是可选的，如果没有提供，将会用 charset_name 默认的 Collation。 SET CHARACTER SET &#39;charset_name&#39;  跟 SET NAMES 类似，等价于下面语句的组合：SET character_set_client = charset_name; SET character_set_results = charset_name; SET collation_connection = @@collation_database; 更多细节，参考 Connection Character Sets and Collations。"},
		{"url": "https://pingcap.com/docs-cn/sql/character-set-configuration/",
		"title": "字符集配置", 
		"content": " 字符集配置 目前 TiDB 还没有相应的配置来设置字符集，默认为 utf8。更多细节"},
		{"url": "https://pingcap.com/docs-cn/sql/literal-values/",
		"title": "字面值", 
		"content": " 字面值 String Literals String Literals 是一个 bytes 或者 characters 的序列，两端被单引号 &#39; 或者双引号 &amp;quot; 包围，例如：&amp;#39;example string&amp;#39; &amp;#34;example string&amp;#34; 如果字符串是连续的，会被合并为一个独立的 string。以下表示是一样的：&amp;#39;a string&amp;#39; &amp;#39;a&amp;#39; &amp;#39; &amp;#39; &amp;#39;string&amp;#39; &amp;#34;a&amp;#34; &amp;#39; &amp;#39; &amp;#34;string&amp;#34; 如果 ANSI_QUOTES SQL MODE 开启了，那么只有单引号内的会被认为是 String Literals，对于双引号内的字符串，会被认为是一个 identifier。binary string 是一串 bytes 组成的字符串，每一个 binary string 有一个叫做 binary 的 character set 和 collation。一个非二进制的字符串是一个由字符组成的字符串，它有除 binary 外的 character set和与之兼容的 collation。对于两种字符串类型，比较都是基于每个字符的数值。对于 binary string 而言，比较单元就是字节，对于非二进制的字符串，那么单元就是字符，而有的字符集支持多字节字符。一个 String Literal 可以拥有一个可选的 character set introducer 和 COLLATE clause，可以用来指派特定的字符集跟 collation（TiDB 对此只是做了语法上的兼容，并不实质做处理)。[_charset_name]&amp;#39;string&amp;#39; [COLLATE collation_name] 例如：SELECT _latin1&amp;#39;string&amp;#39;; SELECT _binary&amp;#39;string&amp;#39;; SELECT _utf8&amp;#39;string&amp;#39; COLLATE utf8_bin; 你可以使用 N&amp;rsquo;literal&amp;rsquo; 或者 n&amp;rsquo;literal&amp;rsquo; 来创建使用 national character set 的字符串，下列语句是一样的：SELECT N&amp;#39;some text&amp;#39;; SELECT n&amp;#39;some text&amp;#39;; SELECT _utf8&amp;#39;some text&amp;#39;; 转义字符： 0: ASCII NUL (X&amp;rsquo;00&amp;rsquo;) 字符 &amp;lsquo;: 单引号 &amp;ldquo;: 双引号 b: 退格符号 n: 换行符 r: 回车符 t: tab 符（制表符） z: ASCII 26 (Ctrl + Z) : 反斜杠  %: % _: _  如果要在 string literal 中使用 &#39; 或者 &amp;quot;，有以下几种办法： 在 &#39; 引用的字符串中，可以用 &#39;&#39; 来表示单引号。 在 &amp;quot; 引用的字符串中，可以用 &amp;quot;&amp;quot; 来表示双引号。 前面接转义符。 在 &#39; 中表示 &amp;quot; 或者在 &amp;quot; 中表示 &#39; 都不需要特别的处理。  更多细节。Numeric Literals 数值字面值包括 integer 跟 Decimal 类型跟浮点数字面值。integer 可以包括 . 作为小数点分隔，数字前可以有 - 或者 + 来表示正数或者负数。精确数值字面值可以表示为如下格式：1, .2, 3.4, -5, -6.78, +9.10.科学记数法也是被允许的，表示为如下格式：1.2E3, 1.2E-3, -1.2E3, -1.2E-3。更多细节。NULL Values NULL 代表数据为空，它是大小写不敏感的，与 N(大小写敏感) 同义。需要注意的是 NULL 跟 0 并不一样，跟空字符串 &#39;&#39; 也不一样。Hexadecimal Literals 十六进制字面值是有 X 和 0x 前缀的字符串，后接表示十六进制的数字。注意 0x 是大小写敏感的，不能表示为 0X。例:X&amp;#39;ac12&amp;#39; X&amp;#39;12AC&amp;#39; x&amp;#39;ac12&amp;#39; x&amp;#39;12AC&amp;#39; 0xac12 0x12AC 以下是不合法的十六进制字面值：X&amp;#39;1z&amp;#39; (z 不是合法的十六进制值) 0X12AC (0X 必须用小写的 0x) 对于使用 X&#39;val&#39; 格式的十六进制字面值，val 必须要有一个数字，可以在前面补一个 0 来避免语法错误。mysql&amp;gt; select X&amp;#39;aff&amp;#39;; ERROR 1105 (HY000): line 0 column 13 near &amp;#34;&amp;#34;hex literal: invalid hexadecimal format, must even numbers, but 3 (total length 13) mysql&amp;gt; select X&amp;#39;0aff&amp;#39;; +---------+ | X&amp;#39;0aff&amp;#39; | +---------+ | | +---------+ 1 row in set (0.00 sec) 默认情况，十六进制字面值是一个二进制字符串。如果需要将一个字符串或者数字转换为十六进制字面值，可以使用内建函数 HEX()：mysql&amp;gt; SELECT HEX(&amp;#39;TiDB&amp;#39;); +-------------+ | HEX(&amp;#39;TiDB&amp;#39;) | +-------------+ | 54694442 | +-------------+ 1 row in set (0.01 sec) mysql&amp;gt; SELECT X&amp;#39;54694442&amp;#39;; +-------------+ | X&amp;#39;54694442&amp;#39; | +-------------+ | TiDB | +-------------+ 1 row in set (0.00 sec) Date and Time Literals Date 跟 Time 字面值有几种格式，例如用字符串表示，或者直接用数字表示。在 TiDB 里面，当 TiDB 期望一个 Date 的时候，它会把 &#39;2017-08-24&#39;， &#39;20170824&#39;，20170824 当做是 Date。TiDB 的 Date 值有以下几种格式： &#39;YYYY-MM-DD&#39; 或者 &#39;YY-MM-DD&#39;，这里的 - 分隔符并不是严格的，可以是任意的标点符号。比如 &#39;2017-08-24&#39;，&#39;2017&amp;amp;08&amp;amp;24&#39;， &#39;2012@12^31&#39; 都是一样的。唯一需要特别对待的是 &amp;lsquo;.&amp;rsquo; 号，它被当做是小数点，用于分隔整数和小数部分。 Date 和 Time 部分可以被 &amp;rsquo;T&amp;rsquo; 分隔，它的作用跟空格符是一样的，例如 2017-8-24 10:42:00 跟 2017-8-24T10:42:00 是一样的。 &#39;YYYYMMDDHHMMSS&#39; 或者 &#39;YYMMDDHHMMSS&#39;，例如 &#39;20170824104520&#39; 和 &#39;170824104520&#39; 被当做是 &#39;2017-08-24 10:45:20&#39;，但是如果你提供了一个超过范围的值，例如&#39;170824304520&#39;，那这就不是一个有效的 Date 字面值。 YYYYMMDDHHMMSS 或者 YYMMDDHHMMSS 注意这里没有单引号或者双引号，是一个数字。例如 20170824104520表示为 &#39;2017-08-24 10:45:20&#39;。  DATETIME 或者 TIMESTAMP 值可以接一个小数部分，用来表示微秒（精度最多到小数点后 6 位），用小数点 . 分隔。Dates 如果 year 部分只有两个数字，这是有歧义的（推荐使用四个数字的格式），TiDB 会尝试用以下的规则来解释： year 值如果在 70-99 范围，那么被转换成 1970-1999。 year 值如果在 00-69 范围，那么被转换成 2000-2069。  对于小于 10 的 month 或者 day 值，&#39;2017-8-4&#39; 跟 &#39;2017-08-04&#39; 是一样的。对于 Time 也是一样，比如 &#39;2017-08-24 1:2:3&#39; 跟 &#39;2017-08-24 01:02:03&#39;是一样的。在需要 Date 或者 Time 的语境下, 对于数值，TiDB 会根据数值的长度来选定指定的格式： 6 个数字，会被解释为 YYMMDD。 12 个数字，会被解释为 YYMMDDHHMMSS。 8 个数字，会解释为 YYYYMMDD。 14 个数字，会被解释为 YYYYMMDDHHMMSS。  对于 Time 类型，TiDB 用以下格式来表示： &#39;D HH:MM:SS&#39;，或者 &#39;HH:MM:SS&#39;，&#39;HH:MM&#39;，&#39;D HH:MM&#39;，&#39;D HH&#39;，&#39;SS&#39;，这里的 D 表示 days，合法的范围是 0-34。 数值 HHMMSS，例如 231010 被解释为&#39;23:10:10&#39;。 数值 SS，MMSS，HHMMSS 都是可以被当做 Time。  Time 类型的小数点也是 .，精度最多小数点后 6 位。更多细节。Boolean Literals 常量 TRUE 和 FALSE 等于 1 和 0，它是大小写不敏感的。mysql&amp;gt; SELECT TRUE, true, tRuE, FALSE, FaLsE, false; +------+------+------+-------+-------+-------+ | TRUE | true | tRuE | FALSE | FaLsE | false | +------+------+------+-------+-------+-------+ | 1 | 1 | 1 | 0 | 0 | 0 | +------+------+------+-------+-------+-------+ 1 row in set (0.00 sec) Bit-Value Literals 位值字面值用 b 或者 0b 做前缀，后接以 0 跟 1 组成的二进制数字。其中 0b 是区分大小写的，0B 是会报错的。合法的 Bit-value： b&amp;rsquo;01&amp;rsquo; B&amp;rsquo;01&amp;rsquo; 0b01  非法的 Bit-value： b&amp;rsquo;2&amp;rsquo; (2 不是二进制数值, 必须为 0 或 1) 0B01 (0B 必须是小写 0b)  默认情况，位值字面值是一个二进制字符串。Bit-value 是作为二进制返回的，所以输出到 MySQL Client 可能会显示不出来，如果要转换为可打印的字符，可以使用内建函数 BIN() 或者 HEX()：CREATE TABLE t (b BIT(8)); INSERT INTO t SET b = b&amp;#39;00010011&amp;#39;; INSERT INTO t SET b = b&amp;#39;1110&amp;#39;; INSERT INTO t SET b = b&amp;#39;100101&amp;#39;; mysql&amp;gt; SELECT b+0, BIN(b), HEX(b) FROM t; +------+--------+--------+ | b+0 | BIN(b) | HEX(b) | +------+--------+--------+ | 19 | 10011 | 13 | | 14 | 1110 | E | | 37 | 100101 | 25 | +------+--------+--------+ 3 rows in set (0.00 sec)"},
		{"url": "https://pingcap.com/docs-cn/sql/util/",
		"title": "实用工具语句", 
		"content": " DESCRIBE 语句 DESCRIBE 和 EXPLAIN 是同义词，另外还可以缩写为 DESC。请参考 EXPLAIN 语句的用法。EXPLAIN 语句 {EXPLAIN | DESCRIBE | DESC} tbl_name [col_name] {EXPLAIN | DESCRIBE | DESC} [explain_type] explainable_stmt explain_type: FORMAT = format_name format_name: &amp;#34;DOT&amp;#34; explainable_stmt: { SELECT statement | DELETE statement | INSERT statement | REPLACE statement | UPDATE statement } EXPLAIN 语句详细信息参考理解 TiDB 执行计划章节。除了 MySQL 标准的结果格式之外，TiDB 还支持输出 DotGraph 结果，这时需要指定 FORMAT = &amp;quot;dot&amp;quot;，示例如下：create table t(a bigint, b bigint); desc format = &amp;#34;dot&amp;#34; select A.a, B.b from t A join t B on A.a &amp;gt; B.b where A.a &amp;lt; 10; TiDB &amp;gt; desc format = &amp;#34;dot&amp;#34; select A.a, B.b from t A join t B on A.a &amp;gt; B.b where A.a &amp;lt; 10;desc format = &amp;#34;dot&amp;#34; select A.a, B.b from t A join t B on A.a &amp;gt; B.b where A.a &amp;lt; 10; +--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | dot contents | +--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | digraph HashRightJoin_7 { subgraph cluster7{ node [style=filled, color=lightgrey] color=black label = &amp;#34;root&amp;#34; &amp;#34;HashRightJoin_7&amp;#34; -&amp;gt; &amp;#34;TableReader_10&amp;#34; &amp;#34;HashRightJoin_7&amp;#34; -&amp;gt; &amp;#34;TableReader_12&amp;#34; } subgraph cluster9{ node [style=filled, color=lightgrey] color=black label = &amp;#34;cop&amp;#34; &amp;#34;Selection_9&amp;#34; -&amp;gt; &amp;#34;TableScan_8&amp;#34; } subgraph cluster11{ node [style=filled, color=lightgrey] color=black label = &amp;#34;cop&amp;#34; &amp;#34;TableScan_11&amp;#34; } &amp;#34;TableReader_10&amp;#34; -&amp;gt; &amp;#34;Selection_9&amp;#34; &amp;#34;TableReader_12&amp;#34; -&amp;gt; &amp;#34;TableScan_11&amp;#34; } | +--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ 1 row in set (0.00 sec) 如果电脑上安装了 dot 程序 (包含在 graphviz 软件包中)，可以通过如下方式生成 PNG 文件：dot xx.dot -T png -O 这里的 xx.dot 是上面的语句返回结果。 如果没有安装 dot，可以将结果拷贝到这个网站，可以得到一个树状图：USE 语句 USE db_name 切换默认 Database，当 SQL 语句中的表没有显示指定 Database 时，即使用默认 Database。"},
		{"url": "https://pingcap.com/recruit-cn/market/operation-manager/",
		"title": "市场运营", 
		"content": " 市场运营 岗位职责  社区活动的维护运营，包括活动的主题策划、内容统筹、人员沟通、现场执行等运营工作； 企业自媒体平台的日常运营，包括内容编辑、发布、维护、管理、互动、提高影响力和关注度； 了解技术社区用户需求，收集反馈，根据运营数据挖掘和分析用户需求； 资料的搜集与编辑整理。  职位要求：  对 toB 的商业和市场具备一定的感知，了解 toB 或技术社区类运营的特点和调性； 有亲和力，具有较强的表达与理解能力以及极强的团队合作意识，善于主动发现问题并及时沟通并解决； 认真负责，逻辑清晰，有良好的文字和语言表达能力； 性格开朗，积极热情，能够快速学习。  待遇： 8K -15K，13薪 + 奖金，优秀者可面议工作地点 北京"},
		{"url": "https://pingcap.com/docs-cn/op-guide/security/",
		"title": "开启 TLS 验证", 
		"content": " 开启 TLS 验证 概述 本文档介绍 TiDB 集群如何开启 TLS 验证，其支持： TiDB 组件之间的双向验证，包括 TiDB、TiKV、PD 相互之间，TiKV Control 与 TiKV、PD Control 与 PD 的双向认证，以及 TiKV peer 之间、PD peer 之间。一旦开启，所有组件之间均使用验证，不支持只开启某一部分的验证。 MySQL Client 与 TiDB 之间的客户端对服务器身份的单向验证以及双向验证。  MySQL Client 与 TiDB 之间使用一套证书，TiDB 集群组件之间使用另外一套证书。TiDB 集群组件间开启 TLS（双向认证） 准备证书 推荐为 TiDB、TiKV、PD 分别准备一个 server 证书，并保证可以相互验证，而它们的各种客户端共用 client 证书。有多种工具可以生成自签名证书，如 openssl，easy-rsa，cfssl。这里提供一个使用 cfssl 生成证书的示例：生成自签名证书。配置证书 TiDB 在 config 文件或命令行参数中设置：[security] # Path of file that contains list of trusted SSL CAs for connection with cluster components. cluster-ssl-ca = &amp;#34;/path/to/ca.pem&amp;#34; # Path of file that contains X509 certificate in PEM format for connection with cluster components. cluster-ssl-cert = &amp;#34;/path/to/tidb-server.pem&amp;#34; # Path of file that contains X509 key in PEM format for connection with cluster components. cluster-ssl-key = &amp;#34;/path/to/tidb-server-key.pem&amp;#34; TiKV 在 config 文件或命令行参数中设置，并设置相应 url 为 https：[security] # set the path for certificates. Empty string means disabling secure connectoins. ca-path = &amp;#34;/path/to/ca.pem&amp;#34; cert-path = &amp;#34;/path/to/client.pem&amp;#34; key-path = &amp;#34;/path/to/client-key.pem&amp;#34; PD 在 config 文件或命令行参数中设置，并设置相应 url 为 https：[security] # Path of file that contains list of trusted SSL CAs. if set, following four settings shouldn&amp;#39;t be empty cacert-path = &amp;#34;/path/to/ca.pem&amp;#34; # Path of file that contains X509 certificate in PEM format. cert-path = &amp;#34;/path/to/server.pem&amp;#34; # Path of file that contains X509 key in PEM format. key-path = &amp;#34;/path/to/server-key.pem&amp;#34; 此时 TiDB 集群各个组件间便开启了双向验证。在使用客户端连接时，需要指定 client 证书，示例：./pd-ctl -u https://127.0.0.1:2379 --cacert /path/to/ca.pem --cert /path/to/pd-client.pem --key /path/to/pd-client-key.pem ./tikv-ctl --host=&amp;#34;127.0.0.1:20160&amp;#34; --ca-path=&amp;#34;/path/to/ca.pem&amp;#34; --cert-path=&amp;#34;/path/to/client.pem&amp;#34; --key-path=&amp;#34;/path/to/clinet-key.pem&amp;#34; MySQL 与 TiDB 间开启 TLS 准备证书 mysql_ssl_rsa_setup --datadir=certs 配置单向认证 在 TiDB 的 config 文件或命令行参数中设置：[security] # Path of file that contains list of trusted SSL CAs. ssl-ca = &amp;#34;&amp;#34; # Path of file that contains X509 certificate in PEM format. ssl-cert = &amp;#34;/path/to/certs/server.pem&amp;#34; # Path of file that contains X509 key in PEM format. ssl-key = &amp;#34;/path/to/certs/server-key.pem&amp;#34; 客户端mysql -u root --host 127.0.0.1 --port 4000 --ssl-mode=REQUIRED 配置双向认证 在 TiDB 的 config 文件或命令行参数中设置：[security] # Path of file that contains list of trusted SSL CAs for connection with mysql client. ssl-ca = &amp;#34;/path/to/certs/ca.pem&amp;#34; # Path of file that contains X509 certificate in PEM format for connection with mysql client. ssl-cert = &amp;#34;/path/to/certs/server.pem&amp;#34; # Path of file that contains X509 key in PEM format for connection with mysql client. ssl-key = &amp;#34;/path/to/certs/server-key.pem&amp;#34; 客户端需要指定 client 证书mysql -u root --host 127.0.0.1 --port 4000 --ssl-cert=/path/to/certs/client-cert.pem --ssl-key=/path/to/certs/client-key.pem --ssl-ca=/path/to/certs/ca.pem --ssl-mode=VERIFY_IDENTITY"},
		{"url": "https://pingcap.com/docs-cn/QUICKSTART/",
		"title": "快速入门指南", 
		"content": " TiDB 快速入门指南 关于 TiDB TiDB 是开源分布式 SQL 数据库，结合了传统的 RDBMS 和 NoSQL 的最佳特性。TiDB 兼容 MySQL，支持无限的水平扩展，具备强一致性和高可用性。TiDB 的目标是为在线事务和分析提供一站式的解决方案。关于本指南 本指南为您介绍如何使用 TiDB-Ansible 快速部署一个 TiDB 集群，并了解 TiDB 的基本操作和管理。TiDB 集群部署 本节具体介绍如何部署一个 TiDB 集群。一个 TiDB 集群由不同的模块组成，包括：TiDB 服务器、TiKV 服务器、Placement Driver (PD) 服务器。架构图如下所示：参考TiDB Ansible 部署方案。TiDB 基本操作 本节具体介绍 TiDB 中基本的增删改查操作。创建、查看和删除数据库 使用 CREATE DATABASE 语句创建数据库。语法如下：CREATE DATABASE db_name [options]; 例如，要创建一个名为 samp_db 的数据库，可使用以下语句：CREATE DATABASE IF NOT EXISTS samp_db; 使用 SHOW DATABASES 语句查看数据库：SHOW DATABASES; 使用 DROP DATABASE 语句删除数据库，例如：DROP DATABASE samp_db; 创建、查看和删除表 使用 CREATE TABLE 语句创建表。语法如下：CREATE TABLE table_name column_name data_type constraint; 例如：CREATE TABLE person ( number INT(11), name VARCHAR(255), birthday DATE ); 如果表已存在，添加 IF NOT EXISTS 可防止发生错误：CREATE TABLE IF NOT EXISTS person ( number INT(11), name VARCHAR(255), birthday DATE ); 使用 SHOW CREATE 语句查看建表语句。例如：SHOW CREATE table person; 使用 SHOW FULL COLUMNS 语句查看表的列。 例如：SHOW FULL COLUMNS FROM person; 使用 DROP TABLE 语句删除表。例如：DROP TABLE person; 或者DROP TABLE IF EXISTS person; 使用 SHOW TABLES 语句查看数据库中的所有表。例如：SHOW TABLES FROM samp_db; 创建、查看和删除索引 对于值不唯一的列，可使用 CREATE INDEX 或 ALTER TABLE 语句。例如：CREATE INDEX person_num ON person (number); 或者ALTER TABLE person ADD INDEX person_num (number); 对于值唯一的列，可以创建唯一索引。例如：CREATE UNIQUE INDEX person_num ON person (number); 或者ALTER TABLE person ADD UNIQUE person_num on (number); 使用 SHOW INDEX 语句查看表内所有索引：SHOW INDEX from person; 使用 ALTER TABLE 或 DROP INDEX 语句来删除索引。与 CREATE INDEX 语句类似，DROP INDEX 也可以嵌入 ALTER TABLE 语句。例如：DROP INDEX person_num ON person; ALTER TABLE person DROP INDEX person_num; 增删改查数据 使用 INSERT 语句向表内插入数据。例如：INSERT INTO person VALUES(&amp;#34;1&amp;#34;,&amp;#34;tom&amp;#34;,&amp;#34;20170912&amp;#34;); 使用 SELECT 语句检索表内数据。例如：SELECT * FROM person; +--------+------+------------+ | number | name | birthday | +--------+------+------------+ | 1 | tom | 2017-09-12 | +--------+------+------------+ 使用 UPDATE 语句修改表内数据。例如：UPDATE person SET birthday=&amp;#39;20171010&amp;#39; WHERE name=&amp;#39;tom&amp;#39;; SELECT * FROM person; +--------+------+------------+ | number | name | birthday | +--------+------+------------+ | 1 | tom | 2017-10-10 | +--------+------+------------+ 使用 DELETE 语句删除表内数据：DELETE FROM person WHERE number=1; SELECT * FROM person; Empty set (0.00 sec) 创建、授权和删除用户 使用 CREATE USER 语句创建一个用户 tiuser，密码为 123456：CREATE USER &amp;#39;tiuser&amp;#39;@&amp;#39;localhost&amp;#39; IDENTIFIED BY &amp;#39;123456&amp;#39;; 授权用户 tiuser 可检索数据库 samp_db 内的表：GRANT SELECT ON samp_db .* TO &amp;#39;tiuser&amp;#39;@&amp;#39;localhost&amp;#39;; 查询用户 tiuser 的权限：SHOW GRANTS for tiuser@localhost; 删除用户 tiuser：DROP USER &amp;#39;tiuser&amp;#39;@&amp;#39;localhost&amp;#39;; TiDB 集群监控 打开浏览器，访问以下监控平台：地址：http://172.16.10.3:3000， 默认帐户和密码为：admin@admin。重要监控指标详解  PD Storage Capacity : TiDB 集群总可用数据库空间大小 Current Storage Size : TiDB 集群目前已用数据库空间大小 Store Status &amp;ndash; up store : TiKV 正常节点数量 Store Status &amp;ndash; down store : TiKV 异常节点数量如果大于 0，证明有节点不正常 Store Status &amp;ndash; offline store : 手动执行下线操作 TiKV 节点数量 Store Status &amp;ndash; Tombstone store : 下线成功的 TiKV 节点数量 Current storage usage : TiKV 集群存储空间占用率超过 80% 应考虑添加 TiKV 节点 99% completed_cmds_duration_seconds : 99% pd-server 请求完成时间小于 5ms average completed_cmds_duration_seconds : pd-server 请求平均完成时间小于 50ms leader balance ratio : leader ratio 最大的节点与最小的节点的差均衡状况下一般小于 5%，节点重启时会比较大 region balance ratio : region ratio 最大的节点与最小的节点的差均衡状况下一般小于 5%，新增/下线节点时会比较大  TiDB handle_requests_duration_seconds : 请求 PD 获取 TSO 响应时间小于 100ms tidb server QPS : 集群的请求量 connection count : 从业务服务器连接到数据库的连接数和业务相关。但是如果连接数发生跳变，需要查明原因。比如突然掉为 0，可以检查网络是否中断； 如果突然上涨，需要检查业务。 statement count : 单位时间内不同类型语句执行的数目 Query Duration 99th percentile : 99% 的 query 时间  TiKV 99% &amp;amp; 99.99% scheduler command duration : 99% &amp;amp; 99.99% 命令执行的时间99% 小于 50ms；99.99% 小于 100ms 95% &amp;amp; 99% storage async_request duration : 95% &amp;amp; 99% Raft 命令执行时间95% 小于 50ms；99% 小于 100ms server report failure message : 发送失败或者收到了错误的 message如果出现了大量的 unreachadble 的消息，表明系统网络出现了问题。如果有 store not match 这样的错误， 表明收到了不属于这个集群发过来的消息 Vote : Raft vote 的频率通常这个值只会在发生 split 的时候有变动，如果长时间出现了 vote 偏高的情况，证明系统出现了严重的问题， 有一些节点无法工作了 95% &amp;amp; 99% coprocessor request duration : 95% &amp;amp; 99% coprocessor 执行时间和业务相关，但通常不会出现持续高位的值 Pending task : 累积的任务数量除了 pd worker，其他任何偏高都属于异常 stall : RocksDB Stall 时间大于 0，表明 RocksDB 忙不过来，需要注意 IO 和 CPU 了 channel full : channel 满了，表明线程太忙无法处理如果大于 0，表明线程已经没法处理了 95% send_message_duration_seconds : 95% 发送消息的时间小于 50ms leader/region : 每个 TiKV 的 leader/region 数量   TiDB 集群扩容缩容方案 TiDB 集群可以在不影响线上服务的情况下进行扩容和缩容。以下缩容示例中，被移除的节点没有混合部署其他服务；如果混合部署了其他服务，不能按如下操作。假设拓扑结构如下所示：   Name Host IP Services     node1 172.16.10.1 PD1   node2 172.16.10.2 PD2   node3 172.16.10.3 PD3, Monitor   node4 172.16.10.4 TiDB1   node5 172.16.10.5 TiDB2   node6 172.16.10.6 TiKV1   node7 172.16.10.7 TiKV2   node8 172.16.10.8 TiKV3   node9 172.16.10.9 TiKV4    扩容 TiDB/TiKV 节点 例如，如果要添加两个 TiDB 节点 (node101、node102)，IP 地址为 172.16.10.101、172.16.10.102，可以进行如下操作： 编辑 inventory.ini 文件，添加节点信息：[tidb_servers] 172.16.10.4 172.16.10.5 172.16.10.101 172.16.10.102 [pd_servers] 172.16.10.1 172.16.10.2 172.16.10.3 [tikv_servers] 172.16.10.6 172.16.10.7 172.16.10.8 172.16.10.9 [monitored_servers] 172.16.10.1 172.16.10.2 172.16.10.3 172.16.10.4 172.16.10.5 172.16.10.6 172.16.10.7 172.16.10.8 172.16.10.9 172.16.10.101 172.16.10.102 [monitoring_servers] 172.16.10.3 [grafana_servers] 172.16.10.3 现在拓扑结构如下所示：   Name Host IP Services     node1 172.16.10.1 PD1   node2 172.16.10.2 PD2   node3 172.16.10.3 PD3, Monitor   node4 172.16.10.4 TiDB1   node5 172.16.10.5 TiDB2   node101 172.16.10.101 TiDB3   node102 172.16.10.102 TiDB4   node6 172.16.10.6 TiKV1   node7 172.16.10.7 TiKV2   node8 172.16.10.8 TiKV3   node9 172.16.10.9 TiKV4    初始化新增节点：ansible-playbook bootstrap.yml -l 172.16.10.101,172.16.10.102 部署新增节点：ansible-playbook deploy.yml -l 172.16.10.101,172.16.10.102 启动新节点服务：ansible-playbook start.yml -l 172.16.10.101,172.16.10.102 更新 Prometheus 配置并重启：ansible-playbook rolling_update_monitor.yml --tags=prometheus 打开浏览器访问监控平台：http://172.16.10.3:3000，监控整个集群和新增节点的状态。  可使用同样的步骤添加 TiKV 节点。但如果要添加 PD 节点，则需手动更新一些配置文件。扩容 PD 节点 例如，如果要添加一个 PD 节点 (node103)，IP 地址为 172.16.10.103，可以进行如下操作： 编辑 inventory.ini 文件，添加节点信息：[tidb_servers] 172.16.10.4 172.16.10.5 [pd_servers] 172.16.10.1 172.16.10.2 172.16.10.3 172.16.10.103 [tikv_servers] 172.16.10.6 172.16.10.7 172.16.10.8 172.16.10.9 [monitored_servers] 172.16.10.4 172.16.10.5 172.16.10.1 172.16.10.2 172.16.10.3 172.16.10.103 172.16.10.6 172.16.10.7 172.16.10.8 172.16.10.9 [monitoring_servers] 172.16.10.3 [grafana_servers] 172.16.10.3 现在拓扑结构如下所示：   Name Host IP Services     node1 172.16.10.1 PD1   node2 172.16.10.2 PD2   node3 172.16.10.3 PD3, Monitor   node103 172.16.10.103 PD4   node4 172.16.10.4 TiDB1   node5 172.16.10.5 TiDB2   node6 172.16.10.6 TiKV1   node7 172.16.10.7 TiKV2   node8 172.16.10.8 TiKV3   node9 172.16.10.9 TiKV4    初始化新增节点：ansible-playbook bootstrap.yml -l 172.16.10.103 部署新增节点：ansible-playbook deploy.yml -l 172.16.10.103 登录新增的 PD 节点，编辑启动脚本：{deploy_dir}/scripts/run_pd.sh 移除 --initial-cluster=&amp;quot;xxxx&amp;quot;  配置 添加 --join=&amp;quot;http://172.16.10.1:2379&amp;quot; 。IP 地址 (172.16.10.1) 可以是集群内现有 PD IP 地址中的任意一个 在新增 PD 节点中手动启动 PD 服务： {deploy_dir}/scripts/start_pd.sh 使用 pd-ctl 检查新节点是否添加成功： ./pd-ctl -u &amp;quot;http://172.16.10.1:2379&amp;quot;   注: pd-ctl 命令用于查询 PD 节点的数量。 滚动升级整个集群：ansible-playbook rolling_update.yml 更新 Prometheus 配置并重启：ansible-playbook rolling_update_monitor.yml --tags=prometheus 打开浏览器访问监控平台：http://172.16.10.3:3000，监控整个集群和新增节点的状态。  缩容 TiDB 节点 例如，如果要移除一个 TiDB 节点 (node5)，IP 地址为 172.16.10.5，可以进行如下操作： 停止 node5 节点上的服务：ansible-playbook stop.yml -l 172.16.10.5 编辑 inventory.ini 文件，移除节点信息：[tidb_servers] 172.16.10.4 #172.16.10.5 # 注释被移除节点 [pd_servers] 172.16.10.1 172.16.10.2 172.16.10.3 [tikv_servers] 172.16.10.6 172.16.10.7 172.16.10.8 172.16.10.9 [monitored_servers] 172.16.10.4 #172.16.10.5 # 注释被移除节点 172.16.10.1 172.16.10.2 172.16.10.3 172.16.10.6 172.16.10.7 172.16.10.8 172.16.10.9 [monitoring_servers] 172.16.10.3 [grafana_servers] 172.16.10.3 现在拓扑结构如下所示：   Name Host IP Services     node1 172.16.10.1 PD1   node2 …"},
		{"url": "https://pingcap.com/docs-cn/sql/control-flow-functions/",
		"title": "控制流程函数", 
		"content": " 控制流程函数    函数名 功能描述     CASE Case 操作符   IF() 构建 if/else   IFNULL() 构建 Null if/else   NULLIF() 如果 expr1 = expr2，返回 NULL    "},
		{"url": "https://pingcap.com/docs-cn/sql/numeric-functions-and-operators/",
		"title": "数值函数与操作符", 
		"content": " 数值函数与操作符 算术操作符    操作符名 功能描述     + 加号   - 减号   * 乘号   / 除号   DIV 整数除法   %, MOD 模运算，取余   - 更改参数符号    数学函数    函数名 功能描述     POW() 返回参数的指定乘方的结果值   POWER() 返回参数的指定乘方的结果值   EXP() 返回 e（自然对数的底）的指定乘方后的值   SQRT() 返回非负数的二次方根   LN() 返回参数的自然对数   LOG() 返回第一个参数的自然对数   LOG2() 返回参数以 2 为底的对数   LOG10() 返回参数以 10 为底的对数   PI() 返回 pi 的值   TAN() 返回参数的正切值   COT() 返回参数的余切值   SIN() 返回参数的正弦值   COS() 返回参数的余弦值   ATAN() 返回参数的反正切值   ATAN2(), ATAN() 返回两个参数的反正切值   ASIN() 返回参数的反正弦值   ACOS() 返回参数的反余弦值   RADIANS() 返回由度转化为弧度的参数   DEGREES() 返回由弧度转化为度的参数   MOD() 返回余数   ABS() 返回参数的绝对值   CEIL() 返回不小于参数的最小整数值   CEILING() 返回不小于参数的最小整数值   FLOOR() 返回不大于参数的最大整数值   ROUND() 返回参数最近似的整数或指定小数位数的数值   RAND() 返回一个随机浮点值   SIGN() 返回参数的符号   CONV() 不同数基间转换数字，返回数字的字符串表示   TRUNCATE() 返回被舍位至指定小数位数的数字   CRC32()  计算循环冗余码校验值并返回一个 32 位无符号值     "},
		{"url": "https://pingcap.com/docs-cn/sql/literal-value-numeric-literals/",
		"title": "数值字面值", 
		"content": " Numeric Literals 数值字面值包括 integer 跟 Decimal 类型跟浮点数字面值。integer 可以包括 . 作为小数点分隔，数字前可以有 - 或者 + 来表示正数或者负数。精确数值字面值可以表示为如下格式：1, .2, 3.4, -5, -6.78, +9.10.科学记数法也是被允许的，表示为如下格式：1.2E3, 1.2E-3, -1.2E3, -1.2E-3。更多细节"},
		{"url": "https://pingcap.com/docs-cn/sql/admin/",
		"title": "数据库管理语句", 
		"content": " TiDB 可以通过一些语句对数据库进行管理，包括设置权限、修改系统变量、查询数据库状态。权限管理 参考权限管理文档。SET 语句 SET 语句有多种作用和形式：设置变量值 SET variable_assignment [, variable_assignment] ... variable_assignment: user_var_name = expr | param_name = expr | local_var_name = expr | [GLOBAL | SESSION] system_var_name = expr | [@@global. | @@session. | @@] system_var_name = expr 这种语法可以设置 TiDB 的变量值，包括系统变量以及用户定义变量。对于用户自定义变量，都是会话范围的变量；对于系统变量，通过 @@global. 或者是 GLOBAL 设置的变量为全局范围变量，否则为会话范围变量，具体参考系统变量一章。SET CHARACTER 语句和 SET NAMES SET {CHARACTER SET | CHARSET} {&amp;#39;charset_name&amp;#39; | DEFAULT} SET NAMES {&amp;#39;charset_name&amp;#39; [COLLATE &amp;#39;collation_name&amp;#39;] | DEFAULT} 这个语句设置这三个会话范围的系统变量：character_set_client，character_set_results，character_set_connection 设置为给定的字符集。目前 character_set_connection 变量的值和 MySQL 有所区别，MySQL 将其设置为 character_set_database 的值。设置密码 SET PASSWORD [FOR user] = password_option password_option: { &amp;#39;auth_string&amp;#39; | PASSWORD(&amp;#39;auth_string&amp;#39;) } 设置用户密码，具体信息参考权限管理。设置隔离级别 SET SESSION TRANSACTION ISOLATION LEVEL READ COMMITTED; 设置事务隔离级别，具体信息参考事务语句。SHOW 语句 TiDB 支持部分 SHOW 语句，用于查看 Database/Table/Column 信息，或者是数据库内部的状态。已经支持的语句：# 已支持，且和 MySQL 行为一致 SHOW CHARACTER SET [like_or_where] SHOW COLLATION [like_or_where] SHOW [FULL] COLUMNS FROM tbl_name [FROM db_name] [like_or_where] SHOW CREATE {DATABASE|SCHEMA} db_name SHOW CREATE TABLE tbl_name SHOW DATABASES [like_or_where] SHOW GRANTS FOR user SHOW INDEX FROM tbl_name [FROM db_name] SHOW PRIVILEGES SHOW [FULL] PROCESSLIST SHOW [GLOBAL | SESSION] STATUS [like_or_where] SHOW TABLE STATUS [FROM db_name] [like_or_where] SHOW [FULL] TABLES [FROM db_name] [like_or_where] SHOW [GLOBAL | SESSION] VARIABLES [like_or_where] SHOW WARNINGS # 已支持，但是返回空结果，目的是提升兼容性 SHOW ENGINE engine_name {STATUS | MUTEX} SHOW [STORAGE] ENGINES SHOW PLUGINS SHOW PROCEDURE STATUS [like_or_where] SHOW TRIGGERS [FROM db_name] [like_or_where] SHOW EVENTS SHOW FUNCTION STATUS [like_or_where] # TiDB 特有语句，用于查看统计信息 SHOW STATS_META [like_or_where] SHOW STATS_HISTOGRAMS [like_or_where] SHOW STATS_BUCKETS [like_or_where] like_or_where: LIKE &amp;#39;pattern&amp;#39; | WHERE expr 说明： * 通过 SHOW 语句展示统计信息请参考统计信息说明 * 关于 SHOW 语句更多信息请参考 MySQL 文档ADMIN 语句 该语句是 TiDB 扩展语法，用于查看 TiDB 自身的状态。ADMIN SHOW DDL ADMIN SHOW DDL JOBS ADMIN CANCEL DDL JOBS &amp;#39;job_id&amp;#39; [, &amp;#39;job_id&amp;#39;] ...  ADMIN SHOW DDL  用于查看当前正在执行的 DDL 作业。 ADMIN SHOW DDL JOBS  用于查看当前 DDL 作业队列中的所有结果（包括正在运行以及等待运行的任务）以及已执行完成的 DDL 作业队列中的最近十条结果。 ADMIN CANCEL DDL JOBS &#39;job_id&#39; [, &#39;job_id&#39;] ...  用于取消正在执行的 DDL 作业，其返回值为对应的作业取消是否成功，如果失败会显示失败的具体原因。这个操作可以同时取消多个 DDL 作业，其中 DDL 作业 ID 可以通过 ADMIN SHOW DDL JOBS 语句来获取。其中如果希望取消的作业已经完成，则取消操作将会失败。"},
		{"url": "https://pingcap.com/docs-cn/sql/dml/",
		"title": "数据操作语言", 
		"content": " TiDB 数据操作语言 数据操作语言（Data Manipulation Language， DML）用于帮助用户实现对数据库的基本操作，比如查询、写入、删除和修改数据库中的数据。TiDB 支持的数据操作语言包括 Select ，Insert, Delete, Update，和 Replace。Select 语句 Select 语句用于从数据库中查询数据。语法定义 SELECT [ALL | DISTINCT | DISTINCTROW ] [HIGH_PRIORITY] [SQL_CACHE | SQL_NO_CACHE] [SQL_CALC_FOUND_ROWS] select_expr [, select_expr ...] [FROM table_references [WHERE where_condition] [GROUP BY {col_name | expr | position} [ASC | DESC], ...] [HAVING where_condition] [ORDER BY {col_name | expr | position} [ASC | DESC], ...] [LIMIT {[offset,] row_count | row_count OFFSET offset}] [FOR UPDATE | LOCK IN SHARE MODE]] 语法元素说明    语法元素 说明     ALL、DISTINCT、DISTINCTROW 查询结果集中可能会包含重复值。指定 DISTINCT/DISTINCTROW 则在查询结果中过滤掉重复的行；指定 ALL 则列出所有的行。默认为 ALL。   HIGH_PRIORITY 该语句为高优先级语句，TiDB 在执行阶段会优先处理这条语句   SQL_CACHE、SQL_NO_CACHE、SQL_CALC_FOUND_ROWS TiDB 出于兼容性解析这三个语法，但是不做任何处理   select_expr 投影操作列表，一般包括列名、表达式，或者是用 &amp;lsquo;*&amp;rsquo; 表示全部列   FROM table_references 表示数据来源，数据来源可以是一个表（select * from t;）或者是多个表 (select * from t1 join t2;) 或者是0个表 (select 1+1 from dual;, 等价于 select 1+1;)   WHERE where_condition Where 子句用于设置过滤条件，查询结果中只会包含满足条件的数据   GROUP BY GroupBy 子句用于对查询结果集进行分组   HAVING where_condition Having 子句与 Where 子句作用类似，Having 子句可以让过滤 GroupBy 后的各种数据，Where 子句用于在聚合前过滤记录。   ORDER BY OrderBy 子句用于指定结果排序顺序，可以按照列、表达式或者是 select_expr 列表中某个位置的字段进行排序。   LIMIT Limit 子句用于限制结果条数。Limit 接受一个或两个数字参数，如果只有一个参数，那么表示返回数据的最大行数；如果是两个参数，那么第一个参数表示返回数据的第一行的偏移量（第一行数据的偏移量是 0），第二个参数指定返回数据的最大条目数。   FOR UPDATE 对查询结果集所有数据上读锁，以监测其他事务对这些的并发修改。TiDB 使用乐观事务模型在语句执行期间不会检测锁冲突，在事务的提交阶段才会检测事务冲突，如果执行 Select For Update 期间，有其他事务修改相关的数据，那么包含 Select For Update 语句的事务会提交失败。   LOCK IN SHARE MODE TiDB 出于兼容性解析这个语法，但是不做任何处理    Insert 语句 Insert 语句用于向数据库中插入数据，TiDB 兼容 MySQL Insert 语句的所有语法。语法定义 InsertStatement: INSERT [LOW_PRIORITY | DELAYED | HIGH_PRIORITY] [IGNORE] [INTO] tbl_name insert_values [ON DUPLICATE KEY UPDATE assignment_list] insert_values: [(col_name [, col_name] ...)] {VALUES | VALUE} (expr_list) [, (expr_list)] ... | SET assignment_list | [(col_name [, col_name] ...)] SELECT ... expr_list: expr [, expr] ... assignment: col_name = expr assignment_list: assignment [, assignment] ... 语法元素说明    语法元素 说明     LOW_PRIORITY 该语句为低优先级语句，TiDB 在执行阶段会降低这条语句的优先级   DELAYED TiDB 出于兼容性解析这个语法，但是不做任何处理   HIGH_PRIORITY 该语句为高优先级语句，TiDB 在执行阶段会优先处理这条语句   IGNORE 如果发生 Uniq Key 冲突，则忽略插入的数据，不报错   tbl_name 要插入的表名   insert_values 待插入的数据，下面一节会详细描述   ON DUPLICATE KEY UPDATE assignment_list 如果发生 Uniq Key 冲突，则舍弃要插入的数据，改用 assignment_list 更新已存在的行    insert_values 待插入的数据集，可以用以下三种方式指定： Value List  将被插入的数据值写入列表中，例如：CREATE TABLE tbl_name ( a int, b int, c int ); INSERT INTO tbl_name VALUES(1,2,3),(4,5,6),(7,8,9); 上面的例子中，(1,2,3),(4,5,6),(7,8,9) 即为 Value List，其中每个括号内部的数据表示一行数据，这个例子中插入了三行数据。Insert 语句也可以只给部分列插入数据，这种情况下，需要在 Value List 之前加上 ColumnName List，如：INSERT INTO tbl_name (a,c) VALUES(1,2),(4,5),(7,8); 上面的例子中，每行数据只指定了 a 和 c 这两列的值，b 列的值会设为 Null。 Assignment List  通过赋值列表指定插入的数据，例如：INSERT INTO tbl_name a=1, b=2, c=3; 这种方式每次只能插入一行数据，每列的值通过赋值列表制定。 Select Statement  待插入的数据集是通过一个 Select 语句获取，要插入的列是通过 Select 语句的 Schema 获得。例如：CREATE TABLE tbl_name1 ( a int, b int, c int ); INSERT INTO tbl_name SELECT * from tbl_name1; 上面的例子中，从 tbl_name1 中查询出数据，插入 tbl_name 中。Delete 语句 Delete 语句用于删除数据库中的数据，TiDB 兼容 MySQL Delete 语句除 PARTITION 之外的所有语法。Delete 语句主要分为单表删除和多表删除两种，下面分开描述。单表删除 这种语法用于删除的数据只会涉及一个表的情况。语法定义 DELETE [LOW_PRIORITY] [QUICK] [IGNORE] FROM tbl_name [WHERE where_condition] [ORDER BY ...] [LIMIT row_count] 多表删除 这种语法用于删除的数据会涉及多张表的情况。一共有两种写法：DELETE [LOW_PRIORITY] [QUICK] [IGNORE] tbl_name[.*] [, tbl_name[.*]] ... FROM table_references [WHERE where_condition] DELETE [LOW_PRIORITY] [QUICK] [IGNORE] FROM tbl_name[.*] [, tbl_name[.*]] ... USING table_references [WHERE where_condition] 删除多个表的数据的时候，可以用这两种语法。这两种写法都可以指定从多个表查询数据，但只删除其中一些表的数据。在第一种语法中，只会删除 FROM 关键字之前的 Table 列表中所列 Table 的表中的数据。对于第二种写法，只会删除 FROM 之后 USING 之前的 Table 列表中的所列 Table 中的数据。语法元素说明    语法元素 说明     LOW_PRIORITY 该语句为低优先级语句，TiDB 在执行阶段会降低这条语句的优先级   QUICK TiDB 出于兼容性解析这个语法，但是不做任何处理   IGNORE TiDB 出于兼容性解析这个语法，但是不做任何处理   tbl_name 要删除数据的表名   WHERE where_condition Where 表达式，只删除满足表达式的那些行   ORDER BY 对待删除数据集进行排序   LIMIT row_count 只对待删除数据集中排序前 row_count 行的内容进行删除    Update 语句 Update 语句用于更新表中的数据。语法定义 Update 语句一共有两种语法，分别用于更新单表数据和多表数据。单表 Update UPDATE [LOW_PRIORITY] [IGNORE] table_reference SET assignment_list [WHERE where_condition] [ORDER BY ...] [LIMIT row_count] assignment: col_name = value assignment_list: assignment [, assignment] ... 单表 Update 语句会更新 Table 中现有行的指定列。SET assignment_list 指定了要更新的列名，以及要赋予地新值。 Where/OrderBy/Limit 子句一起用于从 Table 中查询出待更新的数据。多表 Update UPDATE [LOW_PRIORITY] [IGNORE] table_references SET assignment_list [WHERE where_condition] 多表更新语句用于将 table_references 中满足 Where 子句的数据地指定列赋予新的值。语法元素说明    语法元素 说明     LOW_PRIORITY 该语句为低优先级语句，TiDB 在执行阶段会降低这条语句的优先级   IGNORE TiDB 出于兼容性解析这个语法，但是不做任何处理   table_reference 待更新的 Table 名称   table_references 待更新的 Table 名称列表   SET assignment_list 待更新的列名以及目标值   WHERE where_condition Where 表达式，只更新满足表达式的那些行   ORDER BY 对待更新数据集进行排序   LIMIT row_count 只对待更新数据集中排序前 row_count 行的内容进行更新    Replace 语句 Replace 语句是 MySQL 对标准 SQL 语法的扩展，其行为和 Insert 语句一样，但是当现有数据中有和待插入数据在 PRIMARY KEY 或者 UNIQUE KEY 冲突的情况下，会先删除旧数据，再插入新数据。语法定义 REPLACE [LOW_PRIORITY | DELAYED] [INTO] tbl_name [(col_name [, col_name] ...)] {VALUES | VALUE} (value_list) [, (value_list)] ... REPLACE [LOW_PRIORITY | DELAYED] [INTO] tbl_name SET assignment_list REPLACE [LOW_PRIORITY | DELAYED] [INTO] tbl_name [(col_name [, col_name] ...)] SELECT ... 语法元素说明    语法元素 说明     LOW_PRIORITY 该语句为低优先级语句，TiDB 在执行阶段会降低这条语句的优先级   DELAYED TiDB 出于兼容性解析这个语法，但是不做任何处理   tbl_name 待更新的 Table 名称   value_list 待插入的数据   SET assignment_list 待更新的列名以及目标值   SELECT ... 待插入的数据集，该数据集来自于一个 Select 语句    "},
		{"url": "https://pingcap.com/docs-cn/op-guide/migration/",
		"title": "数据迁移", 
		"content": " 数据迁移 使用 mydumper/loader 全量导入数据 mydumper 是一个更强大的数据迁移工具，具体可以参考 https://github.com/maxbube/mydumper。我们使用 mydumper 从 MySQL 导出数据，然后用 loader 将其导入到 TiDB 里面。 注意：虽然 TiDB 也支持使用 MySQL 官方的 mysqldump 工具来进行数据的迁移工作，但相比于 mydumper / loader，性能会慢很多，大量数据的迁移会花费很多时间，这里我们并不推荐。 mydumper/loader 全量导入数据最佳实践 为了快速的迁移数据 (特别是数据量巨大的库), 可以参考下面建议 mydumper 导出数据至少要拥有 SELECT , RELOAD , LOCK TABLES 权限 使用 mydumper 导出来的数据文件尽可能的小, 最好不要超过 64M, 可以设置参数 -F 64 loader的 -t 参数可以根据 tikv 的实例个数以及负载进行评估调整，例如 3个 tikv 的场景， 此值可以设为 3 *（1 ～ n)；当 tikv 负载过高，loader 以及 tidb 日志中出现大量 backoffer.maxSleep 15000ms is exceeded 可以适当调小该值，当 tikv 负载不是太高的时候，可以适当调大该值。  某次导入示例，以及相关的配置  mydumper 导出后总数据量 214G，单表 8 列，20 亿行数据 集群拓扑  TIKV * 12 TIDB * 4 PD * 3  mydumper -F 设置为 16, loader -t 参数 64  结果：导入时间 11 小时左右，19.4 G/小时从 MySQL 导出数据 我们使用 mydumper 从 MySQL 导出数据，如下:./bin/mydumper -h 127.0.0.1 -P 3306 -u root -t 16 -F 64 -B test -T t1,t2 --skip-tz-utc -o ./var/test 上面，我们使用 -B test 表明是对 test 这个 database 操作，然后用 -T t1,t2 表明只导出 t1，t2 两张表。-t 16 表明使用 16 个线程去导出数据。-F 64 是将实际的 table 切分成多大的 chunk，这里就是 64MB 一个 chunk。--skip-tz-utc 添加这个参数忽略掉 MySQL 与导数据的机器之间时区设置不一致的情况，禁止自动转换。 注意：在阿里云等一些需要 super privilege 的云上面，mydumper 需要加上 --no-locks 参数，否则会提示没有权限操作。 向 TiDB 导入数据  注意：目前 TiDB 支持 UTF8mb4 字符编码，假设 mydumper 导出数据为 latin1 字符编码，请使用 iconv -f latin1 -t utf-8 $file -o /data/imdbload/$basename 命令转换，$file 为已有文件，$basename 为转换后文件。注意：如果 mydumper 使用 -m 参数，会导出不带表结构的数据，这时 loader 无法导入数据。 我们使用 loader 将之前导出的数据导入到 TiDB。Loader 的下载和具体的使用方法见 Loader 使用文档./bin/loader -h 127.0.0.1 -u root -P 4000 -t 32 -d ./var/test 导入成功之后，我们可以用 MySQL 官方客户端进入 TiDB，查看:mysql -h127.0.0.1 -P4000 -uroot mysql&amp;gt; show tables; +----------------+ | Tables_in_test | +----------------+ | t1 | | t2 | +----------------+  mysql&amp;gt; select * from t1; +----+------+ | id | age | +----+------+ | 1 | 1 | | 2 | 2 | | 3 | 3 | +----+------+  mysql&amp;gt; select * from t2; +----+------+ | id | name | +----+------+ | 1 | a | | 2 | b | | 3 | c | +----+------+ 使用 syncer 增量导入数据 上面我们介绍了如何使用 mydumper/loader 将 MySQL 的数据全量导入到 TiDB，但如果后续 MySQL 的数据有更新，我们仍然希望快速导入，使用全量的方式就不合适了。TiDB 提供 syncer 工具能方便的将 MySQL 的数据增量的导入到 TiDB 里面。syncer 属于 TiDB 企业版工具集，如何获取可以参考 下载 TiDB 企业版工具集。下载 TiDB 企业版工具集 (Linux) # 下载 tool 压缩包 wget http://download.pingcap.org/tidb-enterprise-tools-latest-linux-amd64.tar.gz wget http://download.pingcap.org/tidb-enterprise-tools-latest-linux-amd64.sha256 # 检查文件完整性，返回 ok 则正确 sha256sum -c tidb-enterprise-tools-latest-linux-amd64.sha256 # 解开压缩包 tar -xzf tidb-enterprise-tools-latest-linux-amd64.tar.gz cd tidb-enterprise-tools-latest-linux-amd64 假设我们之前已经使用 mydumper/loader 导入了 t1 和 t2 两张表的一些数据，现在我们希望这两张表的任何更新，都是实时的同步到 TiDB 上面。获取同步 position 如上文所提，mydumper 导出的数据目录里面有一个 metadata 文件，里面就包含了我们所需的 position 信息。medadata 文件信息内容举例：Started dump at: 2017-04-28 10:48:10 SHOW MASTER STATUS: Log: mysql-bin.000003 Pos: 930143241 GTID: Finished dump at: 2017-04-28 10:48:11 我们将 position 相关的信息保存到一个 syncer.meta 文件里面，用于 syncer 的同步:# cat syncer.meta binlog-name = &amp;#34;mysql-bin.000003&amp;#34; binlog-pos = 930143241 binlog-gtid = &amp;#34;2bfabd22-fff7-11e6-97f7-f02fa73bcb01:1-23,61ccbb5d-c82d-11e6-ac2e-487b6bd31bf7:1-4&amp;#34;  注意：syncer.meta 只需要第一次使用的时候配置，后续 syncer 同步新的 binlog 之后会自动将其更新到最新的 position。 注意： 如果使用 binlog position 同步则只需要配置 binlog-name binlog-pos; 使用 gtid 同步则需要设置 gtid，且启动 syncer 时带有 --enable-gtid  启动 syncer 启动 syncer 服务之前请详细阅读 Syncer 增量导入syncer 的配置文件 config.toml:log-level = &amp;#34;info&amp;#34; server-id = 101 ## meta 文件地址 meta = &amp;#34;./syncer.meta&amp;#34; worker-count = 16 batch = 10 ## pprof 调试地址, Prometheus 也可以通过该地址拉取 syncer metrics ## 将 127.0.0.1 修改为相应主机 IP 地址 status-addr = &amp;#34;127.0.0.1:10086&amp;#34; ## 跳过 DDL 或者其他语句，格式为 **前缀完全匹配**，如: `DROP TABLE ABC`,则至少需要填入`DROP TABLE`. # skip-sqls = [&amp;#34;ALTER USER&amp;#34;, &amp;#34;CREATE USER&amp;#34;] ## 在使用 route-rules 功能后， ## replicate-do-db &amp;amp; replicate-ignore-db 匹配合表之后(target-schema &amp;amp; target-table )数值 ## 优先级关系: replicate-do-db --&amp;gt; replicate-do-table --&amp;gt; replicate-ignore-db --&amp;gt; replicate-ignore-table ## 指定要同步数据库名；支持正则匹配，表达式语句必须以 `~` 开始 #replicate-do-db = [&amp;#34;~^b.*&amp;#34;,&amp;#34;s1&amp;#34;] ## 指定要同步的 db.table 表 ## db-name 与 tbl-name 不支持 `db-name =&amp;#34;dbname，dbname2&amp;#34;` 格式 #[[replicate-do-table]] #db-name =&amp;#34;dbname&amp;#34; #tbl-name = &amp;#34;table-name&amp;#34; #[[replicate-do-table]] #db-name =&amp;#34;dbname1&amp;#34; #tbl-name = &amp;#34;table-name1&amp;#34; ## 指定要同步的 db.table 表；支持正则匹配，表达式语句必须以 `~` 开始 #[[replicate-do-table]] #db-name =&amp;#34;test&amp;#34; #tbl-name = &amp;#34;~^a.*&amp;#34; ## 指定**忽略**同步数据库；支持正则匹配，表达式语句必须以 `~` 开始 #replicate-ignore-db = [&amp;#34;~^b.*&amp;#34;,&amp;#34;s1&amp;#34;] ## 指定**忽略**同步数据库 ## db-name &amp;amp; tbl-name 不支持 `db-name =&amp;#34;dbname，dbname2&amp;#34;` 语句格式 #[[replicate-ignore-table]] #db-name = &amp;#34;your_db&amp;#34; #tbl-name = &amp;#34;your_table&amp;#34; ## 指定要**忽略**同步数据库名；支持正则匹配，表达式语句必须以 `~` 开始 #[[replicate-ignore-table]] #db-name =&amp;#34;test&amp;#34; #tbl-name = &amp;#34;~^a.*&amp;#34; # sharding 同步规则，采用 wildcharacter # 1. 星号字符 (*) 可以匹配零个或者多个字符, # 例子, doc* 匹配 doc 和 document, 但是和 dodo 不匹配; # 星号只能放在 pattern 结尾，并且一个 pattern 中只能有一个 # 2. 问号字符 (?) 匹配任一一个字符 #[[route-rules]] #pattern-schema = &amp;#34;route_*&amp;#34; #pattern-table = &amp;#34;abc_*&amp;#34; #target-schema = &amp;#34;route&amp;#34; #target-table = &amp;#34;abc&amp;#34; #[[route-rules]] #pattern-schema = &amp;#34;route_*&amp;#34; #pattern-table = &amp;#34;xyz_*&amp;#34; #target-schema = &amp;#34;route&amp;#34; #target-table = &amp;#34;xyz&amp;#34; [from] host = &amp;#34;127.0.0.1&amp;#34; user = &amp;#34;root&amp;#34; password = &amp;#34;&amp;#34; port = 3306 [to] host = &amp;#34;127.0.0.1&amp;#34; user = &amp;#34;root&amp;#34; password = &amp;#34;&amp;#34; port = 4000 启动 syncer:./bin/syncer -config config.toml 2016/10/27 15:22:01 binlogsyncer.go:226: [info] begin to sync binlog from position (mysql-bin.000003, 1280) 2016/10/27 15:22:01 binlogsyncer.go:130: [info] register slave for master server 127.0.0.1:3306 2016/10/27 15:22:01 binlogsyncer.go:552: [info] rotate to (mysql-bin.000003, 1280) 2016/10/27 15:22:01 syncer.go:549: [info] rotate binlog to (mysql-bin.000003, 1280) 在 MySQL 插入新的数据 INSERT INTO t1 VALUES (4, 4), (5, 5); 登录到 TiDB 查看：mysql -h127.0.0.1 -P4000 -uroot -p mysql&amp;gt; select * from t1; +----+------+ | id | age | +----+------+ | 1 | 1 | | 2 | 2 | | 3 | 3 | | 4 | 4 | | 5 | 5 | +----+------+ syncer 每隔 30s 会输出当前的同步统计，如下2017/06/08 01:18:51 syncer.go:934: [info] [syncer]total events = 15, total tps = 130, recent tps = 4, master-binlog = (ON.000001, 11992), master-binlog-gtid=53ea0ed1-9bf8-11e6-8bea-64006a897c73:1-74, syncer-binlog = (ON.000001, 2504), syncer-binlog-gtid = 53ea0ed1-9bf8-11e6-8bea-64006a897c73:1-17 2017/06/08 01:19:21 syncer.go:934: [info] [syncer]total events = 15, total tps = 191, recent tps = 2, master-binlog = (ON.000001, 11992), master-binlog-gtid=53ea0ed1-9bf8-11e6-8bea-64006a897c73:1-74, syncer-binlog = (ON.000001, 2504), syncer-binlog-gtid = 53ea0ed1-9bf8-11e6-8bea-64006a897c73:1-35 可以看到，使用 syncer，我们就能自动的将 MySQL 的更新同步到 TiDB。"},
		{"url": "https://pingcap.com/docs-cn/op-guide/migration-overview/",
		"title": "数据迁移概述", 
		"content": " 数据迁移概述 概述 该文档详细介绍了如何将 MySQL 的数据迁移到 TiDB。这里我们假定 MySQL 以及 TiDB 服务信息如下：   Name Address Port User Password     MySQL 127.0.0.1 3306 root *   TiDB 127.0.0.1 4000 root *    在这个数据迁移过程中，我们会用到下面四个工具: checker 检查 schema 能否被 TiDB 兼容 mydumper 从 MySQL 导出数据 loader 导入数据到 TiDB syncer 增量同步 MySQL 数据到 TiDB  两种迁移场景  第一种场景：只全量导入历史数据 （需要 checker + mydumper + loader）； 第二种场景：全量导入历史数据后，通过增量的方式同步新的数据 （需要 checker + mydumper + loader + syncer）。该场景需要提前开启 binlog 且格式必须为 ROW。  MySQL 开启 binlog 注意： 只有上文提到的第二种场景才需要在 dump 数据之前先开启 binlog MySQL 开启 binlog 功能，参考 Setting the Replication Master Configuration Binlog 格式必须使用 ROW format，这也是 MySQL 5.7 之后推荐的 binlog 格式，可以使用如下语句打开:SET GLOBAL binlog_format = ROW;  使用 checker 进行 Schema 检查 在迁移之前，我们可以使用 TiDB 的 checker 工具，来预先检查 TiDB 是否能支持需要迁移的 table schema。如果 check 某个 table schema 失败，表明 TiDB 当前并不支持，我们不能对该 table 里面的数据进行迁移。checker 包含在 TiDB 工具集里面，我们可以直接下载。下载 TiDB 工具集 (Linux) # 下载 tool 压缩包 wget http://download.pingcap.org/tidb-enterprise-tools-latest-linux-amd64.tar.gz wget http://download.pingcap.org/tidb-enterprise-tools-latest-linux-amd64.sha256 # 检查文件完整性，返回 ok 则正确 sha256sum -c tidb-enterprise-tools-latest-linux-amd64.sha256 # 解开压缩包 tar -xzf tidb-enterprise-tools-latest-linux-amd64.tar.gz cd tidb-enterprise-tools-latest-linux-amd64 使用 checker 检查的一个示范  在 MySQL 的 test database 里面创建几张表，并插入数据:USE test; CREATE TABLE t1 (id INT, age INT, PRIMARY KEY(id)) ENGINE=InnoDB; CREATE TABLE t2 (id INT, name VARCHAR(256), PRIMARY KEY(id)) ENGINE=InnoDB; INSERT INTO t1 VALUES (1, 1), (2, 2), (3, 3); INSERT INTO t2 VALUES (1, &amp;#34;a&amp;#34;), (2, &amp;#34;b&amp;#34;), (3, &amp;#34;c&amp;#34;); 使用 checker 检查 test database 里面所有的 table./bin/checker -host 127.0.0.1 -port 3306 -user root test 2016/10/27 13:11:49 checker.go:48: [info] Checking database test 2016/10/27 13:11:49 main.go:37: [info] Database DSN: root:@tcp(127.0.0.1:3306)/test?charset=utf8 2016/10/27 13:11:49 checker.go:63: [info] Checking table t1 2016/10/27 13:11:49 checker.go:69: [info] Check table t1 succ 2016/10/27 13:11:49 checker.go:63: [info] Checking table t2 2016/10/27 13:11:49 checker.go:69: [info] Check table t2 succ 使用 checker 检查 test database 里面某一个 table这里，假设我们只需要迁移 table t1。./bin/checker -host 127.0.0.1 -port 3306 -user root test t1 2016/10/27 13:13:56 checker.go:48: [info] Checking database test 2016/10/27 13:13:56 main.go:37: [info] Database DSN: root:@tcp(127.0.0.1:3306)/test?charset=utf8 2016/10/27 13:13:56 checker.go:63: [info] Checking table t1 2016/10/27 13:13:56 checker.go:69: [info] Check table t1 succ Check database succ!  一个无法迁移的 table 例子 我们在 MySQL 里面创建如下表：CREATE TABLE t_error ( a INT NOT NULL, PRIMARY KEY (a)) ENGINE=InnoDB TABLESPACE ts1 PARTITION BY RANGE (a) PARTITIONS 3 ( PARTITION P1 VALUES LESS THAN (2), PARTITION P2 VALUES LESS THAN (4) TABLESPACE ts2, PARTITION P3 VALUES LESS THAN (6) TABLESPACE ts3); 使用 checker 进行检查，会报错，表明我们没法迁移 t_error 这张表。./bin/checker -host 127.0.0.1 -port 3306 -user root test t_error 2017/08/04 11:14:35 checker.go:48: [info] Checking database test 2017/08/04 11:14:35 main.go:39: [info] Database DSN: root:@tcp(127.0.0.1:3306)/test?charset=utf8 2017/08/04 11:14:35 checker.go:63: [info] Checking table t1 2017/08/04 11:14:35 checker.go:67: [error] Check table t1 failed with err: line 3 column 29 near &amp;#34; ENGINE=InnoDB DEFAULT CHARSET=latin1 /*!50100 PARTITION BY RANGE (a) (PARTITION P1 VALUES LESS THAN (2) ENGINE = InnoDB, PARTITION P2 VALUES LESS THAN (4) TABLESPACE = ts2 ENGINE = InnoDB, PARTITION P3 VALUES LESS THAN (6) TABLESPACE = ts3 ENGINE = InnoDB) */&amp;#34; (total length 354) github.com/pingcap/tidb/parser/yy_parser.go:96: github.com/pingcap/tidb/parser/yy_parser.go:109: /home/jenkins/workspace/build_tidb_tools_master/go/src/github.com/pingcap/tidb-tools/checker/checker.go:122: parse CREATE TABLE `t1` ( `a` int(11) NOT NULL, PRIMARY KEY (`a`) ) /*!50100 TABLESPACE ts1 */ ENGINE=InnoDB DEFAULT CHARSET=latin1 /*!50100 PARTITION BY RANGE (a) (PARTITION P1 VALUES LESS THAN (2) ENGINE = InnoDB, PARTITION P2 VALUES LESS THAN (4) TABLESPACE = ts2 ENGINE = InnoDB, PARTITION P3 VALUES LESS THAN (6) TABLESPACE = ts3 ENGINE = InnoDB) */ error /home/jenkins/workspace/build_tidb_tools_master/go/src/github.com/pingcap/tidb-tools/checker/checker.go:114: 2017/08/04 11:14:35 main.go:83: [error] Check database test with 1 errors and 0 warnings."},
		{"url": "https://pingcap.com/docs-cn/sql/date-and-time-types/",
		"title": "日期和时间类型", 
		"content": " 日期和时间类型 用于表示日期和时间类型的值是 DATE，TIME，DATETIME，TIMESTAMP 和 YEAR。每一种类型都有自己的有效值的范围，也有一个零值用于表示它是一个无效的值。TIMESTAMP 类型有个自动更新的行为，后面介绍。处理日期和时间类型时，请记住下面这些： 尽管 TiDB 尝试解释不同的格式，日期部分必须是按 年-月-日 的顺序（比如，&amp;rsquo;98-09-04&amp;rsquo;），而不是 月-日-年 或者 日-月-年 的顺序。 日期值中包含两位数字的年份是有歧义的，TiDB 按下面规则解释：  范围在 70-99 之间的被转换成 1970-1999 范围在 00-69 之间的被转换成 2000-2069  如果上下文里面需要的是一个数值，TiDB 自动将日期或时间值转换成数值类型，反之亦然。 如果 TiDB 遇到一个日期或时间值是超过表示范围的，或者无效的，会自动将它转换为该类型的零值。 设置不同的 SQL mode 可以改变 TiDB 的行为。 TiDB 允许 DATE 和 DATETIME 列中出现月份或者日为零的值，比如 &amp;lsquo;2009-00-00&amp;rsquo; 或 &amp;lsquo;2009-01-00&amp;rsquo;。如果这种日期参与计算，比如函数 DATE_SUB() 或者 DATE_ADD()，得到的结果可能会不正确。 TiDB 允许存储零值 &amp;lsquo;0000-00-00&amp;rsquo;，有时候这会比 NULL 值更方便一些。  下面的表格里面显示了不同类型的零值：   Date Type &amp;ldquo;Zero&amp;rdquo; Value     DATE &amp;lsquo;0000-00-00&amp;rsquo;   TIME &amp;lsquo;00:00:00&amp;rsquo;   DATETIME &amp;lsquo;0000-00-00 00:00:00&amp;rsquo;   TIMESTAMP &amp;lsquo;0000-00-00 00:00:00&amp;rsquo;   YEAR 0000    DATE，DATETIME 和 TIMESTAMP 类型 DATE，DATETIME，TIMESTAMP 类型都是相关的。这里描述它们的共同点和区别。DATA 用于只有日期部分，没有时间部分。TiDB 按照 &amp;lsquo;YYYY-MM-DD&amp;rsquo; 格式接受和显示 DATE 类型的值。支持的值的范围是在 &amp;lsquo;1000-01-01&amp;rsquo; 到 &amp;lsquo;9999-12-31&amp;rsquo;。DATETIME 包含了日期和时间部分，格式是 &amp;lsquo;YYYY-MM-DD HH:MM:SS&amp;rsquo;。支持的值的范围是在 &amp;lsquo;1000-01-01 00:00:00&amp;rsquo; 到 &amp;lsquo;9999-12-31 23:59:59&amp;rsquo;。TIMESTAMP 包含了日期和时间部分，值的范围是UTC时间 &amp;lsquo;1970-01-01 00:00:01&amp;rsquo; 到 &amp;lsquo;2038-01-19 03:14:07&amp;rsquo;。DATETIME 和 TIMESTAMP 值可以包含一个最多6位的分数部分，精确到毫秒精度。任何 DATETIME 或 TIMESTAMP 类型的列里面，分数部分都会被存储下来，而不是丢弃。如果包含分数部分，那么值的格式就是 &amp;lsquo;YYYY-MM-DD HH:MM:SS[.fraction]&amp;lsquo;，分数的范围是 000000-999999。分数和其它部分之间必需用小数点分隔。TiDB 将 TIMESTAMP 从当前时区转成 UTC 时区存储，检索时再从 UTC 时区转化到当前时区（注意，DATETIME 并不会这样处理）。每个连接默认的时区是服务器的本地时区，可以通过 time_zone 环境变量进行修改。只要时区保持不变，存储和取回来的值都是一样的。如果存储的是 TIMESTAMP 值，并且时区改变了，那么存储的值和读出来的值会发生变化。不合法的 DATE，DATETIME，TIMESTAMP 值会被自动地转成相应类型的零值（&amp;rsquo;0000-00-00&amp;rsquo; 或 &amp;lsquo;0000-00-00 00:00:00&amp;rsquo;）。注意，TIMESTAMP 类型的值是不允许月份或者日里面出现零的，唯一的例外是零值本身 &amp;lsquo;0000-00-00 00:00:00&amp;rsquo;。两位数的年份是有歧义的，会按照如下规则解释： 00-69 范围被转换成 2000-2069 70-99 范围被转换成 1970-1999  TIME 类型 TIME 类型的值的格式是 &amp;lsquo;HH:MM:SS&amp;rsquo;，值的范围是 &amp;lsquo;-838:59:59&amp;rsquo; 到 &amp;lsquo;838:59:59&amp;rsquo;。时间部分比较大，是因为 TIME 类型不仅用于表示一天里面的时间，也可以用于两个事件之间的时间间隔。TIME 类型可以包含分数部分，如果包含分数部分，那么 TIME 的表示范围则是 &amp;lsquo;-838:59:59.000000&amp;rsquo; 到 &amp;lsquo;838:59:59.000000&amp;rsquo;。注意缩写的时间，&amp;rsquo;11:12&amp;rsquo; 表示的是 &amp;lsquo;11:12:00&amp;rsquo; 而不是 &amp;lsquo;00:11:12&amp;rsquo;，然而 &amp;lsquo;1112&amp;rsquo; 表示的是 &amp;lsquo;00:11:12&amp;rsquo;。这里的区别是是否包含分号 :，处理起来是不一样的。YEAR 类型 YEAR 的值的格式是 YYYY，表示范围从 1901 到 2155，或者是零值 0000。指定 YEAR 的值可以按下列格式： 4位数字从 1901 到 2155 4位字符串从 &amp;lsquo;1901&amp;rsquo; 到 &amp;lsquo;2155&amp;rsquo; 1位或者2位数字，从 1 到 99。相应的，1-69 会被转换为 2001-2069，70-99 会被转换为 1970-1999 1位或者2位字符串，从 &amp;lsquo;0&amp;rsquo; 到 &amp;lsquo;99&amp;rsquo; 数值的 0 会被当作 0000，而字符串的 &amp;lsquo;0&amp;rsquo; 或 &amp;lsquo;00&amp;rsquo; 会被当作 2000  不合法的的 YEAR 的值被会自动转换成 0000。TIMESTAMP 和 DATETIME 的自动初始化和更新 TIMESTAMP 和 DATETIME 列可以被自动初始化或者更新为当前时间。对于表里面任意的 TIMESTAMP 或者 DATETIME 列，可以将默认值或者自动更新值指定为 current timestamp。通过在列定义时指定 DEFAULT CURRENT_TIMESTAMP 和 ON UPDATE CURRENT_TIMESTAMP 可以设置这些属性。DEFAULT 也可以指定成某个特定的值，比如 DEFAULT 0 或者 DEFAULT &#39;2000-01-01 00:00:00&#39;。CREATE TABLE t1 ( ts TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP, dt DATETIME DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP ); 除非指定了 NOT NULL，否则 DATETIME 的默认值是 NULL，如果 NOT NULL 时不指定默认值，默认值是 0。CREATE TABLE t1 ( dt1 DATETIME ON UPDATE CURRENT_TIMESTAMP, -- default NULL dt2 DATETIME NOT NULL ON UPDATE CURRENT_TIMESTAMP -- default 0 ); 时间值中的小数部分 TIME，DATETIME，TIMESTAMP 支持分数部分，可以精确到毫秒精度。 使用 type_name(fsp) 来定义支持分数精度的列，其中 type_name 可以是 TIME，DATETIME 或者 TIMESTAMP，例如：  CREATE TABLE t1 (t TIME(3), dt DATETIME(6)); fsp 必须是从 0 到 6。0 表示没有分数部分，如果 fsp 忽略，则默认就是 0。 插入一条 TIME，DATETIME 或者 TIMESTAMP 涉及到分数部分时，如果分数位不够，或者过多，可能会涉及到 rounding。例如：  mysql&amp;gt; CREATE TABLE fractest( c1 TIME(2), c2 DATETIME(2), c3 TIMESTAMP(2) ); Query OK, 0 rows affected (0.33 sec) mysql&amp;gt; INSERT INTO fractest VALUES &amp;gt; (&amp;#39;17:51:04.777&amp;#39;, &amp;#39;2014-09-08 17:51:04.777&amp;#39;, &amp;#39;2014-09-08 17:51:04.777&amp;#39;); Query OK, 1 row affected (0.03 sec) mysql&amp;gt; SELECT * FROM fractest; +-------------|------------------------|------------------------+ | c1 | c2 | c3 | +-------------|------------------------|------------------------+ | 17:51:04.78 | 2014-09-08 17:51:04.78 | 2014-09-08 17:51:04.78 | +-------------|------------------------|------------------------+ 1 row in set (0.00 sec) 日期和时间类型转换 有时候可能会需要在日期类型间进行转换，某些转换可能会丢失信息。例如，DATE，DATETIME，TIMESTAMP 的值都是有各自的表示范围的。TIMESTAMP 不可以早于 UTC 时间的 1970 年，或者晚于 UTC 时间的 &amp;lsquo;2038-01-19 03:14:07&amp;rsquo;，这意味着 &amp;lsquo;1968-01-01&amp;rsquo; 是一个合理的 DATE 或者 DATETIME 日期值，但是转换成 TIMESTAMP 时会变成 0。DATE 的转换： 转成 DATETIME 或者 TIMESTAMP 会添加时间部分 &amp;lsquo;00:00:00&amp;rsquo;，因为 DATE 不包含时间信息 转成 TIME 会变成 &amp;lsquo;00:00:00&amp;rsquo;  DATETIME 和 TIMESTAMP 的转换： 转成 DATE 会丢弃时间和分数部分，比如 &amp;lsquo;1999-12-31 23:59:59.499&amp;rsquo; 变成 &amp;lsquo;1999-12-31&amp;rsquo; 转成 TIME 会丢弃日期部分，因为 TIME 里面不包含日期信息  将 TIME 转成其它时间日期格式时， 自动地使用 CURRENT_DATE() 作为日期部分，最后生成的是 TIME 的时间加上 CURRENT_DATE() 之后得到的日期，也就是说如果 TIME 的值不是在 &amp;lsquo;00:00:00&amp;rsquo; 到 &amp;lsquo;23:59:59&amp;rsquo; 范围内，转换之后的日期部分并不是当天。TIME 转成 DATE 也是类似过程，然后丢弃时间部分。使用 CASE() 函数可以显示的转换类型，比如：date_col = CAST(datetime_col AS DATE) 将 TIME 和 DATETIME 转换成数值格式：mysql&amp;gt; SELECT CURTIME(), CURTIME()+0, CURTIME(3)+0; +-----------|-------------|--------------+ | CURTIME() | CURTIME()+0 | CURTIME(3)+0 | +-----------|-------------|--------------+ | 09:28:00 | 92800 | 92800.887 | +-----------|-------------|--------------+ mysql&amp;gt; SELECT NOW(), NOW()+0, NOW(3)+0; +---------------------|----------------|--------------------+ | NOW() | NOW()+0 | NOW(3)+0 | +---------------------|----------------|--------------------+ | 2012-08-15 09:28:00 | 20120815092800 | 20120815092800.889 | +---------------------|----------------|--------------------+ 日期中的两位数字的年 日期中包含两位数字的年是带有歧义的，因为不知道世纪。对于 DATETIME，DATE 和 TIMESTAMP 类型，TiDB 采用如下规则处理歧义： 范围在 00-69 被转换成 2000-2069 范围在 70-99 被转换成 1970-1999  对于 YEAR 也是类似过程，只有一个例外，数字的 00 插入到 YEAR(4) 会变成 0000 而不是 2000。如果想解释成 2000，则必需指定成 2000 或者是 &amp;lsquo;0&amp;rsquo; 或 &amp;lsquo;00&amp;rsquo;。有些函数比如 MIN() 和 MAX() 对于两位数字的年处理可能不太好，使用四位格式会比较合适。"},
		{"url": "https://pingcap.com/docs-cn/sql/time-zone/",
		"title": "时区支持", 
		"content": "TiDB 使用的时区由 time_zone 全局变量和 session 变量决定。time_zone 的初始值是机器当前的系统时区 &amp;lsquo;SYSTEM&amp;rsquo; 。在运行过程中可以修改全局时区：mysql&amp;gt; SET GLOBAL time_zone = timezone; TiDB 还可以通过设置 session 变量 time_zone 为每个连接维护各自的时区。默认条件下，这个值取的是全局变量 time_zone 的值。修改 session 使用的时区：mysql&amp;gt; SET time_zone = timezone; 查看当前使用的时区的值：mysql&amp;gt; SELECT @@global.time_zone, @@session.time_zone; 设置 time_zone 的值的格式： &amp;lsquo;SYSTEM&amp;rsquo; 表明使用系统时间 相对于 UTC 时间的偏移，比如 &amp;lsquo;+10:00&amp;rsquo; 或者 &amp;lsquo;-6:00&amp;rsquo; 某个时区的名字，比如 &amp;lsquo;Europe/Helsinki&amp;rsquo;， &amp;lsquo;US/Eastern&amp;rsquo; 或 &amp;lsquo;MET&amp;rsquo;  NOW() 和 CURTIME() 的返回值都受到时区设置的影响。注意，只有 Timestamp 数据类型的值是受时区影响的。可以理解为， Timestamp 数据类型的实际表示使用的是 (字面值 + 时区信息)。其它时间和日期类型，比如 Datetime/Date/Time 是不包含时区信息的，所以也不受到时区变化的影响。mysql&amp;gt; create table t (ts timestamp, dt datetime); Query OK, 0 rows affected (0.02 sec) mysql&amp;gt; set @@time_zone = &amp;#39;UTC&amp;#39;; Query OK, 0 rows affected (0.00 sec) mysql&amp;gt; insert into t values (&amp;#39;2017-09-30 11:11:11&amp;#39;, &amp;#39;2017-09-30 11:11:11&amp;#39;); Query OK, 1 row affected (0.00 sec) mysql&amp;gt; set @@time_zone = &amp;#39;+8:00&amp;#39;; Query OK, 0 rows affected (0.00 sec) mysql&amp;gt; select * from t; +---------------------|---------------------+ | ts | dt | +---------------------|---------------------+ | 2017-09-30 19:11:11 | 2017-09-30 11:11:11 | +---------------------|---------------------+ 1 row in set (0.00 sec) 上面的例子中，无论怎么调整时区的值， Datetime 类型字段的值是不受影响的，而 Timestamp 则随着时区改变，显示的值会发生变化。其实 Timestamp 持久化到存储的值始终没有变化过，只是根据时区的不同显示值不同。Timestamp 类型和 Datetime 等类型的值，两者相互转换的过程中，会涉及到时区。这种情况一律基于 session 的当前 time_zone 时区处理。另外，用户在导数据的过程中，也要需注意主库和从库之间的时区设定是否一致。"},
		{"url": "https://pingcap.com/docs-cn/sql/privilege/",
		"title": "权限管理", 
		"content": " 权限管理 权限管理概述 TiDB的权限管理系统是按照 MySQL 的权限管理进行实现，大部分的 MySQL 的语法和权限类型都是支持的。如果发现行为跟 MySQL 不一致的地方，欢迎报告 issue。示例 用户账户操作 更改密码 set password for &amp;#39;root&amp;#39;@&amp;#39;%&amp;#39; = &amp;#39;xxx&amp;#39;; 添加用户 create user &amp;#39;test&amp;#39;@&amp;#39;127.0.0.1&amp;#39; identified by &amp;#39;xxx&amp;#39;; 用户名是大小写敏感的。host则支持模糊匹配，比如：create user &amp;#39;test&amp;#39;@&amp;#39;192.168.10.%&amp;#39;; 允许 test 用户从 192.168.10 子网的任何一个主机登陆。如果没有指定 host，则默认是所有 IP 均可登陆。如果没有指定密码，默认为空：create user &amp;#39;test&amp;#39;; 等价于create user &amp;#39;test&amp;#39;@&amp;#39;%&amp;#39; identified by &amp;#39;&amp;#39;; 删除用户 drop user &amp;#39;test&amp;#39;@&amp;#39;%&amp;#39;; 这个操作会清除用户在 mysql.user 表里面的记录项，并且清除在授权表里面的相关记录。忘记root密码 使用一个特殊的启动参数启动 TiDB（需要root权限）：sudo ./tidb-server -skip-grant-table=true 这个参数启动，TiDB 会跳过权限系统，然后使用 root 登陆以后修改密码：mysql -h 127.0.0.1 -P 4000 -u root 权限相关操作 授予权限 授予 xxx 用户对数据库 test 的读权限：grant Select on test.* to &amp;#39;xxx&amp;#39;@&amp;#39;%&amp;#39;; 为 test 用户授予所有数据库，全部权限：grant all privileges on *.* to &amp;#39;xxx&amp;#39;@&amp;#39;%&amp;#39;; 如果 grant 的目标用户不存在，TiDB 会自动创建用户。mysql&amp;gt; select * from mysql.user where user=&amp;#39;xxxx&amp;#39;; Empty set (0.00 sec) mysql&amp;gt; grant all privileges on test.* to &amp;#39;xxxx&amp;#39;@&amp;#39;%&amp;#39; identified by &amp;#39;yyyyy&amp;#39;; Query OK, 0 rows affected (0.00 sec) mysql&amp;gt; select user,host from mysql.user where user=&amp;#39;xxxx&amp;#39;; +------|------+ | user | host | +------|------+ | xxxx | % | +------|------+ 1 row in set (0.00 sec) 例子中 xxxx@% 就是自动添加进去的用户。grant 对于数据库或者表的授权，不检查数据库或表是否存在。mysql&amp;gt; select * from test.xxxx; ERROR 1146 (42S02): Table &amp;#39;test.xxxx&amp;#39; doesn&amp;#39;t exist mysql&amp;gt; grant all privileges on test.xxxx to xxxx; Query OK, 0 rows affected (0.00 sec) mysql&amp;gt; select user,host from mysql.tables_priv where user=&amp;#39;xxxx&amp;#39;; +------|------+ | user | host | +------|------+ | xxxx | % | +------|------+ 1 row in set (0.00 sec) grant 可以模糊匹配地授予数据库和表mysql&amp;gt; grant all privileges on `te%`.* to genius; Query OK, 0 rows affected (0.00 sec) mysql&amp;gt; select user,host,db from mysql.db where user=&amp;#39;genius&amp;#39;; +--------|------|-----+ | user | host | db | +--------|------|-----+ | genius | % | te% | +--------|------|-----+ 1 row in set (0.00 sec) 这个例子中通过 % 模糊匹配，所有 te 开头的数据库，都被授予了权限。收回权限 revoke语句与grant对应：revoke all privileges on `test`.* from &amp;#39;genius&amp;#39;@&amp;#39;localhost&amp;#39;; 注意 revoke 收回权限时只做精确匹配，若找不到记录则报错。而 grant 授予权限时可以使用模糊匹配。mysql&amp;gt; revoke all privileges on `te%`.* from &amp;#39;genius&amp;#39;@&amp;#39;%&amp;#39;; ERROR 1141 (42000): There is no such grant defined for user &amp;#39;genius&amp;#39; on host &amp;#39;%&amp;#39;  关于模糊匹配和转义，字符串和 identifier&amp;gt; mysql&amp;gt; grant all privileges on `te%`.* to &amp;#39;genius&amp;#39;@&amp;#39;localhost&amp;#39;; &amp;gt; Query OK, 0 rows affected (0.00 sec) &amp;gt; ``` &amp;gt; &amp;gt; 这个例子是精确匹配名叫 `te%` 的数据库，注意到用了 `` 转义字符。 &amp;gt; &amp;gt; 以单引号包含的，是一个字符串。以反引号包含的，是一个 identifier。注意下面区别： &amp;gt; &amp;gt; ``` &amp;gt; mysql&amp;gt; grant all privileges on &amp;#39;test&amp;#39;.* to &amp;#39;genius&amp;#39;@&amp;#39;localhost&amp;#39;; &amp;gt; ERROR 1064 (42000): You have an error in your SQL syntax; check the &amp;gt; manual that corresponds to your MySQL server version for the right &amp;gt; syntax to use near &amp;#39;&amp;#39;test&amp;#39;.* to &amp;#39;genius&amp;#39;@&amp;#39;localhost&amp;#39;&amp;#39; at line 1 &amp;gt; &amp;gt; mysql&amp;gt; grant all privileges on `test`.* to &amp;#39;genius&amp;#39;@&amp;#39;localhost&amp;#39;; &amp;gt; Query OK, 0 rows affected (0.00 sec) &amp;gt; ``` &amp;gt; &amp;gt; 如果一些特殊的关键字想做为表名，可以用反引号包含起来。比如： &amp;gt; &amp;gt; ``` &amp;gt; mysql&amp;gt; create table `select` (id int); &amp;gt; Query OK, 0 rows affected (0.27 sec) &amp;gt; ``` #### 查看为用户分配的权限 `SHOW GRANT` 语句可以查看为用户分配了哪些权限。 ```sql show grants for &amp;#39;root&amp;#39;@&amp;#39;%&amp;#39;; 更精确的方式，可以通过直接查看授权表的数据实现。比如想知道，test@% 该用户是否拥有对 db1.t 的 Insert 权限。先查看该用户是否拥有全局 Insert 权限：select Insert from mysql.user where user=&amp;#39;test&amp;#39; and host=&amp;#39;%&amp;#39;; 如果没有，再查看该用户是否拥有 db1 数据库级别的 Insert权限：select Insert from mysql.db where user=&amp;#39;test&amp;#39; and host=&amp;#39;%&amp;#39;; 如果仍然没有，则继续判断是否拥有 db1.t 这张表的 Insert 权限：select tables_priv from mysql.tables_priv where user=&amp;#39;test&amp;#39; and host=&amp;#39;%&amp;#39; and db=&amp;#39;db1&amp;#39;; 权限系统的实现 授权表 有几张系统表是非常特殊的表，权限相关的数据全部存储在这几张表内。 mysql.user 用户账户，全局权限 mysql.db 数据库级别的权限 mysql.tables_priv 表级别的权限 mysql.columns_priv 列级别的权限  这几张表包含了数据的生效范围和权限信息。例如，mysql.user 表的部分数据：mysql&amp;gt; select User,Host,Select_priv,Insert_priv from mysql.user limit 1; +------|------|-------------|-------------+ | User | Host | Select_priv | Insert_priv | +------|------|-------------|-------------+ | root | % | Y | Y | +------|------|-------------|-------------+ 1 row in set (0.00 sec) 这条记录中，Host 和 User 决定了 root 用户从任意主机（%）发送过来的连接请求可以被接受，而 Select_priv 和 Insert_priv 表示用户拥有全局的 Select 和 Insert 权限。mysql.user 这张表里面的生效范围是全局的。mysql.db 表里面包含的 Host 和 User 决定了用户可以访问哪些数据库，权限列的生效范围是数据库。理论上，所有权限管理相关的操作，都可以通过直接对授权表的 CRUD 操作完成。实现层面其实也只是包装了一层语法糖。例如删除用户会执行：delete from mysql.user where user=&amp;#39;test&amp;#39;; 但是不推荐用户手动修改授权表。连接验证 当客户端发送连接请求时，TiDB 服务器会对登陆操作进行验证。验证过程先检查 mysql.user 表，当某条记录的 User 和 Host 和连接请求匹配上了，再去验证 Password。用户身份基于两部分信息，发起连接的客户端的 Host，以及用户名 User。如果 User不为空，则用户名必须精确匹配。User+Host 可能会匹配 user 表里面多行，为了处理这种情况，user 表的行是排序过的，客户端连接时会依次去匹配，并使用首次匹配到的那一行做权限验证。排序是按 Host 在前，User 在后。请求验证 连接成功之后，请求验证会检测执行操作是否拥有足够的权限。对于数据库相关请求 (INSERT，UPDATE)，先检查 mysql.user 表里面的用户全局权限，如果权限够，则直接可以访问。如果全局权限不足，则再检查 mysql.db 表。user 表的权限是全局的，并且不管默认数据库是哪一个。比如 user 里面有 DELETE 权限，任何一行，任何的表，任何的数据库。db表里面，User 为空是匹配匿名用户，User 里面不能有通配符。Host和Db列里面可以有 % 和 _，可以模式匹配。user 和 db 读到内存也是排序的。tables_priv 和 columns_priv 中使用 % 是类似的，但是在Db, Table_name, Column_name 这些列不能包含 %。加载进来时排序也是类似的。生效时机 TiDB 启动时，将一些权限检查的表加载到内存，之后使用缓存的数据来验证权限。系统会周期性的将授权表从数据库同步到缓存，生效则是由同步的周期决定，目前这个值设定的是5分钟。修改了授权表，如果需要立即生效，可以手动调用：flush privileges; 限制和约束 一些使用频率偏低的权限当前版本的实现中还未做检查，比如 FILE/USAGE/SHUTDOWN/EXECUTE/PROCESS/INDEX 等等，未来会陆续完善。现阶段对权限的支持还没有做到 column 级别。Create User 语句 CREATE USER [IF NOT EXISTS] user [auth_spec] [, user [auth_spec]] ... auth_spec: { IDENTIFIED BY &amp;#39;auth_string&amp;#39; | IDENTIFIED BY PASSWORD &amp;#39;hash_string&amp;#39; } user 参见用户账号名。 IDENTIFIED BY &amp;lsquo;auth_string&amp;rsquo;  设置登录密码，auth_string 将会被 TiDB 经过加密存储在 mysql.user 表中。 IDENTIFIED BY PASSWORD &amp;lsquo;hash_string&amp;rsquo;  设置登录密码，hash_string 将会被 TiDB 经过加密存储在 mysql.user 表中。目前这个行为和 MySQL 不一致，会在接下来的版本中修改为和 MySQL 一致的行为。"},
		{"url": "https://pingcap.com/docs-cn/sql/comment-syntax/",
		"title": "注释语法", 
		"content": " 注释语法 TiDB 支持三种注释风格： 用 # 注释一行 用 -- 注释一行，用 -- 注释必须要在其之后留出至少一个空格。 用 /* */ 注释一块，可以注释多行。  例：mysql&amp;gt; SELECT 1+1; # This comment continues to the end of line +------+ | 1+1 | +------+ | 2 | +------+ 1 row in set (0.00 sec) mysql&amp;gt; SELECT 1+1; -- This comment continues to the end of line +------+ | 1+1 | +------+ | 2 | +------+ 1 row in set (0.00 sec) mysql&amp;gt; SELECT 1 /* this is an in-line comment */ + 1; +--------+ | 1 + 1 | +--------+ | 2 | +--------+ 1 row in set (0.01 sec) mysql&amp;gt; SELECT 1+ -&amp;gt; /* /*&amp;gt; this is a /*&amp;gt; multiple-line comment /*&amp;gt; */ -&amp;gt; 1; +-------+ | 1+ 1 | +-------+ | 2 | +-------+ 1 row in set (0.00 sec) mysql&amp;gt; SELECT 1+1--1; +--------+ | 1+1--1 | +--------+ | 3 | +--------+ 1 row in set (0.01 sec) TiDB 也跟 MySQL 保持一致，支持一种 C 风格注释的变体：/*! Specific code */ 在这种格式中，TiDB 会执行注释中的语句，这个语法是为了让这些 SQL 在其他的数据库中被忽略，而在 TiDB 中被执行。例如： SELECT /*! STRAIGHT_JOIN */ col1 FROM table1,table2 WHERE ...在 TiDB 中，这种写法等价于 SELECT STRAIGHT_JOIN col1 FROM table1,table2 WHERE ...如果注释中指定了 Server 版本号，例如 /*!50110 KEY_BLOCK_SIZE=1024 */，在 MySQL 中表示只有 MySQL 的版本大于等于 5.1.10 才会处理这个 comment 中的内容。但是在 TiDB 中，这个版本号不会起作用，所有的 comment 都会处理。还有一种注释会被当做是优化器 Hint 特殊对待：SELECT /*+ hint */ FROM ...; 由于 hint 包含在类似 /*+ xxx */ 的 comment 里，MySQL 客户端在 5.7.7 之前，会默认把 comment 清除掉，如果需要在旧的客户端使用 hint，需要在启动客户端时加上 &amp;ndash;comments 选项，例如 mysql -h 127.0.0.1 -P 4000 -uroot &amp;ndash;comments目前支持的 TiDB 特有的 Hint 有以下： TIDB_SMJ(t1, t2)SELECT /*+ TIDB_SMJ(t1, t2) */ * from t1，t2 where t1.id = t2.id提示优化器使用 Sort Merge Join 算法，这个算法通常会占用更少的内存，但执行时间会更久。当数据量太大，或系统内存不足时，建议尝试使用。 TIDB_INLJ(t1, t2)SELECT /*+ TIDB_INLJ(t1, t2) */ * from t1，t2 where t1.id = t2.id提示优化器使用 Index Nested Loop Join 算法，这个算法可能会在某些场景更快，消耗更少系统资源，有的场景会更慢，消耗更多系统资源。对于外表经过 WHERE 条件过滤后结果集较小（小于 1 万行）的场景，可以尝试使用。TIDB_INLJ() 中的参数是建立查询计划时，驱动表（外表）的候选表。即 TIDB_INLJ(t1) 只会考虑使用 t1 作为驱动表构建查询计划。  更多细节。"},
		{"url": "https://pingcap.com/recruit-cn/sales/channel-co-director/",
		"title": "渠道合作总监", 
		"content": " 渠道合作总监 岗位职责  根据公司产品（分布式数据库）特性，负责相关合作伙伴的建立、维护、发展与管理 制定相关合作伙伴的拓展计划 执行并完成相关合作伙伴拓展计划和销售任务 配合销售及市场部门完成相关工作  职位要求  本科以上学历,三年以上软件行业渠道销售经验 与主要IT系统集成商、行业应用软件开发商具有良好的合作关系 具有良好的渠道拓展能力和丰富的渠道资源和渠道管理经验 具有数据库软件等基础软件渠道销售经验者优先，有技术背景优先 强烈的责任心、良好的沟通能力、团队协作能力  待遇 20K - 35K , 13薪 + 业绩奖金，优秀者可面议工作地点 北京"},
		{"url": "https://pingcap.com/docs-cn/sql/understanding-the-query-execution-plan/",
		"title": "理解 TiDB 执行计划", 
		"content": " 理解 TiDB 执行计划 TiDB 优化器会根据当前数据表的实际情况来选择最优的执行计划，执行计划由一系列的 operator 构成，这里我们详细解释一下 TiDB 中 EXPLAIN 语句返回的执行计划信息。使用 EXPLAIN 来优化 SQL 语句 EXPLAIN 语句的返回结果提供了 TiDB 执行 SQL 查询的详细信息： EXPLAIN 可以和 SELECT, DELETE, INSERT, REPLACE, 以及 UPDATE 语句一起使用； 执行 EXPLAIN，TiDB 会返回被 EXPLAIN 的 SQL 语句经过优化器后的最终物理执行计划。也就是说，EXPLAIN 展示了 TiDB 执行该 SQL 语句的完整信息，比如以什么样的顺序，什么方式 JOIN 两个表，表达式树长什么样等等。详细请看 EXPLAIN 输出格式； TiDB 目前还不支持 EXPLAIN [options] FOR CONNECTION connection_id，我们将在未来支持它，详细请看：#4351；  通过观察 EXPLAIN 的结果，你可以知道如何给数据表添加索引使得执行计划使用索引从而加速 SQL 语句的执行速度；你也可以使用 EXPLAIN 来检查优化器是否选择了最优的顺序来 JOIN 数据表。EXPLAIN 输出格式 目前 TiDB 的 EXPLAIN 会输出 6 列，分别是：id，parents，children，task，operator info 和 count，执行计划中每个 operator 都由这 6 列属性来描述，EXPLAIN 结果中每一行描述一个 operator。下面详细解释每个属性的含义：   属性名 含义     id operator 的 id，在整个执行计划中唯一的标识一个 operator   parents 这个 operator 的 parent。目前的执行计划可以看做是一个 operator 构成的树状结构，数据从 child 流向 parent，每个 operator 的 parent 有且仅有一个   children 这个 operator 的 children，也即是这个 operator 的数据来源   task 当前这个 operator 属于什么 task。目前的执行计划分成为两种 task，一种叫 root task，在 tidb-server 上执行，一种叫 cop task，并行的在 tikv 上执行。当前的执行计划在 task 级别的拓扑关系是一个 root task 后面可以跟许多 cop task，root task 使用 cop task 的输出结果作为输入。cop task 中执行的也即是 tidb 下推到 tikv 上的任务，每个 cop task 分散在 tikv 集群中，由多个进程共同执行   operator info 每个 operator 的详细信息。各个 operator 的 operator info 各有不同，我们将在 Operator Info 中详细介绍   count 预计当前 operator 将会输出的数据条数，基于统计信息以及 operator 的执行逻辑估算而来    概述 Task 简介 目前 TiDB 的计算任务隶属于两种不同的 task: cop task 和 root task。cop task 是指被下推到 KV 端分布式执行的计算任务，root task 是指在 TiDB 端单点执行的计算任务。SQL 优化的目标之一是将计算尽可能的下推到 KV 端执行。表数据和索引数据 TiDB 的表数据是指一张表的原始数据，存放在 TiKV 中。对于每行表数据，它的 key 是一个 64 位整数，称为 Handle ID。如果一张表存在 int 类型的主键，我们会把主键的值当作表数据的 Handle ID，否则由系统自动生成 Handle ID。表数据的 value 由这一行的所有数据编码而成。在读取表数据的时候，我们可以按照 Handle ID 递增的顺序返回。TiDB 的索引数据和表数据一样，也存放在 TiKV 中。它的 key 是由索引列编码的有序 bytes，value 是这一行索引数据对应的 Handle ID，通过 Handle ID 我们可以读取这一行的非索引列。在读取索引数据的时候，我们按照索引列递增的顺序返回，如果有多个索引列，我们首先保证第 1 列递增，并且在第 i 列相等的情况下，保证第 i + 1 列递增。范围查询 在 WHERE/HAVING/ON 条件中，我们会分析主键或索引键的查询返回。如数字、日期类型的比较符，如大于、小于、等于以及大于等于、小于等于，字符类型的 LIKE 符号等。 值得注意的是，我们只支持比较符一端是列，另一端是常量，或可以计算成某一常量的情况，类似 year(birth_day) &amp;lt; 1992 的查询条件是不能利用索引的。还要注意应尽可能使用同一类型进行比较，以避免引入额外的 cast 操作而导致不能利用索引，如 user_id = 123456，如果 user_id 是字符串，需要将 123456 也写成字符串常量的形式。 针对同一列的范围查询条件使用 AND 和 OR 组合后，等于对范围求交集或者并集。对于多维组合索引，我们可以写多个列的条件。例如对组合索引(a, b, c)，当 a 为等值查询时，可以继续求 b 的查询范围，当 b 也为等值查询时，可以继续求 c 的查询范围，反之如果 a 为非等值查询，则只能求 a 的范围。Operator Info TableReader 和 TableScan TableScan 表示在 KV 端对表数据进行扫描，TableReader 表示在 TiDB 端从 TiKV 端读取，属于同一功能的两个算子。table 表示 SQL 语句中的表名，如果表名被重命名，则显示重命名。range 表示扫描的数据范围，如果在查询中不指定 WHERE/HAVING/ON 条件，则会选择全表扫描，如果在 int 类型的主键上有范围查询条件，会选择范围查询。keep order 表示 table scan 是否按顺序返回。IndexReader 和 IndexLookUp Index 在 TiDB 端的读取方式有两种：IndexReader 表示直接从索引中读取索引列，适用于 SQL 语句中仅引用了该索引相关的列或主键；IndexLookUp 表示从索引中过滤部分数据，仅返回这些数据的 Handle ID，通过 Handle ID 再次查找表数据，这种方式需要两次从 TiKV 获取数据。Index 的读取方式是由优化器自动选择的。IndexScan 是 KV 端读取索引数据的算子，和 TableScan 功能类似。table 表示 SQL 语句中的表名，如果表名被重命名，则显示重命名。index 表示索引名。range 表示扫描的数据范围。out of order 表示 index scan 是否按照顺序返回。注意在 TiDB 中，多列或者非 int 列构成的主键是当作唯一索引处理的。Selection Selection 表示 SQL 语句中的选择条件，通常出现在 WHERE/HAVING/ON 子句中。Projection Projection 对应 SQL 语句中的 SELECT 列表，功能是将每一条输入数据映射成新的输出数据。Aggregation Aggregation 对应 SQL 语句中的 Group By 语句或者没有 Group By 语句但是存在聚合函数，例如 count 或 sum 函数等。TiDB 支持两种聚合算法：Hash Aggregation 以及 Stream Aggregation（待补充）。Hash Aggregation 是基于哈希的聚合算法，如果 Hash Aggregation 紧邻 Table 或者 Index 的读取算子，则聚合算子会在 TiKV 端进行预聚合，以提高计算的并行度和减少网络开销。Join TiDB 支持 Inner Join 以及 Left/Right Outer Join，并会自动将可以化简的外连接转换为 Inner Join。TiDB 支持三种 Join 算法：Hash Join，Sort Merge Join 和 Index Look up Join。Hash Join 的原理是将参与连接的小表预先装载到内存中，读取大表的所有数据进行连接。Sort Merge Join 会利用输入数据的有序信息，同时读取两张表的数据并依次进行比较。Index Look Up Join 会读取外表的数据，并对内表进行主键或索引键查询。Apply Apply 是 TiDB 用来描述子查询的一种算子，行为类似于 Nested Loop，即每次从外表中取一条数据，带入到内表的关联列中，并执行，最后根据 Apply 内联的 Join 算法进行连接计算。值得注意的是，Apply 一般会被查询优化器自动转换为 Join 操作。用户在编写 SQL 的过程中应尽量避免 Apply 算子的出现。"},
		{"url": "https://pingcap.com/docs-cn/op-guide/generate-self-signed-certificates/",
		"title": "生成自签名证书", 
		"content": " 生成自签名证书 概述 本文档提供使用 cfssl 生成自签名证书的示例。假设实例集群拓扑如下：   Name Host IP Services     node1 172.16.10.1 PD1, TiDB1   node2 172.16.10.2 PD2, TiDB2   node3 172.16.10.3 PD3   node4 172.16.10.4 TiKV1   node5 172.16.10.5 TiKV2   node6 172.16.10.6 TiKV3    下载 cfssl 假设使用 x86_64 Linux 主机：mkdir ~/bin curl -s -L -o ~/bin/cfssl https://pkg.cfssl.org/R1.2/cfssl_linux-amd64 curl -s -L -o ~/bin/cfssljson https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64 chmod +x ~/bin/{cfssl,cfssljson} export PATH=$PATH:~/bin 初始化证书颁发机构 生成 cfssl 的默认配置，以便于之后修改：mkdir ~/cfssl cd ~/cfssl cfssl print-defaults config &amp;gt; ca-config.json cfssl print-defaults csr &amp;gt; ca-csr.json 生成证书 证书介绍  tidb-server certificate 由 TiDB 使用，为其他组件和客户端验证 TiDB 身份。 tikv-server certificate 由 TiKV 使用，为其他组件和客户端验证 TiKV 身份。 pd-server certificate 由 PD 使用，为其他组件和客户端验证 PD 身份。 client certificate 用于通过 PD、TiKV、TiDB 验证客户端。例如 pd-ctl，tikv-ctl，pd-recover。  配置 CA 选项 根据实际需求修改 ca-config.json ：{ &amp;#34;signing&amp;#34;: { &amp;#34;default&amp;#34;: { &amp;#34;expiry&amp;#34;: &amp;#34;43800h&amp;#34; }, &amp;#34;profiles&amp;#34;: { &amp;#34;server&amp;#34;: { &amp;#34;expiry&amp;#34;: &amp;#34;43800h&amp;#34;, &amp;#34;usages&amp;#34;: [ &amp;#34;signing&amp;#34;, &amp;#34;key encipherment&amp;#34;, &amp;#34;server auth&amp;#34;, &amp;#34;client auth&amp;#34; ] }, &amp;#34;client&amp;#34;: { &amp;#34;expiry&amp;#34;: &amp;#34;43800h&amp;#34;, &amp;#34;usages&amp;#34;: [ &amp;#34;signing&amp;#34;, &amp;#34;key encipherment&amp;#34;, &amp;#34;client auth&amp;#34; ] } } } } 根据实际需求修改 ca-csr.json ：{ &amp;#34;CN&amp;#34;: &amp;#34;My own CA&amp;#34;, &amp;#34;key&amp;#34;: { &amp;#34;algo&amp;#34;: &amp;#34;rsa&amp;#34;, &amp;#34;size&amp;#34;: 2048 }, &amp;#34;names&amp;#34;: [ { &amp;#34;C&amp;#34;: &amp;#34;CN&amp;#34;, &amp;#34;L&amp;#34;: &amp;#34;Beijing&amp;#34;, &amp;#34;O&amp;#34;: &amp;#34;PingCAP&amp;#34;, &amp;#34;ST&amp;#34;: &amp;#34;Beijing&amp;#34; } ] } 生成 CA 证书 cfssl gencert -initca ca-csr.json | cfssljson -bare ca - 将会生成以下几个文件：ca-key.pem ca.csr ca.pem 生成服务器端证书 hostname 中为各组件的 IP 地址，以及 127.0.0.1echo &amp;#39;{&amp;#34;CN&amp;#34;:&amp;#34;tidb-server&amp;#34;,&amp;#34;hosts&amp;#34;:[&amp;#34;&amp;#34;],&amp;#34;key&amp;#34;:{&amp;#34;algo&amp;#34;:&amp;#34;rsa&amp;#34;,&amp;#34;size&amp;#34;:2048}}&amp;#39; | cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=server -hostname=&amp;#34;172.16.10.1,172.16.10.2,127.0.0.1&amp;#34; - | cfssljson -bare tidb-server echo &amp;#39;{&amp;#34;CN&amp;#34;:&amp;#34;tikv-server&amp;#34;,&amp;#34;hosts&amp;#34;:[&amp;#34;&amp;#34;],&amp;#34;key&amp;#34;:{&amp;#34;algo&amp;#34;:&amp;#34;rsa&amp;#34;,&amp;#34;size&amp;#34;:2048}}&amp;#39; | cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=server -hostname=&amp;#34;172.16.10.4,172.16.10.5,172.16.10.6,127.0.0.1&amp;#34; - | cfssljson -bare tikv-server echo &amp;#39;{&amp;#34;CN&amp;#34;:&amp;#34;pd-server&amp;#34;,&amp;#34;hosts&amp;#34;:[&amp;#34;&amp;#34;],&amp;#34;key&amp;#34;:{&amp;#34;algo&amp;#34;:&amp;#34;rsa&amp;#34;,&amp;#34;size&amp;#34;:2048}}&amp;#39; | cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=server -hostname=&amp;#34;172.16.10.1,172.16.10.2,172.16.10.3,127.0.0.1&amp;#34; - | cfssljson -bare pd-server 将会生成以下几个文件：tidb-server-key.pem tikv-server-key.pem pd-server-key.pem tidb-server.csr tikv-server.csr pd-server.csr tidb-server.pem tikv-server.pem pd-server.pem 生成客户端证书 echo &amp;#39;{&amp;#34;CN&amp;#34;:&amp;#34;client&amp;#34;,&amp;#34;hosts&amp;#34;:[&amp;#34;&amp;#34;],&amp;#34;key&amp;#34;:{&amp;#34;algo&amp;#34;:&amp;#34;rsa&amp;#34;,&amp;#34;size&amp;#34;:2048}}&amp;#39; | cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=client -hostname=&amp;#34;&amp;#34; - | cfssljson -bare client 将会生成以下几个文件：client-key.pem client.csr client.pem "},
		{"url": "https://pingcap.com/docs-cn/op-guide/offline-ansible-deployment/",
		"title": "离线 TiDB Ansible 部署方案", 
		"content": " 离线 TiDB Ansible 部署方案 准备机器  下载机一台 该机器需开放外网访问，用于下载 TiDB-Ansible、TiDB 及相关软件安装包。 推荐安装 CentOS 7.3 及以上版本 Linux 操作系统。  部署目标机器若干及部署中控机一台 系统要求及配置参考准备机器。 可以无法访问外网。   在中控机器上离线安装 Ansible 及其依赖  CentOS 7 系统 Ansible 离线安装方式： 下载 Ansible  离线安装包 ，上传至中控机。 # tar -xzvf ansible-2.4-rpms.el7.tar.gz  # cd ansible-2.4-rpms.el7  # rpm -ivh PyYAML*rpm libyaml*rpm python-babel*rpm python-backports*rpm python-backports-ssl_match_hostname*rpm python-cffi*rpm python-enum34*rpm python-httplib2*rpm python-idna*rpm python-ipaddress*rpm python-jinja2*rpm python-markupsafe*rpm python-paramiko*rpm python-passlib*rpm python-ply*rpm python-pycparser*rpm python-setuptools*rpm python-six*rpm python2-cryptography*rpm python2-jmespath*rpm python2-pyasn1*rpm sshpass*rpm  # rpm -ivh ansible-2.4.2.0-2.el7.noarch.rpm 安装完成后，可通过 ansible --version 查看版本：# ansible --version  ansible 2.4.2.0  在下载机上下载 TiDB-Ansible 及 TiDB 安装包  在下载机上安装 Ansible请按以下方式在 CentOS 7 系统的下载机上在线安装 Ansible。 通过 epel 源安装， 会自动安装 Ansible 相关依赖(如 Jinja2==2.7.2 MarkupSafe==0.11)，安装完成后，可通过 ansible --version 查看版本，请务必确认是 Ansible 2.4 及以上版本，否则会有兼容问题。# yum install epel-release # yum install ansible curl # ansible --version  ansible 2.4.2.0 下载 tidb-ansible使用以下命令从 Github TiDB-Ansible 项目 上下载 TiDB-Ansible 相应版本，默认的文件夹名称为 tidb-ansible。下载 GA 版本：git clone -b release-1.0 https://github.com/pingcap/tidb-ansible.git 或下载 master 版本：git clone https://github.com/pingcap/tidb-ansible.git  注： 生产环境请下载 GA 版本部署 TiDB。 执行 local_prepare.yml playbook，联网下载 TiDB binary 到下载机cd tidb-ansible ansible-playbook local_prepare.yml 将执行完以上命令之后的 tidb-ansible 文件夹拷贝到中控机 /home/tidb 目录下，文件属主权限需是 tidb 用户。  分配机器资源，编辑 inventory.ini 文件 参考分配机器资源，编辑 inventory.ini 文件即可。部署任务  参考部署任务即可。 ansible-playbook local_prepare.yml 该 playbook 不需要再执行。  测试集群 参考测试集群即可。"},
		{"url": "https://pingcap.com/docs-cn/sql/variable/",
		"title": "系统变量", 
		"content": " 系统变量 MySQL 系统变量 (System Variables) 是一些系统参数，用于调整数据库运行时的行为，根据变量的作用范围分为全局范围有效（Global Scope）以及会话级别有效（Session Scope）。TiDB 支持 MySQL5.7 的所有系统变量，大部分变量仅仅是为了兼容性而支持，不会影响运行时行为。设置系统变量 通过 SET 语句可以修改系统变量的值。进行修改时，还要考虑变量可修改的范围，不是所有的变量都能在全局/会话范围内进行修改。具体的可修改范围参考 MySQL 动态变量文档。全局范围值  在变量名前加 GLOBAL 关键词或者是使用 @@global. 作为修饰符:  SET GLOBAL autocommit = 1; SET @@global.autocommit = 1; 会话范围值  在变量名前加 SESSION 关键词或者是使用 @@session. 作为修饰符，或者是不加任何修饰符:  SET SESSION autocommit = 1; SET @@session.autocommit = 1; SET @@autocommit = 1;  LOCAL 以及 @@local. 是 SESSION 以及 @@session. 的同义词  TiDB 支持的 MySQL 系统变量 下列系统变量是 TiDB 真正支持并且行为和 MySQL 一致：   变量名 作用域 说明     autocommit GLOBAL | SESSION 是否自动 Commit 事务   sql_mode GLOBAL | SESSION 支持部分 MySQL SQL mode，   time_zone GLOBAL | SESSION 数据库所使用的时区   tx_isolation GLOBAL | SESSION 事务隔离级别    TiDB 特有的系统变量 参见 TiDB 专用系统变量。"},
		{"url": "https://pingcap.com/docs-cn/sql/statistics/",
		"title": "统计信息简介", 
		"content": " 统计信息简介 TiDB 优化器会根据统计信息来选择最优的执行计划。统计信息收集了表级别和列级别的信息，表的统计信息包括总行数，以及修改的行数。列的统计信息包括不同值的数量，NULL 的数量，以及该列的直方图信息。统计信息的收集 手动收集 你可以通过执行 ANALYZE 语句来收集统计信息。语法：ANALYZE TABLE TableNameList &amp;gt; 该语句会收集 TableNameList 中所有表的统计信息。 ANALYZE TABLE TableName INDEX IndexNameList &amp;gt; 该语句会收集 TableName 中所有的 IndexNameList 中的索引列的统计信息。 自动更新 在发生增加，删除以及修改语句时，TiDB 会自动更新表的总行数以及修改的行数。这些信息会定期持久化下来， 更新的周期是 5 * stats-lease, stats-lease 的默认值是 3s，如果将其指定为 0，那么将不会自动更新。控制 ANALYZE 并发度 执行 ANALYZE 语句的时候，你可以通过一些参数来调整并发度，以控制对系统的影响。tidb_build_stats_concurrency 目前 ANALYZE 执行的时候会被切分成一个个小的任务，每个任务只负责某一个列或者索引。tidb_build_stats_concurrency 可以控制同时执行的任务的数量，其默认值是 4。tidb_distsql_scan_concurrency 在执行分析普通列任务的时候，tidb_distsql_scan_concurrency 可以用于控制一次读取的 Region 数量，其默认值是 10。tidb_index_serial_scan_concurrency 在执行分析索引列任务的时候，tidb_index_serial_scan_concurrency 可以用于控制一次读取的 Region 数量，其默认值是 1。统计信息的查看 你可以通过一些语句来查看统计信息的状态。表的元信息 你可以通过 SHOW STATS_META 来查看表的总行数以及修改的行数等信息。语法：SHOW STATS_META [ShowLikeOrWhere] &amp;gt; 该语句会输出所有表的总行数以及修改行数等信息，你可以通过使用 ShowLikeOrWhere 来筛选需要的信息。 目前 SHOW STATS_META 会输出 5 列，具体如下：   语法元素 说明     db_name 数据库名   table_name 表名   update_time 更新时间   modify_count 修改的行数   row_count 总行数    列的元信息 你可以通过 SHOW STATS_HISTOGRAMS 来查看列的不同值数量以及 NULL 数量等信息。语法：SHOW STATS_HISTOGRAMS [ShowLikeOrWhere] &amp;gt; 该语句会输出所有列的不同值数量以及 NULL 数量等信息，你可以通过使用 ShowLikeOrWhere 来筛选需要的信息。 目前 SHOW STATS_HISTOGRAMS 会输出 7 列，具体如下：   语法元素 说明     db_name 数据库名   table_name 表名   column_name 列名   is_index 是否是索引列   update_time 更新时间   distinct_count 不同值数量   null_count NULL 的数量    直方图桶的信息 你可以通过 SHOW STATS_BUCKETS 来查看直方图每个桶的信息。语法：SHOW STATS_BUCKETS [ShowLikeOrWhere] &amp;gt; 该语句会输出所有桶的信息，你可以通过使用 ShowLikeOrWhere 来筛选需要的信息。 目前 SHOW STATS_BUCKETS 会输出 9 列，具体如下：   语法元素 说明     db_name 数据库名   table_name 表名   column_name 列名   is_index 是否是索引列   bucket_id 桶的编号   count 所有落在这个桶及之前桶中值的数量   repeats 最大值出现的次数   lower_bound 最小值   upper_bound 最大值    删除统计信息 可以通过执行 DROP STATS 语句来删除统计信息。语法：DROP STATS TableName &amp;gt; 该语句会删除 TableName 中所有的统计信息。"},
		{"url": "https://pingcap.com/recruit-cn/sales/sales-director/",
		"title": "行业销售总监", 
		"content": " 行业销售总监 岗位职责  负责分布式数据库产品的业务拓展、合作及销售 深入掌握和分析行业市场信息，把握最新销售信息，为公司提供业务发展战略依据 负责目标顾客的开发与维护、项目谈判，调配各种资源达成公司制定销售指标，扩大产品的市场占有率 负责拓展新客户和新业务，积极了解客户的需求并进行专业分析和评估，制定合理销售方案 完成整体业绩指标，包括销售额和回款额  职位要求  本科或以上学历，通信、计算机、企业管理、市场营销等相关专业 五年以上IT产品厂商和集成商销售经验，出色的过往销售业绩 有特定行业的客户和渠道资源，比如政府、银行、保险、电信、能源等 出众的沟通表达能力、抗压能力、管理能力，具有敬业精神及团队合作意识 如有数据库技术经验或售前实施经验有加分  待遇 25K - 35K , 13薪 + 业绩奖金，优秀者可面议工作地点 北京"},
		{"url": "https://pingcap.com/docs-cn/sql/type-conversion-in-expression-evaluation/",
		"title": "表达式求值的类型转换", 
		"content": " 表达式求值的类型转换 TiDB 中表达式求值的类型转换与 MySQL 基本一致，详情参见 MySQL 表达式求值的类型转换。"},
		{"url": "https://pingcap.com/docs-cn/sql/expression-syntax/",
		"title": "表达式语法", 
		"content": " 表达式语法(Expression Syntax) 在 TiDB 中，以下规则是表达式的语法，你可以在 parser/parser.y 中找到定义。TiDB 的语法解析是基于 yacc 的。Expression: singleAtIdentifier assignmentEq Expression | Expression logOr Expression | Expression &amp;#34;XOR&amp;#34; Expression | Expression logAnd Expression | &amp;#34;NOT&amp;#34; Expression | Factor IsOrNotOp trueKwd | Factor IsOrNotOp falseKwd | Factor IsOrNotOp &amp;#34;UNKNOWN&amp;#34; | Factor Factor: Factor IsOrNotOp &amp;#34;NULL&amp;#34; | Factor CompareOp PredicateExpr | Factor CompareOp singleAtIdentifier assignmentEq PredicateExpr | Factor CompareOp AnyOrAll SubSelect | PredicateExpr PredicateExpr: PrimaryFactor InOrNotOp &amp;#39;(&amp;#39; ExpressionList &amp;#39;)&amp;#39; | PrimaryFactor InOrNotOp SubSelect | PrimaryFactor BetweenOrNotOp PrimaryFactor &amp;#34;AND&amp;#34; PredicateExpr | PrimaryFactor LikeOrNotOp PrimaryExpression LikeEscapeOpt | PrimaryFactor RegexpOrNotOp PrimaryExpression | PrimaryFactor PrimaryFactor: PrimaryFactor &amp;#39;|&amp;#39; PrimaryFactor | PrimaryFactor &amp;#39;&amp;amp;&amp;#39; PrimaryFactor | PrimaryFactor &amp;#34;&amp;lt;&amp;lt;&amp;#34; PrimaryFactor | PrimaryFactor &amp;#34;&amp;gt;&amp;gt;&amp;#34; PrimaryFactor | PrimaryFactor &amp;#39;+&amp;#39; PrimaryFactor | PrimaryFactor &amp;#39;-&amp;#39; PrimaryFactor | PrimaryFactor &amp;#39;*&amp;#39; PrimaryFactor | PrimaryFactor &amp;#39;/&amp;#39; PrimaryFactor | PrimaryFactor &amp;#39;%&amp;#39; PrimaryFactor | PrimaryFactor &amp;#34;DIV&amp;#34; PrimaryFactor | PrimaryFactor &amp;#34;MOD&amp;#34; PrimaryFactor | PrimaryFactor &amp;#39;^&amp;#39; PrimaryFactor | PrimaryExpression PrimaryExpression: Operand | FunctionCallKeyword | FunctionCallNonKeyword | FunctionCallAgg | FunctionCallGeneric | Identifier jss stringLit | Identifier juss stringLit | SubSelect | &amp;#39;!&amp;#39; PrimaryExpression | &amp;#39;~&amp;#39; PrimaryExpression | &amp;#39;-&amp;#39; PrimaryExpression | &amp;#39;+&amp;#39; PrimaryExpression | &amp;#34;BINARY&amp;#34; PrimaryExpression | PrimaryExpression &amp;#34;COLLATE&amp;#34; StringName "},
		{"url": "https://pingcap.com/docs-cn/op-guide/location-awareness/",
		"title": "跨机房部署方案", 
		"content": " 跨机房部署方案 概述 PD 能够根据 TiKV 集群的拓扑结构进行调度，使得 TiKV 的容灾能力最大化。阅读本章前，请先确保阅读 Binary 部署方案 和 Docker 部署方案。TiKV 上报拓扑信息 可以通过 TiKV 的启动参数或者配置文件来让 TiKV 上报拓扑信息给 PD。假设拓扑结构分为三级：zone &amp;gt; rack &amp;gt; host，可以通过 labels 来指定这些信息。启动参数：tikv-server --labels zone=&amp;lt;zone&amp;gt;,rack=&amp;lt;rack&amp;gt;,host=&amp;lt;host&amp;gt; 配置文件：[server] labels = &amp;#34;zone=&amp;lt;zone&amp;gt;,rack=&amp;lt;rack&amp;gt;,host=&amp;lt;host&amp;gt;&amp;#34; PD 理解 TiKV 拓扑结构 可以通过 PD 的配置文件让 PD 理解 TiKV 集群的拓扑结构。[replication] max-replicas = 3 location-labels = [&amp;#34;zone&amp;#34;, &amp;#34;rack&amp;#34;, &amp;#34;host&amp;#34;] 其中 location-labels 需要与 TiKV 的 labels 名字对应，这样 PD 才能知道这些 labels 代表了 TiKV 的拓扑结构。PD 基于 TiKV 拓扑结构进行调度 PD 能够根据我们提供的拓扑信息作出最优的调度，我们只需要关心什么样的拓扑结构能够达到我们想要的效果。假设我们使用三副本，并且希望一个数据中心挂掉的情况下能够正常服务，我们至少需要四个数据中心 （理论上三个数据中心即可，但是当前实现无法保证）。假设我们有四个数据中心 (zone)，每个数据中心有两个机架 (rack)，每个机架上有两个主机 (host)。 每个主机上面启动一个 TiKV 实例：# zone=z1 tikv-server --labels zone=z1,rack=r1,host=h1 tikv-server --labels zone=z1,rack=r1,host=h2 tikv-server --labels zone=z1,rack=r2,host=h1 tikv-server --labels zone=z1,rack=r2,host=h2 # zone=z2 tikv-server --labels zone=z2,rack=r1,host=h1 tikv-server --labels zone=z2,rack=r1,host=h2 tikv-server --labels zone=z2,rack=r2,host=h1 tikv-server --labels zone=z2,rack=r2,host=h2 # zone=z3 tikv-server --labels zone=z3,rack=r1,host=h1 tikv-server --labels zone=z3,rack=r1,host=h2 tikv-server --labels zone=z3,rack=r2,host=h1 tikv-server --labels zone=z3,rack=r2,host=h2 # zone=z4 tikv-server --labels zone=z4,rack=r1,host=h1 tikv-server --labels zone=z4,rack=r1,host=h2 tikv-server --labels zone=z4,rack=r2,host=h1 tikv-server --labels zone=z4,rack=r2,host=h2 也就是说，我们有 16 个 TiKV 实例，分布在 4 个不同的数据中心，8 个不同的机架，16 个不同的机器。在这种拓扑结构下，PD 会优先把每一份数据的不同副本调度到不同的数据中心。 这时候如果其中一个数据中心挂了，不会影响正常服务。 如果这个数据中心一段时间内恢复不了，PD 会把这个数据中心的副本迁移出去。总的来说，PD 能够根据当前的拓扑结构使得集群容灾能力最大化，所以如果我们希望达到某个级别的容灾能力， 就需要根据拓扑机构在不同的地理位置提供多于备份数 (max-replicas) 的机器。"},
		{"url": "https://pingcap.com/docs-cn/op-guide/recommendation/",
		"title": "软件和硬件环境要求", 
		"content": " TiDB 软件和硬件环境要求 概述 TiDB 作为一款开源分布式 NewSQL 数据库，可以很好的部署和运行在 Intel 架构服务器环境及主流虚拟化环境，并支持绝大多数的主流硬件网络。作为一款高性能数据库系统，TiDB 支持主流的 Linux 操作系统环境。Linux 操作系统版本要求    Linux 操作系统平台 版本     Red Hat Enterprise Linux 7.3 及以上   CentOS 7.3 及以上   Oracle Enterprise Linux 7.3 及以上   Ubuntu LTS 16.04 及以上     注： TiDB 只支持 Red Hat 兼容内核 (RHCK) 的 Oracle Enterprise Linux，不支持 Oracle Enterprise Linux 提供的 Unbreakable Enterprise Kernel。 TiDB 对 Linux 操作系统的以上支持包括部署和运行在物理服务器以及 VMware、KVM、XEN 主流虚拟化环境。   服务器要求 TiDB 支持部署和运行在 Intel x86-64 架构的 64 位通用硬件服务器平台。对于开发，测试，及生产环境的服务器硬件配置有以下要求和建议：开发及测试环境    组件 CPU 内存 本地存储 网络 实例数量(最低要求)     TiDB 8核+ 16 GB+ SAS, 200 GB+ 千兆网卡 1（可与 PD 同机器）   PD 8核+ 16 GB+ SAS, 200 GB+ 千兆网卡 1（可与 TiDB 同机器）   TiKV 8核+ 32 GB+ SSD, 200 GB+ 千兆网卡 3       服务器总计 4     注： 验证测试环境中的 TiDB 和 PD 可以部署在同一台服务器上。 如进行性能相关的测试，避免采用低性能存储和网络硬件配置，防止对测试结果的正确性产生干扰。 如果仅验证功能，建议使用 Docker Compose 部署方案单机进行测试。   生产环境    组件 CPU 内存 硬盘类型 网络 实例数量(最低要求)     TiDB 16核+ 48 GB+ SAS 万兆网卡（2块最佳） 2   PD 8核+ 16 GB+ SSD 万兆网卡（2块最佳） 3   TiKV 16核+ 48 GB+ SSD 万兆网卡（2块最佳） 3   监控 8核+ 16 GB+ SAS 千兆网卡 1       服务器总计 9     注： 生产环境中的 TiDB 和 PD 可以部署和运行在同服务器上，如对性能和可靠性有更高的要求，应尽可能分开部署。 生产环境强烈推荐使用更高的配置。 TiKV 硬盘大小建议不要超过 800G 以防止硬盘损坏时，数据恢复耗时过长   网络要求 TiDB 作为开源分布式 NewSQL 数据库，其正常运行需要网络环境提供如下的网络端口配置要求，管理员可根据实际环境中 TiDB 组件部署的方案，在网络侧和主机侧启用相关端口：   组件 默认端口 说明     TiDB 4000 应用及 DBA 工具访问通信端口   TiDB 10080 TiDB 状态信息上报通信端口   TiKV 20160 TiKV 通信端口   PD 2379 提供 TiDB 和 PD 通信端口   PD 2380 PD 集群节点间通信端口   Prometheus 9090 Prometheus 服务通信端口   Pushgateway 9091 TiDB, TiKV, PD 监控聚合和上报端口   Node_exporter 9100 TiDB 集群每个节点的系统信息上报通信端口   Grafana 3000 Web 监控服务对外服务和客户端(浏览器)访问端口    客户端 Web 浏览器要求 TiDB 提供了基于 Prometheus 和 Grafana 技术平台作为 TiDB 分布式数据库集群的可视化监控数据展现方案。建议用户采用高版本的微软 IE, Google Chrome，Mozilla Firefox 访问 Grafana 监控入口。"},
		{"url": "https://pingcap.com/docs-cn/op-guide/dashboard-overview-info/",
		"title": "重要监控指标详解", 
		"content": " 重要监控指标详解 使用 ansible 部署 tidb 集群时，一键部署监控系统 (prometheus/grafana)，监控架构请看 TiDB 监控框架概述目前 grafana dashboard 整体分为四个 dashboard，node_export，PD，TIDB，TIKV。 内容较多，主要在于尽快让 TIDB 开发确认问题。对于日常运维，我们单独挑选出重要的 metrics 放在 overview 页面，方便日常运维人员观察集群组件(PD, TIDB, TIKV)使用状态以及集群使用状态 。以下为 overview dashboard 说明：说明  PD Storage Capacity : tidb 集群总可用数据库空间大小 Current Storage Size : tidb 集群目前已用数据库空间大小 Store Status &amp;ndash; up store : tikv 正常节点数量 Store Status &amp;ndash; down store : tikv 异常节点数量如果大于0，证明有节点不正常 Store Status &amp;ndash; offline store : 手动执行下线操作tikv节点数量 Store Status &amp;ndash; Tombstone store : 下线成功的tikv节点数量 Current storage usage : tikv 集群存储空间占用率超过 80% 应考虑添加 tikv 节点 99% completed_cmds_duration_seconds : 99% pd-server 请求完成时间小于 5ms average completed_cmds_duration_seconds : pd-server 请求平均完成时间小于 50ms leader balance ratio : leader ratio 最大的节点与最小的节点的差均衡状况下一般小于 5%，节点重启时会比较大 region balance ratio : region ratio 最大的节点与最小的节点的差均衡状况下一般小于 5%，新增/下线节点时会比较大  TiDB handle_requests_duration_seconds : 请求PD获取TSO响应时间小于100ms tidb server QPS : 集群的请求量和业务相关 connection count : 从业务服务器连接到数据库的连接数和业务相关。但是如果连接数发生跳变，需要查明原因。比如突然掉为0，可以检查网络是否中断；如果突然上涨，需要检查业务。 statement count : 单位时间内不同类型语句执行的数目这个和业务相关 Query Duration 99th percentile : 99% 的query时间  TiKV 99% &amp;amp; 99.99% scheduler command duration : 99% &amp;amp; 99.99% 命令执行的时间99% 小于 50ms；99.99% 小于100ms 95% &amp;amp; 99% storage async_request duration : 95% &amp;amp; 99% Raft 命令执行时间95% 小于 50ms；99% 小于100ms server report failure message : 发送失败或者收到了错误的 message如果出现了大量的 unreachadble 的消息，表明系统网络出现了问题。如果有 store not match 这样的错误，表明收到了不属于这个集群发过来的消息 Vote : Raft vote 的频率通常这个值只会在发生 split 的时候有变动，如果长时间出现了 vote 偏高的情况，证明系统出现了严重的问题，有一些节点无法工作了 95% &amp;amp; 99% coprocessor request duration : 95% &amp;amp; 99% coprocessor 执行时间和业务相关，但通常不会出现持续高位的值 Pending task : 累积的任务数量除了 pd worker，其他任何偏高都属于异常 stall : RocksDB Stall 时间大于 0，表明 RocksDB 忙不过来，需要注意 IO 和 CPU 了 channel full : channel 满了，表明线程太忙无法处理如果大于 0，表明线程已经没法处理了 95% send_message_duration_seconds : 95% 发送消息的时间小于50ms leader/region : 每个tikv的leader/region数量和业务相关   图例 "},
		{"url": "https://pingcap.com/docs-cn/sql/error/",
		"title": "错误码与故障诊断", 
		"content": " 错误码与故障诊断 本篇文档描述在使用 TiDB 过程中会遇到的问题以及解决方法。错误码 TiDB 兼容 MySQL 的错误码，在大多数情况下，返回和 MySQL 一样的错误码。另外还有一些特有的错误码：   错误码 说明     9001 请求 PD 超时，请检查 PD Server 状态/监控/日志以及 TiDB Server 与 PD Server 之间的网络   9002 请求 TiKV 超时，请检查 TiKV Server 状态/监控/日志以及 TiDB Server 与 TiKV Server 之间的网络   9003 TiKV 操作繁忙，一般出现在数据库负载比较高时，请检查 TiKV Server 状态/监控/日志   9004 当数据库上承载的业务存在大量的事务冲突时，会遇到这种错误，请检查业务代码   9005 某个 Raft Group 不可用，如副本数目不足，出现在 TiKV 比较繁忙或者是 TiKV 节点停机的时候，请检查 TiKV Server 状态/监控/日志   9006 GC Life Time 间隔时间过短，长事务本应读到的数据可能被清理了,应增加GC Life Time   9500 单个事务过大，原因及解决方法请参考这里    故障诊断 参见故障诊断文档以及 FAQ。"},
		{"url": "https://pingcap.com/recruit-cn/sales/senior-business-manager/",
		"title": "高级业务拓展（销售）经理", 
		"content": " 高级业务拓展（销售）经理 岗位职责  负责分布式数据库产品的业务拓展、合作及销售，偏新客户拓展 收集整理市场信息，推动合作伙伴、分销渠道的横向和纵向发展 总结行业特性、配合售前部门梳理行业解决方案 完成整体业绩指标，包括销售额和回款额  职位要求  本科或以上学历，通信、计算机、企业管理、市场营销等相关专业 三年以上软件、数据、信息、咨询服务产品的销售、渠道经验 有潜在客户和渠道资源优先，比如互联网金融行业、游戏行业、券商保险行业、政府行业 出众的沟通表达能力和抗压能力，具有敬业精神及团队合作意识 如有数据库技术经验或售前实施经验有加分  待遇 15K - 30K , 13薪 + 业绩奖金，优秀者可面议工作地点 北京"}]
